#include "tt-decomp_init.h"

__attribute__((alias("__imp__util_9410"))) PPC_WEAK_FUNC(util_9410);
PPC_FUNC_IMPL(__imp__util_9410) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lbz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 4);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82239440
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82239440:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82239454
	if (ctx.r8.u32 != 0) {
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_82239454:
	// clrlwi r7,r29,24
	ctx.r7.u64 = var_r29 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82239480
	if (ctx.r7.u32 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// stw r3,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r3.u32);
		// lbz r6,4(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 4);
		// ori r5,r6,1
		ctx.r5.u64 = ctx.r6.u64 | 1;
		// stb r5,4(r31)
		PPC_STORE_U8(var_r31 + 4, ctx.r5.u8);
		return;
	}
loc_82239480:
	// stw r30,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r30);
	// lbz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 4);
	// rlwinm r3,r4,0,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// stb r3,4(r31)
	PPC_STORE_U8(var_r31 + 4, ctx.r3.u8);
	return;
}

__attribute__((alias("__imp__jumptable_9498"))) PPC_WEAK_FUNC(jumptable_9498);
PPC_FUNC_IMPL(__imp__jumptable_9498) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,8
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// mr r26,r7
	var_r26 = ctx.r7.u32;
	// bl 0x822665c0
	cmSampleCamMachineBank_65C0_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,12
	ctx.r4.s64 = 12;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82239510
	if (ctx.r3.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// stb r11,8(r3)
		PPC_STORE_U8(ctx.r3.u32 + 8, ctx.r11.u8);
		// stb r11,9(r3)
		PPC_STORE_U8(ctx.r3.u32 + 9, ctx.r11.u8);
		// stw r11,4(r3)
		PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
		// b 0x82239514
	} else {
	loc_82239510:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82239514:
	// mr r5,r27
	ctx.r5.u64 = var_r27;
	// stw r3,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x8223a2e8
	jumptable_A2E8(ctx, base);
	// mr r5,r26
	ctx.r5.u64 = var_r26;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
	// bl 0x8223a370
	jumptable_A370(ctx, base);
	// lbz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
	// andi. r6,r7,253
	ctx.r6.u64 = ctx.r7.u64 & 253;
	ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// stb r6,4(r31)
	PPC_STORE_U8(var_r31 + 4, ctx.r6.u8);
	return;
}

__attribute__((alias("__imp__jumptable_9548"))) PPC_WEAK_FUNC(jumptable_9548);
PPC_FUNC_IMPL(__imp__jumptable_9548) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,8
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// bl 0x822665c0
	cmSampleCamMachineBank_65C0_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,12
	ctx.r4.s64 = 12;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x822395bc
	if (ctx.r3.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// stb r11,8(r3)
		PPC_STORE_U8(ctx.r3.u32 + 8, ctx.r11.u8);
		// stb r11,9(r3)
		PPC_STORE_U8(ctx.r3.u32 + 9, ctx.r11.u8);
		// stw r11,4(r3)
		PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
		// b 0x822395c0
	} else {
	loc_822395BC:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_822395C0:
	// mr r5,r27
	ctx.r5.u64 = var_r27;
	// stw r3,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x8223a2e8
	jumptable_A2E8(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
	// bl 0x8223a500
	rage_A500(ctx, base);
	// lbz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
	// andi. r6,r7,253
	ctx.r6.u64 = ctx.r7.u64 & 253;
	ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// stb r6,4(r31)
	PPC_STORE_U8(var_r31 + 4, ctx.r6.u8);
	return;
}

__attribute__((alias("__imp__jumptable_95F0"))) PPC_WEAK_FUNC(jumptable_95F0);
PPC_FUNC_IMPL(__imp__jumptable_95F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,8
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// bl 0x822665c0
	cmSampleCamMachineBank_65C0_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,12
	ctx.r4.s64 = 12;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82239664
	if (ctx.r3.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// stb r11,8(r3)
		PPC_STORE_U8(ctx.r3.u32 + 8, ctx.r11.u8);
		// stb r11,9(r3)
		PPC_STORE_U8(ctx.r3.u32 + 9, ctx.r11.u8);
		// stw r11,4(r3)
		PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
		// b 0x82239668
	} else {
	loc_82239664:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82239668:
	// mr r5,r27
	ctx.r5.u64 = var_r27;
	// stw r3,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x8223a2e8
	jumptable_A2E8(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
	// bl 0x8223a410
	rage_A410(ctx, base);
	// lbz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
	// andi. r6,r7,253
	ctx.r6.u64 = ctx.r7.u64 & 253;
	ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// stb r6,4(r31)
	PPC_STORE_U8(var_r31 + 4, ctx.r6.u8);
	return;
}

__attribute__((alias("__imp__jumptable_9698"))) PPC_WEAK_FUNC(jumptable_9698);
PPC_FUNC_IMPL(__imp__jumptable_9698) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,8
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// bl 0x822665c0
	cmSampleCamMachineBank_65C0_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,12
	ctx.r4.s64 = 12;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82239710
	if (ctx.r3.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// stb r11,8(r3)
		PPC_STORE_U8(ctx.r3.u32 + 8, ctx.r11.u8);
		// stb r11,9(r3)
		PPC_STORE_U8(ctx.r3.u32 + 9, ctx.r11.u8);
		// stw r11,4(r3)
		PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
		// b 0x82239714
	} else {
	loc_82239710:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82239714:
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// stw r3,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x8223a2e8
	jumptable_A2E8(ctx, base);
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// bl 0x8223a488
	rage_A488(ctx, base);
	// lbz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
	// andi. r6,r7,253
	ctx.r6.u64 = ctx.r7.u64 & 253;
	ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// stb r6,4(r31)
	PPC_STORE_U8(var_r31 + 4, ctx.r6.u8);
	return;
}

__attribute__((alias("__imp__parStreamInXml_9748_h"))) PPC_WEAK_FUNC(parStreamInXml_9748_h);
PPC_FUNC_IMPL(__imp__parStreamInXml_9748_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lhz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 12);
	// cmpwi cr6,r10,0
	// ble cr6,0x82239794
	if (ctx.r10.s32 > 0) {
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// li r9,0
		ctx.r9.s64 = 0;
		// rlwinm r10,r10,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// add r4,r10,r3
		ctx.r4.u64 = ctx.r10.u64 + ctx.r3.u64;
		// stb r9,0(r11)
		PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r9.u8);
		// lbz r5,80(r1)
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
		// bl 0x82239a70
		parStreamInXml_9A70(ctx, base);
		// lbz r8,4(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 4);
		// ori r7,r8,2
		ctx.r7.u64 = ctx.r8.u64 | 2;
		// stb r7,4(r31)
		PPC_STORE_U8(var_r31 + 4, ctx.r7.u8);
	}
loc_82239794:
	// blr
	return;
}

__attribute__((alias("__imp__rage_97A8"))) PPC_WEAK_FUNC(rage_97A8);
PPC_FUNC_IMPL(__imp__rage_97A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=144, savegprlr_28
	// lbz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// li r30,0
	var_r30 = 0;
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x822397d4
	if (ctx.r10.u32 == 0) {
		// mr r11,r30
		ctx.r11.u64 = var_r30;
	}
loc_822397D4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82239900
	if (ctx.r8.u32 != 0) {
		// lhz r7,12(r3)
		ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
		// li r5,0
		ctx.r5.s64 = 0;
		// lwz r31,8(r3)
		var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 8));
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// rotlwi r11,r7,2
		ctx.r11.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stb r30,88(r1)
		PPC_STORE_U8(ctx.r1.u32 + 88, (uint8_t)var_r30);
		// stb r30,89(r1)
		PPC_STORE_U8(ctx.r1.u32 + 89, (uint8_t)var_r30);
		// add r29,r11,r31
		var_r29 = (uint32_t)(ctx.r11.u64 + var_r31);
		// stw r30,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, var_r30);
		// bl 0x8223a2e8
		jumptable_A2E8(ctx, base);
		// subf r6,r31,r29
		ctx.r6.s64 = (int64_t)(int32_t)var_r29 - (int64_t)(int32_t)var_r31;
		// srawi r7,r6,2
		ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3) != 0);
		ctx.r7.s64 = ctx.r6.s32 >> 2;
		// cmpwi cr6,r7,0
		// ble cr6,0x82239898
		if (ctx.r7.s32 > 0) {
			// lwz r4,80(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		loc_82239824:
			// srawi r6,r7,1
			ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
			ctx.r6.s64 = ctx.r7.s32 >> 1;
			// mr r10,r4
			ctx.r10.u64 = ctx.r4.u64;
			// rlwinm r11,r6,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// add r5,r11,r31
			ctx.r5.u64 = ctx.r11.u64 + var_r31;
			// lwz r3,0(r5)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
			// lwz r11,0(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		loc_8223983C:
			// lbz r9,0(r11)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
			// lbz r8,0(r10)
			ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
			// cmpwi cr6,r9,0
			// subf r8,r8,r9
			ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
			// beq cr6,0x82239860
			if (ctx.r9.s32 == 0) goto loc_82239860;
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// cmpwi cr6,r8,0
			// beq cr6,0x8223983c
			if (ctx.r8.s32 == 0) goto loc_8223983C;
		loc_82239860:
			// cmpwi cr6,r8,0
			// li r11,1
			ctx.r11.s64 = 1;
			// blt cr6,0x82239870
			if (ctx.r8.s32 >= 0) {
				// mr r11,r30
				ctx.r11.u64 = var_r30;
			}
		loc_82239870:
			// clrlwi r11,r11,24
			ctx.r11.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r11,0
			// beq cr6,0x8223988c
			if (ctx.r11.u32 != 0) {
				// subf r11,r6,r7
				ctx.r11.s64 = ctx.r7.s64 - ctx.r6.s64;
				// addi r31,r5,4
				var_r31 = (uint32_t)(ctx.r5.s64 + 4);
				// addi r7,r11,-1
				ctx.r7.s64 = ctx.r11.s64 + -1;
				// b 0x82239890
			} else {
			loc_8223988C:
				// mr r7,r6
				ctx.r7.u64 = ctx.r6.u64;
			}
		loc_82239890:
			// cmpwi cr6,r7,0
			// bgt cr6,0x82239824
			if (ctx.r7.s32 > 0) goto loc_82239824;
		}
	loc_82239898:
		// cmplw cr6,r31,r29
		// beq cr6,0x822398ec
		if (var_r31 != var_r29) {
			// lwz r31,0(r31)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */);
			// mr r10,r28
			ctx.r10.u64 = var_r28;
			// lwz r11,0(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
		loc_822398AC:
			// lbz r9,0(r11)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
			// lbz r8,0(r10)
			ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
			// cmpwi cr6,r9,0
			// subf r8,r8,r9
			ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
			// beq cr6,0x822398d0
			if (ctx.r9.s32 == 0) goto loc_822398D0;
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// cmpwi cr6,r8,0
			// beq cr6,0x822398ac
			if (ctx.r8.s32 == 0) goto loc_822398AC;
		loc_822398D0:
			// cmpwi cr6,r8,0
			// bne cr6,0x822398ec
			if (ctx.r8.s32 != 0) goto loc_822398EC;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bl 0x8223a258
			rage_A258(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			return;
		}
	loc_822398EC:
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x8223a258
		rage_A258(ctx, base);
	loc_822398F4:
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_82239900:
	// lhz r5,12(r3)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// cmpwi cr6,r5,0
	// ble cr6,0x822398f4
	if (ctx.r5.s32 <= 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
	// lwz r4,8(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
loc_82239918:
	// lwz r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// cmplwi cr6,r3,0
	// beq cr6,0x8223995c
	if (ctx.r3.u32 != 0) {
		// rotlwi r10,r3,0
		ctx.r10.u64 = ctx.r3.u32;
		// mr r11,r28
		ctx.r11.u64 = var_r28;
		// lwz r10,0(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	loc_82239930:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x82239954
		if (ctx.r9.s32 == 0) goto loc_82239954;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239930
		if (ctx.r8.s32 == 0) goto loc_82239930;
	loc_82239954:
		// cmpwi cr6,r8,0
		// beq cr6,0x82239978
		if (ctx.r8.s32 == 0) {
			// rlwinm r7,r6,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r3,r7,r4
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
			return;
		}
	}
loc_8223995C:
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmpw cr6,r6,r5
	// blt cr6,0x82239918
	if (ctx.r6.s32 < ctx.r5.s32) goto loc_82239918;
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__rage_9988"))) PPC_WEAK_FUNC(rage_9988);
PPC_FUNC_IMPL(__imp__rage_9988) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r27,0
	var_r27 = 0;
	// lbz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 4);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x822399b4
	if (ctx.r10.u32 == 0) {
		// mr r11,r27
		ctx.r11.u64 = var_r27;
	}
loc_822399B4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x822399c8
	if (ctx.r8.u32 != 0) {
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_822399C8:
	// lhz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 12);
	// mr r29,r27
	var_r29 = (uint32_t)(var_r27);
	// cmpwi cr6,r11,0
	// ble cr6,0x82239a10
	if (ctx.r11.s32 > 0) {
		// mr r30,r27
		var_r30 = (uint32_t)(var_r27);
	loc_822399DC:
		// lwz r7,8(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwzx r28,r7,r30
		var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + var_r30));
		// cmplwi cr6,r28,0
		// beq cr6,0x822399fc
		if (var_r28 != 0) {
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// bl 0x8223a258
			rage_A258(ctx, base);
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		}
	loc_822399FC:
		// lhz r11,12(r31)
		ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 12);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r29,r11
		// blt cr6,0x822399dc
		if ((int32_t)var_r29 < ctx.r11.s32) goto loc_822399DC;
	}
loc_82239A10:
	// stw r27,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* rage_GameObject::vtable@+0x0 */ var_r27);
	// stb r27,4(r31)
	PPC_STORE_U8(var_r31 + 4, (uint8_t)var_r27);
	// lwz r30,8(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
	// cmplwi cr6,r30,0
	// beq cr6,0x82239a58
	if (var_r30 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r6,r3,24
		ctx.r6.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r6,0
		// bne cr6,0x82239a58
		if (ctx.r6.u32 != 0) {
			// stw r27,8(r31)
			PPC_STORE_U32(var_r31 + 8, var_r27);
			// sth r27,12(r31)
			PPC_STORE_U16(var_r31 + 12, (uint16_t)var_r27);
			// sth r27,14(r31)
			PPC_STORE_U16(var_r31 + 14, (uint16_t)var_r27);
			return;
		}
		// lwz r5,0(r13)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r3,4
		ctx.r3.s64 = 4;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// lwzx r3,r3,r5
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r5.u32);
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_82239A58:
	// stw r27,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r27);
	// sth r27,12(r31)
	PPC_STORE_U16(var_r31 + 12, (uint16_t)var_r27);
	// sth r27,14(r31)
	PPC_STORE_U16(var_r31 + 14, (uint16_t)var_r27);
	return;
}

__attribute__((alias("__imp__parStreamInXml_9A70"))) PPC_WEAK_FUNC(parStreamInXml_9A70);
PPC_FUNC_IMPL(__imp__parStreamInXml_9A70) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// cmplw cr6,r30,r29
	// beq cr6,0x82239b3c
	if (var_r30 != var_r29) {
		// subf r11,r30,r29
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 - (int64_t)(int32_t)var_r30;
		// li r10,0
		ctx.r10.s64 = 0;
		// srawi r31,r11,2
		ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
		var_r31 = (uint32_t)(ctx.r11.s32 >> 2);
		// mr r11,r31
		ctx.r11.u64 = var_r31;
		// cmpwi cr6,r11,1
		// beq cr6,0x82239ab8
	while (ctx.r11.s32 != 1) {
		loc_82239AA8:
			// srawi r11,r11,1
			ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
			ctx.r11.s64 = ctx.r11.s32 >> 1;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// cmpwi cr6,r11,1
			// bne cr6,0x82239aa8
	}
	loc_82239AB8:
		// mr r7,r28
		ctx.r7.u64 = var_r28;
		// rlwinm r6,r10,1,0,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// li r5,0
		ctx.r5.s64 = 0;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82239b48
		parStreamInXml_9B48_2h(ctx, base);
		// stb r28,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r28);
		// cmpwi cr6,r31,16
		// ble cr6,0x82239b28
		if ((int32_t)var_r31 > 16) {
			// addi r31,r30,64
			var_r31 = (uint32_t)(var_r30 + 64);
			// cmplw cr6,r30,r31
			// beq cr6,0x82239afc
			if (var_r30 != var_r31) {
				// addi r5,r1,80
				ctx.r5.s64 = ctx.r1.s64 + 80;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x82239e88
				parStreamInXml_9E88_h(ctx, base);
				// lbz r28,80(r1)
				var_r28 = (uint32_t)(PPC_LOAD_U8(ctx.r1.u32 + 80));
			}
		loc_82239AFC:
			// cmplw cr6,r31,r29
			// beq cr6,0x82239b3c
			if (var_r31 == var_r29) {
				return;
			}
		loc_82239B04:
			// mr r5,r28
			ctx.r5.u64 = var_r28;
			// lwz r4,0(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8223a0a0
			parStreamInXml_A0A0_2hr(ctx, base);
			// addi r31,r31,4
			var_r31 = (uint32_t)(var_r31 + 4);
			// cmplw cr6,r31,r29
			// bne cr6,0x82239b04
			if (var_r31 != var_r29) goto loc_82239B04;
			return;
		}
	loc_82239B28:
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// stb r28,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r28);
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82239e88
		parStreamInXml_9E88_h(ctx, base);
	}
loc_82239B3C:
	return;
}

__attribute__((alias("__imp__parStreamInXml_9B48_2h"))) PPC_WEAK_FUNC(parStreamInXml_9B48_2h);
PPC_FUNC_IMPL(__imp__parStreamInXml_9B48_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_27
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// subf r11,r30,r29
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 - (int64_t)(int32_t)var_r30;
	// mr r27,r7
	var_r27 = ctx.r7.u32;
	// rlwinm r10,r11,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// cmpwi cr6,r10,64
	// ble cr6,0x82239cc4
	if (ctx.r10.s32 > 64) {
	loc_82239B74:
		// cmpwi cr6,r28,0
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// beq cr6,0x82239cb0
		if ((int32_t)var_r28 == 0) goto loc_82239CB0;
		// subf r9,r30,r29
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 - (int64_t)(int32_t)var_r30;
		// mr r6,r27
		ctx.r6.u64 = var_r27;
		// srawi r8,r9,2
		ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
		ctx.r8.s64 = ctx.r9.s32 >> 2;
		// addi r5,r29,-4
		ctx.r5.s64 = (int64_t)(int32_t)var_r29 + -4;
		// srawi r7,r8,1
		ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
		ctx.r7.s64 = ctx.r8.s32 >> 1;
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// addze r4,r7
		temp.s64 = ctx.r7.s64 + ctx.xer.ca;
		ctx.xer.ca = temp.u32 < ctx.r7.u32;
		ctx.r4.s64 = temp.s64;
		// rlwinm r11,r4,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// add r4,r11,r30
		ctx.r4.u64 = ctx.r11.u64 + var_r30;
		// bl 0x82239cd0
		parStreamInXml_9CD0_2hr(ctx, base);
		// lwz r5,0(r3)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// mr r7,r29
		ctx.r7.u64 = var_r29;
		// mr r31,r30
		var_r31 = (uint32_t)(var_r30);
	loc_82239BB4:
		// lwz r6,0(r5)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	loc_82239BB8:
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// mr r10,r6
		ctx.r10.u64 = ctx.r6.u64;
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	loc_82239BC4:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x82239be8
		if (ctx.r9.s32 == 0) goto loc_82239BE8;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239bc4
		if (ctx.r8.s32 == 0) goto loc_82239BC4;
	loc_82239BE8:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x82239bf8
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82239BF8:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x82239c0c
		if (ctx.r3.u32 == 0) goto loc_82239C0C;
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// b 0x82239bb8
		goto loc_82239BB8;
	loc_82239C0C:
		// addi r7,r7,-4
		ctx.r7.s64 = ctx.r7.s64 + -4;
		// mr r11,r6
		ctx.r11.u64 = ctx.r6.u64;
		// lwz r10,0(r7)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// lwz r10,0(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	loc_82239C1C:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x82239c40
		if (ctx.r9.s32 == 0) goto loc_82239C40;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239c1c
		if (ctx.r8.s32 == 0) goto loc_82239C1C;
	loc_82239C40:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x82239c50
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82239C50:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// bne cr6,0x82239c0c
		if (ctx.r3.u32 != 0) goto loc_82239C0C;
		// cmplw cr6,r31,r7
		// bge cr6,0x82239c7c
		if (var_r31 >= ctx.r7.u32) goto loc_82239C7C;
		// lwz r10,0(r7)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// stw r10,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// stw r11,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r11.u32);
		// b 0x82239bb4
		goto loc_82239BB4;
	loc_82239C7C:
		// mr r7,r27
		ctx.r7.u64 = var_r27;
		// mr r6,r28
		ctx.r6.u64 = var_r28;
		// li r5,0
		ctx.r5.s64 = 0;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82239b48
		parStreamInXml_9B48_2h(ctx, base);
		// subf r9,r30,r31
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 - (int64_t)(int32_t)var_r30;
		// mr r29,r31
		var_r29 = (uint32_t)(var_r31);
		// rlwinm r8,r9,0,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFC;
		// cmpwi cr6,r8,64
		// bgt cr6,0x82239b74
		if (ctx.r8.s32 > 64) goto loc_82239B74;
		return;
	loc_82239CB0:
		// mr r7,r27
		ctx.r7.u64 = var_r27;
		// li r6,0
		ctx.r6.s64 = 0;
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// bl 0x82239f50
		parStreamInXml_9F50_2h(ctx, base);
	}
loc_82239CC4:
	return;
}

__attribute__((alias("__imp__parStreamInXml_9CD0_2hr"))) PPC_WEAK_FUNC(parStreamInXml_9CD0_2hr);
PPC_FUNC_IMPL(__imp__parStreamInXml_9CD0_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r30);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 0);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r31,0(r10)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 0));
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// mr r11,r31
	ctx.r11.u64 = var_r31;
loc_82239CF8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x82239d1c
	if (ctx.r9.s32 == 0) goto loc_82239D1C;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	// beq cr6,0x82239cf8
	if (ctx.r8.s32 == 0) goto loc_82239CF8;
loc_82239D1C:
	// cmpwi cr6,r8,0
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x82239d2c
	if (ctx.r8.s32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82239D2C:
	// clrlwi r5,r11,24
	ctx.r5.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r5,0
	// beq cr6,0x82239de0
	if (ctx.r5.u32 != 0) {
		// lwz r10,0(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// mr r11,r6
		ctx.r11.u64 = ctx.r6.u64;
		// lwz r7,0(r10)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// mr r10,r7
		ctx.r10.u64 = ctx.r7.u64;
	loc_82239D48:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x82239d6c
		if (ctx.r9.s32 == 0) goto loc_82239D6C;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239d48
		if (ctx.r8.s32 == 0) goto loc_82239D48;
	loc_82239D6C:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x82239d7c
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82239D7C:
		// clrlwi r5,r11,24
		ctx.r5.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// bne cr6,0x82239e78
		if (ctx.r5.u32 != 0) goto loc_82239E78;
		// mr r10,r7
		ctx.r10.u64 = ctx.r7.u64;
		// mr r11,r31
		ctx.r11.u64 = var_r31;
	loc_82239D90:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r4,0(r10)
		ctx.r4.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r4,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r4.s64;
		// beq cr6,0x82239db4
		if (ctx.r9.s32 == 0) goto loc_82239DB4;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239d90
		if (ctx.r8.s32 == 0) goto loc_82239D90;
	loc_82239DB4:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x82239dc4
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82239DC4:
		// clrlwi r10,r11,24
		ctx.r10.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// bne cr6,0x82239e7c
		if (ctx.r10.u32 != 0) {
			// ld r30,-16(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// ld r31,-8(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
			// blr
			return;
		}
	loc_82239DD0:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// ld r30,-16(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// ld r31,-8(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
		// blr
		return;
	}
loc_82239DE0:
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r11,r31
	ctx.r11.u64 = var_r31;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_82239DF0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x82239e14
	if (ctx.r9.s32 == 0) goto loc_82239E14;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	// beq cr6,0x82239df0
	if (ctx.r8.s32 == 0) goto loc_82239DF0;
loc_82239E14:
	// cmpwi cr6,r8,0
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x82239e24
	if (ctx.r8.s32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82239E24:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// bne cr6,0x82239dd0
	if (ctx.r11.u32 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// ld r30,-16(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// ld r31,-8(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
		// blr
		return;
	}
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_82239E38:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x82239e5c
	if (ctx.r9.s32 == 0) goto loc_82239E5C;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	// beq cr6,0x82239e38
	if (ctx.r8.s32 == 0) goto loc_82239E38;
loc_82239E5C:
	// cmpwi cr6,r8,0
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x82239e6c
	if (ctx.r8.s32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82239E6C:
	// clrlwi r6,r11,24
	ctx.r6.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// bne cr6,0x82239e7c
	if (ctx.r6.u32 == 0) {
	loc_82239E78:
		// mr r3,r4
		ctx.r3.u64 = ctx.r4.u64;
	}
loc_82239E7C:
	// ld r30,-16(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__parStreamInXml_9E88_h"))) PPC_WEAK_FUNC(parStreamInXml_9E88_h);
PPC_FUNC_IMPL(__imp__parStreamInXml_9E88_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r26,r4
	var_r26 = ctx.r4.u32;
	// addi r31,r29,4
	var_r31 = (uint32_t)(var_r29 + 4);
	// mr r27,r5
	var_r27 = ctx.r5.u32;
	// cmplw cr6,r31,r26
	// beq cr6,0x82239f44
	if (var_r31 != var_r26) {
		// subf r28,r29,r31
		var_r28 = var_r31 - var_r29;
	loc_82239EB0:
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r30,0(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 0));
		// lbz r5,0(r27)
		ctx.r5.u64 = PPC_LOAD_U8(var_r27 + 0);
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
	loc_82239EC4:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x82239ee8
		if (ctx.r9.s32 == 0) goto loc_82239EE8;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239ec4
		if (ctx.r8.s32 == 0) goto loc_82239EC4;
	loc_82239EE8:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x82239ef8
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82239EF8:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r6,0
		// beq cr6,0x82239f28
		if (ctx.r6.u32 != 0) {
			// cmpwi cr6,r28,0
			// ble cr6,0x82239f20
			if ((int32_t)var_r28 > 0) {
				// subf r11,r28,r31
				ctx.r11.s64 = (int64_t)(int32_t)var_r31 - (int64_t)(int32_t)var_r28;
				// mr r5,r28
				ctx.r5.u64 = var_r28;
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// addi r3,r11,4
				ctx.r3.s64 = ctx.r11.s64 + 4;
				// bl 0x8242ff70
				memmove(ctx, base);
			}
		loc_82239F20:
			// stw r30,0(r29)
			PPC_STORE_U32(var_r29 + 0, var_r30);
			// b 0x82239f34
		} else {
		loc_82239F28:
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8223a0a0
			parStreamInXml_A0A0_2hr(ctx, base);
		}
	loc_82239F34:
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// addi r28,r28,4
		var_r28 = (uint32_t)(var_r28 + 4);
		// cmplw cr6,r31,r26
		// bne cr6,0x82239eb0
		if (var_r31 != var_r26) goto loc_82239EB0;
	}
loc_82239F44:
	return;
}

__attribute__((alias("__imp__parStreamInXml_9F50_2h"))) PPC_WEAK_FUNC(parStreamInXml_9F50_2h);
PPC_FUNC_IMPL(__imp__parStreamInXml_9F50_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_24
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r25,r5
	var_r25 = ctx.r5.u32;
	// subf r24,r30,r28
	var_r24 = var_r28 - var_r30;
	// mr r26,r7
	var_r26 = ctx.r7.u32;
	// srawi r27,r24,2
	ctx.xer.ca = ((int32_t)var_r24 < 0) & ((var_r24 & 0x3) != 0);
	var_r27 = (uint32_t)((int32_t)var_r24 >> 2);
	// cmpwi cr6,r27,2
	// blt cr6,0x82239fd0
	if ((int32_t)var_r27 >= 2) {
		// addi r11,r27,-2
		ctx.r11.s64 = (int64_t)(int32_t)var_r27 + -2;
		// mr r5,r27
		ctx.r5.u64 = var_r27;
		// srawi r10,r11,1
		ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
		ctx.r10.s64 = ctx.r11.s32 >> 1;
		// addze r31,r10
		temp.s64 = ctx.r10.s64 + ctx.xer.ca;
		ctx.xer.ca = temp.u32 < ctx.r10.u32;
		var_r31 = (uint32_t)(temp.s64);
		// rlwinm r11,r31,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// add r29,r11,r30
		var_r29 = (uint32_t)(ctx.r11.u64 + var_r30);
		// lwz r6,0(r29)
		ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 0);
		// bl 0x8223a108
		parStreamInXml_A108_wrh(ctx, base);
		// cmpwi cr6,r31,0
		// beq cr6,0x82239fd0
	while ((int32_t)var_r31 != 0) {
		loc_82239FA8:
			// addi r29,r29,-4
			var_r29 = (uint32_t)(var_r29 + -4);
			// addi r31,r31,-1
			var_r31 = (uint32_t)(var_r31 + -1);
			// mr r7,r26
			ctx.r7.u64 = var_r26;
			// mr r5,r27
			ctx.r5.u64 = var_r27;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwz r6,0(r29)
			ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 0);
			// bl 0x8223a108
			parStreamInXml_A108_wrh(ctx, base);
			// cmpwi cr6,r31,0
			// bne cr6,0x82239fa8
	}
	}
loc_82239FD0:
	// mr r31,r28
	var_r31 = (uint32_t)(var_r28);
	// cmplw cr6,r28,r25
	// bge cr6,0x8223a050
while (var_r31 < var_r25) {
	loc_82239FDC:
		// lwz r29,0(r30)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// lwz r6,0(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lwz r10,0(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r11,0(r6)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	loc_82239FEC:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x8223a010
		if (ctx.r9.s32 == 0) goto loc_8223A010;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x82239fec
		if (ctx.r8.s32 == 0) goto loc_82239FEC;
	loc_8223A010:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x8223a020
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A020:
		// clrlwi r5,r11,24
		ctx.r5.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// beq cr6,0x8223a044
		if (ctx.r5.u32 == 0) goto loc_8223A044;
		// mr r7,r26
		ctx.r7.u64 = var_r26;
		// stw r29,0(r31)
		PPC_STORE_U32(var_r31 + 0, var_r29);
		// mr r5,r27
		ctx.r5.u64 = var_r27;
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8223a108
		parStreamInXml_A108_wrh(ctx, base);
	loc_8223A044:
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmplw cr6,r31,r25
		// blt cr6,0x82239fdc
}
loc_8223A050:
	// cmpwi cr6,r27,1
	// ble cr6,0x8223a094
	if ((int32_t)var_r27 > 1) {
		// mr r11,r24
		ctx.r11.u64 = var_r24;
		// addi r29,r30,-4
		var_r29 = (uint32_t)(var_r30 + -4);
	loc_8223A060:
		// lwz r10,0(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r31,r11,-4
		var_r31 = (uint32_t)(ctx.r11.s64 + -4);
		// mr r7,r26
		ctx.r7.u64 = var_r26;
		// lwzx r6,r29,r11
		ctx.r6.u64 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// srawi r5,r31,2
		ctx.xer.ca = ((int32_t)var_r31 < 0) & ((var_r31 & 0x3) != 0);
		ctx.r5.s64 = (int32_t)var_r31 >> 2;
		// stwx r10,r29,r11
		PPC_STORE_U32(var_r29 + ctx.r11.u32, ctx.r10.u32);
		// bl 0x8223a108
		parStreamInXml_A108_wrh(ctx, base);
		// mr r11,r31
		ctx.r11.u64 = var_r31;
		// rlwinm r9,r11,0,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
		// cmpwi cr6,r9,4
		// bgt cr6,0x8223a060
		if (ctx.r9.s32 > 4) goto loc_8223A060;
	}
loc_8223A094:
	return;
}

__attribute__((alias("__imp__parStreamInXml_A0A0_2hr"))) PPC_WEAK_FUNC(parStreamInXml_A0A0_2hr);
PPC_FUNC_IMPL(__imp__parStreamInXml_A0A0_2hr) {
	PPC_FUNC_PROLOGUE();
	// addi r7,r3,-4
	ctx.r7.s64 = ctx.r3.s64 + -4;
loc_8223A0A4:
	// lwz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
loc_8223A0B0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x8223a0d4
	if (ctx.r9.s32 == 0) goto loc_8223A0D4;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	// beq cr6,0x8223a0b0
	if (ctx.r8.s32 == 0) goto loc_8223A0B0;
loc_8223A0D4:
	// cmpwi cr6,r8,0
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x8223a0e4
	if (ctx.r8.s32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8223A0E4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223a100
	if (ctx.r11.u32 == 0) {
		// stw r4,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r4.u32);
		// blr
		return;
	}
	// stw r6,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r6.u32);
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// b 0x8223a0a4
	goto loc_8223A0A4;
}

__attribute__((alias("__imp__parStreamInXml_A108_wrh"))) PPC_WEAK_FUNC(parStreamInXml_A108_wrh);
PPC_FUNC_IMPL(__imp__parStreamInXml_A108_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// addi r11,r4,1
	ctx.r11.s64 = ctx.r4.s64 + 1;
	// stb r7,55(r1)
	PPC_STORE_U8(ctx.r1.u32 + 55, ctx.r7.u8);
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r8,r5
	// bge cr6,0x8223a1a8
	if (ctx.r8.s32 < ctx.r5.s32) {
	loc_8223A124:
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// add r11,r11,r3
		ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
		// lwz r10,-4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r10,0(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// lwz r11,0(r9)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	loc_8223A13C:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r7,0(r10)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r7,r7,r9
		ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
		// beq cr6,0x8223a160
		if (ctx.r9.s32 == 0) goto loc_8223A160;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r7,0
		// beq cr6,0x8223a13c
		if (ctx.r7.s32 == 0) goto loc_8223A13C;
	loc_8223A160:
		// cmpwi cr6,r7,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x8223a170
		if (ctx.r7.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A170:
		// clrlwi r10,r11,24
		ctx.r10.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x8223a180
		if (ctx.r10.u32 != 0) {
			// addi r8,r8,-1
			ctx.r8.s64 = ctx.r8.s64 + -1;
		}
	loc_8223A180:
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r7,r4,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r9,r8,1
		ctx.r9.s64 = ctx.r8.s64 + 1;
		// mr r4,r8
		ctx.r4.u64 = ctx.r8.u64;
		// rlwinm r8,r9,1,0,30
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// lwzx r10,r11,r3
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
		// cmpw cr6,r8,r5
		// stwx r10,r7,r3
		PPC_STORE_U32(ctx.r7.u32 + ctx.r3.u32, ctx.r10.u32);
		// blt cr6,0x8223a124
		if (ctx.r8.s32 < ctx.r5.s32) goto loc_8223A124;
		// cmpw cr6,r8,r5
		ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r5.s32, ctx.xer);
	}
loc_8223A1A8:
	// bne cr6,0x8223a1c4
	if (ctx.cr6.eq) {
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r9,r4,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// add r7,r11,r3
		ctx.r7.u64 = ctx.r11.u64 + ctx.r3.u64;
		// addi r4,r8,-1
		ctx.r4.s64 = ctx.r8.s64 + -1;
		// lwz r5,-4(r7)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
		// stwx r5,r9,r3
		PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, ctx.r5.u32);
	}
loc_8223A1C4:
	// addi r11,r4,-1
	ctx.r11.s64 = ctx.r4.s64 + -1;
	// cmpw cr6,r4,r31
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// addze r7,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r7.s64 = temp.s64;
	// ble cr6,0x8223a248
	while (ctx.r4.s32 > (int32_t)var_r31) {
	loc_8223A1D8:
		// rlwinm r9,r7,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwz r10,0(r6)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// lwzx r5,r9,r3
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
		// lwz r11,0(r5)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	loc_8223A1E8:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmpwi cr6,r9,0
		// subf r8,r8,r9
		ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
		// beq cr6,0x8223a20c
		if (ctx.r9.s32 == 0) goto loc_8223A20C;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r8,0
		// beq cr6,0x8223a1e8
		if (ctx.r8.s32 == 0) goto loc_8223A1E8;
	loc_8223A20C:
		// cmpwi cr6,r8,0
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x8223a21c
		if (ctx.r8.s32 >= 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A21C:
		// clrlwi r10,r11,24
		ctx.r10.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x8223a248
		if (ctx.r10.u32 == 0) {
			// rlwinm r5,r4,2,0,29
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// stwx r6,r5,r3
			PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r6.u32);
			// ld r31,-8(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
			// blr
			return;
		}
		// rlwinm r9,r4,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r8,r7,-1
		ctx.r8.s64 = ctx.r7.s64 + -1;
		// mr r4,r7
		ctx.r4.u64 = ctx.r7.u64;
		// srawi r7,r8,1
		ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
		ctx.r7.s64 = ctx.r8.s32 >> 1;
		// cmpw cr6,r4,r31
		ctx.cr6.compare<int32_t>(ctx.r4.s32, (int32_t)var_r31, ctx.xer);
		// addze r7,r7
		temp.s64 = ctx.r7.s64 + ctx.xer.ca;
		ctx.xer.ca = temp.u32 < ctx.r7.u32;
		ctx.r7.s64 = temp.s64;
		// stwx r5,r9,r3
		PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, ctx.r5.u32);
		// bgt cr6,0x8223a1d8
}
loc_8223A248:
	// rlwinm r5,r4,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r6,r5,r3
	PPC_STORE_U32(ctx.r5.u32 + ctx.r3.u32, ctx.r6.u32);
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__rage_A258"))) PPC_WEAK_FUNC(rage_A258);
PPC_FUNC_IMPL(__imp__rage_A258) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,9(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 9);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8223a284
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8223A284:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8223a298
	if (ctx.r8.u32 != 0) {
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A298:
	// lbz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 8);
	// cmplwi cr6,r7,0
	// bne cr6,0x8223a2d0
	if (ctx.r7.u32 == 0) {
		// lbz r6,9(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 9);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r5,r6,0,30,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r5,0
		// bne cr6,0x8223a2bc
		if (ctx.r5.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A2BC:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x8223a2d0
		if (ctx.r3.u32 == 0) {
			// blr
			return;
		}
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4)/* rage_GameObject::flags@+0x4 */;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A2D0:
	// blr
	return;
}

__attribute__((alias("__imp__jumptable_A2E8"))) PPC_WEAK_FUNC(jumptable_A2E8);
PPC_FUNC_IMPL(__imp__jumptable_A2E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lbz r11,9(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 9);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8223a318
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8223A318:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8223a32c
	if (ctx.r8.u32 != 0) {
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A32C:
	// clrlwi r7,r29,24
	ctx.r7.u64 = var_r29 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x8223a358
	if (ctx.r7.u32 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// stw r3,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r3.u32);
		// lbz r6,9(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 9);
		// ori r5,r6,1
		ctx.r5.u64 = ctx.r6.u64 | 1;
		// stb r5,9(r31)
		PPC_STORE_U8(var_r31 + 9, ctx.r5.u8);
		return;
	}
loc_8223A358:
	// stw r30,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r30);
	// lbz r4,9(r31)
	ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 9);
	// rlwinm r3,r4,0,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFE;
	// stb r3,9(r31)
	PPC_STORE_U8(var_r31 + 9, ctx.r3.u8);
	return;
}

__attribute__((alias("__imp__jumptable_A370"))) PPC_WEAK_FUNC(jumptable_A370);
PPC_FUNC_IMPL(__imp__jumptable_A370) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// li r29,0
	var_r29 = 0;
	// lbz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 8);
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a3c4
	if (ctx.r11.u32 == 0) {
		// lbz r10,9(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 9);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r9,r10,0,30,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r9,0
		// bne cr6,0x8223a3b0
		if (ctx.r9.u32 == 0) {
			// mr r11,r29
			ctx.r11.u64 = var_r29;
		}
	loc_8223A3B0:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8223a3c4
		if (ctx.r7.u32 == 0) goto loc_8223A3C4;
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A3C4:
	// clrlwi r6,r28,24
	ctx.r6.u64 = var_r28 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8223a3f4
	if (ctx.r6.u32 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// stw r3,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r3.u32);
		// lbz r5,9(r31)
		ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 9);
		// ori r4,r5,2
		ctx.r4.u64 = ctx.r5.u64 | 2;
		// stb r4,9(r31)
		PPC_STORE_U8(var_r31 + 9, ctx.r4.u8);
		// stb r29,8(r31)
		PPC_STORE_U8(var_r31 + 8, (uint8_t)var_r29);
		return;
	}
loc_8223A3F4:
	// stw r30,4(r31)
	PPC_STORE_U32(var_r31 + 4, var_r30);
	// lbz r3,9(r31)
	ctx.r3.u64 = PPC_LOAD_U8(var_r31 + 9);
	// andi. r11,r3,253
	ctx.r11.u64 = ctx.r3.u64 & 253;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stb r11,9(r31)
	PPC_STORE_U8(var_r31 + 9, ctx.r11.u8);
	// stb r29,8(r31)
	PPC_STORE_U8(var_r31 + 8, (uint8_t)var_r29);
	return;
}

__attribute__((alias("__imp__rage_A410"))) PPC_WEAK_FUNC(rage_A410);
PPC_FUNC_IMPL(__imp__rage_A410) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lbz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 8);
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a464
	if (ctx.r11.u32 == 0) {
		// lbz r10,9(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 9);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r9,r10,0,30,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r9,0
		// bne cr6,0x8223a450
		if (ctx.r9.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A450:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8223a464
		if (ctx.r7.u32 == 0) {
			// li r6,1
			ctx.r6.s64 = 1;
			// stw r30,4(r31)
			PPC_STORE_U32(var_r31 + 4,/* rage_GameObject::flags@+0x4 */ var_r30);
			// stb r6,8(r31)
			PPC_STORE_U8(var_r31 + 8, ctx.r6.u8);
			// blr
			return;
		}
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4)/* rage_GameObject::flags@+0x4 */;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A464:
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r30,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* rage_GameObject::flags@+0x4 */ var_r30);
	// stb r6,8(r31)
	PPC_STORE_U8(var_r31 + 8, ctx.r6.u8);
	// blr
	return;
}

__attribute__((alias("__imp__rage_A488"))) PPC_WEAK_FUNC(rage_A488);
PPC_FUNC_IMPL(__imp__rage_A488) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// lbz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 8);
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a4dc
	if (ctx.r11.u32 == 0) {
		// lbz r10,9(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 9);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r9,r10,0,30,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r9,0
		// bne cr6,0x8223a4c8
		if (ctx.r9.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A4C8:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8223a4dc
		if (ctx.r7.u32 == 0) goto loc_8223A4DC;
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4)/* rage_GameObject::flags@+0x4 */;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A4DC:
	// li r6,2
	ctx.r6.s64 = 2;
	// stfs f31,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 4,/* rage_GameObject::flags@+0x4 */ temp.u32);
	// stb r6,8(r31)
	PPC_STORE_U8(var_r31 + 8, ctx.r6.u8);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__rage_A500"))) PPC_WEAK_FUNC(rage_A500);
PPC_FUNC_IMPL(__imp__rage_A500) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lbz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 8);
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a554
	if (ctx.r11.u32 == 0) {
		// lbz r10,9(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 9);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r9,r10,0,30,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r9,0
		// bne cr6,0x8223a540
		if (ctx.r9.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223A540:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8223a554
		if (ctx.r7.u32 == 0) {
			// li r6,3
			ctx.r6.s64 = 3;
			// stb r30,4(r31)
			PPC_STORE_U8(var_r31 + 4, (uint8_t)var_r30);
			// stb r6,8(r31)
			PPC_STORE_U8(var_r31 + 8, ctx.r6.u8);
			// blr
			return;
		}
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4)/* rage_GameObject::flags@+0x4 */;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8223A554:
	// li r6,3
	ctx.r6.s64 = 3;
	// stb r30,4(r31)
	PPC_STORE_U8(var_r31 + 4, (uint8_t)var_r30);
	// stb r6,8(r31)
	PPC_STORE_U8(var_r31 + 8, ctx.r6.u8);
	// blr
	return;
}

__attribute__((alias("__imp__jumptable_A578_h"))) PPC_WEAK_FUNC(jumptable_A578_h);
PPC_FUNC_IMPL(__imp__jumptable_A578_h) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,3
	// bne cr6,0x8223a58c
	if (ctx.r11.u32 == 3) {
		// lbz r3,4(r3)
		ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// blr
		return;
	}
loc_8223A58C:
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a5c8
	if (ctx.r11.u32 == 0) {
		// lwz r11,4(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// lbz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// extsb r11,r11
		ctx.r11.s64 = ctx.r11.s8;
		// cmpwi cr6,r11,102
		// beq cr6,0x8223a5bc
		if (ctx.r11.s32 != 102) {
			// cmpwi cr6,r11,70
			// beq cr6,0x8223a5bc
			if (ctx.r11.s32 == 70) goto loc_8223A5BC;
			// cmpwi cr6,r11,48
			// li r11,1
			ctx.r11.s64 = 1;
			// bne cr6,0x8223a5c0
			if (ctx.r11.s32 != 48) {
				// clrlwi r3,r11,24
				ctx.r3.u64 = ctx.r11.u32 & 0xFF;
				// blr
				return;
			}
		}
	loc_8223A5BC:
		// li r11,0
		ctx.r11.s64 = 0;
	loc_8223A5C0:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// blr
		return;
	}
loc_8223A5C8:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__parStreamInXml_A5D0"))) PPC_WEAK_FUNC(parStreamInXml_A5D0);
PPC_FUNC_IMPL(__imp__parStreamInXml_A5D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=96, manual
	// lbz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,2
	// bne cr6,0x8223a5fc
	if (ctx.r11.u32 == 2) {
		// lfs f1,4(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		ctx.f1.f64 = double(temp.f32);
		// blr
		return;
	}
loc_8223A5FC:
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a620
	if (ctx.r11.u32 == 0) {
		// lwz r3,4(r3)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// bl 0x82432b80
		jumptable_2B80(ctx, base);
		// frsp f1,f1
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = double(float(ctx.f1.f64));
		// blr
		return;
	}
loc_8223A620:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f1,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__fiAsciiTokenizer_A638_wrh"))) PPC_WEAK_FUNC(fiAsciiTokenizer_A638_wrh);
PPC_FUNC_IMPL(__imp__fiAsciiTokenizer_A638_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// lbz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// cmplwi cr6,r11,3
	// bgt cr6,0x8223a6f8
	if (ctx.r11.u32 > 3) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// blr
		return;
	}
	// lis r12,-32220
	// addi r12,r12,-22928
	ctx.r12.s64 = ctx.r12.s64 + -22928;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		// lwz r3,4(r3)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* fiAsciiTokenizer::flags@+0x4 */;
		// blr
		return;
	case 1:
		goto loc_8223A698;
	case 2:
		goto loc_8223A6A8;
	case 3:
		goto loc_8223A6C8;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_8223A680:
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* fiAsciiTokenizer::flags@+0x4 */;
	// blr
	return;
loc_8223A698:
	// lis r11,-32253
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* fiAsciiTokenizer::flags@+0x4 */;
	// addi r4,r11,15328
	ctx.r4.s64 = ctx.r11.s64 + 15328;
	// b 0x8223a6f0
	goto loc_8223A6F0;
loc_8223A6A8:
	// lis r11,-32252
	// lfs f1,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* fiAsciiTokenizer::flags@+0x4 */;
	ctx.f1.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stfd f1,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.f1.u64);
	// addi r4,r11,-22180
	ctx.r4.s64 = ctx.r11.s64 + -22180;
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// bl 0x82430030
	fiAsciiTokenizer_0030_g(ctx, base);
	// b 0x8223a6f8
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
loc_8223A6C8:
	// lbz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// cmplwi cr6,r11,0
	// beq cr6,0x8223a6e0
	if (ctx.r11.u32 == 0) goto loc_8223A6E0;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r5,r11,32076
	ctx.r5.s64 = ctx.r11.s64 + 32076;
	// b 0x8223a6e8
	goto loc_8223A6E8;
loc_8223A6E0:
	// lis r11,-32252
	ctx.r11.s64 = -2113667072;
	// addi r5,r11,-6456
	ctx.r5.s64 = ctx.r11.s64 + -6456;
loc_8223A6E8:
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// addi r4,r11,31820
	ctx.r4.s64 = ctx.r11.s64 + 31820;
loc_8223A6F0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82430030
	fiAsciiTokenizer_0030_g(ctx, base);
loc_8223A6F8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_A710_g"))) PPC_WEAK_FUNC(atSingleton_A710_g);
PPC_FUNC_IMPL(__imp__atSingleton_A710_g) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r3,16
	ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
	// cmplwi cr6,r11,11
	// bge cr6,0x8223a724
	if (ctx.r11.u32 < 11) {
		// li r3,11
		ctx.r3.s64 = 11;
		// blr
		return;
	}
loc_8223A724:
	// cmplwi cr6,r11,29
	// bge cr6,0x8223a734
	if (ctx.r11.u32 < 29) {
		// li r3,29
		ctx.r3.s64 = 29;
		// blr
		return;
	}
loc_8223A734:
	// cmplwi cr6,r11,59
	// bge cr6,0x8223a744
	if (ctx.r11.u32 < 59) {
		// li r3,59
		ctx.r3.s64 = 59;
		// blr
		return;
	}
loc_8223A744:
	// cmplwi cr6,r11,107
	// bge cr6,0x8223a754
	if (ctx.r11.u32 < 107) {
		// li r3,107
		ctx.r3.s64 = 107;
		// blr
		return;
	}
loc_8223A754:
	// cmplwi cr6,r11,191
	// bge cr6,0x8223a764
	if (ctx.r11.u32 < 191) {
		// li r3,191
		ctx.r3.s64 = 191;
		// blr
		return;
	}
loc_8223A764:
	// cmplwi cr6,r11,331
	// bge cr6,0x8223a774
	if (ctx.r11.u32 < 331) {
		// li r3,331
		ctx.r3.s64 = 331;
		// blr
		return;
	}
loc_8223A774:
	// cmplwi cr6,r11,563
	// bge cr6,0x8223a784
	if (ctx.r11.u32 < 563) {
		// li r3,563
		ctx.r3.s64 = 563;
		// blr
		return;
	}
loc_8223A784:
	// cmplwi cr6,r11,953
	// bge cr6,0x8223a794
	if (ctx.r11.u32 < 953) {
		// li r3,953
		ctx.r3.s64 = 953;
		// blr
		return;
	}
loc_8223A794:
	// cmplwi cr6,r11,1609
	// bge cr6,0x8223a7a4
	if (ctx.r11.u32 < 1609) {
		// li r3,1609
		ctx.r3.s64 = 1609;
		// blr
		return;
	}
loc_8223A7A4:
	// cmplwi cr6,r11,2729
	// bge cr6,0x8223a7b4
	if (ctx.r11.u32 < 2729) {
		// li r3,2729
		ctx.r3.s64 = 2729;
		// blr
		return;
	}
loc_8223A7B4:
	// cmplwi cr6,r11,4621
	// bge cr6,0x8223a7c4
	if (ctx.r11.u32 < 4621) {
		// li r3,4621
		ctx.r3.s64 = 4621;
		// blr
		return;
	}
loc_8223A7C4:
	// cmplwi cr6,r11,7841
	// bge cr6,0x8223a7d4
	if (ctx.r11.u32 < 7841) {
		// li r3,7841
		ctx.r3.s64 = 7841;
		// blr
		return;
	}
loc_8223A7D4:
	// cmplwi cr6,r11,13297
	// bge cr6,0x8223a7e4
	if (ctx.r11.u32 < 13297) {
		// li r3,13297
		ctx.r3.s64 = 13297;
		// blr
		return;
	}
loc_8223A7E4:
	// cmplwi cr6,r11,22571
	// bge cr6,0x8223a7f4
	if (ctx.r11.u32 < 22571) {
		// li r3,22571
		ctx.r3.s64 = 22571;
		// blr
		return;
	}
loc_8223A7F4:
	// cmplwi cr6,r11,38351
	// bge cr6,0x8223a804
	if (ctx.r11.u32 < 38351) {
		// li r3,-27185
		// blr
		return;
	}
loc_8223A804:
	// cmplwi cr6,r11,65167
	// li r3,-369
	// bltlr cr6
	if (ctx.r11.u32 < 65167) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_A818_g"))) PPC_WEAK_FUNC(atSingleton_A818_g);
PPC_FUNC_IMPL(__imp__atSingleton_A818_g) {
	PPC_FUNC_PROLOGUE();
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// cmpwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.s32 == 0) return;
loc_8223A830:
	// rlwinm r9,r3,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// rlwinm r11,r3,0,0,3
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xF0000000;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223a854
	if (ctx.r11.u32 != 0) {
		// rlwinm r9,r11,8,24,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFF;
		// xor r8,r9,r11
		ctx.r8.u64 = ctx.r9.u64 ^ ctx.r11.u64;
		// xor r3,r8,r3
		ctx.r3.u64 = ctx.r8.u64 ^ ctx.r3.u64;
	}
loc_8223A854:
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// extsb r11,r7
	ctx.r11.s64 = ctx.r7.s8;
	// cmpwi cr6,r11,0
	// bne cr6,0x8223a830
	if (ctx.r11.s32 != 0) goto loc_8223A830;
	// blr
	return;
}

__attribute__((alias("__imp__SinglesNetworkClient_A868_g"))) PPC_WEAK_FUNC(SinglesNetworkClient_A868_g);
PPC_FUNC_IMPL(__imp__SinglesNetworkClient_A868_g) {
	PPC_FUNC_PROLOGUE();
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
loc_8223A87C:
	// extsb r10,r11
	ctx.r10.s64 = ctx.r11.s8;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r10,97
	// blt cr6,0x8223a89c
	if (ctx.r10.s32 >= 97) {
		// cmpwi cr6,r10,122
		// bgt cr6,0x8223a89c
		if (ctx.r10.s32 > 122) goto loc_8223A89C;
		// addi r8,r10,-32
		ctx.r8.s64 = ctx.r10.s64 + -32;
		// extsb r11,r8
		ctx.r11.s64 = ctx.r8.s8;
	}
loc_8223A89C:
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// rlwinm r10,r3,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r3,0,0,3
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xF0000000;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223a8c0
	if (ctx.r11.u32 != 0) {
		// rlwinm r7,r11,8,24,31
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFF;
		// xor r6,r7,r11
		ctx.r6.u64 = ctx.r7.u64 ^ ctx.r11.u64;
		// xor r3,r6,r3
		ctx.r3.u64 = ctx.r6.u64 ^ ctx.r3.u64;
	}
loc_8223A8C0:
	// lbz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// cmplwi cr6,r11,0
	// bne cr6,0x8223a87c
	if (ctx.r11.u32 != 0) goto loc_8223A87C;
	// blr
	return;
}

__attribute__((alias("__imp__aud_A8D0"))) PPC_WEAK_FUNC(aud_A8D0);
PPC_FUNC_IMPL(__imp__aud_A8D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lhz r10,6(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 6);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + var_r30;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r10
	// ble cr6,0x8223a938
	if (ctx.r11.s32 > ctx.r10.s32) {
		// addi r9,r11,15
		ctx.r9.s64 = ctx.r11.s64 + 15;
		// rlwinm r28,r9,0,0,27
		var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0);
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// lwz r4,0(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmplwi cr6,r4,0
		// beq cr6,0x8223a928
		if (ctx.r4.u32 != 0) {
			// lhz r5,6(r31)
			ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
			// bl 0x82434100
			memcpy(ctx, base);
		}
	loc_8223A928:
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// sth r28,6(r31)
		PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r28);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// stw r29,0(r31)
		PPC_STORE_U32(var_r31 + 0, var_r29);
	}
loc_8223A938:
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bl 0x82434100
	memcpy(ctx, base);
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// li r7,0
	ctx.r7.s64 = 0;
	// add r6,r11,r10
	ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stbx r7,r6,r30
	PPC_STORE_U8(ctx.r6.u32 + var_r30, ctx.r7.u8);
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// add r5,r11,r30
	ctx.r5.u64 = ctx.r11.u64 + var_r30;
	// sth r5,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r5.u16);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_A978_2h"))) PPC_WEAK_FUNC(phJoint3Dof_A978_2h);
PPC_FUNC_IMPL(__imp__phJoint3Dof_A978_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lfs f13,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phJoint3Dof::vtable@+0x0 */;
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phJoint3Dof::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// fabs f12,f13
	ctx.f12.u64 = ctx.f13.u64 & ~0x8000000000000000;
	// fabs f11,f0
	ctx.f11.u64 = ctx.f0.u64 & ~0x8000000000000000;
	// fcmpu cr6,f12,f11
	// blt cr6,0x8223a9b4
	if (ctx.f12.f64 >= ctx.f11.f64) {
		// bso cr6,0x8223a9b4
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223A990, "bso");
		// lis r11,-32253
		// lfs f12,8(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f12.f64 = double(temp.f32);
		// fneg f11,f12
		ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
		// stfs f13,8(r4)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
		// stfs f11,0(r4)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
		// lfs f0,-12016(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,4(r4)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
		// b 0x8223a9d0
	} else {
	loc_8223A9B4:
		// lis r11,-32253
		// lfs f13,8(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fneg f0,f0
		ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// stfs f13,4(r4)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
		// stfs f0,8(r4)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
		// lfs f12,-12016(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		ctx.f12.f64 = double(temp.f32);
		// stfs f12,0(r4)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	}
loc_8223A9D0:
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32158
	// vmsum3fp128 v11,v13,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r11,r11,-25616
	ctx.r11.s64 = ctx.r11.s64 + -25616;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// vrsqrtefp v10,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18496
	ctx.r11.s64 = ctx.r11.s64 + -18496;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcmpeqfp v0,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vsel v0,v10,v9,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
	// vmulfp128 v10,v0,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v9,v8,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v12,v11,v10,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v12.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v12,v9,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v0,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmulfp128 v0,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v0,v11,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBox_AA50_wrh"))) PPC_WEAK_FUNC(phBoundBox_AA50_wrh);
PPC_FUNC_IMPL(__imp__phBoundBox_AA50_wrh) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// cmplwi cr6,r4,5
	// bgtlr cr6
	if (ctx.r4.u32 > 5) return;
	// lis r12,-32220
	// addi r12,r12,-21904
	ctx.r12.s64 = ctx.r12.s64 + -21904;
	// rlwinm r0,r4,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r4.u64) {
	case 0:
		// lfs f0,4(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r5)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
		// lfs f13,8(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,4(r5)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// blr
		return;
	case 1:
		// lfs f12,8(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f12.f64 = double(temp.f32);
		// stfs f12,0(r5)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
		// lfs f11,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
		ctx.f11.f64 = double(temp.f32);
		// stfs f11,4(r5)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// blr
		return;
	case 2:
		// lfs f10,8(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f10.f64 = double(temp.f32);
		// stfs f10,0(r5)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
		// lfs f9,0(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
		ctx.f9.f64 = double(temp.f32);
		// stfs f9,4(r5)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// blr
		return;
	case 3:
		// lfs f8,0(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
		ctx.f8.f64 = double(temp.f32);
		// stfs f8,0(r5)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
		// lfs f7,8(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f7.f64 = double(temp.f32);
		// stfs f7,4(r5)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// blr
		return;
	case 4:
		// lfs f6,0(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
		ctx.f6.f64 = double(temp.f32);
		// stfs f6,0(r5)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
		// lfs f5,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
		ctx.f5.f64 = double(temp.f32);
		// stfs f5,4(r5)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// blr
		return;
	case 5:
		// lfs f4,4(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
		ctx.f4.f64 = double(temp.f32);
		// stfs f4,0(r5)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
		// lfs f3,0(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
		ctx.f3.f64 = double(temp.f32);
		// stfs f3,4(r5)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// blr
		return;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_8223AA88:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f13,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_AB00_g"))) PPC_WEAK_FUNC(phBoundCapsule_AB00_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_AB00_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x824301d8
	phBoundCapsule_01D8_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// frsp f31,f0
	var_f31 = double(float(ctx.f0.f64));
	// bl 0x824302b0
	phBoundCapsule_02B0_g(ctx, base);
	// lfs f0,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// frsp f13,f1
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lfs f12,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f0,f31
	ctx.f10.f64 = double(float(ctx.f0.f64 * var_f31));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * var_f31));
	// fmadds f9,f12,f13,f10
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + ctx.f10.f64));
	// stfs f9,8(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fmsubs f0,f0,f13,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f11.f64));
	// stfs f0,4(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_AB70_g"))) PPC_WEAK_FUNC(phBoundCapsule_AB70_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_AB70_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x824301d8
	phBoundCapsule_01D8_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// frsp f31,f0
	var_f31 = double(float(ctx.f0.f64));
	// bl 0x824302b0
	phBoundCapsule_02B0_g(ctx, base);
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f0.f64 = double(temp.f32);
	// frsp f12,f1
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lfs f13,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f0,f31
	ctx.f11.f64 = double(float(ctx.f0.f64 * var_f31));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * var_f31));
	// fmsubs f13,f13,f12,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 - ctx.f11.f64));
	// stfs f13,8(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fmadds f9,f0,f12,f10
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f9,0(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_ABE0_g"))) PPC_WEAK_FUNC(phBoundCapsule_ABE0_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_ABE0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v0,v0,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v13,v13,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f1,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f0,f1
	// beq cr6,0x8223ac94
	if (ctx.f0.f64 != ctx.f1.f64) {
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v12,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32248
		// vmsum3fp128 v11,v12,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lfs f13,-25520(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25520);  /* glob:0x82029c50 */
		ctx.f13.f64 = double(temp.f32);
		// stvx v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f12,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f12.f64 = double(temp.f32);
		// fdivs f0,f12,f0
		ctx.f0.f64 = double(float(ctx.f12.f64 / ctx.f0.f64));
		// fcmpu cr6,f0,f13
		// bgt cr6,0x8223ac94
		if (ctx.f0.f64 > ctx.f13.f64) {
			// blr
			return;
		}
		// lis r11,-32164
		ctx.r11.s64 = -2107899904;
		// lfs f13,22840(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22840);  /* glob:lbl_825C5938 @ 0x825c5938 */
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// bge cr6,0x8223ac88
		if (ctx.f0.f64 < ctx.f13.f64) {
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfs f1,-25808(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25808);  /* glob:lbl_82079B30 @ 0x82079b30 */
			ctx.f1.f64 = double(temp.f32);
			// b 0x8223ac94
		} else {
		loc_8223AC88:
			// fmr f1,f0
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = ctx.f0.f64;
			// bl 0x82430e90
			phBoundCapsule_0E90_g(ctx, base);
			// frsp f1,f1
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = double(float(ctx.f1.f64));
		}
	}
loc_8223AC94:
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_ACB0_p45"))) PPC_WEAK_FUNC(phBoundCapsule_ACB0_p45);
PPC_FUNC_IMPL(__imp__phBoundCapsule_ACB0_p45) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f13,8(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bgt cr6,0x8223acec
	if (ctx.f0.f64 <= ctx.f13.f64) {
		// bso cr6,0x8223acec
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223ACDC, "bso");
		// vaddfp v11,v13,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v11,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	}
loc_8223ACEC:
	// vmsum3fp128 v10,v0,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// stvx v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	// blelr cr6
	if (ctx.f13.f64 <= ctx.f0.f64) return;
	// fdivs f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// stfs f12,-16(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vmaddfp v8,v0,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v8,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_AD30_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_AD30_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_AD30_g) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32248
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-25796(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25796);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32253
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fmuls f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f11,f10,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f7,f11,f12
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmuls f8,f12,f13
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f6,f11,f13
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f13,f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// fmuls f12,f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmuls f0,f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// fadds f5,f10,f7
	ctx.f5.f64 = double(float(ctx.f10.f64 + ctx.f7.f64));
	// stfs f5,4(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// fsubs f4,f10,f7
	ctx.f4.f64 = double(float(ctx.f10.f64 - ctx.f7.f64));
	// stfs f4,16(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// fsubs f3,f9,f6
	ctx.f3.f64 = double(float(ctx.f9.f64 - ctx.f6.f64));
	// stfs f3,8(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// fadds f2,f6,f9
	ctx.f2.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// stfs f2,32(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// fadds f9,f11,f8
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f8.f64));
	// stfs f9,24(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// fadds f1,f12,f13
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fadds f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fadds f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f8,f8,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f11.f64));
	// stfs f8,36(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// fsubs f7,f0,f1
	ctx.f7.f64 = double(float(ctx.f0.f64 - ctx.f1.f64));
	// stfs f7,0(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// fsubs f6,f0,f12
	ctx.f6.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// stfs f6,20(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// fsubs f5,f0,f10
	ctx.f5.f64 = double(float(ctx.f0.f64 - ctx.f10.f64));
	// stfs f5,40(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 40, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_ADE0_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_ADE0_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_ADE0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x8223b218
	LocomotionStateAnim_B218_g(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8223b218
	LocomotionStateAnim_B218_g(ctx, base);
	// lfs f11,104(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32253
	// fmuls f9,f12,f11
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f13.f64 = double(temp.f32);
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// lfs f11,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f11.f64 = double(temp.f32);
	// lfs f7,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f7.f64 = double(temp.f32);
	// lfs f10,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f6,f13,f0,f9
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f5,f8,f11,f6
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f11.f64 + ctx.f6.f64));
	// fmadds f4,f7,f10,f5
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fcmpu cr6,f4,f0
	// bge cr6,0x8223ae74
	if (ctx.f4.f64 < ctx.f0.f64) {
		// fneg f3,f11
		ctx.f3.u64 = ctx.f11.u64 ^ 0x8000000000000000;
		// stfs f3,80(r1)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// fneg f2,f13
		ctx.f2.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// stfs f2,84(r1)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// fneg f1,f12
		ctx.f1.u64 = ctx.f12.u64 ^ 0x8000000000000000;
		// stfs f1,88(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		// fneg f0,f10
		ctx.f0.u64 = ctx.f10.u64 ^ 0x8000000000000000;
		// stfs f0,92(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	}
loc_8223AE74:
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8223aed0
	phBoundCapsule_AED0_g(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x8223ad30
	LocomotionStateAnim_AD30_g(ctx, base);
	// addi r11,r30,48
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 48;
	// addi r10,r31,48
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 48;
	// stfs f31,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r9,r29,48
	ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 48;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v13,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// vmaddfp v0,v13,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phBoundCapsule_AED0_g"))) PPC_WEAK_FUNC(phBoundCapsule_AED0_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_AED0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_28
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// fmr f30,f1
	var_f30 = ctx.f1.f64;
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82137bf8
	phBoundCapsule_7BF8_g(ctx, base);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r28,r11,-12024
	var_r28 = (uint32_t)(ctx.r11.s64 + -12024);  // lbl_8202D108 @ 0x8202d108
	// lfs f0,4(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r28 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f1,f0
	var_f31 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// bl 0x824301d8
	phBoundCapsule_01D8_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// lfs f13,8(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// beq cr6,0x8223afc8
	if (ctx.f0.f64 != ctx.f13.f64) {
		// fmuls f30,f31,f30
		var_f30 = double(float(var_f31 * var_f30));
		// lfs f13,0(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + 0);
		ctx.f13.f64 = double(temp.f32);
		// fdivs f29,f13,f0
		var_f29 = double(float(ctx.f13.f64 / ctx.f0.f64));
		// fsubs f1,f31,f30
		ctx.f1.f64 = double(float(var_f31 - var_f30));
		// bl 0x824301d8
		phBoundCapsule_01D8_g(ctx, base);
		// fmr f0,f1
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = ctx.f1.f64;
		// fmr f1,f30
		ctx.f1.f64 = var_f30;
		// frsp f0,f0
		ctx.f0.f64 = double(float(ctx.f0.f64));
		// fmuls f31,f0,f29
		var_f31 = double(float(ctx.f0.f64 * var_f29));
		// bl 0x824301d8
		phBoundCapsule_01D8_g(ctx, base);
		// frsp f11,f1
		ctx.fpscr.disableFlushMode();
		ctx.f11.f64 = double(float(ctx.f1.f64));
		// lfs f13,0(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,0(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f12.f64 = double(temp.f32);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// fmuls f0,f11,f29
		ctx.f0.f64 = double(float(ctx.f11.f64 * var_f29));
		// fmuls f10,f13,f0
		ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// fmadds f9,f12,f31,f10
		ctx.f9.f64 = double(float(ctx.f12.f64 * var_f31 + ctx.f10.f64));
		// stfs f9,0(r30)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r30 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
		// lfs f8,4(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f8.f64 = double(temp.f32);
		// fmuls f7,f8,f31
		ctx.f7.f64 = double(float(ctx.f8.f64 * var_f31));
		// lfs f6,4(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f6.f64 = double(temp.f32);
		// fmadds f5,f6,f0,f7
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f7.f64));
		// stfs f5,4(r30)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(var_r30 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
		// lfs f4,8(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 8);
		ctx.f4.f64 = double(temp.f32);
		// fmuls f3,f4,f31
		ctx.f3.f64 = double(float(ctx.f4.f64 * var_f31));
		// lfs f2,8(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 8);
		ctx.f2.f64 = double(temp.f32);
		// fmadds f1,f2,f0,f3
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f3.f64));
		// stfs f1,8(r30)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r30 + 8, temp.u32);
		// lfs f13,12(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 12);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f12,f13,f31
		ctx.f12.f64 = double(float(ctx.f13.f64 * var_f31));
		// lfs f11,12(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 12);
		ctx.f11.f64 = double(temp.f32);
		// fmadds f10,f11,f0,f12
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f12.f64));
		// stfs f10,12(r30)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r30 + 12, temp.u32);
		// bl 0x820c42d8
		util_42D8(ctx, base);
		return;
	}
loc_8223AFC8:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(var_r31 + 0);
	// std r11,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.r11.u64);
	// ld r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U64(var_r31 + 8);
	// std r10,8(r30)
	PPC_STORE_U64(var_r30 + 8, ctx.r10.u64);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_AFF0_g"))) PPC_WEAK_FUNC(phBoundCapsule_AFF0_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_AFF0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// fmr f30,f1
	var_f30 = ctx.f1.f64;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// bl 0x82137bf8
	phBoundCapsule_7BF8_g(ctx, base);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r29,r11,-12024
	var_r29 = (uint32_t)(ctx.r11.s64 + -12024);  // lbl_8202D108 @ 0x8202d108
	// lfs f0,4(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f1,f0
	var_f31 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// bl 0x824301d8
	phBoundCapsule_01D8_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// lfs f13,8(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// beq cr6,0x8223b0cc
	if (ctx.f0.f64 != ctx.f13.f64) {
		// fmuls f30,f31,f30
		var_f30 = double(float(var_f31 * var_f30));
		// lfs f13,0(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f13.f64 = double(temp.f32);
		// fdivs f29,f13,f0
		var_f29 = double(float(ctx.f13.f64 / ctx.f0.f64));
		// fsubs f1,f31,f30
		ctx.f1.f64 = double(float(var_f31 - var_f30));
		// bl 0x824301d8
		phBoundCapsule_01D8_g(ctx, base);
		// fmr f0,f1
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = ctx.f1.f64;
		// fmr f1,f30
		ctx.f1.f64 = var_f30;
		// frsp f0,f0
		ctx.f0.f64 = double(float(ctx.f0.f64));
		// fmuls f31,f0,f29
		var_f31 = double(float(ctx.f0.f64 * var_f29));
		// bl 0x824301d8
		phBoundCapsule_01D8_g(ctx, base);
		// frsp f10,f1
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = double(float(ctx.f1.f64));
		// lfs f13,0(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f13.f64 = double(temp.f32);
		// fmuls f12,f13,f31
		ctx.f12.f64 = double(float(ctx.f13.f64 * var_f31));
		// lfs f9,4(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f9.f64 = double(temp.f32);
		// fmuls f8,f9,f31
		ctx.f8.f64 = double(float(ctx.f9.f64 * var_f31));
		// lfs f7,8(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 8);
		ctx.f7.f64 = double(temp.f32);
		// fmuls f6,f31,f7
		ctx.f6.f64 = double(float(var_f31 * ctx.f7.f64));
		// lfs f11,0(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f11.f64 = double(temp.f32);
		// lfs f5,12(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 12);
		ctx.f5.f64 = double(temp.f32);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// fmuls f4,f5,f31
		ctx.f4.f64 = double(float(ctx.f5.f64 * var_f31));
		// fmuls f0,f10,f29
		ctx.f0.f64 = double(float(ctx.f10.f64 * var_f29));
		// fmadds f3,f11,f0,f12
		ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f12.f64));
		// stfs f3,0(r31)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(var_r31 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
		// lfs f2,4(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f2.f64 = double(temp.f32);
		// fmadds f1,f2,f0,f8
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f8.f64));
		// stfs f1,4(r31)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r31 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
		// lfs f13,8(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f12,f13,f0,f6
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f6.f64));
		// stfs f12,8(r31)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r31 + 8, temp.u32);
		// lfs f11,12(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 12);
		ctx.f11.f64 = double(temp.f32);
		// fmadds f10,f11,f0,f4
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f4.f64));
		// stfs f10,12(r31)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r31 + 12, temp.u32);
		// bl 0x820c42d8
		util_42D8(ctx, base);
	}
loc_8223B0CC:
	return;
}

__attribute__((alias("__imp__phBoundCapsule_B0E0_p45"))) PPC_WEAK_FUNC(phBoundCapsule_B0E0_p45);
PPC_FUNC_IMPL(__imp__phBoundCapsule_B0E0_p45) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// bl 0x820c4340
	util_4340(ctx, base);
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stfs f0,0(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// lis r11,-32253
	// lfs f13,4(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f12,8(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stfs f1,0(r29)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// lfs f12,132(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	// ble cr6,0x8223b184
	if (ctx.f13.f64 > ctx.f12.f64) {
		// fcmpu cr6,f13,f0
		// beq cr6,0x8223b164
		if (ctx.f13.f64 != ctx.f0.f64) {
			// fsqrts f0,f13
			ctx.f0.f64 = double(float(sqrt(ctx.f13.f64)));
			// lfs f13,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f13.f64 = double(temp.f32);
			// fdivs f0,f13,f0
			ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
		}
	loc_8223B164:
		// stfs f0,96(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vmulfp128 v11,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v11,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	}
loc_8223B184:
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lvx128 v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phBoundCapsule_B198_fw"))) PPC_WEAK_FUNC(phBoundCapsule_B198_fw);
PPC_FUNC_IMPL(__imp__phBoundCapsule_B198_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// lis r11,-32253
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lfs f0,-12020(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12020);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// bl 0x820d04f0
	phBoundCapsule_04F0_g(ctx, base);
	// lfs f13,8(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f9,12(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 12, temp.u32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f13,8(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// stfs f10,0(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_B218_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_B218_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_B218_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=112, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lfs f12,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 20);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f12,f13
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// lfs f11,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f11.f64 = double(temp.f32);
	// fadds f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f11.f64));
	// fcmpu cr6,f0,f12
	// blt cr6,0x8223b294
	if (ctx.f0.f64 < ctx.f12.f64) goto loc_8223B294;
	// bso cr6,0x8223b294
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223B248, "bso");
	// fcmpu cr6,f0,f13
	// bso cr6,0x8223b258
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223B250, "bso");
	// bge cr6,0x8223b2b4
	if (ctx.f0.f64 >= ctx.f13.f64) goto loc_8223B2B4;
loc_8223B258:
	// fcmpu cr6,f13,f11
	ctx.fpscr.disableFlushMode();
	// blt cr6,0x8223b268
	if (ctx.f13.f64 >= ctx.f11.f64) {
		// li r11,2
		ctx.r11.s64 = 2;
		// bns cr6,0x8223b26c
		// UNIMPLEMENTED: bns
		PPC_UNIMPLEMENTED(0x8223B264, "bns");
	}
loc_8223B268:
	// li r11,3
	ctx.r11.s64 = 3;
loc_8223B26C:
	// lis r12,-32220
	// addi r12,r12,-19836
	ctx.r12.s64 = ctx.r12.s64 + -19836;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		goto loc_8223B2C8;
	case 1:
		goto loc_8223B33C;
	case 2:
		goto loc_8223B3B8;
	case 3:
		goto loc_8223B438;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_8223B294:
	// fcmpu cr6,f12,f13
	ctx.fpscr.disableFlushMode();
	// bso cr6,0x8223b258
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223B298, "bso");
	// blt cr6,0x8223b258
	if (ctx.f12.f64 < ctx.f13.f64) goto loc_8223B258;
	// fcmpu cr6,f12,f11
	// blt cr6,0x8223b268
	if (ctx.f12.f64 < ctx.f11.f64) goto loc_8223B268;
	// bso cr6,0x8223b268
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223B2A8, "bso");
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8223b26c
	goto loc_8223B26C;
loc_8223B2B4:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	// blt cr6,0x8223b268
	if (ctx.f0.f64 < ctx.f11.f64) goto loc_8223B268;
	// bso cr6,0x8223b268
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223B2BC, "bso");
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x8223b26c
	goto loc_8223B26C;
loc_8223B2C8:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r29,r11,-11900
	var_r29 = (uint32_t)(ctx.r11.s64 + -11900);  // lbl_8202D184 @ 0x8202d184
	// lfs f13,-124(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + -124);
	ctx.f13.f64 = double(temp.f32);
	// fadds f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lfs f0,-120(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -120);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32163
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f13,12(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// lfs f12,24(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 24);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,-32640(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32640);  /* glob:lbl_82028080 @ 0x82028080 */
	ctx.f0.f64 = double(temp.f32);
	// fsubs f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,0(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// lfs f8,32(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,4(r30)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	// lfs f4,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,16(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,8(r30)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	return;
loc_8223B33C:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r29,r11,-11900
	var_r29 = (uint32_t)(ctx.r11.s64 + -11900);  // lbl_8202D184 @ 0x8202d184
	// lfs f13,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f13,f12,f13,f0
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 - ctx.f0.f64));
	// lfs f0,-124(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -124);
	ctx.f0.f64 = double(temp.f32);
	// fadds f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lfs f0,-120(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -120);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32163
	// fmuls f13,f12,f0
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f13,0(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// lfs f11,24(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 24);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,-32640(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32640);  /* glob:lbl_82028080 @ 0x82028080 */
	ctx.f0.f64 = double(temp.f32);
	// fsubs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f8,12(r30)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// lfs f7,16(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fadds f5,f7,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 + ctx.f6.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,4(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	// lfs f3,32(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fadds f1,f3,f2
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f2.f64));
	// fmuls f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f0,8(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	return;
loc_8223B3B8:
	// lis r11,-32253
	// lfs f12,20(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 20);
	ctx.f12.f64 = double(temp.f32);
	// addi r29,r11,-11900
	var_r29 = (uint32_t)(ctx.r11.s64 + -11900);  // lbl_8202D184 @ 0x8202d184
	// lfs f13,0(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f11,f12,f13,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 - ctx.f0.f64));
	// lfs f0,-124(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -124);
	ctx.f0.f64 = double(temp.f32);
	// fadds f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f10,f1
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// lfs f0,-120(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -120);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32163
	// fmuls f13,f10,f0
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f13,4(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	// lfs f9,32(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f0,-32640(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32640);  /* glob:lbl_82028080 @ 0x82028080 */
	ctx.f0.f64 = double(temp.f32);
	// fsubs f7,f9,f8
	ctx.f7.f64 = double(float(ctx.f9.f64 - ctx.f8.f64));
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f6,12(r30)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fadds f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfs f2,0(r30)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// lfs f1,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f1.f64 = double(temp.f32);
	// lfs f13,24(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f1,f13
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,8(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	return;
loc_8223B438:
	// lis r11,-32253
	// lfs f10,40(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f10.f64 = double(temp.f32);
	// addi r29,r11,-11900
	var_r29 = (uint32_t)(ctx.r11.s64 + -11900);  // lbl_8202D184 @ 0x8202d184
	// lfs f13,0(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f9,f10,f13,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 - ctx.f0.f64));
	// lfs f0,-124(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -124);
	ctx.f0.f64 = double(temp.f32);
	// fadds f1,f9,f0
	ctx.f1.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f8,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f1.f64));
	// lfs f0,-120(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + -120);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32163
	// fmuls f13,f8,f0
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f13,8(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// lfs f7,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,16(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,-32640(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32640);  /* glob:lbl_82028080 @ 0x82028080 */
	ctx.f0.f64 = double(temp.f32);
	// fsubs f5,f7,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 - ctx.f6.f64));
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,12(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// lfs f3,32(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fadds f1,f3,f2
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f2.f64));
	// fmuls f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f13,0(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// lfs f12,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,24(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 24);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,4(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	return;
}

__attribute__((alias("__imp__crIKSolverGleicherLimb_B4B8"))) PPC_WEAK_FUNC(crIKSolverGleicherLimb_B4B8);
PPC_FUNC_IMPL(__imp__crIKSolverGleicherLimb_B4B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, manual
	// lvx128 v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32158
	// lvx128 v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v6,v11,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmsum3fp128 v5,v10,v10
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// addi r11,r11,-25616
	ctx.r11.s64 = ctx.r11.s64 + -25616;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// li r31,1
	var_r31 = 1;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// vrsqrtefp v4,v6
	simde_mm_store_ps(ctx.v4.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v6.f32))));
	// vrsqrtefp v3,v5
	simde_mm_store_ps(ctx.v3.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v5.f32))));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18496
	ctx.r11.s64 = ctx.r11.s64 + -18496;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// vcmpeqfp v13,v4,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vor v9,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vcmpeqfp v12,v3,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsel v13,v4,v8,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8))));
	// vsel v12,v3,v8,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8))));
	// vmulfp128 v8,v13,v13
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v3,v12,v12
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v4,v7,v13
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v7,v7,v12
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vnmsubfp v9,v6,v8,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v9.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vnmsubfp v0,v5,v3,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v3.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v13,v9,v4,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v4.f32)), simde_mm_load_ps(ctx.v13.f32)));
	// vmaddfp v12,v0,v7,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v7.f32)), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v0,v11,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v10,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r11,-12016
	ctx.r10.s64 = ctx.r11.s64 + -12016;
	// lis r11,-32248
	// lfs f11,-8(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f0,f13,f11
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// fabs f12,f0
	ctx.f12.u64 = ctx.f0.u64 & ~0x8000000000000000;
	// lfs f0,-25840(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25840);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f12,f0
	// bge cr6,0x8223b5c0
	if (ctx.f12.f64 < ctx.f0.f64) {
		// lfs f0,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
		// stfs f0,4(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		// stfs f0,8(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
		// stfs f11,12(r3)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
		// li r3,1
		ctx.r3.s64 = 1;
		// blr
		return;
	}
loc_8223B5C0:
	// fadds f8,f13,f11
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// fabs f13,f8
	ctx.f13.u64 = ctx.f8.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f0
	// bge cr6,0x8223b648
	if (ctx.f13.f64 < ctx.f0.f64) {
		// lfs f0,4(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		ctx.f0.f64 = double(temp.f32);
		// li r31,0
		var_r31 = 0;
		// lfs f13,0(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,8(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
		ctx.f12.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8223b600
		if (ctx.f13.f64 >= ctx.f0.f64) {
			// bso cr6,0x8223b600
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223B5E8, "bso");
			// fcmpu cr6,f13,f12
			// blt cr6,0x8223b610
			if (ctx.f13.f64 < ctx.f12.f64) goto loc_8223B610;
			// bso cr6,0x8223b610
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223B5F4, "bso");
			// li r11,0
			ctx.r11.s64 = 0;
			// b 0x8223b614
		} else {
		loc_8223B600:
			// fcmpu cr6,f0,f12
			ctx.fpscr.disableFlushMode();
			// blt cr6,0x8223b610
			if (ctx.f0.f64 >= ctx.f12.f64) {
				// li r11,1
				ctx.r11.s64 = 1;
				// bns cr6,0x8223b614
				// UNIMPLEMENTED: bns
				PPC_UNIMPLEMENTED(0x8223B60C, "bns");
			}
		loc_8223B610:
			// li r11,2
			ctx.r11.s64 = 2;
		}
	loc_8223B614:
		// cmplwi cr6,r11,1
		// blt cr6,0x8223b638
		if (ctx.r11.u32 >= 1) {
			// beq cr6,0x8223b638
			if (ctx.cr6.eq) goto loc_8223B638;
			// cmplwi cr6,r11,3
			// bge cr6,0x8223b648
			if (ctx.r11.u32 >= 3) goto loc_8223B648;
			// lfs f13,0(r10)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
			ctx.f13.f64 = double(temp.f32);
			// fmr f0,f11
			ctx.f0.f64 = ctx.f11.f64;
			// fmr f12,f13
			ctx.f12.f64 = ctx.f13.f64;
			// b 0x8223b654
			goto loc_8223B654;
		}
	loc_8223B638:
		// lfs f0,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// fmr f13,f11
		ctx.f13.f64 = ctx.f11.f64;
		// fmr f12,f0
		ctx.f12.f64 = ctx.f0.f64;
		// b 0x8223b654
	} else {
	loc_8223B648:
		// lfs f13,88(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f13.f64 = double(temp.f32);
		// lfs f0,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f0.f64 = double(temp.f32);
		// lfs f12,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f12.f64 = double(temp.f32);
	}
loc_8223B654:
	// lfs f11,100(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f5,f11,f12
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f6,f9,f13
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f8,12(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// fmsubs f2,f9,f0,f5
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 - ctx.f5.f64));
	// stfs f2,8(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// fmsubs f4,f11,f13,f7
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f13.f64 - ctx.f7.f64));
	// stfs f4,0(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// fmsubs f3,f10,f12,f6
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 - ctx.f6.f64));
	// stfs f3,4(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// bl 0x820c42d8
	util_42D8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__util_B6A8"))) PPC_WEAK_FUNC(util_B6A8);
PPC_FUNC_IMPL(__imp__util_B6A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_29
	// lis r11,-32158
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,-25616
	ctx.r11.s64 = ctx.r11.s64 + -25616;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lvx128 v12,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// vmsum3fp128 v6,v0,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// vmsum3fp128 v5,v13,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18496
	ctx.r11.s64 = ctx.r11.s64 + -18496;
	// stvx v5,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// vor v10,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// lvx128 v3,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v5,v3,0
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// vnmsubfp v13,v12,v5,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v5.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r30,r11,-12016
	var_r30 = (uint32_t)(ctx.r11.s64 + -12016);  // lbl_8202D110 @ 0x8202d110
	// lis r11,-32248
	// lvx128 v4,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v6,v4,0
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.u32), 0xFF));
	// lfs f0,-8(r30)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(var_r30 + -8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-25840(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25840);
	ctx.f13.f64 = double(temp.f32);
	// vnmsubfp v0,v12,v6,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v6.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v6,v0,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vrsqrtefp v5,v6
	simde_mm_store_ps(ctx.v5.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v6.f32))));
	// vcmpeqfp v12,v5,v9
	simde_mm_store_ps(ctx.v12.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vsel v12,v5,v8,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8))));
	// vmulfp128 v5,v12,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v4,v7,v12
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vnmsubfp v10,v6,v5,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v5.f32)), simde_mm_load_ps(ctx.v10.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v12,v10,v4,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v4.f32)), simde_mm_load_ps(ctx.v12.f32)));
	// vmsum3fp128 v10,v13,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmulfp128 v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vrsqrtefp v6,v10
	simde_mm_store_ps(ctx.v6.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v10.f32))));
	// stvx v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcmpeqfp v0,v6,v9
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vsel v0,v6,v8,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8))));
	// vmulfp128 v9,v0,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v8,v7,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v11,v10,v9,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v11.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v11,v8,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v2,v12,v0
	simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f1.f64 = double(temp.f32);
	// fsubs f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
	// fabs f12,f12
	ctx.f12.u64 = ctx.f12.u64 & ~0x8000000000000000;
	// fcmpu cr6,f12,f13
	// bge cr6,0x8223b7fc
	if (ctx.f12.f64 < ctx.f13.f64) {
		// lfs f13,0(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 0);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,0(r29)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r29 + 0, temp.u32);
		// stfs f13,4(r29)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r29 + 4, temp.u32);
		// stfs f13,8(r29)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r29 + 8, temp.u32);
		// stfs f0,12(r29)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r29 + 12, temp.u32);
		return;
	}
loc_8223B7FC:
	// fadds f11,f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// fabs f0,f11
	ctx.f0.u64 = ctx.f11.u64 & ~0x8000000000000000;
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223b828
	if (ctx.f0.f64 < ctx.f13.f64) {
		// lis r11,-32248
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lfs f1,-25808(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25808);
		ctx.f1.f64 = double(temp.f32);
		// bl 0x8223b198
		phBoundCapsule_B198_fw(ctx, base);
		return;
	}
loc_8223B828:
	// lfs f13,100(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f9,f0
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f12,f11
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fmuls f6,f10,f13
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmsubs f4,f10,f11,f7
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f7.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmsubs f5,f0,f13,f8
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f8.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmsubs f3,f9,f12,f6
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f6.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// bl 0x82430e90
	phBoundCapsule_0E90_g(ctx, base);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 0);
	ctx.f0.f64 = double(temp.f32);
	// frsp f1,f1
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// lvx128 v1,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v31,v1,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v31.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f2,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// fcmpu cr6,f2,f0
	// blt cr6,0x8223b8ac
	if (ctx.f2.f64 >= ctx.f0.f64) {
		// bso cr6,0x8223b8ac
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223B894, "bso");
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8223b198
		phBoundCapsule_B198_fw(ctx, base);
		return;
	}
loc_8223B8AC:
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// vsubfp v30,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v30,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223b198
	phBoundCapsule_B198_fw(ctx, base);
	return;
}

__attribute__((alias("__imp__util_B8D0"))) PPC_WEAK_FUNC(util_B8D0);
PPC_FUNC_IMPL(__imp__util_B8D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f11,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f9,f10,f11,f12
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f12.f64));
	// fnmsubs f4,f7,f8,f9
	ctx.f4.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f9.f64)));
	// fnmsubs f3,f5,f6,f4
	ctx.f3.f64 = double(float(-(ctx.f5.f64 * ctx.f6.f64 - ctx.f4.f64)));
	// stfs f3,12(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// lfs f2,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f2,f1
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f7,f13,f12,f0
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f0.f64));
	// fmadds f6,f11,f10,f7
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f7.f64));
	// fnmsubs f5,f9,f8,f6
	ctx.f5.f64 = double(float(-(ctx.f9.f64 * ctx.f8.f64 - ctx.f6.f64)));
	// stfs f5,0(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f4,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f1,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f9,f1,f0,f2
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f2.f64));
	// fmadds f8,f13,f12,f9
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fnmsubs f7,f11,f10,f8
	ctx.f7.f64 = double(float(-(ctx.f11.f64 * ctx.f10.f64 - ctx.f8.f64)));
	// stfs f7,4(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f6,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f6,f5
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f3,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f3,f2,f4
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f4.f64));
	// fmadds f10,f1,f0,f11
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fnmsubs f9,f13,f12,f10
	ctx.f9.f64 = double(float(-(ctx.f13.f64 * ctx.f12.f64 - ctx.f10.f64)));
	// stfs f9,8(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__util_B9A8"))) PPC_WEAK_FUNC(util_B9A8);
PPC_FUNC_IMPL(__imp__util_B9A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f12,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f0,f13,f10
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f10.f64));
	// fmadds f4,f9,f8,f5
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f5.f64));
	// fmadds f3,f6,f7,f4
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64 + ctx.f4.f64));
	// stfs f3,12(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// lfs f2,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f2,f1
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f7,f13,f12,f0
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 - ctx.f0.f64));
	// fnmsubs f6,f11,f10,f7
	ctx.f6.f64 = double(float(-(ctx.f11.f64 * ctx.f10.f64 - ctx.f7.f64)));
	// fmadds f5,f9,f8,f6
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f6.f64));
	// stfs f5,0(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f4,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f1,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f9,f1,f0,f2
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 - ctx.f2.f64));
	// fnmsubs f8,f13,f12,f9
	ctx.f8.f64 = double(float(-(ctx.f13.f64 * ctx.f12.f64 - ctx.f9.f64)));
	// fmadds f7,f11,f10,f8
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f8.f64));
	// stfs f7,4(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f6,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f6,f5
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f3,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmsubs f11,f3,f2,f4
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 - ctx.f4.f64));
	// fnmsubs f10,f1,f0,f11
	ctx.f10.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// fmadds f9,f13,f12,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f9,8(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_BA80_h"))) PPC_WEAK_FUNC(phBoundCapsule_BA80_h);
PPC_FUNC_IMPL(__imp__phBoundCapsule_BA80_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8223bbb0
	util_BBB0(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8223b4b8
	crIKSolverGleicherLimb_B4B8(ctx, base);
	// lfs f8,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f13,f8
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f10,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f0.f64 = double(temp.f32);
	// lfs f7,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f2,f0,f10
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f11,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f3,f0,f7
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f9,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f31,f0,f8
	var_f31 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// lfs f12,12(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f6,8(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,4(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f4.f64 = double(temp.f32);
	// fmadds f1,f11,f10,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f1.f64));
	// fmadds f2,f11,f9,f2
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fmadds f3,f13,f10,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 + ctx.f3.f64));
	// fmsubs f31,f12,f10,f31
	var_f31 = double(float(ctx.f12.f64 * ctx.f10.f64 - var_f31));
	// fmadds f1,f12,f7,f1
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 + ctx.f1.f64));
	// fmadds f2,f12,f8,f2
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f8.f64 + ctx.f2.f64));
	// fmadds f3,f12,f9,f3
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f3.f64));
	// fnmsubs f0,f0,f9,f1
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f9.f64 - ctx.f1.f64)));
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f10,f13,f7,f2
	ctx.f10.f64 = double(float(-(ctx.f13.f64 * ctx.f7.f64 - ctx.f2.f64)));
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f13,f13,f9,f31
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - var_f31)));
	// fnmsubs f12,f11,f8,f3
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f9,f6,f0
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fnmsubs f2,f11,f7,f13
	ctx.f2.f64 = double(float(-(ctx.f11.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// stfs f2,92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmadds f8,f5,f12,f9
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fmadds f1,f4,f10,f8
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f10.f64 + ctx.f8.f64));
	// bl 0x82430fe0
	phBoundCapsule_0FE0_g(ctx, base);
	// frsp f7,f1
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f1.f64));
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f0,-11900(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -11900);  /* glob:lbl_8202D184 @ 0x8202d184 */
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32248
	// fmuls f1,f7,f0
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f0,-25808(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25808);  /* glob:0x82029b30 */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// ble cr6,0x8223bb74
	if (ctx.f1.f64 > ctx.f0.f64) {
		// lis r11,-32253
		ctx.r11.s64 = -2113732608;
		// lfs f0,-16340(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16340);  /* glob:lbl_8202C02C @ 0x8202c02c */
		ctx.f0.f64 = double(temp.f32);
		// fsubs f1,f1,f0
		ctx.f1.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
		// b 0x8223bb90
	} else {
	loc_8223BB74:
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfs f0,-25812(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25812);  /* glob:lbl_82079B2C @ 0x82079b2c */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f1,f0
		// bge cr6,0x8223bb90
		if (ctx.f1.f64 >= ctx.f0.f64) {
			// addi r1,r1,144
			ctx.r1.s64 = ctx.r1.s64 + 144;
			// lwz r12,-8(r1)
			ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
			// mtlr r12
			ctx.lr = ctx.r12.u64;
			// lfd f31,-32(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
			// ld r30,-24(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
			// ld r31,-16(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// blr
			return;
		}
		// lis r11,-32253
		ctx.r11.s64 = -2113732608;
		// lfs f0,-16340(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16340);  /* glob:lbl_8202C02C @ 0x8202c02c */
		ctx.f0.f64 = double(temp.f32);
		// fadds f1,f1,f0
		ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	}
loc_8223BB90:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__util_BBB0"))) PPC_WEAK_FUNC(util_BBB0);
PPC_FUNC_IMPL(__imp__util_BBB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lfs f11,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32253
	// lfs f13,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// stfs f11,0(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f13,8(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// stfs f12,4(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f9
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// lfs f7,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmadds f3,f7,f6,f8
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f8.f64));
	// fmadds f2,f5,f4,f3
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 + ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,-16(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,12(r3)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f13
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmsubs f11,f12,f0,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f13.f64));
	// stfs f11,-16(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// vmaddfp v0,v13,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,12(r3)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,-16(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// lfs f0,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f11,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f9,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f7,f0,f9
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// lvx128 v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// fmsubs f5,f13,f0,f8
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 - ctx.f8.f64));
	// stfs f5,-16(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// fmsubs f3,f12,f9,f6
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 - ctx.f6.f64));
	// stfs f3,-8(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -8, temp.u32);
	// fmsubs f4,f11,f10,f7
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 - ctx.f7.f64));
	// stfs f4,-12(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -12, temp.u32);
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v8,v9,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__util_BCB0"))) PPC_WEAK_FUNC(util_BCB0);
PPC_FUNC_IMPL(__imp__util_BCB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lfs f11,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32253
	// lfs f13,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// stfs f11,0(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// stfs f13,8(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// stfs f12,4(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f9
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// lfs f7,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmadds f3,f7,f6,f8
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f8.f64));
	// fmadds f2,f5,f4,f3
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 + ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,-16(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,12(r3)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f13
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmsubs f11,f12,f0,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f13.f64));
	// stfs f11,-16(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// vmaddfp v0,v13,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vor v10,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfs f10,12(r3)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,-16(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// lfs f0,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f11,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f9,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f7,f0,f9
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// lvx128 v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// fmsubs f5,f13,f0,f8
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 - ctx.f8.f64));
	// stfs f5,-16(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// fmsubs f3,f12,f9,f6
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 - ctx.f6.f64));
	// stfs f3,-8(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -8, temp.u32);
	// fmsubs f4,f11,f10,f7
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 - ctx.f7.f64));
	// stfs f4,-12(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -12, temp.u32);
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lvx128 v8,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vnmsubfp v10,v8,v0,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v10.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_BDB0_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_BDB0_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_BDB0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// addi r6,r31,16
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 16;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r30,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 16;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// bl 0x8223bbb0
	util_BBB0(ctx, base);
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223b8d0
	util_B8D0(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_BE20_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_BE20_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_BE20_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, savegprlr_29
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r31,r6,16
	var_r31 = (uint32_t)(ctx.r6.s64 + 16);
	// addi r4,r30,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 0);
	ctx.f0.f64 = double(temp.f32);
	// mr r5,r31
	ctx.r5.u64 = var_r31;
	// fneg f13,f0
	ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f13,0(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// lfs f12,4(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fneg f11,f12
	ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f11,4(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// lfs f10,8(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fneg f9,f10
	ctx.f9.u64 = ctx.f10.u64 ^ 0x8000000000000000;
	// stfs f9,8(r6)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r6.u32 + 8, temp.u32);
	// lfs f8,12(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 12);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,12(r6)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r6.u32 + 12, temp.u32);
	// bl 0x8223bbb0
	util_BBB0(ctx, base);
	// lis r11,-32160
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vsubfp v12,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223bbb0
	util_BBB0(ctx, base);
	// addi r10,r29,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 16;
	// lvx128 v11,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v11,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v10,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223b9a8
	util_B9A8(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundGeometry_BED0"))) PPC_WEAK_FUNC(phBoundGeometry_BED0);
PPC_FUNC_IMPL(__imp__phBoundGeometry_BED0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r21 = 0;
	double var_f27 = 0.0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	double var_f28 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f87c
	ctx.lr = 0x8223BED8;
	__savegprlr_21(ctx, base);
	// addi r12,r1,-96
	ctx.r12.s64 = ctx.r1.s64 + -96;
	// bl 0x82436614
	__savefpr_27(ctx, base);
	// li r12,-160
	ctx.r12.s64 = -160;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// fmr f27,f1
	ctx.fpscr.disableFlushMode();
	var_f27 = ctx.f1.f64;
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// mr r24,r7
	var_r24 = ctx.r7.u32;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// li r5,0
	ctx.r5.s64 = 0;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r27,1
	var_r27 = 1;
	// lvx128 v10,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r22,r8
	var_r22 = ctx.r8.u32;
	// vsubfp v0,v10,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v12,r0,r24
	ea = (var_r24) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r25,r9
	var_r25 = ctx.r9.u32;
	// mr r23,r10
	var_r23 = ctx.r10.u32;
	// mr r8,r5
	ctx.r8.u64 = ctx.r5.u64;
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// vmsum3fp128 v0,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fabs f0,f13
	ctx.f0.u64 = ctx.f13.u64 & ~0x8000000000000000;
	// fcmpu cr6,f0,f27
	// ble cr6,0x8223bf4c
	if (ctx.f0.f64 > var_f27) {
	loc_8223BF44:
		// li r3,3
		ctx.r3.s64 = 3;
		// b 0x8223c39c
		// addi r1,r1,320
		ctx.r1.s64 = ctx.r1.s64 + 320;
		// li r0,-160
		ctx.r0.s64 = -160;
		// lvx128 v127,r1,r0
		ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r12,r1,-96
		ctx.r12.s64 = ctx.r1.s64 + -96;
		// bl 0x82436660
		__restfpr_27(ctx, base);
		// b 0x8242f8cc
		__restgprlr_21(ctx, base);
		return;
	}
loc_8223BF4C:
	// lis r11,-32253
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// addi r3,r11,-11900
	ctx.r3.s64 = ctx.r11.s64 + -11900;
	// cmpwi cr6,r6,0
	// lfs f30,-116(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + -116);
	var_f30 = double(temp.f32);
	// ble cr6,0x8223c064
	if (ctx.r6.s32 > 0) {
		// addi r10,r1,144
		ctx.r10.s64 = ctx.r1.s64 + 144;
		// mr r7,r29
		ctx.r7.u64 = var_r29;
		// li r28,-1
		var_r28 = (uint32_t)(-1);
		// li r30,-2
		var_r30 = (uint32_t)(-2);
	loc_8223BF74:
		// addi r11,r31,1
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 1;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v13,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vpermwi128 v9,v12,99
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x9C));
		// divw r8,r11,r6
		ctx.r8.s32 = ctx.r6.s32 ? ctx.r11.s32 / ctx.r6.s32 : 0;
		// vpermwi128 v8,v12,135
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x78));
		// rotlwi r9,r11,1
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
		// stw r5,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r5.u32);
		// mullw r8,r8,r6
		ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r6.s32);
		// subf r8,r8,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r8.s64;
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// rlwinm r21,r8,4,0,27
		var_r21 = (uint32_t)(__builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0);
		// andc r9,r6,r9
		ctx.r9.u64 = ctx.r6.u64 & ~ctx.r9.u64;
		// twllei r6,0
		if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
		// twlgei r9,-1
		if (ctx.r9.s32 == -1 || ctx.r9.u32 > 4294967295u) __builtin_trap();
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// lvx128 v11,r21,r29
		ea = (var_r21 + var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp128 v127,v11,v0
		simde_mm_store_ps(ctx.v127.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vpermwi128 v0,v127,135
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x78));
		// vpermwi128 v7,v127,99
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x9C));
		// vmulfp128 v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vnmsubfp v0,v7,v8,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v9,v0,v13
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// stvx v9,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f30
		// blt cr6,0x8223bfec
		if (ctx.f0.f64 >= var_f30) {
			// bso cr6,0x8223bfec
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223BFE0, "bso");
			// stw r27,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, var_r27);
			// b 0x8223c050
		} else {
		loc_8223BFEC:
			// vmsum3fp128 v8,v127,v13
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
			// addi r9,r1,96
			ctx.r9.s64 = ctx.r1.s64 + 96;
			// mr r4,r5
			ctx.r4.u64 = ctx.r5.u64;
			// stvx v8,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f12,96(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			ctx.f12.f64 = double(temp.f32);
			// fcmpu cr6,f12,f30
			// bge cr6,0x8223c010
			if (ctx.f12.f64 < var_f30) {
				// stw r28,0(r10)
				PPC_STORE_U32(ctx.r10.u32 + 0, var_r28);
				// b 0x8223c050
			} else {
			loc_8223C010:
				// vsubfp v0,v10,v11
				ctx.fpscr.enableFlushMode();
				simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
				// addi r9,r1,112
				ctx.r9.s64 = ctx.r1.s64 + 112;
				// vmsum3fp128 v7,v127,v0
				simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
				// stvx v7,r0,r9
				ea = (ctx.r9.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lfs f11,112(r1)
				ctx.fpscr.disableFlushModeUnconditional();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
				ctx.f11.f64 = double(temp.f32);
				// fcmpu cr6,f11,f30
				// ble cr6,0x8223c034
				if (ctx.f11.f64 > var_f30) {
					// stw r30,0(r10)
					PPC_STORE_U32(ctx.r10.u32 + 0, var_r30);
					// b 0x8223c050
				} else {
				loc_8223C034:
					// vmsum3fp128 v6,v12,v13
					ctx.fpscr.enableFlushMode();
					simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
					// addi r9,r1,128
					ctx.r9.s64 = ctx.r1.s64 + 128;
					// stvx v6,r0,r9
					ea = (ctx.r9.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// lfs f10,128(r1)
					ctx.fpscr.disableFlushModeUnconditional();
					temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
					ctx.f10.f64 = double(temp.f32);
					// fcmpu cr6,f10,f30
					// bgt cr6,0x8223c0cc
					if (ctx.f10.f64 > var_f30) goto loc_8223C0CC;
					// bso cr6,0x8223c0cc
					// UNIMPLEMENTED: bso
					PPC_UNIMPLEMENTED(0x8223C04C, "bso");
				}
			}
		}
	loc_8223C050:
		// mr r31,r11
		var_r31 = ctx.r11.u32;
		// addi r7,r7,16
		ctx.r7.s64 = ctx.r7.s64 + 16;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpw cr6,r31,r6
		// blt cr6,0x8223bf74
		if ((int32_t)var_r31 < ctx.r6.s32) goto loc_8223BF74;
	}
loc_8223C064:
	// clrlwi r9,r4,24
	ctx.r9.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x8223c258
	if (ctx.r9.u32 == 0) goto loc_8223C258;
	// stfs f13,128(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stw r5,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r5.u32);
	// stfs f27,112(r1)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lwz r8,412(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// lwz r5,404(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// fsubs f13,f27,f13
	ctx.f13.f64 = double(float(var_f27 - ctx.f13.f64));
	// li r3,2
	ctx.r3.s64 = 2;
	// stfs f13,0(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v21,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v21,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v21.u32), 0xFF));
	// lvx128 v20,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v20,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v20.u32), 0xFF));
	// vnmsubfp v10,v12,v0,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v10.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v10,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r24
	ea = (var_r24) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vnmsubfp v0,v12,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r22
	ea = (var_r22) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v19,r0,r24
	ea = (var_r24) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v19,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8223c39c
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// li r0,-160
	ctx.r0.s64 = -160;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r12,r1,-96
	ctx.r12.s64 = ctx.r1.s64 + -96;
	// bl 0x82436660
	__restfpr_27(ctx, base);
	// b 0x8242f8cc
	__restgprlr_21(ctx, base);
	return;
loc_8223C0CC:
	// vmsum3fp128 v5,v13,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vmsum3fp128 v4,v0,v0
	simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// vmsum3fp128 v3,v127,v127
	simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lfs f13,0(r3)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32163
	// stvx v5,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f29,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	var_f29 = double(temp.f32);
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f0,f29,f12
	ctx.f0.f64 = double(float(var_f29 - ctx.f12.f64));
	// lfs f11,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f11.f64 = double(temp.f32);
	// fadds f9,f12,f29
	ctx.f9.f64 = double(float(ctx.f12.f64 + var_f29));
	// lfs f12,-124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + -124);
	ctx.f12.f64 = double(temp.f32);
	// fdivs f28,f12,f11
	var_f28 = double(float(ctx.f12.f64 / ctx.f11.f64));
	// fmuls f7,f0,f0
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// lfs f0,-32640(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32640);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f8,f9,f13,f11
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f11.f64));
	// fnmsubs f6,f7,f28,f8
	ctx.f6.f64 = double(float(-(ctx.f7.f64 * var_f28 - ctx.f8.f64)));
	// fmuls f31,f6,f0
	var_f31 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fcmpu cr6,f31,f30
	// ble cr6,0x8223c14c
	if (var_f31 > var_f30) {
		// fmuls f5,f27,f27
		ctx.f5.f64 = double(float(var_f27 * var_f27));
		// fcmpu cr6,f31,f5
		// bgt cr6,0x8223bf44
		if (var_f31 > ctx.f5.f64) {
			// li r3,3
			ctx.r3.s64 = 3;
			// b 0x8223c39c
			// addi r1,r1,320
			ctx.r1.s64 = ctx.r1.s64 + 320;
			// li r0,-160
			ctx.r0.s64 = -160;
			// lvx128 v127,r1,r0
			ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r12,r1,-96
			ctx.r12.s64 = ctx.r1.s64 + -96;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8cc
			__restgprlr_21(ctx, base);
			return;
		}
		// fmr f1,f31
		ctx.f1.f64 = var_f31;
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f0,f1
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = double(float(ctx.f1.f64));
		// b 0x8223c150
	} else {
	loc_8223C14C:
		// fmr f0,f30
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = var_f30;
	}
loc_8223C150:
	// fsubs f4,f29,f31
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = double(float(var_f29 - var_f31));
	// lwz r30,412(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 412));
	// fsubs f3,f27,f0
	ctx.f3.f64 = double(float(var_f27 - ctx.f0.f64));
	// stw r31,0(r23)
	PPC_STORE_U32(var_r23 + 0, var_r31);
	// stfs f3,0(r30)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// fmuls f1,f4,f28
	ctx.f1.f64 = double(float(ctx.f4.f64 * var_f28));
	// fcmpu cr6,f1,f30
	// ble cr6,0x8223c194
	if (ctx.f1.f64 > var_f30) {
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f2,f1
		ctx.fpscr.disableFlushMode();
		ctx.f2.f64 = double(float(ctx.f1.f64));
		// stfs f2,128(r1)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// addi r5,r1,128
		ctx.r5.s64 = ctx.r1.s64 + 128;
		// lvx128 v2,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v2,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
		// vmulfp128 v1,v127,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v1,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// b 0x8223c1a0
	} else {
	loc_8223C194:
		// lvx128 v0,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v31,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_setzero_si128());
		// stvx v31,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223C1A0:
	// rlwinm r4,r31,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
	// lvx128 v30,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r31,404(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 404));
	// lis r11,-32248
	// lvx128 v0,r4,r29
	ea = (ctx.r4.u32 + var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v30,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lfs f0,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
	ctx.f0.f64 = double(temp.f32);
	// stvx v0,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v29,v0,v0
	simde_mm_store_ps(ctx.v29.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v29,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,128(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// ble cr6,0x8223c224
	if (ctx.f1.f64 > ctx.f0.f64) {
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f1,f1
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = double(float(ctx.f1.f64));
		// stfs f1,128(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// lis r11,-32163
		// lvx128 v28,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,-18496
		ctx.r11.s64 = ctx.r11.s64 + -18496;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,128
		ctx.r11.s64 = ctx.r1.s64 + 128;
		// lvx128 v27,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v27,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v27.u32), 0xFF));
		// vrefp v13,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v0,v13,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v26,v28,v0
		simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v26,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// b 0x8223c22c
	} else {
	loc_8223C224:
		// lvx128 v25,r0,r24
		ea = (var_r24) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v25,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223C22C:
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 0);
	ctx.f0.f64 = double(temp.f32);
	// li r3,1
	ctx.r3.s64 = 1;
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lvx128 v24,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v23,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lvx128 v22,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v22,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v22.u32), 0xFF));
	// vnmsubfp v24,v23,v0,v24
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v24.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v23.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v24.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v24,r0,r22
	ea = (var_r22) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8223c39c
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// li r0,-160
	ctx.r0.s64 = -160;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r12,r1,-96
	ctx.r12.s64 = ctx.r1.s64 + -96;
	// bl 0x82436660
	__restfpr_27(ctx, base);
	// b 0x8242f8cc
	__restgprlr_21(ctx, base);
	return;
loc_8223C258:
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// cmpwi cr6,r6,0
	// ble cr6,0x8223c2d0
	if (ctx.r6.s32 > 0) {
		// addi r7,r1,144
		ctx.r7.s64 = ctx.r1.s64 + 144;
	loc_8223C268:
		// addi r11,r9,1
		ctx.r11.s64 = ctx.r9.s64 + 1;
		// lwz r10,0(r7)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// twllei r6,0
		if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
		// rotlwi r8,r11,1
		ctx.r8.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
		// divw r4,r11,r6
		ctx.r4.s32 = ctx.r6.s32 ? ctx.r11.s32 / ctx.r6.s32 : 0;
		// addi r3,r8,-1
		ctx.r3.s64 = ctx.r8.s64 + -1;
		// mullw r8,r4,r6
		ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
		// andc r5,r6,r3
		ctx.r5.u64 = ctx.r6.u64 & ~ctx.r3.u64;
		// subf r8,r8,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r8.s64;
		// cmpwi cr6,r10,-2
		// twlgei r5,-1
		if (ctx.r5.s32 == -1 || ctx.r5.u32 > 4294967295u) __builtin_trap();
		// beq cr6,0x8223c2ac
		if (ctx.r10.s32 != -2) {
			// rlwinm r4,r8,2,0,29
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r3,r1,144
			ctx.r3.s64 = ctx.r1.s64 + 144;
			// lwzx r5,r4,r3
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
			// cmpwi cr6,r5,-1
			// bne cr6,0x8223c2c0
			if (ctx.r5.s32 != -1) goto loc_8223C2C0;
		}
	loc_8223C2AC:
		// rlwinm r4,r8,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r3,r1,144
		ctx.r3.s64 = ctx.r1.s64 + 144;
		// lwzx r5,r4,r3
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
		// cmpw cr6,r10,r5
		// bne cr6,0x8223c2d0
		if (ctx.r10.s32 != ctx.r5.s32) goto loc_8223C2D0;
	loc_8223C2C0:
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// cmpw cr6,r9,r6
		// blt cr6,0x8223c268
		if (ctx.r9.s32 < ctx.r6.s32) goto loc_8223C268;
	}
loc_8223C2D0:
	// cmpw cr6,r9,r6
	// beq cr6,0x8223bf44
	if (ctx.r9.s32 == ctx.r6.s32) {
		// li r3,3
		ctx.r3.s64 = 3;
		// b 0x8223c39c
		// addi r1,r1,320
		ctx.r1.s64 = ctx.r1.s64 + 320;
		// li r0,-160
		ctx.r0.s64 = -160;
		// lvx128 v127,r1,r0
		ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r12,r1,-96
		ctx.r12.s64 = ctx.r1.s64 + -96;
		// bl 0x82436660
		__restfpr_27(ctx, base);
		// b 0x8242f8cc
		__restgprlr_21(ctx, base);
		return;
	}
	// rlwinm r4,r8,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// fmuls f12,f27,f27
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(var_f27 * var_f27));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r31,404(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 404));
	// lvx128 v0,r4,r29
	ea = (ctx.r4.u32 + var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v18,v0,v0
	simde_mm_store_ps(ctx.v18.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v18,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,144(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f12
	// bgt cr6,0x8223bf44
	if (ctx.f1.f64 > ctx.f12.f64) {
		// li r3,3
		ctx.r3.s64 = 3;
		// b 0x8223c39c
		// addi r1,r1,320
		ctx.r1.s64 = ctx.r1.s64 + 320;
		// li r0,-160
		ctx.r0.s64 = -160;
		// lvx128 v127,r1,r0
		ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r12,r1,-96
		ctx.r12.s64 = ctx.r1.s64 + -96;
		// bl 0x82436660
		__restfpr_27(ctx, base);
		// b 0x8242f8cc
		__restgprlr_21(ctx, base);
		return;
	}
	// lis r11,-32248
	// stw r8,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r8.u32);
	// lfs f0,-25512(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// ble cr6,0x8223c364
	if (ctx.f1.f64 > ctx.f0.f64) {
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// lis r11,-32163
		// frsp f30,f1
		ctx.fpscr.disableFlushMode();
		var_f30 = double(float(ctx.f1.f64));
		// stfs f30,144(r1)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
		// addi r11,r11,-18496
		ctx.r11.s64 = ctx.r11.s64 + -18496;
		// lvx128 v17,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,144
		ctx.r11.s64 = ctx.r1.s64 + 144;
		// lvx128 v16,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v16,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v16.u32), 0xFF));
		// vrefp v13,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v0,v13,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v15,v17,v0
		simde_mm_store_ps(ctx.v15.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v17.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v15,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// b 0x8223c36c
	} else {
	loc_8223C364:
		// lvx128 v14,r0,r24
		ea = (var_r24) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v14,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223C36C:
	// fsubs f0,f27,f30
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(var_f27 - var_f30));
	// stfs f0,144(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lvx128 v13,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,412(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// li r3,0
	ctx.r3.s64 = 0;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// vnmsubfp v13,v12,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v13,r0,r22
	ea = (var_r22) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_8223C39C:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// li r0,-160
	ctx.r0.s64 = -160;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r12,r1,-96
	ctx.r12.s64 = ctx.r1.s64 + -96;
	// bl 0x82436660
	__restfpr_27(ctx, base);
	// b 0x8242f8cc
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_C3B8_p45"))) PPC_WEAK_FUNC(phBoundCapsule_C3B8_p45);
PPC_FUNC_IMPL(__imp__phBoundCapsule_C3B8_p45) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// lfs f12,-25512(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// fcmpu cr6,f0,f12
	// bgt cr6,0x8223c3f8
	if (ctx.f0.f64 <= ctx.f12.f64) {
		// bso cr6,0x8223c3f8
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223C3E8, "bso");
		// li r3,1
		ctx.r3.s64 = 1;
		// stvx v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	}
loc_8223C3F8:
	// vmsum3fp128 v11,v0,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// fcmpu cr6,f13,f12
	// bgt cr6,0x8223c428
	if (ctx.f13.f64 <= ctx.f12.f64) {
		// bso cr6,0x8223c428
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223C414, "bso");
		// vaddfp v10,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// li r3,2
		ctx.r3.s64 = 2;
		// stvx v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	}
loc_8223C428:
	// fadds f12,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// li r3,0
	ctx.r3.s64 = 0;
	// fdivs f11,f0,f12
	ctx.f11.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
	// stfs f11,-16(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lvx128 v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v8,v0,v13
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__util_C460"))) PPC_WEAK_FUNC(util_C460);
PPC_FUNC_IMPL(__imp__util_C460) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	ctx.r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v12,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// vpermwi128 v9,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v8,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v7,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v0,v11,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vpermwi128 v10,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f13.f64 = double(temp.f32);
	// vnmsubfp v0,v9,v10,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vpermwi128 v12,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// vmulfp128 v0,v8,v12
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vnmsubfp v0,v7,v11,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmsum3fp128 v12,v0,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x8223c510
	if (ctx.f0.f64 > ctx.f13.f64) {
		// lis r11,-32253
		// addi r11,r11,-12024
		ctx.r11.s64 = ctx.r11.s64 + -12024;
		// lfs f13,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// bne cr6,0x8223c4ec
		if (ctx.f0.f64 == ctx.f13.f64) {
			// fmr f0,f13
			ctx.f0.f64 = ctx.f13.f64;
			// b 0x8223c4f8
		} else {
		loc_8223C4EC:
			// fsqrts f0,f0
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
			// lfs f13,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f13.f64 = double(temp.f32);
			// fdivs f0,f13,f0
			ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
		}
	loc_8223C4F8:
		// stfs f0,-16(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
		// addi r7,r1,-16
		ctx.r7.s64 = ctx.r1.s64 + -16;
		// lvx128 v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vmulfp128 v0,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// b 0x8223c58c
	} else {
	loc_8223C510:
		// lis r11,-32160
		// addi r4,r1,-16
		ctx.r4.s64 = ctx.r1.s64 + -16;
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// addi r6,r1,-32
		ctx.r6.s64 = ctx.r1.s64 + -32;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v0,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmsum3fp128 v10,v0,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v10,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// ble cr6,0x8223c580
		if (ctx.f0.f64 > ctx.f13.f64) {
			// lis r11,-32253
			// addi r11,r11,-12024
			ctx.r11.s64 = ctx.r11.s64 + -12024;
			// lfs f13,8(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f0,f13
			// bne cr6,0x8223c55c
			if (ctx.f0.f64 == ctx.f13.f64) {
				// fmr f0,f13
				ctx.f0.f64 = ctx.f13.f64;
				// b 0x8223c568
			} else {
			loc_8223C55C:
				// fsqrts f0,f0
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
				// lfs f13,0(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
				ctx.f13.f64 = double(temp.f32);
				// fdivs f0,f13,f0
				ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
			}
		loc_8223C568:
			// stfs f0,-16(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
			// addi r11,r1,-16
			ctx.r11.s64 = ctx.r1.s64 + -16;
			// lvx128 v9,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v13,v9,0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
			// vmulfp128 v0,v0,v13
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
			// b 0x8223c58c
		} else {
		loc_8223C580:
			// lis r11,-32163
			// addi r11,r11,-18544
			ctx.r11.s64 = ctx.r11.s64 + -18544;
			// lvx128 v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	}
loc_8223C58C:
	// cmplwi cr6,r5,0
	// beq cr6,0x8223c598
	if (ctx.r5.u32 != 0) {
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223C598:
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vmsum3fp128 v8,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// blr
	return;
}

__attribute__((alias("__imp__ph_C5B8"))) PPC_WEAK_FUNC(ph_C5B8);
PPC_FUNC_IMPL(__imp__ph_C5B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x8223C5C0;
	__savegprlr_29(ctx, base);
	// stfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f31.u64);
	// li r12,-64
	ctx.r12.s64 = -64;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v11,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmsum3fp128 v8,v0,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vpermwi128 v9,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vmsum3fp128 v13,v13,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vpermwi128 v10,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vmulfp128 v127,v11,v12
	simde_mm_store_ps(ctx.v127.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// stvx v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vnmsubfp128 v127,v9,v10,v127
	simde_mm_store_ps(ctx.v127.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v127.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// lfs f13,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// vmsum3fp128 v7,v127,v127
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
	// fmuls f11,f12,f1
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
	// stvx128 v127,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f11
	// bge cr6,0x8223c710
	if (ctx.f1.f64 < ctx.f11.f64) {
		// lvx128 v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v10,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// lvx128 v12,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v9,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vsubfp v11,v12,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vpermwi128 v5,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vpermwi128 v12,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// lis r11,-32158
		// cmplwi cr6,r31,0
		ctx.cr6.compare<uint32_t>(var_r31, 0, ctx.xer);
		// addi r11,r11,-25616
		ctx.r11.s64 = ctx.r11.s64 + -25616;
		// lvx128 v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32160
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// vpermwi128 v0,v11,99
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0x9C));
		// vpermwi128 v4,v11,135
		simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0x78));
		// lvx128 v7,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32163
		// vmulfp128 v0,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r11,r11,-18480
		ctx.r11.s64 = ctx.r11.s64 + -18480;
		// lvx128 v6,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32163
		// addi r11,r11,-18496
		ctx.r11.s64 = ctx.r11.s64 + -18496;
		// vnmsubfp v0,v10,v4,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v4.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v12,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vpermwi128 v10,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vmulfp128 v0,v9,v12
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v12,v5,v10,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v10,v12,v12
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
		// vrsqrtefp v9,v10
		simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v10.f32))));
		// vcmpeqfp v0,v9,v8
		simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vsel v0,v9,v7,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8))));
		// vmulfp128 v9,v0,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v8,v6,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vnmsubfp v13,v10,v9,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v8,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v0,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// beq cr6,0x8223c6e8
		if (!(ctx.cr6.eq)) {
			// stvx v0,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_8223C6E8:
		// vmsum3fp128 v6,v0,v11
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// addi r6,r1,112
		ctx.r6.s64 = ctx.r1.s64 + 112;
		// stvx v6,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f10,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f10.f64 = double(temp.f32);
		// fneg f1,f10
		ctx.f1.u64 = ctx.f10.u64 ^ 0x8000000000000000;
		// addi r1,r1,192
		ctx.r1.s64 = ctx.r1.s64 + 192;
		// li r0,-64
		ctx.r0.s64 = -64;
		// lvx128 v127,r1,r0
		ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfd f31,-40(r1)
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
	}
loc_8223C710:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f31,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	var_f31 = double(temp.f32);
	// fcmpu cr6,f1,f31
	// ble cr6,0x8223c7b0
	if (ctx.f1.f64 > var_f31) {
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f9,f1
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = double(float(ctx.f1.f64));
		// stfs f9,112(r1)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lvx128 v13,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32163
		// lvx128 v12,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,112
		ctx.r4.s64 = ctx.r1.s64 + 112;
		// vsubfp v11,v12,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// addi r11,r11,-18496
		ctx.r11.s64 = ctx.r11.s64 + -18496;
		// lvx128 v5,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v5,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vrefp v13,v12
		simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v0,v13,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v0,v127,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmsum3fp128 v4,v0,v11
		simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvx v4,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f0.f64 = double(temp.f32);
		// fmr f1,f0
		ctx.f1.f64 = ctx.f0.f64;
		// fcmpu cr6,f0,f31
		// bge cr6,0x8223c790
		if (ctx.f0.f64 < var_f31) {
			// lis r11,-32160
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// addi r11,r11,26448
			ctx.r11.s64 = ctx.r11.s64 + 26448;
			// lvx128 v13,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vsubfp v0,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		}
	loc_8223C790:
		// cmplwi cr6,r31,0
		// beq cr6,0x8223c840
		if (var_r31 == 0) {
			// addi r1,r1,192
			ctx.r1.s64 = ctx.r1.s64 + 192;
			// li r0,-64
			ctx.r0.s64 = -64;
			// lvx128 v127,r1,r0
			ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfd f31,-40(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// stvx v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r1,r1,192
		ctx.r1.s64 = ctx.r1.s64 + 192;
		// li r0,-64
		ctx.r0.s64 = -64;
		// lvx128 v127,r1,r0
		ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfd f31,-40(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
	}
loc_8223C7B0:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// fcmpu cr6,f8,f31
	// bgt cr6,0x8223c800
	if (ctx.f8.f64 <= var_f31) {
		// bso cr6,0x8223c800
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223C7C4, "bso");
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,112
		ctx.r11.s64 = ctx.r1.s64 + 112;
		// lvx128 v3,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// vsubfp v2,v3,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v2.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32)));
		// mr r4,r6
		ctx.r4.u64 = ctx.r6.u64;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// stvx v2,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// addi r1,r1,192
		ctx.r1.s64 = ctx.r1.s64 + 192;
		// li r0,-64
		ctx.r0.s64 = -64;
		// lvx128 v127,r1,r0
		ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfd f31,-40(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
	}
loc_8223C800:
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v1,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r5,r31
	ctx.r5.u64 = var_r31;
	// vsubfp v31,v1,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223c460
	util_C460(ctx, base);
	// cmplwi cr6,r31,0
	// beq cr6,0x8223c840
	if (var_r31 != 0) {
		// lis r11,-32160
		// lvx128 v30,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v29,v0,v30
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v30.f32)));
		// stvx v29,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223C840:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_C858_w"))) PPC_WEAK_FUNC(phBoundCapsule_C858_w);
PPC_FUNC_IMPL(__imp__phBoundCapsule_C858_w) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f1,8(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// fcmpu cr6,f0,f1
	// bgt cr6,0x8223c88c
	if (ctx.f0.f64 <= ctx.f1.f64) {
		// bnslr cr6
		// UNIMPLEMENTED: bnslr
		PPC_UNIMPLEMENTED(0x8223C888, "bnslr");
	}
loc_8223C88C:
	// vmsum3fp128 v12,v0,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	// bgt cr6,0x8223c8b0
	if (ctx.f13.f64 <= ctx.f0.f64) {
		// bso cr6,0x8223c8b0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223C8A4, "bso");
		// lfs f1,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f1.f64 = double(temp.f32);
		// blr
		return;
	}
loc_8223C8B0:
	// fdivs f1,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// blr
	return;
}

__attribute__((alias("__imp__util_C8B8"))) PPC_WEAK_FUNC(util_C8B8);
PPC_FUNC_IMPL(__imp__util_C8B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=384, savegprlr_26
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// mr r27,r7
	var_r27 = ctx.r7.u32;
	// mr r26,r8
	var_r26 = ctx.r8.u32;
	// bl 0x8223d1c0
	phBoundCapsule_D1C0_w(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223c8f8
	if (ctx.r11.u32 != 0) {
		// li r3,1
		ctx.r3.s64 = 1;
		return;
	}
loc_8223C8F8:
	// lvx128 v11,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lvx128 v10,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v5,v0,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v0,v10,v11
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// lvx128 v13,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// vaddfp v6,v13,v10
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223c858
	phBoundCapsule_C858_w(ctx, base);
	// vsubfp v13,v10,v5
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v5.f32)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// fmr f8,f1
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f8.f64 = ctx.f1.f64;
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223c858
	phBoundCapsule_C858_w(ctx, base);
	// lis r11,-32253
	// fmr f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f1.f64;
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f11,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f8,f11
	// bne cr6,0x8223caa8
	if (ctx.f8.f64 == ctx.f11.f64) {
		// fcmpu cr6,f12,f11
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bne cr6,0x8223c98c
		if (ctx.f12.f64 == ctx.f11.f64) {
			// vsubfp v12,v11,v10
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
			// addi r8,r1,80
			ctx.r8.s64 = ctx.r1.s64 + 80;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stvx v12,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f11,0(r26)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
	loc_8223C98C:
		// lfs f9,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// li r5,0
		ctx.r5.s64 = 0;
		// fcmpu cr6,f12,f9
		// bne cr6,0x8223ca34
		if (ctx.f12.f64 == ctx.f9.f64) {
			// vsubfp v11,v11,v10
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
			// addi r7,r1,256
			ctx.r7.s64 = ctx.r1.s64 + 256;
			// addi r3,r1,256
			ctx.r3.s64 = ctx.r1.s64 + 256;
			// stvx v11,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c460
			util_C460(ctx, base);
			// lvx128 v10,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r6,r1,128
			ctx.r6.s64 = ctx.r1.s64 + 128;
			// vsubfp v9,v10,v6
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32)));
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,128
			ctx.r3.s64 = ctx.r1.s64 + 128;
			// fmr f12,f1
			ctx.fpscr.disableFlushModeUnconditional();
			ctx.f12.f64 = ctx.f1.f64;
			// stvx v9,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c460
			util_C460(ctx, base);
			// fcmpu cr6,f12,f1
			ctx.fpscr.disableFlushMode();
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bge cr6,0x8223ca0c
			if (ctx.f12.f64 < ctx.f1.f64) {
				// lvx128 v0,r0,r29
				ea = (var_r29) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// addi r5,r1,80
				ctx.r5.s64 = ctx.r1.s64 + 80;
				// lvx128 v13,r0,r30
				ea = (var_r30) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vsubfp v8,v13,v0
				ctx.fpscr.enableFlushModeUnconditional();
				simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
				// stvx v8,r0,r5
				ea = (ctx.r5.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// bl 0x8223c858
				phBoundCapsule_C858_w(ctx, base);
				// stfs f1,0(r27)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f1.f64);
				PPC_STORE_U32(var_r27 + 0, temp.u32);
				// li r3,0
				ctx.r3.s64 = 0;
				// stfs f11,0(r26)
				temp.f32 = float(ctx.f11.f64);
				PPC_STORE_U32(var_r26 + 0, temp.u32);
				return;
			}
		loc_8223CA0C:
			// lvx128 v0,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r1,80
			ctx.r11.s64 = ctx.r1.s64 + 80;
			// vsubfp v7,v0,v6
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
			// stvx v7,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f9,0(r26)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
	loc_8223CA34:
		// vsubfp v6,v11,v10
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
		// addi r10,r1,224
		ctx.r10.s64 = ctx.r1.s64 + 224;
		// addi r3,r1,224
		ctx.r3.s64 = ctx.r1.s64 + 224;
		// stvx v6,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// lvx128 v4,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,160
		ctx.r9.s64 = ctx.r1.s64 + 160;
		// vsubfp v3,v4,v5
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v5.f32)));
		// li r5,0
		ctx.r5.s64 = 0;
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// fmr f10,f1
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f10.f64 = ctx.f1.f64;
		// addi r3,r1,160
		ctx.r3.s64 = ctx.r1.s64 + 160;
		// stvx v3,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// fcmpu cr6,f10,f1
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223cd58
		if (ctx.f10.f64 >= ctx.f1.f64) {
			// stfs f9,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f12,0(r26)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
		// lvx128 v0,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// vsubfp v2,v13,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v2.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stvx v2,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r27)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f11,0(r26)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r26 + 0, temp.u32);
		return;
	}
loc_8223CAA8:
	// lfs f9,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f8,f9
	// bne cr6,0x8223cbfc
	if (ctx.f8.f64 == ctx.f9.f64) {
		// fcmpu cr6,f12,f11
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bne cr6,0x8223cb5c
		if (ctx.f12.f64 == ctx.f11.f64) {
			// vsubfp v1,v11,v10
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
			// addi r7,r1,288
			ctx.r7.s64 = ctx.r1.s64 + 288;
			// li r5,0
			ctx.r5.s64 = 0;
			// addi r3,r1,288
			ctx.r3.s64 = ctx.r1.s64 + 288;
			// stvx v1,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c460
			util_C460(ctx, base);
			// lvx128 v31,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r6,r1,192
			ctx.r6.s64 = ctx.r1.s64 + 192;
			// vsubfp v30,v31,v6
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v6.f32)));
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,192
			ctx.r3.s64 = ctx.r1.s64 + 192;
			// fmr f12,f1
			ctx.fpscr.disableFlushModeUnconditional();
			ctx.f12.f64 = ctx.f1.f64;
			// stvx v30,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c460
			util_C460(ctx, base);
			// fcmpu cr6,f12,f1
			ctx.fpscr.disableFlushMode();
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bge cr6,0x8223cb34
			if (ctx.f12.f64 < ctx.f1.f64) {
				// lvx128 v0,r0,r29
				ea = (var_r29) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// addi r5,r1,80
				ctx.r5.s64 = ctx.r1.s64 + 80;
				// lvx128 v13,r0,r30
				ea = (var_r30) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vsubfp v29,v13,v0
				ctx.fpscr.enableFlushModeUnconditional();
				simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
				// stvx v29,r0,r5
				ea = (ctx.r5.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// bl 0x8223c858
				phBoundCapsule_C858_w(ctx, base);
				// stfs f1,0(r27)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f1.f64);
				PPC_STORE_U32(var_r27 + 0, temp.u32);
				// li r3,0
				ctx.r3.s64 = 0;
				// stfs f11,0(r26)
				temp.f32 = float(ctx.f11.f64);
				PPC_STORE_U32(var_r26 + 0, temp.u32);
				return;
			}
		loc_8223CB34:
			// lvx128 v0,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r1,80
			ctx.r11.s64 = ctx.r1.s64 + 80;
			// vsubfp v28,v0,v6
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
			// stvx v28,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f9,0(r26)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
	loc_8223CB5C:
		// fcmpu cr6,f12,f9
		ctx.fpscr.disableFlushMode();
		// bne cr6,0x8223cb8c
		if (ctx.f12.f64 == ctx.f9.f64) {
			// vsubfp v27,v11,v6
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v27.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v6.f32)));
			// addi r10,r1,80
			ctx.r10.s64 = ctx.r1.s64 + 80;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stvx v27,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f9,0(r26)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
	loc_8223CB8C:
		// vsubfp v26,v11,v6
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v26.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v6.f32)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// li r5,0
		ctx.r5.s64 = 0;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// stvx v26,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// lvx128 v25,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// vsubfp v24,v25,v5
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v24.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v5.f32)));
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// fmr f11,f1
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f11.f64 = ctx.f1.f64;
		// stvx v24,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// fcmpu cr6,f11,f1
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223cd58
		if (ctx.f11.f64 >= ctx.f1.f64) {
			// stfs f9,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f12,0(r26)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// vsubfp v23,v0,v6
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v23.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stvx v23,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r27)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f9,0(r26)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r26 + 0, temp.u32);
		return;
	}
loc_8223CBFC:
	// fcmpu cr6,f12,f11
	ctx.fpscr.disableFlushMode();
	// li r5,0
	ctx.r5.s64 = 0;
	// bne cr6,0x8223cc80
	if (ctx.f12.f64 == ctx.f11.f64) {
		// vsubfp v22,v11,v10
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v22.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
		// addi r6,r1,144
		ctx.r6.s64 = ctx.r1.s64 + 144;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,144
		ctx.r3.s64 = ctx.r1.s64 + 144;
		// stvx v22,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,176
		ctx.r11.s64 = ctx.r1.s64 + 176;
		// lvx128 v21,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// vsubfp v20,v21,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v20.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v21.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r3,r1,176
		ctx.r3.s64 = ctx.r1.s64 + 176;
		// fmr f12,f1
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f12.f64 = ctx.f1.f64;
		// stvx v20,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// fcmpu cr6,f12,f1
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223cd44
		if (ctx.f12.f64 >= ctx.f1.f64) {
			// stfs f11,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f8,0(r26)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
		// lvx128 v0,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// vsubfp v19,v13,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v19.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stvx v19,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r27)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f11,0(r26)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r26 + 0, temp.u32);
		return;
	}
loc_8223CC80:
	// fcmpu cr6,f12,f9
	ctx.fpscr.disableFlushMode();
	// bne cr6,0x8223cd00
	if (ctx.f12.f64 == ctx.f9.f64) {
		// vsubfp v18,v11,v6
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v18.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v6.f32)));
		// addi r9,r1,208
		ctx.r9.s64 = ctx.r1.s64 + 208;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,208
		ctx.r3.s64 = ctx.r1.s64 + 208;
		// stvx v18,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,240
		ctx.r8.s64 = ctx.r1.s64 + 240;
		// lvx128 v17,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// li r5,0
		ctx.r5.s64 = 0;
		// vsubfp v16,v17,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v16.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v17.f32), simde_mm_load_ps(ctx.v0.f32)));
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// addi r3,r1,240
		ctx.r3.s64 = ctx.r1.s64 + 240;
		// fmr f12,f1
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f12.f64 = ctx.f1.f64;
		// stvx v16,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c460
		util_C460(ctx, base);
		// fcmpu cr6,f12,f1
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223cd44
		if (ctx.f12.f64 >= ctx.f1.f64) {
			// stfs f11,0(r27)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(var_r27 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f8,0(r26)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r26 + 0, temp.u32);
			return;
		}
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// vsubfp v15,v0,v6
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v15.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stvx v15,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r27)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f9,0(r26)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r26 + 0, temp.u32);
		return;
	}
loc_8223CD00:
	// vsubfp v14,v10,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v14.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r6,r1,272
	ctx.r6.s64 = ctx.r1.s64 + 272;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// stvx v14,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223c460
	util_C460(ctx, base);
	// lvx128 v63,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,304
	ctx.r11.s64 = ctx.r1.s64 + 304;
	// vsubfp128 v62,v63,v5
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v62.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v63.f32), simde_mm_load_ps(ctx.v5.f32)));
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// fmr f10,f1
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f10.f64 = ctx.f1.f64;
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// stvx128 v62,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223c460
	util_C460(ctx, base);
	// fcmpu cr6,f10,f1
	ctx.fpscr.disableFlushMode();
	// bge cr6,0x8223cd58
	if (ctx.f10.f64 < ctx.f1.f64) {
	loc_8223CD44:
		// stfs f11,0(r27)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f8,0(r26)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r26 + 0, temp.u32);
		return;
	}
loc_8223CD58:
	// stfs f9,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stfs f12,0(r26)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r26 + 0, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_CD70"))) PPC_WEAK_FUNC(phBoundCapsule_CD70);
PPC_FUNC_IMPL(__imp__phBoundCapsule_CD70) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	double var_f27 = 0.0;
	double var_f26 = 0.0;
	double var_f28 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f898
	ctx.lr = 0x8223CD78;
	__savegprlr_28(ctx, base);
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82436610
	__savefpr_26(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	var_f31 = ctx.f1.f64;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// fmr f29,f2
	var_f29 = ctx.f2.f64;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// mr r28,r8
	var_r28 = ctx.r8.u32;
	// bl 0x8223d350
	phBoundCapsule_D350_2hr(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223cdc0
	if (ctx.r11.u32 != 0) {
		// li r3,1
		ctx.r3.s64 = 1;
		// addi r1,r1,256
		ctx.r1.s64 = ctx.r1.s64 + 256;
		// addi r12,r1,-40
		ctx.r12.s64 = ctx.r1.s64 + -40;
		// bl 0x8243665c
		__restfpr_26(ctx, base);
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	}
loc_8223CDC0:
	// fsubs f0,f29,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(var_f29 - var_f31));
	// lis r11,-32248
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v13,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lfs f12,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32253
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// fabs f13,f0
	ctx.f13.u64 = ctx.f0.u64 & ~0x8000000000000000;
	// lfs f27,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	var_f27 = double(temp.f32);
	// lfs f26,-8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	var_f26 = double(temp.f32);
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fcmpu cr6,f13,f12
	// ble cr6,0x8223ce04
	if (ctx.f13.f64 > ctx.f12.f64) {
		// fdivs f13,f26,f0
		ctx.f13.f64 = double(float(var_f26 / ctx.f0.f64));
		// b 0x8223ce08
	} else {
	loc_8223CE04:
		// fmr f13,f27
		ctx.fpscr.disableFlushMode();
		ctx.f13.f64 = var_f27;
	}
loc_8223CE08:
	// lfs f0,4(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	// ble cr6,0x8223ce30
	if (ctx.f0.f64 > var_f31) {
		// fcmpu cr6,f0,f29
		// bge cr6,0x8223ce28
		if (ctx.f0.f64 < var_f29) {
			// fsubs f0,f0,f31
			ctx.f0.f64 = double(float(ctx.f0.f64 - var_f31));
			// fmuls f28,f0,f13
			var_f28 = double(float(ctx.f0.f64 * ctx.f13.f64));
			// b 0x8223ce34
			goto loc_8223CE34;
		}
	loc_8223CE28:
		// fmr f28,f26
		ctx.fpscr.disableFlushMode();
		var_f28 = var_f26;
		// b 0x8223ce34
	} else {
	loc_8223CE30:
		// fmr f28,f27
		ctx.fpscr.disableFlushMode();
		var_f28 = var_f27;
	}
loc_8223CE34:
	// lfs f0,116(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	// ble cr6,0x8223ce5c
	if (ctx.f0.f64 > var_f31) {
		// fcmpu cr6,f0,f29
		// bge cr6,0x8223ce54
		if (ctx.f0.f64 < var_f29) {
			// fsubs f12,f0,f31
			ctx.f12.f64 = double(float(ctx.f0.f64 - var_f31));
			// fmuls f30,f12,f13
			var_f30 = double(float(ctx.f12.f64 * ctx.f13.f64));
			// b 0x8223ce60
			goto loc_8223CE60;
		}
	loc_8223CE54:
		// fmr f30,f26
		ctx.fpscr.disableFlushMode();
		var_f30 = var_f26;
		// b 0x8223ce60
	} else {
	loc_8223CE5C:
		// fmr f30,f27
		ctx.fpscr.disableFlushMode();
		var_f30 = var_f27;
	}
loc_8223CE60:
	// fcmpu cr6,f28,f27
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8223cf5c
	if (var_f28 <= var_f27) {
		// bso cr6,0x8223cf5c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223CE68, "bso");
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// fcmpu cr6,f30,f27
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f11,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f10,f11,f31
		ctx.f10.f64 = double(float(ctx.f11.f64 - var_f31));
		// stfs f10,84(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// bgt cr6,0x8223ce8c
		if (var_f30 <= var_f27) {
			// bns cr6,0x8223cf34
			// UNIMPLEMENTED: bns
			PPC_UNIMPLEMENTED(0x8223CE88, "bns");
		}
	loc_8223CE8C:
		// fcmpu cr6,f30,f26
		ctx.fpscr.disableFlushMode();
		// blt cr6,0x8223cf00
		if (var_f30 >= var_f26) {
			// bso cr6,0x8223cf00
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223CE94, "bso");
			// li r5,0
			ctx.r5.s64 = 0;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bl 0x8223c460
			util_C460(ctx, base);
			// lfs f9,4(r30)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
			ctx.f9.f64 = double(temp.f32);
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// fsubs f8,f9,f29
			ctx.f8.f64 = double(float(ctx.f9.f64 - var_f29));
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stfs f8,84(r1)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// fmr f12,f1
			ctx.f12.f64 = ctx.f1.f64;
			// bl 0x8223c460
			util_C460(ctx, base);
			// fcmpu cr6,f12,f1
			ctx.fpscr.disableFlushMode();
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bge cr6,0x8223d038
			if (ctx.f12.f64 >= ctx.f1.f64) goto loc_8223D038;
			// lfs f7,4(r30)
			temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
			ctx.f7.f64 = double(temp.f32);
			// fsubs f6,f7,f31
			ctx.f6.f64 = double(float(ctx.f7.f64 - var_f31));
			// stfs f6,84(r1)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r29)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
			// stfs f27,0(r28)
			temp.f32 = float(var_f27);
			PPC_STORE_U32(var_r28 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r1,r1,256
			ctx.r1.s64 = ctx.r1.s64 + 256;
			// addi r12,r1,-40
			ctx.r12.s64 = ctx.r1.s64 + -40;
			// bl 0x8243665c
			__restfpr_26(ctx, base);
			// b 0x8242f8e8
			__restgprlr_28(ctx, base);
			return;
		}
	loc_8223CF00:
		// li r5,0
		ctx.r5.s64 = 0;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x8223c460
		util_C460(ctx, base);
		// lfs f13,112(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f5,f13,f13
		ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
		// lfs f0,120(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
		ctx.f0.f64 = double(temp.f32);
		// fmr f31,f1
		var_f31 = ctx.f1.f64;
		// fmadds f1,f0,f0,f5
		ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f5.f64));
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f4,f1
		ctx.fpscr.disableFlushMode();
		ctx.f4.f64 = double(float(ctx.f1.f64));
		// fcmpu cr6,f31,f4
		// bge cr6,0x8223d1a4
		if (var_f31 >= ctx.f4.f64) goto loc_8223D1A4;
	loc_8223CF34:
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
	loc_8223CF3C:
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r29)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
		// stfs f27,0(r28)
		temp.f32 = float(var_f27);
		PPC_STORE_U32(var_r28 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// addi r1,r1,256
		ctx.r1.s64 = ctx.r1.s64 + 256;
		// addi r12,r1,-40
		ctx.r12.s64 = ctx.r1.s64 + -40;
		// bl 0x8243665c
		__restfpr_26(ctx, base);
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	}
loc_8223CF5C:
	// fcmpu cr6,f28,f26
	ctx.fpscr.disableFlushMode();
	// blt cr6,0x8223d058
	if (var_f28 < var_f26) goto loc_8223D058;
	// bso cr6,0x8223d058
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223CF64, "bso");
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// fcmpu cr6,f30,f27
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f3,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f3,f29
	ctx.f2.f64 = double(float(ctx.f3.f64 - var_f29));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// bgt cr6,0x8223cff0
	if (var_f30 > var_f27) goto loc_8223CFF0;
	// bso cr6,0x8223cff0
	// UNIMPLEMENTED: bso
	PPC_UNIMPLEMENTED(0x8223CF84, "bso");
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8223c460
	util_C460(ctx, base);
	// lfs f0,4(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// fsubs f13,f0,f31
	ctx.f13.f64 = double(float(ctx.f0.f64 - var_f31));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmr f12,f1
	ctx.f12.f64 = ctx.f1.f64;
	// bl 0x8223c460
	util_C460(ctx, base);
	// fcmpu cr6,f1,f12
	ctx.fpscr.disableFlushMode();
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// blt cr6,0x8223cf3c
	if (ctx.f1.f64 < ctx.f12.f64) goto loc_8223CF3C;
	// lfs f12,4(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f12,f29
	ctx.f11.f64 = double(float(ctx.f12.f64 - var_f29));
	// stfs f11,100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// bl 0x8223c858
	phBoundCapsule_C858_w(ctx, base);
	// stfs f1,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// stfs f26,0(r28)
	temp.f32 = float(var_f26);
	PPC_STORE_U32(var_r28 + 0, temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x8243665c
	__restfpr_26(ctx, base);
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
loc_8223CFF0:
	// fcmpu cr6,f30,f26
	ctx.fpscr.disableFlushMode();
	// blt cr6,0x8223cffc
	if (var_f30 >= var_f26) {
		// bns cr6,0x8223d030
		// UNIMPLEMENTED: bns
		PPC_UNIMPLEMENTED(0x8223CFF8, "bns");
	}
loc_8223CFFC:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8223c460
	util_C460(ctx, base);
	// lfs f13,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f13,f13
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// fmadds f1,f0,f0,f10
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f10.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f9,f1
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// fcmpu cr6,f31,f9
	// bge cr6,0x8223d1a4
	if (var_f31 < ctx.f9.f64) {
	loc_8223D030:
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
	loc_8223D038:
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r29)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
		// stfs f26,0(r28)
		temp.f32 = float(var_f26);
		PPC_STORE_U32(var_r28 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// addi r1,r1,256
		ctx.r1.s64 = ctx.r1.s64 + 256;
		// addi r12,r1,-40
		ctx.r12.s64 = ctx.r1.s64 + -40;
		// bl 0x8243665c
		__restfpr_26(ctx, base);
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	loc_8223D058:
		// fcmpu cr6,f30,f27
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x8223d0d4
		if (var_f30 <= var_f27) {
			// bso cr6,0x8223d0d4
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223D060, "bso");
			// addi r7,r1,128
			ctx.r7.s64 = ctx.r1.s64 + 128;
			// li r5,0
			ctx.r5.s64 = 0;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,128
			ctx.r3.s64 = ctx.r1.s64 + 128;
			// stvx v0,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f8,132(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
			ctx.f8.f64 = double(temp.f32);
			// fsubs f7,f8,f31
			ctx.f7.f64 = double(float(ctx.f8.f64 - var_f31));
			// stfs f7,132(r1)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
			// bl 0x8223c460
			util_C460(ctx, base);
			// lfs f13,8(r30)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r30 + 8);
			ctx.f13.f64 = double(temp.f32);
			// fmuls f6,f13,f13
			ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
			// lfs f0,0(r30)
			temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
			ctx.f0.f64 = double(temp.f32);
			// fmr f31,f1
			var_f31 = ctx.f1.f64;
			// fmadds f1,f0,f0,f6
			ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f6.f64));
			// bl 0x824301d0
			phBoundCapsule_01D0_g(ctx, base);
			// frsp f5,f1
			ctx.fpscr.disableFlushMode();
			ctx.f5.f64 = double(float(ctx.f1.f64));
			// fcmpu cr6,f31,f5
			// bge cr6,0x8223d188
			if (var_f31 >= ctx.f5.f64) goto loc_8223D188;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,128
			ctx.r3.s64 = ctx.r1.s64 + 128;
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r29)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
			// stfs f27,0(r28)
			temp.f32 = float(var_f27);
			PPC_STORE_U32(var_r28 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r1,r1,256
			ctx.r1.s64 = ctx.r1.s64 + 256;
			// addi r12,r1,-40
			ctx.r12.s64 = ctx.r1.s64 + -40;
			// bl 0x8243665c
			__restfpr_26(ctx, base);
			// b 0x8242f8e8
			__restgprlr_28(ctx, base);
			return;
		}
	loc_8223D0D4:
		// fcmpu cr6,f30,f26
		ctx.fpscr.disableFlushMode();
		// bne cr6,0x8223d14c
		if (var_f30 == var_f26) {
			// addi r6,r1,144
			ctx.r6.s64 = ctx.r1.s64 + 144;
			// li r5,0
			ctx.r5.s64 = 0;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,144
			ctx.r3.s64 = ctx.r1.s64 + 144;
			// stvx v0,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f4,148(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
			ctx.f4.f64 = double(temp.f32);
			// fsubs f3,f4,f29
			ctx.f3.f64 = double(float(ctx.f4.f64 - var_f29));
			// stfs f3,148(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
			// bl 0x8223c460
			util_C460(ctx, base);
			// lfs f13,0(r30)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
			ctx.f13.f64 = double(temp.f32);
			// fmuls f2,f13,f13
			ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
			// lfs f0,8(r30)
			temp.u32 = PPC_LOAD_U32(var_r30 + 8);
			ctx.f0.f64 = double(temp.f32);
			// fmr f31,f1
			var_f31 = ctx.f1.f64;
			// fmadds f1,f0,f0,f2
			ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f2.f64));
			// bl 0x824301d0
			phBoundCapsule_01D0_g(ctx, base);
			// frsp f1,f1
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = double(float(ctx.f1.f64));
			// fcmpu cr6,f31,f1
			// bge cr6,0x8223d188
			if (var_f31 >= ctx.f1.f64) goto loc_8223D188;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// addi r3,r1,144
			ctx.r3.s64 = ctx.r1.s64 + 144;
			// bl 0x8223c858
			phBoundCapsule_C858_w(ctx, base);
			// stfs f1,0(r29)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
			// stfs f26,0(r28)
			temp.f32 = float(var_f26);
			PPC_STORE_U32(var_r28 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r1,r1,256
			ctx.r1.s64 = ctx.r1.s64 + 256;
			// addi r12,r1,-40
			ctx.r12.s64 = ctx.r1.s64 + -40;
			// bl 0x8243665c
			__restfpr_26(ctx, base);
			// b 0x8242f8e8
			__restgprlr_28(ctx, base);
			return;
		}
	loc_8223D14C:
		// lfs f13,0(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f13.f64 = double(temp.f32);
		// fmuls f13,f13,f13
		ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
		// lfs f0,8(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 8);
		ctx.f0.f64 = double(temp.f32);
		// fmadds f1,f0,f0,f13
		ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f13.f64));
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// lfs f13,112(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f13.f64 = double(temp.f32);
		// fmr f31,f1
		var_f31 = ctx.f1.f64;
		// fmuls f12,f13,f13
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
		// lfs f0,120(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
		ctx.f0.f64 = double(temp.f32);
		// fmadds f1,f0,f0,f12
		ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f12.f64));
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f11,f31
		ctx.fpscr.disableFlushMode();
		ctx.f11.f64 = double(float(var_f31));
		// frsp f10,f1
		ctx.f10.f64 = double(float(ctx.f1.f64));
		// fcmpu cr6,f11,f10
		// bge cr6,0x8223d1a4
		if (ctx.f11.f64 >= ctx.f10.f64) goto loc_8223D1A4;
	loc_8223D188:
		// stfs f27,0(r29)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f27);
		PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f28,0(r28)
		temp.f32 = float(var_f28);
		PPC_STORE_U32(var_r28 + 0, temp.u32);
		// addi r1,r1,256
		ctx.r1.s64 = ctx.r1.s64 + 256;
		// addi r12,r1,-40
		ctx.r12.s64 = ctx.r1.s64 + -40;
		// bl 0x8243665c
		__restfpr_26(ctx, base);
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	}
loc_8223D1A4:
	// stfs f26,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f26);
	PPC_STORE_U32(var_r29 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stfs f30,0(r28)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(var_r28 + 0, temp.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x8243665c
	__restfpr_26(ctx, base);
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_D1C0_w"))) PPC_WEAK_FUNC(phBoundCapsule_D1C0_w);
PPC_FUNC_IMPL(__imp__phBoundCapsule_D1C0_w) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v0,v13,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r9,r11,-12016
	ctx.r9.s64 = ctx.r11.s64 + -12016;
	// lfs f12,-8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x8223d2dc
	if (ctx.f0.f64 > ctx.f13.f64) {
		// lvx128 v12,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// vmsum3fp128 v8,v12,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// fdivs f0,f12,f0
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f0.f64 = double(float(ctx.f12.f64 / ctx.f0.f64));
		// lvx128 v10,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vor v0,v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
		// lvx128 v9,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// vsubfp v11,v10,v9
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)));
		// stvx v8,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v7,v11,v13
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// stvx v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f9,96(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f8,f9,f0
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// stfs f8,96(r1)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// addi r11,r1,96
		ctx.r11.s64 = ctx.r1.s64 + 96;
		// lvx128 v5,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f11,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f10,f11,f0
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// stfs f10,80(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lvx128 v6,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v8,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// vnmsubfp v0,v13,v8,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vspltw v8,v5,0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
		// vnmsubfp v11,v13,v8,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v11.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v4,v0,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v4,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,96(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// ble cr6,0x8223d29c
		if (ctx.f0.f64 > ctx.f13.f64) {
			// vmsum3fp128 v3,v11,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// stvx v3,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f7,96(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			ctx.f7.f64 = double(temp.f32);
			// fdivs f6,f7,f0
			ctx.f6.f64 = double(float(ctx.f7.f64 / ctx.f0.f64));
			// fneg f5,f6
			ctx.f5.u64 = ctx.f6.u64 ^ 0x8000000000000000;
			// stfs f5,0(r7)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// b 0x8223d2a4
		} else {
		loc_8223D29C:
			// lfs f0,-4(r9)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
			ctx.f0.f64 = double(temp.f32);
			// stfs f0,0(r7)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		}
	loc_8223D2A4:
		// lfs f4,0(r7)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		ctx.f4.f64 = double(temp.f32);
		// mr r4,r6
		ctx.r4.u64 = ctx.r6.u64;
		// stfs f4,96(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// lvx128 v2,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v2,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
		// vmaddfp v0,v12,v0,v10
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v10.f32)));
		// vsubfp v1,v9,v0
		simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v1,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r8)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// b 0x8223d304
	} else {
	loc_8223D2DC:
		// lvx128 v13,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,96
		ctx.r11.s64 = ctx.r1.s64 + 96;
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// vsubfp v31,v13,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lfs f0,-4(r9)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r8)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// stvx v31,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8223c858
		phBoundCapsule_C858_w(ctx, base);
		// stfs f1,0(r7)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	}
loc_8223D304:
	// lfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x8223d334
	if (ctx.f0.f64 > ctx.f13.f64) {
		// fcmpu cr6,f0,f12
		// bge cr6,0x8223d334
		if (ctx.f0.f64 >= ctx.f12.f64) goto loc_8223D334;
		// lfs f0,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// ble cr6,0x8223d334
		if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8223D334;
		// fcmpu cr6,f0,f12
		// li r11,1
		ctx.r11.s64 = 1;
		// blt cr6,0x8223d338
		if (ctx.f0.f64 < ctx.f12.f64) {
			// clrlwi r3,r11,24
			ctx.r3.u64 = ctx.r11.u32 & 0xFF;
			// blr
			return;
		}
	}
loc_8223D334:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8223D338:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_D350_2hr"))) PPC_WEAK_FUNC(phBoundCapsule_D350_2hr);
PPC_FUNC_IMPL(__imp__phBoundCapsule_D350_2hr) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32163
	// lfs f11,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// lfs f13,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-32648(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32648);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f12,f11,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmadds f10,f13,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f0.f64));
	// fabs f12,f12
	ctx.f12.u64 = ctx.f12.u64 & ~0x8000000000000000;
	// fcmpu cr6,f10,f12
	// bge cr6,0x8223d4c0
	if (ctx.f10.f64 < ctx.f12.f64) {
		// lis r11,-32253
		// addi r11,r11,-12024
		ctx.r11.s64 = ctx.r11.s64 + -12024;
		// lfs f0,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f11,f0
		// ble cr6,0x8223d39c
		if (ctx.f11.f64 > ctx.f0.f64) {
			// lfs f9,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
			ctx.f9.f64 = double(temp.f32);
			// fadds f12,f9,f11
			ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
			// b 0x8223d3a4
		} else {
		loc_8223D39C:
			// lfs f12,4(r3)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
			ctx.f12.f64 = double(temp.f32);
			// fadds f9,f12,f11
			ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
		}
	loc_8223D3A4:
		// fcmpu cr6,f2,f1
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223d3b8
		if (ctx.f2.f64 > ctx.f1.f64) {
			// fmr f10,f2
			ctx.f10.f64 = ctx.f2.f64;
			// fmr f0,f1
			ctx.f0.f64 = ctx.f1.f64;
			// b 0x8223d3c0
		} else {
		loc_8223D3B8:
			// fmr f10,f1
			ctx.fpscr.disableFlushMode();
			ctx.f10.f64 = ctx.f1.f64;
			// fmr f0,f2
			ctx.f0.f64 = ctx.f2.f64;
		}
	loc_8223D3C0:
		// fcmpu cr6,f12,f0
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223d42c
		if (ctx.f12.f64 > ctx.f0.f64) {
			// fcmpu cr6,f9,f10
			// bge cr6,0x8223d42c
			if (ctx.f9.f64 >= ctx.f10.f64) goto loc_8223D42C;
			// fcmpu cr6,f9,f0
			// bge cr6,0x8223d3dc
			if (ctx.f9.f64 < ctx.f0.f64) {
				// fmr f9,f0
				ctx.f9.f64 = ctx.f0.f64;
			}
		loc_8223D3DC:
			// fcmpu cr6,f12,f10
			ctx.fpscr.disableFlushMode();
			// ble cr6,0x8223d3e8
			if (ctx.f12.f64 > ctx.f10.f64) {
				// fmr f12,f10
				ctx.f12.f64 = ctx.f10.f64;
			}
		loc_8223D3E8:
			// fadds f12,f9,f12
			ctx.fpscr.disableFlushMode();
			ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
			// lfs f0,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// fsubs f13,f2,f1
			ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
			// lfs f8,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
			ctx.f8.f64 = double(temp.f32);
			// lis r11,-32248
			// fmsubs f7,f12,f0,f8
			ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f8.f64));
			// fabs f10,f13
			ctx.f10.u64 = ctx.f13.u64 & ~0x8000000000000000;
			// fdivs f6,f7,f11
			ctx.f6.f64 = double(float(ctx.f7.f64 / ctx.f11.f64));
			// lfs f11,-25512(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
			ctx.f11.f64 = double(temp.f32);
			// stfs f6,0(r7)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// fcmpu cr6,f10,f11
			// ble cr6,0x8223d420
			if (ctx.f10.f64 > ctx.f11.f64) {
				// fmsubs f5,f12,f0,f1
				ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f1.f64));
				// fdivs f0,f5,f13
				ctx.f0.f64 = double(float(ctx.f5.f64 / ctx.f13.f64));
			}
		loc_8223D420:
			// li r3,1
			ctx.r3.s64 = 1;
			// stfs f0,0(r8)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// blr
			return;
		}
	loc_8223D42C:
		// fcmpu cr6,f12,f0
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x8223d47c
		if (ctx.f12.f64 <= ctx.f0.f64) {
			// bso cr6,0x8223d47c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223D434, "bso");
			// fadds f12,f0,f12
			ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
			// lfs f0,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// fsubs f13,f2,f1
			ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
			// lfs f4,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
			ctx.f4.f64 = double(temp.f32);
			// lis r11,-32248
			// fmsubs f3,f12,f0,f4
			ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f4.f64));
			// fabs f10,f13
			ctx.f10.u64 = ctx.f13.u64 & ~0x8000000000000000;
			// fdivs f2,f3,f11
			ctx.f2.f64 = double(float(ctx.f3.f64 / ctx.f11.f64));
			// lfs f11,-25512(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
			ctx.f11.f64 = double(temp.f32);
			// stfs f2,0(r7)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// fcmpu cr6,f10,f11
			// ble cr6,0x8223d470
			if (ctx.f10.f64 > ctx.f11.f64) {
				// fmsubs f1,f12,f0,f1
				ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f1.f64));
				// fdivs f0,f1,f13
				ctx.f0.f64 = double(float(ctx.f1.f64 / ctx.f13.f64));
			}
		loc_8223D470:
			// stfs f0,0(r8)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
	loc_8223D47C:
		// fadds f12,f10,f9
		ctx.fpscr.disableFlushMode();
		ctx.f12.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// lfs f0,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f0.f64 = double(temp.f32);
		// fsubs f13,f2,f1
		ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
		// lfs f8,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f8.f64 = double(temp.f32);
		// lis r11,-32248
		// fmsubs f7,f12,f0,f8
		ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f8.f64));
		// fabs f10,f13
		ctx.f10.u64 = ctx.f13.u64 & ~0x8000000000000000;
		// fdivs f6,f7,f11
		ctx.f6.f64 = double(float(ctx.f7.f64 / ctx.f11.f64));
		// lfs f11,-25512(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
		ctx.f11.f64 = double(temp.f32);
		// stfs f6,0(r7)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// fcmpu cr6,f10,f11
		// ble cr6,0x8223d4b4
		if (ctx.f10.f64 > ctx.f11.f64) {
			// fmsubs f5,f12,f0,f1
			ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f1.f64));
			// fdivs f0,f5,f13
			ctx.f0.f64 = double(float(ctx.f5.f64 / ctx.f13.f64));
		}
	loc_8223D4B4:
		// stfs f0,0(r8)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8223D4C0:
	// lis r11,-32248
	// lfs f13,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f13,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f13,-25512(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// fcmpu cr6,f0,f13
	// lfs f9,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// ble cr6,0x8223d514
	if (ctx.f0.f64 > ctx.f13.f64) {
		// addi r10,r1,-16
		ctx.r10.s64 = ctx.r1.s64 + -16;
		// lvx128 v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f4,0(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundCapsule::vtable@+0x0 */;
		ctx.f4.f64 = double(temp.f32);
		// lfs f3,8(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f3.f64 = double(temp.f32);
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f12,-16(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f10,f4,f12
		ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
		// lfs f13,-8(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f8,f3,f13,f10
		ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f13.f64 + ctx.f10.f64));
		// fdivs f7,f8,f0
		ctx.f7.f64 = double(float(ctx.f8.f64 / ctx.f0.f64));
		// fneg f0,f7
		ctx.f0.u64 = ctx.f7.u64 ^ 0x8000000000000000;
		// b 0x8223d518
	} else {
	loc_8223D514:
		// fmr f0,f9
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = ctx.f9.f64;
	}
loc_8223D518:
	// fsubs f13,f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
	// lis r10,-32248
	// stfs f0,0(r7)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// lfs f10,-25840(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25840);
	ctx.f10.f64 = double(temp.f32);
	// fabs f12,f13
	ctx.f12.u64 = ctx.f13.u64 & ~0x8000000000000000;
	// fcmpu cr6,f12,f10
	// ble cr6,0x8223d548
	if (ctx.f12.f64 > ctx.f10.f64) {
		// lfs f6,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f6.f64 = double(temp.f32);
		// fmadds f5,f11,f0,f6
		ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f6.f64));
		// fsubs f4,f5,f1
		ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f1.f64));
		// fdivs f13,f4,f13
		ctx.f13.f64 = double(float(ctx.f4.f64 / ctx.f13.f64));
		// b 0x8223d54c
	} else {
	loc_8223D548:
		// fmr f13,f9
		ctx.fpscr.disableFlushMode();
		ctx.f13.f64 = ctx.f9.f64;
	}
loc_8223D54C:
	// lfs f12,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,0(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fcmpu cr6,f0,f12
	// ble cr6,0x8223d57c
	if (ctx.f0.f64 > ctx.f12.f64) {
		// lfs f11,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f0,f11
		// bge cr6,0x8223d57c
		if (ctx.f0.f64 >= ctx.f11.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fcmpu cr6,f13,f12
		// ble cr6,0x8223d57c
		if (ctx.f13.f64 <= ctx.f12.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fcmpu cr6,f13,f11
		// li r3,1
		ctx.r3.s64 = 1;
		// bltlr cr6
		if (ctx.f13.f64 < ctx.f11.f64) return;
	}
loc_8223D57C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__ph_D588"))) PPC_WEAK_FUNC(ph_D588);
PPC_FUNC_IMPL(__imp__ph_D588) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	double var_f29 = 0.0;
	double var_f28 = 0.0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f888
	ctx.lr = 0x8223D590;
	__savegprlr_24(ctx, base);
	// addi r12,r1,-72
	ctx.r12.s64 = ctx.r1.s64 + -72;
	// bl 0x82436618
	__savefpr_28(ctx, base);
	// li r12,-128
	ctx.r12.s64 = -128;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r30,r9
	var_r30 = ctx.r9.u32;
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r25,r4
	var_r25 = ctx.r4.u32;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// lvx128 v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v12,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// mr r27,r8
	var_r27 = ctx.r8.u32;
	// vpermwi128 v0,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// mr r24,r10
	var_r24 = ctx.r10.u32;
	// vpermwi128 v13,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vmulfp128 v127,v12,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v127.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vnmsubfp128 v127,v13,v0,v127
	simde_mm_store_ps(ctx.v127.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v127.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmsum3fp128 v0,v127,v127
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// ble cr6,0x8223d888
	if (ctx.f1.f64 > ctx.f0.f64) {
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,112
		ctx.r9.s64 = ctx.r1.s64 + 112;
		// lvx128 v12,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v13,v13,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v0,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmsum3fp128 v13,v13,v127
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
		// vmsum3fp128 v12,v0,v127
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f29,96(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		var_f29 = double(temp.f32);
		// lfs f28,112(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		var_f28 = double(temp.f32);
		// stfs f29,80(r1)
		temp.f32 = float(var_f29);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// stfs f28,84(r1)
		temp.f32 = float(var_f28);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// lwz r8,80(r1)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lwz r6,84(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// rlwinm r7,r8,1,31,31
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
		// rlwinm r5,r6,1,31,31
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
		// eqv r4,r7,r5
		ctx.r4.u64 = ~(ctx.r7.u64 ^ ctx.r5.u64);
		// clrlwi r3,r4,31
		ctx.r3.u64 = ctx.r4.u32 & 0x1;
		// cmplwi cr6,r3,0
		// bne cr6,0x8223d888
		if (ctx.r3.u32 != 0) goto loc_8223D888;
		// lis r11,-32253
		ctx.r11.s64 = -2113732608;
		// addi r26,r11,-12024
		var_r26 = (uint32_t)(ctx.r11.s64 + -12024);  // lbl_8202D108 @ 0x8202d108
		// lfs f30,8(r26)
		temp.u32 = PPC_LOAD_U32(var_r26 + 8);
		var_f30 = double(temp.f32);
		// fcmpu cr6,f28,f30
		// bne cr6,0x8223d678
		if (var_f28 == var_f30) {
			// fcmpu cr6,f29,f30
			// beq cr6,0x8223d888
			if (var_f29 == var_f30) goto loc_8223D888;
		}
	loc_8223D678:
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f0,f1
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = double(float(ctx.f1.f64));
		// lfs f31,0(r26)
		temp.u32 = PPC_LOAD_U32(var_r26 + 0);
		var_f31 = double(temp.f32);
		// fdivs f0,f31,f0
		ctx.f0.f64 = double(float(var_f31 / ctx.f0.f64));
		// fmuls f13,f28,f0
		ctx.f13.f64 = double(float(var_f28 * ctx.f0.f64));
		// fmuls f12,f29,f0
		ctx.f12.f64 = double(float(var_f29 * ctx.f0.f64));
		// fsubs f12,f13,f12
		ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
		// fdivs f13,f13,f12
		ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f12.f64));
		// fcmpu cr6,f13,f30
		// bge cr6,0x8223d6a8
		if (ctx.f13.f64 < var_f30) {
			// fcmpu cr6,f13,f31
			// bgt cr6,0x8223d888
			if (ctx.f13.f64 > var_f31) goto loc_8223D888;
		}
	loc_8223D6A8:
		// stfs f0,112(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// stfs f13,96(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// lvx128 v13,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v8,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// lvx128 v6,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v5,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v7,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// lvx128 v12,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,112
		ctx.r11.s64 = ctx.r1.s64 + 112;
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// addi r9,r1,112
		ctx.r9.s64 = ctx.r1.s64 + 112;
		// lvx128 v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// lvx128 v10,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v11,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vmulfp128 v0,v127,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmaddfp v11,v5,v11,v6
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v6.f32)));
		// vpermwi128 v10,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vpermwi128 v9,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vsubfp v11,v11,v12
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vmulfp128 v0,v10,v8
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vnmsubfp v0,v9,v7,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v8,v11,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v8,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f11,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f11,f30
		// bgt cr6,0x8223d888
		if (ctx.f11.f64 > var_f30) goto loc_8223D888;
		// lis r11,-32160
		// lvx128 v12,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v4,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v8,v4,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v0,v11,v12
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vsubfp v0,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmsum3fp128 v7,v0,v8
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// stvx v7,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f10,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f10.f64 = double(temp.f32);
		// fcmpu cr6,f10,f30
		// bgt cr6,0x8223d888
		if (ctx.f10.f64 > var_f30) goto loc_8223D888;
		// vpermwi128 v8,v12,99
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x9C));
		// vaddfp v11,v0,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vpermwi128 v7,v12,135
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x78));
		// addi r7,r1,112
		ctx.r7.s64 = ctx.r1.s64 + 112;
		// vmulfp128 v0,v10,v8
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vnmsubfp v0,v9,v7,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v3,v11,v0
		simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v3,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f9,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f9.f64 = double(temp.f32);
		// fcmpu cr6,f9,f30
		// bgt cr6,0x8223d888
		if (ctx.f9.f64 > var_f30) goto loc_8223D888;
		// vsubfp v12,v11,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// addi r6,r1,112
		ctx.r6.s64 = ctx.r1.s64 + 112;
		// vsubfp v0,v4,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vaddfp v13,v12,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmsum3fp128 v2,v13,v0
		simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v2,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f8,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f8.f64 = double(temp.f32);
		// fcmpu cr6,f8,f30
		// bgt cr6,0x8223d888
		if (ctx.f8.f64 > var_f30) goto loc_8223D888;
		// stfs f13,112(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// lwz r4,356(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
		// addi r11,r1,128
		ctx.r11.s64 = ctx.r1.s64 + 128;
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// lvx128 v1,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v1,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.u32), 0xFF));
		// vmaddfp v31,v5,v0,v6
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v6.f32)));
		// stvx v31,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v9,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vpermwi128 v8,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// lvx128 v12,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v7,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// lvx128 v11,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v13,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// lvx128 v10,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v11,v11,v12
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vmulfp128 v0,v9,v8
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vsubfp v12,v10,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v0,v7,v13,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v30,v0,v0
		simde_mm_store_ps(ctx.v30.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v29,v11,v0
		simde_mm_store_ps(ctx.v29.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v28,v12,v0
		simde_mm_store_ps(ctx.v28.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v30,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v29,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v28,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f1,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f1.f64 = double(temp.f32);
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f7,f1
		ctx.fpscr.disableFlushMode();
		ctx.f7.f64 = double(float(ctx.f1.f64));
		// lfs f6,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f6.f64 = double(temp.f32);
		// lfs f5,128(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f5.f64 = double(temp.f32);
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfs f11,-25840(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25840);  /* glob:lbl_82079B10 @ 0x82079b10 */
		ctx.f11.f64 = double(temp.f32);
		// fdivs f0,f31,f7
		ctx.f0.f64 = double(float(var_f31 / ctx.f7.f64));
		// fmuls f13,f6,f0
		ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
		// fmuls f0,f5,f0
		ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
		// fsubs f0,f13,f0
		ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// fabs f12,f0
		ctx.f12.u64 = ctx.f0.u64 & ~0x8000000000000000;
		// fcmpu cr6,f12,f11
		// ble cr6,0x8223d888
		if (ctx.f12.f64 <= ctx.f11.f64) goto loc_8223D888;
		// fdivs f0,f13,f0
		ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
		// fcmpu cr6,f0,f30
		// blt cr6,0x8223d888
		if (ctx.f0.f64 < var_f30) goto loc_8223D888;
		// bso cr6,0x8223d888
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223D850, "bso");
		// fcmpu cr6,f0,f31
		// bgt cr6,0x8223d888
		if (ctx.f0.f64 > var_f31) goto loc_8223D888;
		// bso cr6,0x8223d888
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223D85C, "bso");
		// stfs f0,128(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// li r3,1
		ctx.r3.s64 = 1;
		// lvx128 v0,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,128
		ctx.r9.s64 = ctx.r1.s64 + 128;
		// lvx128 v27,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v27,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v27.u32), 0xFF));
		// vmaddfp v26,v0,v12,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v26,r0,r24
		ea = (var_r24) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// b 0x8223d88c
	} else {
	loc_8223D888:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8223D88C:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// li r0,-128
	ctx.r0.s64 = -128;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r12,r1,-72
	ctx.r12.s64 = ctx.r1.s64 + -72;
	// bl 0x82436664
	__restfpr_28(ctx, base);
	// b 0x8242f8d8
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundBox_D8A8_2h"))) PPC_WEAK_FUNC(phBoundBox_D8A8_2h);
PPC_FUNC_IMPL(__imp__phBoundBox_D8A8_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x8223D8B0;
	__savegprlr_29(ctx, base);
	// lis r11,-32253
	// lfs f0,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// li r30,0
	var_r30 = 0;
	// addi r29,r11,-12020
	var_r29 = (uint32_t)(ctx.r11.s64 + -12020);  // lbl_8202D10C @ 0x8202d10c
	// mr r31,r30
	var_r31 = (uint32_t)(var_r30);
	// lfs f4,4(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 4)/* phBoundBox::flags@+0x4 */;
	ctx.f4.f64 = double(temp.f32);
	// fcmpu cr6,f0,f4
	// ble cr6,0x8223d8e0
	if (ctx.f0.f64 > ctx.f4.f64) {
		// lfs f12,8(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f12.f64 = double(temp.f32);
		// lfs f0,8(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
		ctx.f0.f64 = double(temp.f32);
		// lfs f10,8(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
		ctx.f10.f64 = double(temp.f32);
		// b 0x8223d98c
	} else {
	loc_8223D8E0:
		// fcmpu cr6,f0,f4
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223d904
		if (ctx.f0.f64 < ctx.f4.f64) {
			// lfs f13,8(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// lfs f12,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
			ctx.f12.f64 = double(temp.f32);
			// lfs f9,8(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
			ctx.f9.f64 = double(temp.f32);
			// lfs f0,4(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// lfs f11,8(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
			ctx.f11.f64 = double(temp.f32);
			// lfs f10,4(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
			ctx.f10.f64 = double(temp.f32);
			// b 0x8223d998
			goto loc_8223D998;
		}
	loc_8223D904:
		// lfs f0,4(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f4
		// ble cr6,0x8223d92c
		if (ctx.f0.f64 > ctx.f4.f64) {
			// lfs f13,8(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// lfs f12,0(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
			ctx.f12.f64 = double(temp.f32);
			// lfs f9,8(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
			ctx.f9.f64 = double(temp.f32);
			// lfs f0,0(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f11,8(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
			ctx.f11.f64 = double(temp.f32);
			// lfs f10,0(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
			ctx.f10.f64 = double(temp.f32);
			// b 0x8223d998
			goto loc_8223D998;
		}
	loc_8223D92C:
		// fcmpu cr6,f0,f4
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223d950
		if (ctx.f0.f64 < ctx.f4.f64) {
			// lfs f13,0(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
			ctx.f13.f64 = double(temp.f32);
			// lfs f12,8(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			ctx.f12.f64 = double(temp.f32);
			// lfs f9,0(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
			ctx.f9.f64 = double(temp.f32);
			// lfs f0,8(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
			ctx.f0.f64 = double(temp.f32);
			// lfs f11,0(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
			ctx.f11.f64 = double(temp.f32);
			// lfs f10,8(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
			ctx.f10.f64 = double(temp.f32);
			// b 0x8223d998
			goto loc_8223D998;
		}
	loc_8223D950:
		// lfs f0,8(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f4
		// ble cr6,0x8223d978
		if (ctx.f0.f64 > ctx.f4.f64) {
			// lfs f13,0(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
			ctx.f13.f64 = double(temp.f32);
			// lfs f12,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
			ctx.f12.f64 = double(temp.f32);
			// lfs f9,0(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
			ctx.f9.f64 = double(temp.f32);
			// lfs f0,4(r4)
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// lfs f11,0(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
			ctx.f11.f64 = double(temp.f32);
			// lfs f10,4(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
			ctx.f10.f64 = double(temp.f32);
			// b 0x8223d998
			goto loc_8223D998;
		}
	loc_8223D978:
		// fcmpu cr6,f0,f4
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223db68
		if (ctx.f0.f64 >= ctx.f4.f64) {
			// lfs f0,0(r29)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r29 + 0)/* phBoundBox::vtable@+0x0 */;
			ctx.f0.f64 = double(temp.f32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stfs f0,0(r7)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// stfs f0,0(r8)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// lfs f12,0(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
		ctx.f12.f64 = double(temp.f32);
		// lfs f0,0(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// lfs f10,0(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		ctx.f10.f64 = double(temp.f32);
	}
loc_8223D98C:
	// lfs f11,4(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundBox::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
loc_8223D998:
	// lfs f5,-4(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + -4);
	ctx.f5.f64 = double(temp.f32);
	// li r6,2
	ctx.r6.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// fcmpu cr6,f9,f4
	// beq cr6,0x8223da54
	if (ctx.f9.f64 != ctx.f4.f64) {
		// fdivs f8,f5,f9
		ctx.f8.f64 = double(float(ctx.f5.f64 / ctx.f9.f64));
		// fsubs f7,f11,f13
		ctx.f7.f64 = double(float(ctx.f11.f64 - ctx.f13.f64));
		// fmuls f7,f7,f8
		ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
		// fmadds f6,f0,f7,f12
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 + ctx.f12.f64));
		// fabs f6,f6
		ctx.f6.u64 = ctx.f6.u64 & ~0x8000000000000000;
		// fcmpu cr6,f6,f10
		// bgt cr6,0x8223d9e0
		if (ctx.f6.f64 <= ctx.f10.f64) {
			// bso cr6,0x8223d9e0
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223D9C8, "bso");
			// stfs f7,0(r7)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// mr r31,r5
			var_r31 = ctx.r5.u32;
			// stfs f7,0(r8)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// stw r30,0(r9)
			PPC_STORE_U32(ctx.r9.u32 + 0, var_r30);
			// stw r30,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, var_r30);
		}
	loc_8223D9E0:
		// fneg f3,f11
		ctx.fpscr.disableFlushMode();
		ctx.f3.u64 = ctx.f11.u64 ^ 0x8000000000000000;
		// fsubs f2,f3,f13
		ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
		// fmuls f8,f2,f8
		ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
		// fmadds f1,f0,f8,f12
		ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f12.f64));
		// fabs f7,f1
		ctx.f7.u64 = ctx.f1.u64 & ~0x8000000000000000;
		// fcmpu cr6,f7,f10
		// bgt cr6,0x8223da54
		if (ctx.f7.f64 > ctx.f10.f64) goto loc_8223DA54;
		// bso cr6,0x8223da54
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223D9FC, "bso");
		// cmpwi cr6,r31,0
		// bne cr6,0x8223da20
		if ((int32_t)var_r31 == 0) {
			// stfs f8,0(r7)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// mr r11,r5
			ctx.r11.u64 = ctx.r5.u64;
			// stfs f8,0(r8)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// stw r6,0(r9)
			PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
			// stw r6,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
			// b 0x8223da50
		} else {
		loc_8223DA20:
			// lfs f7,0(r7)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f7.f64 = double(temp.f32);
			// fcmpu cr6,f8,f7
			// bge cr6,0x8223da38
			if (ctx.f8.f64 < ctx.f7.f64) {
				// stfs f8,0(r7)
				temp.f32 = float(ctx.f8.f64);
				PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
				// stw r6,0(r9)
				PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
				// b 0x8223da4c
			} else {
			loc_8223DA38:
				// lfs f6,0(r8)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
				ctx.f6.f64 = double(temp.f32);
				// fcmpu cr6,f8,f6
				// ble cr6,0x8223da4c
				if (ctx.f8.f64 <= ctx.f6.f64) goto loc_8223DA4C;
				// stfs f8,0(r8)
				temp.f32 = float(ctx.f8.f64);
				PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
				// stw r6,0(r10)
				PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
			}
		loc_8223DA4C:
			// mr r11,r6
			ctx.r11.u64 = ctx.r6.u64;
		}
	loc_8223DA50:
		// mr r31,r11
		var_r31 = ctx.r11.u32;
	}
loc_8223DA54:
	// fcmpu cr6,f0,f4
	ctx.fpscr.disableFlushMode();
	// beq cr6,0x8223db60
	if (ctx.f0.f64 != ctx.f4.f64) {
		// cmpwi cr6,r31,2
		// bge cr6,0x8223db60
		if ((int32_t)var_r31 >= 2) goto loc_8223DB60;
		// fdivs f8,f5,f0
		ctx.f8.f64 = double(float(ctx.f5.f64 / ctx.f0.f64));
		// fsubs f3,f10,f12
		ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f12.f64));
		// fmuls f0,f3,f8
		ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
		// fmadds f2,f0,f9,f13
		ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f9.f64 + ctx.f13.f64));
		// fabs f7,f2
		ctx.f7.u64 = ctx.f2.u64 & ~0x8000000000000000;
		// fcmpu cr6,f7,f11
		// bgt cr6,0x8223dad8
		if (ctx.f7.f64 <= ctx.f11.f64) {
			// bso cr6,0x8223dad8
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223DA80, "bso");
			// cmpwi cr6,r31,0
			// bne cr6,0x8223daa4
			if ((int32_t)var_r31 == 0) {
				// stfs f0,0(r7)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
				// mr r11,r5
				ctx.r11.u64 = ctx.r5.u64;
				// stfs f0,0(r8)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
				// stw r5,0(r9)
				PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
				// stw r5,0(r10)
				PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r5.u32);
				// b 0x8223dad4
			} else {
			loc_8223DAA4:
				// lfs f1,0(r7)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
				ctx.f1.f64 = double(temp.f32);
				// fcmpu cr6,f0,f1
				// bge cr6,0x8223dabc
				if (ctx.f0.f64 < ctx.f1.f64) {
					// stfs f0,0(r7)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
					// stw r5,0(r9)
					PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
					// b 0x8223dad0
				} else {
				loc_8223DABC:
					// lfs f7,0(r8)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
					ctx.f7.f64 = double(temp.f32);
					// fcmpu cr6,f0,f7
					// ble cr6,0x8223dad0
					if (ctx.f0.f64 <= ctx.f7.f64) goto loc_8223DAD0;
					// stfs f0,0(r8)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
					// stw r5,0(r10)
					PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r5.u32);
				}
			loc_8223DAD0:
				// mr r11,r6
				ctx.r11.u64 = ctx.r6.u64;
			}
		loc_8223DAD4:
			// mr r31,r11
			var_r31 = ctx.r11.u32;
		}
	loc_8223DAD8:
		// cmpwi cr6,r31,2
		// bge cr6,0x8223db60
		if ((int32_t)var_r31 >= 2) goto loc_8223DB60;
		// fneg f6,f10
		ctx.fpscr.disableFlushMode();
		ctx.f6.u64 = ctx.f10.u64 ^ 0x8000000000000000;
		// fsubs f3,f6,f12
		ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f12.f64));
		// fmuls f0,f3,f8
		ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
		// fmadds f2,f0,f9,f13
		ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f9.f64 + ctx.f13.f64));
		// fabs f13,f2
		ctx.f13.u64 = ctx.f2.u64 & ~0x8000000000000000;
		// fcmpu cr6,f13,f11
		// bgt cr6,0x8223db60
		if (ctx.f13.f64 > ctx.f11.f64) goto loc_8223DB60;
		// bso cr6,0x8223db60
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223DAFC, "bso");
		// cmpwi cr6,r31,0
		// bne cr6,0x8223db24
		if ((int32_t)var_r31 == 0) {
			// li r6,3
			ctx.r6.s64 = 3;
			// stfs f0,0(r7)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// stfs f0,0(r8)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// mr r11,r5
			ctx.r11.u64 = ctx.r5.u64;
			// stw r6,0(r9)
			PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
			// stw r6,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
			// b 0x8223db5c
		} else {
		loc_8223DB24:
			// lfs f1,0(r7)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f1.f64 = double(temp.f32);
			// fcmpu cr6,f0,f1
			// bge cr6,0x8223db40
			if (ctx.f0.f64 < ctx.f1.f64) {
				// li r10,3
				ctx.r10.s64 = 3;
				// stfs f0,0(r7)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
				// stw r10,0(r9)
				PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
				// b 0x8223db58
			} else {
			loc_8223DB40:
				// lfs f13,0(r8)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
				ctx.f13.f64 = double(temp.f32);
				// fcmpu cr6,f0,f13
				// ble cr6,0x8223db58
				if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8223DB58;
				// li r9,3
				ctx.r9.s64 = 3;
				// stfs f0,0(r8)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
				// stw r9,0(r10)
				PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
			}
		loc_8223DB58:
			// mr r11,r6
			ctx.r11.u64 = ctx.r6.u64;
		}
	loc_8223DB5C:
		// mr r31,r11
		var_r31 = ctx.r11.u32;
	}
loc_8223DB60:
	// cmpwi cr6,r31,0
	// bne cr6,0x8223db7c
	if ((int32_t)var_r31 == 0) {
	loc_8223DB68:
		// lfs f0,0(r29)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r29 + 0)/* phBoundBox::vtable@+0x0 */;
		ctx.f0.f64 = double(temp.f32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stfs f0,0(r7)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// stfs f0,0(r8)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
	}
loc_8223DB7C:
	// lfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f4
	// bgt cr6,0x8223db9c
	if (ctx.f0.f64 <= ctx.f4.f64) {
		// bso cr6,0x8223db9c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223DB88, "bso");
		// lfs f12,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f12.f64 = double(temp.f32);
		// fcmpu cr6,f12,f4
		// bso cr6,0x8223db9c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223DB94, "bso");
		// ble cr6,0x8223dbb8
		if (ctx.f12.f64 <= ctx.f4.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
	}
loc_8223DB9C:
	// fcmpu cr6,f0,f5
	ctx.fpscr.disableFlushMode();
	// blt cr6,0x8223dbc0
	if (ctx.f0.f64 >= ctx.f5.f64) {
		// bso cr6,0x8223dbc0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223DBA4, "bso");
		// lfs f11,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f11,f5
		// blt cr6,0x8223dbc0
		if (ctx.f11.f64 < ctx.f5.f64) {
			// li r3,1
			ctx.r3.s64 = 1;
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// bso cr6,0x8223dbc0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223DBB4, "bso");
	loc_8223DBB8:
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
	}
loc_8223DBC0:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundGeometry_DBC8_wrh"))) PPC_WEAK_FUNC(phBoundGeometry_DBC8_wrh);
PPC_FUNC_IMPL(__imp__phBoundGeometry_DBC8_wrh) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v7,v13,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v7,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f12,8(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fcmpu cr6,f0,f12
	// blt cr6,0x8223dc2c
	if (ctx.f0.f64 >= ctx.f12.f64) {
		// lvx128 v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,-16
		ctx.r10.s64 = ctx.r1.s64 + -16;
		// vsubfp v13,v13,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// lwz r7,84(r1)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// vmsum3fp128 v11,v13,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,0(r7)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// fcmpu cr6,f13,f12
		// blt cr6,0x8223dc34
		if (ctx.f13.f64 < ctx.f12.f64) goto loc_8223DC34;
		// bso cr6,0x8223dc34
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223DC28, "bso");
	}
loc_8223DC2C:
	// li r3,2
	ctx.r3.s64 = 2;
	// blr
	return;
loc_8223DC34:
	// fsubs f13,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lwz r6,92(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fcmpu cr6,f0,f12
	// blt cr6,0x8223dd20
	if (ctx.f0.f64 >= ctx.f12.f64) {
		// lfs f13,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// bgt cr6,0x8223dd20
		if (ctx.f0.f64 > ctx.f13.f64) {
			// li r3,3
			ctx.r3.s64 = 3;
			// blr
			return;
		}
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,-16
		ctx.r5.s64 = ctx.r1.s64 + -16;
		// vsubfp v0,v0,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// lvx128 v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v12,v11,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vpermwi128 v11,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// lis r11,-32248
		// vpermwi128 v10,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// lfs f0,-25516(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25516);
		ctx.f0.f64 = double(temp.f32);
		// vpermwi128 v9,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vpermwi128 v8,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vmulfp128 v0,v9,v11
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vnmsubfp v0,v8,v10,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v10,v12,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f11,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f11,f0
		// bge cr6,0x8223dcac
		if (ctx.f11.f64 < ctx.f0.f64) {
			// li r3,1
			ctx.r3.s64 = 1;
			// blr
			return;
		}
	loc_8223DCAC:
		// vmsum3fp128 v9,v7,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// addi r4,r1,-16
		ctx.r4.s64 = ctx.r1.s64 + -16;
		// stvx v9,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f13,f12
		// blt cr6,0x8223dd18
		if (ctx.f13.f64 >= ctx.f12.f64) {
			// fcmpu cr6,f13,f11
			// bgt cr6,0x8223dd18
			if (ctx.f13.f64 > ctx.f11.f64) {
				// li r3,4
				ctx.r3.s64 = 4;
				// blr
				return;
			}
			// vpermwi128 v0,v12,135
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x78));
			// addi r3,r1,-16
			ctx.r3.s64 = ctx.r1.s64 + -16;
			// vpermwi128 v11,v7,99
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0x9C));
			// vpermwi128 v12,v12,99
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x9C));
			// vpermwi128 v10,v7,135
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0x78));
			// vmulfp128 v0,v0,v11
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
			// vnmsubfp v0,v12,v10,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmsum3fp128 v8,v13,v0
			simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// stvx v8,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f0,-16(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
			ctx.f0.f64 = double(temp.f32);
			// fcmpu cr6,f0,f12
			// blt cr6,0x8223dd10
			if (ctx.f0.f64 >= ctx.f12.f64) {
				// fadds f12,f0,f13
				ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
				// fcmpu cr6,f12,f11
				// bgt cr6,0x8223dd10
				if (ctx.f12.f64 > ctx.f11.f64) {
					// li r3,5
					ctx.r3.s64 = 5;
					// blr
					return;
				}
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
		loc_8223DD10:
			// li r3,5
			ctx.r3.s64 = 5;
			// blr
			return;
		}
	loc_8223DD18:
		// li r3,4
		ctx.r3.s64 = 4;
		// blr
		return;
	}
loc_8223DD20:
	// li r3,3
	ctx.r3.s64 = 3;
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_DD28_2hr"))) PPC_WEAK_FUNC(phBoundGeometry_DD28_2hr);
PPC_FUNC_IMPL(__imp__phBoundGeometry_DD28_2hr) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-32
	ctx.r11.s64 = ctx.r1.s64 + -32;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v9,v13,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v13,v11,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// vmsum3fp128 v11,v9,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,-48(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f13,-44(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// stfs f13,0(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lwz r3,-48(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r7,-44(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// rlwinm r11,r3,1,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0x1;
	// rlwinm r6,r7,1,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// eqv r5,r6,r11
	ctx.r5.u64 = ~(ctx.r6.u64 ^ ctx.r11.u64);
	// clrlwi r3,r5,31
	ctx.r3.u64 = ctx.r5.u32 & 0x1;
	// cmplwi cr6,r3,0
	// beq cr6,0x8223dda0
	if (ctx.r3.u32 != 0) {
		// li r3,3
		ctx.r3.s64 = 3;
		// blr
		return;
	}
loc_8223DDA0:
	// fsubs f13,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f9,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	// blt cr6,0x8223df8c
	if (ctx.f0.f64 >= ctx.f9.f64) {
		// lfs f7,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f7.f64 = double(temp.f32);
		// fcmpu cr6,f0,f7
		// bgt cr6,0x8223df8c
		if (ctx.f0.f64 > ctx.f7.f64) {
			// li r3,4
			ctx.r3.s64 = 4;
			// blr
			return;
		}
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,-16
		ctx.r9.s64 = ctx.r1.s64 + -16;
		// vsubfp v10,v13,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// lvx128 v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v13,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// lvx128 v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v8,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vsubfp v11,v11,v12
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// lis r11,-32164
		ctx.r11.s64 = -2107899904;
		// lfs f12,22840(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22840);  /* glob:lbl_825C5938 @ 0x825c5938 */
		ctx.f12.f64 = double(temp.f32);
		// vpermwi128 v6,v10,135
		simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x78));
		// vpermwi128 v5,v10,99
		simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x9C));
		// vmulfp128 v13,v6,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vnmsubfp v13,v5,v8,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v8,v11,v13
		simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// stvx v8,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f0.f64 = double(temp.f32);
		// fmr f10,f0
		ctx.f10.f64 = ctx.f0.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x8223de28
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fmr f11,f7
			ctx.f11.f64 = ctx.f7.f64;
			// b 0x8223de30
		} else {
		loc_8223DE28:
			// fmr f11,f12
			ctx.fpscr.disableFlushMode();
			ctx.f11.f64 = ctx.f12.f64;
			// fneg f10,f0
			ctx.f10.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		}
	loc_8223DE30:
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfs f8,-25516(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25516);  /* glob:lbl_82079C54 @ 0x82079c54 */
		ctx.f8.f64 = double(temp.f32);
		// fcmpu cr6,f10,f8
		// bge cr6,0x8223de48
		if (ctx.f10.f64 < ctx.f8.f64) {
			// li r3,1
			ctx.r3.s64 = 1;
			// blr
			return;
		}
	loc_8223DE48:
		// vmsum3fp128 v7,v9,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// addi r8,r1,-16
		ctx.r8.s64 = ctx.r1.s64 + -16;
		// stvx v7,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f6,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f6.f64 = double(temp.f32);
		// fmuls f13,f6,f11
		ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
		// fcmpu cr6,f13,f9
		// blt cr6,0x8223deb4
		if (ctx.f13.f64 >= ctx.f9.f64) {
			// fcmpu cr6,f13,f10
			// bgt cr6,0x8223deb4
			if (ctx.f13.f64 > ctx.f10.f64) goto loc_8223DEB4;
			// vpermwi128 v13,v11,135
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0x78));
			// addi r7,r1,-16
			ctx.r7.s64 = ctx.r1.s64 + -16;
			// vpermwi128 v8,v9,99
			simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0x9C));
			// vpermwi128 v11,v11,99
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0x9C));
			// vpermwi128 v7,v9,135
			simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0x78));
			// vmulfp128 v13,v13,v8
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)));
			// vnmsubfp v13,v11,v7,v13
			simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmsum3fp128 v4,v0,v13
			simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
			// stvx v4,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f5,-16(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
			ctx.f5.f64 = double(temp.f32);
			// fmuls f0,f5,f11
			ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
			// fcmpu cr6,f0,f9
			// blt cr6,0x8223deac
			if (ctx.f0.f64 >= ctx.f9.f64) {
				// fadds f4,f0,f13
				ctx.f4.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
				// fcmpu cr6,f4,f10
				// ble cr6,0x8223df74
				if (ctx.f4.f64 <= ctx.f10.f64) {
					// li r3,0
					ctx.r3.s64 = 0;
					// blr
					return;
				}
			}
		loc_8223DEAC:
			// li r3,6
			ctx.r3.s64 = 6;
			// b 0x8223deb8
		} else {
		loc_8223DEB4:
			// li r3,5
			ctx.r3.s64 = 5;
		}
	loc_8223DEB8:
		// cmplwi cr6,r10,0
		// beqlr cr6
		if (ctx.r10.u32 == 0) return;
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v11,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vsubfp v13,v13,v12
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vpermwi128 v12,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// addi r6,r1,-16
		ctx.r6.s64 = ctx.r1.s64 + -16;
		// vpermwi128 v8,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// vpermwi128 v7,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// vmulfp128 v13,v8,v12
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v13,v7,v11,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmsum3fp128 v3,v10,v13
		simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// stvx v3,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f0.f64 = double(temp.f32);
		// fmr f11,f0
		ctx.f11.f64 = ctx.f0.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x8223df04
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fmr f12,f7
			ctx.f12.f64 = ctx.f7.f64;
			// b 0x8223df08
		} else {
		loc_8223DF04:
			// fneg f11,f0
			ctx.fpscr.disableFlushMode();
			ctx.f11.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		}
	loc_8223DF08:
		// fcmpu cr6,f11,f8
		ctx.fpscr.disableFlushMode();
		// bge cr6,0x8223df18
		if (ctx.f11.f64 < ctx.f8.f64) {
			// li r3,2
			ctx.r3.s64 = 2;
			// blr
			return;
		}
	loc_8223DF18:
		// vmsum3fp128 v2,v9,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// addi r5,r1,-16
		ctx.r5.s64 = ctx.r1.s64 + -16;
		// stvx v2,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f3,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f3.f64 = double(temp.f32);
		// fmuls f13,f3,f12
		ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
		// fcmpu cr6,f13,f9
		// blt cr6,0x8223df84
		if (ctx.f13.f64 >= ctx.f9.f64) {
			// fcmpu cr6,f13,f11
			// bgt cr6,0x8223df84
			if (ctx.f13.f64 > ctx.f11.f64) {
				// li r3,7
				ctx.r3.s64 = 7;
				// blr
				return;
			}
			// vpermwi128 v13,v9,99
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0x9C));
			// addi r4,r1,-16
			ctx.r4.s64 = ctx.r1.s64 + -16;
			// vpermwi128 v12,v9,135
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0x78));
			// vmulfp128 v13,v6,v13
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v13.f32)));
			// vnmsubfp v13,v5,v12,v13
			simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmsum3fp128 v1,v0,v13
			simde_mm_store_ps(ctx.v1.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
			// stvx v1,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f2,-16(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
			ctx.f2.f64 = double(temp.f32);
			// fmuls f0,f2,f12
			ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
			// fcmpu cr6,f0,f9
			// blt cr6,0x8223df7c
			if (ctx.f0.f64 >= ctx.f9.f64) {
				// fadds f1,f0,f13
				ctx.f1.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
				// fcmpu cr6,f1,f11
				// bgt cr6,0x8223df7c
				if (ctx.f1.f64 > ctx.f11.f64) {
					// li r3,8
					ctx.r3.s64 = 8;
					// blr
					return;
				}
			loc_8223DF74:
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
		loc_8223DF7C:
			// li r3,8
			ctx.r3.s64 = 8;
			// blr
			return;
		}
	loc_8223DF84:
		// li r3,7
		ctx.r3.s64 = 7;
		// blr
		return;
	}
loc_8223DF8C:
	// li r3,4
	ctx.r3.s64 = 4;
	// blr
	return;
}

__attribute__((alias("__imp__phBoundSphere_DF98"))) PPC_WEAK_FUNC(phBoundSphere_DF98);
PPC_FUNC_IMPL(__imp__phBoundSphere_DF98) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	double var_f27 = 0.0;
	double var_f30 = 0.0;
	double var_f28 = 0.0;
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x8223DFA0;
	__savegprlr_29(ctx, base);
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82436614
	__savefpr_27(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// vmsum3fp128 v11,v13,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// vmsum3fp128 v12,v0,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// mr r29,r8
	var_r29 = ctx.r8.u32;
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f27,8(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	var_f27 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	var_f30 = double(temp.f32);
	// fsubs f12,f1,f13
	ctx.f12.f64 = double(float(ctx.f1.f64 - ctx.f13.f64));
	// fmuls f0,f30,f30
	ctx.f0.f64 = double(float(var_f30 * var_f30));
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	var_f28 = double(temp.f32);
	// fmadds f0,f12,f28,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 * var_f28 + ctx.f0.f64));
	// fcmpu cr6,f0,f27
	// blt cr6,0x8223e10c
	if (ctx.f0.f64 >= var_f27) {
		// fcmpu cr6,f13,f1
		// bgt cr6,0x8223e080
		if (ctx.f13.f64 <= ctx.f1.f64) {
			// bso cr6,0x8223e080
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223E018, "bso");
			// vaddfp v0,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
			// addi r8,r1,112
			ctx.r8.s64 = ctx.r1.s64 + 112;
			// vmsum3fp128 v9,v0,v0
			simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// stvx v9,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f11,112(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
			ctx.f11.f64 = double(temp.f32);
			// fcmpu cr6,f11,f1
			// bso cr6,0x8223e03c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8223E034, "bso");
			// ble cr6,0x8223e10c
			if (ctx.f11.f64 <= ctx.f1.f64) {
				// li r3,0
				ctx.r3.s64 = 0;
				// addi r1,r1,208
				ctx.r1.s64 = ctx.r1.s64 + 208;
				// addi r12,r1,-32
				ctx.r12.s64 = ctx.r1.s64 + -32;
				// bl 0x82436660
				__restfpr_27(ctx, base);
				// b 0x8242f8ec
				__restgprlr_29(ctx, base);
				return;
			}
		loc_8223E03C:
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfs f13,-25512(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f28,f13
			// blt cr6,0x8223e10c
			if (var_f28 < ctx.f13.f64) {
				// li r3,0
				ctx.r3.s64 = 0;
				// addi r1,r1,208
				ctx.r1.s64 = ctx.r1.s64 + 208;
				// addi r12,r1,-32
				ctx.r12.s64 = ctx.r1.s64 + -32;
				// bl 0x82436660
				__restfpr_27(ctx, base);
				// b 0x8242f8ec
				__restgprlr_29(ctx, base);
				return;
			}
			// cmplwi cr6,r31,0
			// beq cr6,0x8223e06c
			if (var_r31 == 0) {
				// li r3,1
				ctx.r3.s64 = 1;
				// addi r1,r1,208
				ctx.r1.s64 = ctx.r1.s64 + 208;
				// addi r12,r1,-32
				ctx.r12.s64 = ctx.r1.s64 + -32;
				// bl 0x82436660
				__restfpr_27(ctx, base);
				// b 0x8242f8ec
				__restgprlr_29(ctx, base);
				return;
			}
			// fmr f1,f0
			ctx.f1.f64 = ctx.f0.f64;
			// bl 0x824301d0
			phBoundCapsule_01D0_g(ctx, base);
			// frsp f10,f1
			ctx.fpscr.disableFlushMode();
			ctx.f10.f64 = double(float(ctx.f1.f64));
			// fsubs f9,f10,f30
			ctx.f9.f64 = double(float(ctx.f10.f64 - var_f30));
			// fdivs f8,f9,f28
			ctx.f8.f64 = double(float(ctx.f9.f64 / var_f28));
			// stfs f8,0(r31)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
		loc_8223E06C:
			// li r3,1
			ctx.r3.s64 = 1;
			// addi r1,r1,208
			ctx.r1.s64 = ctx.r1.s64 + 208;
			// addi r12,r1,-32
			ctx.r12.s64 = ctx.r1.s64 + -32;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
	loc_8223E080:
		// fmr f29,f0
		ctx.fpscr.disableFlushMode();
		var_f29 = ctx.f0.f64;
		// lfs f31,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);  /* glob:0x82080000 */
		var_f31 = double(temp.f32);
		// fdivs f28,f31,f28
		var_f28 = double(float(var_f31 / var_f28));
		// fmr f1,f29
		ctx.f1.f64 = var_f29;
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f7,f1
		ctx.fpscr.disableFlushMode();
		ctx.f7.f64 = double(float(ctx.f1.f64));
		// fadds f6,f7,f30
		ctx.f6.f64 = double(float(ctx.f7.f64 + var_f30));
		// fmuls f5,f6,f28
		ctx.f5.f64 = double(float(ctx.f6.f64 * var_f28));
		// fneg f0,f5
		ctx.f0.u64 = ctx.f5.u64 ^ 0x8000000000000000;
		// fcmpu cr6,f0,f27
		// blt cr6,0x8223e10c
		if (ctx.f0.f64 < var_f27) {
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r1,r1,208
			ctx.r1.s64 = ctx.r1.s64 + 208;
			// addi r12,r1,-32
			ctx.r12.s64 = ctx.r1.s64 + -32;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// fcmpu cr6,f0,f31
		// bgt cr6,0x8223e10c
		if (ctx.f0.f64 > var_f31) {
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r1,r1,208
			ctx.r1.s64 = ctx.r1.s64 + 208;
			// addi r12,r1,-32
			ctx.r12.s64 = ctx.r1.s64 + -32;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// cmplwi cr6,r31,0
		// beq cr6,0x8223e0c0
		if (var_r31 != 0) {
			// stfs f0,0(r31)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
		}
	loc_8223E0C0:
		// clrlwi r7,r29,24
		ctx.r7.u64 = var_r29 & 0xFF;
		// cmplwi cr6,r7,0
		// bne cr6,0x8223e06c
		if (ctx.r7.u32 != 0) {
			// li r3,1
			ctx.r3.s64 = 1;
			// addi r1,r1,208
			ctx.r1.s64 = ctx.r1.s64 + 208;
			// addi r12,r1,-32
			ctx.r12.s64 = ctx.r1.s64 + -32;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// fmr f1,f29
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f29;
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f4,f1
		ctx.fpscr.disableFlushMode();
		ctx.f4.f64 = double(float(ctx.f1.f64));
		// fsubs f3,f4,f30
		ctx.f3.f64 = double(float(ctx.f4.f64 - var_f30));
		// fmuls f0,f3,f28
		ctx.f0.f64 = double(float(ctx.f3.f64 * var_f28));
		// fcmpu cr6,f0,f31
		// bgt cr6,0x8223e06c
		if (ctx.f0.f64 > var_f31) {
			// li r3,1
			ctx.r3.s64 = 1;
			// addi r1,r1,208
			ctx.r1.s64 = ctx.r1.s64 + 208;
			// addi r12,r1,-32
			ctx.r12.s64 = ctx.r1.s64 + -32;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8ec
			__restgprlr_29(ctx, base);
			return;
		}
		// bso cr6,0x8223e06c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8223E0E8, "bso");
		// cmplwi cr6,r30,0
		// beq cr6,0x8223e0f8
		if (var_r30 != 0) {
			// stfs f0,0(r30)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r30 + 0, temp.u32);
		}
	loc_8223E0F8:
		// li r3,2
		ctx.r3.s64 = 2;
		// addi r1,r1,208
		ctx.r1.s64 = ctx.r1.s64 + 208;
		// addi r12,r1,-32
		ctx.r12.s64 = ctx.r1.s64 + -32;
		// bl 0x82436660
		__restfpr_27(ctx, base);
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
	}
loc_8223E10C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82436660
	__restfpr_27(ctx, base);
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__pongCameraMgr_E120_2hr"))) PPC_WEAK_FUNC(pongCameraMgr_E120_2hr);
PPC_FUNC_IMPL(__imp__pongCameraMgr_E120_2hr) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stb r7,55(r1)
	PPC_STORE_U8(ctx.r1.u32 + 55, ctx.r7.u8);
	// vmsum3fp128 v13,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lfs f0,12(r5)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f12,f0
	// bge cr6,0x8223e15c
	if (ctx.f12.f64 < ctx.f0.f64) {
	loc_8223E154:
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8223E15C:
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vmsum3fp128 v11,v12,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	// beq cr6,0x8223e154
	if (ctx.f13.f64 == ctx.f0.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// fdivs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f13.f64));
	// li r3,1
	ctx.r3.s64 = 1;
	// fneg f11,f12
	ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f11,0(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_E190"))) PPC_WEAK_FUNC(phBoundCapsule_E190);
PPC_FUNC_IMPL(__imp__phBoundCapsule_E190) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=240, savegprlr_24
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r26,r6
	var_r26 = ctx.r6.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r25,r4
	var_r25 = ctx.r4.u32;
	// lis r11,-32248
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r24,r10
	var_r24 = ctx.r10.u32;
	// lvx128 v13,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v12,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v11,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vsubfp v13,v11,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// lfs f1,-24324(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24324);
	ctx.f1.f64 = double(temp.f32);
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// mr r28,r8
	var_r28 = ctx.r8.u32;
	// mr r31,r9
	var_r31 = ctx.r9.u32;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223c5b8
	ph_C5B8(ctx, base);
	// fmr f3,f1
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = ctx.f1.f64;
	// fcmpu cr6,f3,f31
	// ble cr6,0x8223e224
	if (ctx.f3.f64 > var_f31) {
	loc_8223E214:
		// li r3,3
		ctx.r3.s64 = 3;
		return;
	}
loc_8223E224:
	// lwz r27,332(r1)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 332));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r8,r27
	ctx.r8.u64 = var_r27;
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8223c8b8
	util_C8B8(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// lfs f0,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f0.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	// beq cr6,0x8223e2b8
	if (ctx.r9.u32 != 0) {
		// lis r11,-32160
		// stfs f0,112(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// addi r7,r1,128
		ctx.r7.s64 = ctx.r1.s64 + 128;
		// lwz r4,324(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// fsubs f13,f31,f3
		ctx.f13.f64 = double(float(var_f31 - ctx.f3.f64));
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// stfs f13,0(r24)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r24 + 0, temp.u32);
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// li r8,0
		ctx.r8.s64 = 0;
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// li r3,1
		ctx.r3.s64 = 1;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v10,v0,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// lvx128 v9,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stw r8,0(r28)
		PPC_STORE_U32(var_r28 + 0, ctx.r8.u32);
		// stvx v10,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmaddfp v8,v11,v13,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v8,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	}
loc_8223E2B8:
	// lis r11,-32253
	// addi r11,r11,-12020
	ctx.r11.s64 = ctx.r11.s64 + -12020;
	// lfs f11,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f11
	// ble cr6,0x8223e454
	if (ctx.f0.f64 > ctx.f11.f64) {
		// fcmpu cr6,f0,f12
	} else {
		if (ctx.f0.f64 < ctx.f12.f64) {
			// lfs f13,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f13.f64 = double(temp.f32);
			// lfs f12,80(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f12.f64 = double(temp.f32);
			// fcmpu cr6,f12,f13
			// bge cr6,0x8223e2fc
			if (ctx.f12.f64 < ctx.f13.f64) {
			// li r3,1
			ctx.r3.s64 = 1;
			// lvx128 v7,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v7,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stw r3,0(r28)
			PPC_STORE_U32(var_r28 + 0, ctx.r3.u32);
			// b 0x8223e30c
			} else {
			loc_8223E2FC:
			// li r11,2
			ctx.r11.s64 = 2;
			// lvx128 v6,r0,r26
			ea = (var_r26) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v6,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stw r11,0(r28)
			PPC_STORE_U32(var_r28 + 0, ctx.r11.u32);
			}
			loc_8223E30C:
			// stfs f0,144(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// lvx128 v0,r0,r29
			ea = (var_r29) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r8,324(r1)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
			// fmuls f10,f31,f31
			ctx.f10.f64 = double(float(var_f31 * var_f31));
			// lvx128 v5,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,144
			ctx.r9.s64 = ctx.r1.s64 + 144;
			// addi r7,r1,144
			ctx.r7.s64 = ctx.r1.s64 + 144;
			// lvx128 v4,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v13,v4,0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.u32), 0xFF));
			// vmaddfp v0,v5,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v0,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v3,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vsubfp v0,v3,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vmsum3fp128 v13,v0,v0
			simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// stvx v0,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f1,144(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
			ctx.f1.f64 = double(temp.f32);
			// fcmpu cr6,f1,f10
			// bgt cr6,0x8223e214
			if (ctx.f1.f64 > ctx.f10.f64) {
			// li r3,3
			ctx.r3.s64 = 3;
			return;
			}
			// fcmpu cr6,f1,f11
			// ble cr6,0x8223e3bc
			if (ctx.f1.f64 > ctx.f11.f64) {
			// bl 0x824301d0
			phBoundCapsule_01D0_g(ctx, base);
			// frsp f0,f1
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(ctx.f1.f64));
			// lis r11,-32163
			// lvx128 v2,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r11,r11,-18496
			ctx.r11.s64 = ctx.r11.s64 + -18496;
			// lvx128 v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fneg f9,f0
			ctx.f9.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f9,144(r1)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
			// fsubs f8,f31,f0
			ctx.f8.f64 = double(float(var_f31 - ctx.f0.f64));
			// stfs f8,0(r24)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r24 + 0, temp.u32);
			// addi r6,r1,144
			ctx.r6.s64 = ctx.r1.s64 + 144;
			// lvx128 v1,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v12,v1,0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.u32), 0xFF));
			// vrefp v13,v12
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
			// vnmsubfp v0,v13,v12,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmaddfp v0,v13,v0,v13
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
			// vmulfp128 v31,v2,v0
			simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v31,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			return;
			}
			loc_8223E3BC:
			// lvx128 v0,r0,r26
			ea = (var_r26) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r5,0(r28)
			ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
			// lvx128 v13,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lis r11,-32160
			// vsubfp v0,v13,v0
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stfs f31,0(r24)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.f32 = float(var_f31);
			PPC_STORE_U32(var_r24 + 0, temp.u32);
			// cmpwi cr6,r5,2
			// addi r11,r11,26448
			ctx.r11.s64 = ctx.r11.s64 + 26448;
			// stvx v0,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bne cr6,0x8223e3f0
			if (ctx.r5.s32 == 2) {
			// lvx128 v13,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vsubfp v30,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v30,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			}
			loc_8223E3F0:
			// lvx128 v13,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lis r10,-32158
			// vmsum3fp128 v11,v13,v13
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
			// lvx128 v9,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r10,r10,-25616
			ctx.r10.s64 = ctx.r10.s64 + -25616;
			// lis r11,-32163
			// li r3,0
			ctx.r3.s64 = 0;
			// addi r11,r11,-18480
			ctx.r11.s64 = ctx.r11.s64 + -18480;
			// lvx128 v0,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v8,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lis r11,-32163
			// addi r11,r11,-18496
			ctx.r11.s64 = ctx.r11.s64 + -18496;
			// vrsqrtefp v10,v11
			simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
			// lvx128 v12,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vcmpeqfp v0,v10,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vsel v0,v10,v9,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
			// vmulfp128 v10,v0,v0
			simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vmulfp128 v9,v8,v0
			simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vnmsubfp v12,v11,v10,v12
			simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v12.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmaddfp v0,v12,v9,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
			// vmulfp128 v29,v13,v0
			simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v29,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			return;
		}
	}
loc_8223E454:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223e480
	if (ctx.f0.f64 < ctx.f13.f64) {
		// lwz r4,324(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
		// lvx128 v28,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stfs f11,0(r27)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
		// li r3,4
		ctx.r3.s64 = 4;
		// stvx v28,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	}
loc_8223E480:
	// lwz r3,324(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// lvx128 v27,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f12,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// stvx v27,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r3,4
	ctx.r3.s64 = 4;
	return;
}

__attribute__((alias("__imp__phBoundQuadtree_E4A0_2hr"))) PPC_WEAK_FUNC(phBoundQuadtree_E4A0_2hr);
PPC_FUNC_IMPL(__imp__phBoundQuadtree_E4A0_2hr) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32253
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v13,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v9,v12,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vsubfp v8,v11,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lfs f0,-12016(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,-64(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// fmr f8,f0
	ctx.f8.f64 = ctx.f0.f64;
	// addi r11,r1,-64
	ctx.r11.s64 = ctx.r1.s64 + -64;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v7,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// beq cr6,0x8223e4e8
	if (!(ctx.cr6.eq)) {
		// lvx128 v12,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v7,v12,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	}
loc_8223E4E8:
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r9,32
	ctx.r7.s64 = ctx.r9.s64 + 32;
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// vmsum3fp128 v4,v0,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// addi r4,r1,-48
	ctx.r4.s64 = ctx.r1.s64 + -48;
	// vmsum3fp128 v3,v0,v13
	simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lfs f11,4(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// vmsum3fp128 v6,v0,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v5,v0,v10
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// lfs f10,8(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// stvx v4,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f5,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f5.f64 = double(temp.f32);
	// fabs f12,f5
	ctx.f12.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// lfs f7,-64(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	ctx.f7.f64 = double(temp.f32);
	// fabs f0,f7
	ctx.f0.u64 = ctx.f7.u64 & ~0x8000000000000000;
	// lfs f6,-48(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	ctx.f6.f64 = double(temp.f32);
	// fabs f13,f6
	ctx.f13.u64 = ctx.f6.u64 & ~0x8000000000000000;
	// fmuls f4,f11,f0
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmadds f3,f10,f13,f4
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f4.f64));
	// fmadds f0,f12,f9,f3
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f3.f64));
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fneg f13,f0
	ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// fcmpu cr6,f12,f0
	// bgt cr6,0x8223e63c
	if (ctx.f12.f64 <= ctx.f0.f64) {
		// fcmpu cr6,f12,f13
		// blt cr6,0x8223e63c
		if (ctx.f12.f64 < ctx.f13.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// li r11,0
		ctx.r11.s64 = 0;
	loc_8223E57C:
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,-16
		ctx.r8.s64 = ctx.r1.s64 + -16;
		// vmsum3fp128 v2,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// addi r7,r1,-32
		ctx.r7.s64 = ctx.r1.s64 + -32;
		// vmsum3fp128 v1,v0,v9
		simde_mm_store_ps(ctx.v1.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// addi r5,r1,-48
		ctx.r5.s64 = ctx.r1.s64 + -48;
		// vmsum3fp128 v31,v0,v8
		simde_mm_store_ps(ctx.v31.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// cmplwi cr6,r6,0
		ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
		// stvx v2,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v1,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v31,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,-16(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		ctx.f0.f64 = double(temp.f32);
		// lfs f2,-32(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
		ctx.f2.f64 = double(temp.f32);
		// fadds f12,f2,f0
		ctx.f12.f64 = double(float(ctx.f2.f64 + ctx.f0.f64));
		// lfs f1,-48(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
		ctx.f1.f64 = double(temp.f32);
		// fadds f13,f1,f0
		ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
		// fsubs f11,f0,f12
		ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// fsel f10,f11,f12,f0
		ctx.f10.f64 = ctx.f11.f64 >= 0.0 ? ctx.f12.f64 : ctx.f0.f64;
		// fsel f12,f11,f0,f12
		ctx.f12.f64 = ctx.f11.f64 >= 0.0 ? ctx.f0.f64 : ctx.f12.f64;
		// beq cr6,0x8223e5e0
		if (!(ctx.cr6.eq)) {
			// vmsum3fp128 v30,v0,v7
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v30.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
			// addi r4,r1,-64
			ctx.r4.s64 = ctx.r1.s64 + -64;
			// stvx v30,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f11,-64(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -64);
			ctx.f11.f64 = double(temp.f32);
			// fadds f8,f11,f0
			ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
		}
	loc_8223E5E0:
		// fsubs f7,f10,f13
		ctx.fpscr.disableFlushMode();
		ctx.f7.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
		// lfs f11,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f6,f12,f13
		ctx.f6.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
		// cmplwi cr6,r6,0
		// fneg f9,f11
		ctx.f9.u64 = ctx.f11.u64 ^ 0x8000000000000000;
		// fsel f0,f7,f13,f10
		ctx.f0.f64 = ctx.f7.f64 >= 0.0 ? ctx.f13.f64 : ctx.f10.f64;
		// fsel f13,f6,f12,f13
		ctx.f13.f64 = ctx.f6.f64 >= 0.0 ? ctx.f12.f64 : ctx.f13.f64;
		// beq cr6,0x8223e610
		if (ctx.r6.u32 != 0) {
			// fsubs f5,f0,f8
			ctx.f5.f64 = double(float(ctx.f0.f64 - ctx.f8.f64));
			// fsubs f4,f13,f8
			ctx.f4.f64 = double(float(ctx.f13.f64 - ctx.f8.f64));
			// fsel f0,f5,f8,f0
			ctx.f0.f64 = ctx.f5.f64 >= 0.0 ? ctx.f8.f64 : ctx.f0.f64;
			// fsel f13,f4,f13,f8
			ctx.f13.f64 = ctx.f4.f64 >= 0.0 ? ctx.f13.f64 : ctx.f8.f64;
		}
	loc_8223E610:
		// fcmpu cr6,f0,f11
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x8223e63c
		if (ctx.f0.f64 > ctx.f11.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fcmpu cr6,f13,f9
		// blt cr6,0x8223e63c
		if (ctx.f13.f64 < ctx.f9.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,3
		// blt cr6,0x8223e57c
		if (ctx.r11.s32 < 3) goto loc_8223E57C;
		// li r3,1
		ctx.r3.s64 = 1;
		// blr
		return;
	}
loc_8223E63C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phLevelNew_E648_wrh"))) PPC_WEAK_FUNC(phLevelNew_E648_wrh);
PPC_FUNC_IMPL(__imp__phLevelNew_E648_wrh) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32163
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v12,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f12,-12(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32248
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// vsubfp v11,v11,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v10,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lfs f0,-25840(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25840);
	ctx.f0.f64 = double(temp.f32);
	// fadds f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fadds f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// vaddfp v12,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v10,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lfs f10,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f8.f64 = double(temp.f32);
	// fabs f0,f10
	ctx.f0.u64 = ctx.f10.u64 & ~0x8000000000000000;
	// lfs f6,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f6.f64 = double(temp.f32);
	// fabs f7,f8
	ctx.f7.u64 = ctx.f8.u64 & ~0x8000000000000000;
	// fabs f6,f6
	ctx.f6.u64 = ctx.f6.u64 & ~0x8000000000000000;
	// vmulfp128 v0,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vpermwi128 v13,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// lfs f5,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f5.f64 = double(temp.f32);
	// fabs f12,f5
	ctx.f12.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lfs f4,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f4.f64 = double(temp.f32);
	// vmulfp128 v0,v11,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lfs f3,-8(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f3.f64 = double(temp.f32);
	// fabs f8,f4
	ctx.f8.u64 = ctx.f4.u64 & ~0x8000000000000000;
	// fabs f10,f3
	ctx.f10.u64 = ctx.f3.u64 & ~0x8000000000000000;
	// fadds f2,f12,f13
	ctx.f2.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// vnmsubfp v0,v10,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// fcmpu cr6,f0,f2
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.cr6.compare(ctx.f0.f64, ctx.f2.f64);
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f5.f64 = double(temp.f32);
	// fabs f0,f1
	ctx.f0.u64 = ctx.f1.u64 & ~0x8000000000000000;
	// lfs f4,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f4.f64 = double(temp.f32);
	// fabs f5,f5
	ctx.f5.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// fabs f4,f4
	ctx.f4.u64 = ctx.f4.u64 & ~0x8000000000000000;
	// bgt cr6,0x8223e770
	if (!(ctx.cr6.gt)) {
		// fadds f3,f8,f9
		ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
		// fcmpu cr6,f7,f3
		// bgt cr6,0x8223e770
		if (ctx.f7.f64 > ctx.f3.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fadds f2,f10,f11
		ctx.f2.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
		// fcmpu cr6,f6,f2
		// bgt cr6,0x8223e770
		if (ctx.f6.f64 > ctx.f2.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fmuls f1,f10,f9
		ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
		// fmadds f7,f11,f8,f1
		ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f1.f64));
		// fcmpu cr6,f0,f7
		// bgt cr6,0x8223e770
		if (ctx.f0.f64 > ctx.f7.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fmuls f6,f11,f12
		ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
		// fmadds f3,f10,f13,f6
		ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f6.f64));
		// fcmpu cr6,f5,f3
		// bgt cr6,0x8223e770
		if (ctx.f5.f64 > ctx.f3.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// fmuls f2,f9,f12
		ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
		// li r3,1
		ctx.r3.s64 = 1;
		// fmadds f1,f8,f13,f2
		ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f2.f64));
		// fcmpu cr6,f4,f1
		// blelr cr6
		if (ctx.f4.f64 <= ctx.f1.f64) return;
	}
loc_8223E770:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phPolytree_E778_2h"))) PPC_WEAK_FUNC(phPolytree_E778_2h);
PPC_FUNC_IMPL(__imp__phPolytree_E778_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32164
	// lfs f13,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r8,-1
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// lfs f10,22840(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22840);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32253
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f13,f8
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// bne cr6,0x8223e7c8
	if (ctx.f13.f64 == ctx.f8.f64) {
		// lfs f0,0(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// fabs f0,f0
		ctx.f0.u64 = ctx.f0.u64 & ~0x8000000000000000;
		// lfs f13,0(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// ble cr6,0x8223e820
		if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8223E820;
	loc_8223E7C0:
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8223E7C8:
	// fdivs f0,f7,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f7.f64 / ctx.f13.f64));
	// lfs f12,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f8
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r9,1
	ctx.r9.s64 = 1;
	// li r10,1
	ctx.r10.s64 = 1;
	// ble cr6,0x8223e800
	if (ctx.f13.f64 > ctx.f8.f64) {
		// fneg f11,f13
		ctx.f11.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// li r8,1
		ctx.r8.s64 = 1;
		// fsubs f10,f13,f12
		ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
		// fsubs f6,f11,f12
		ctx.f6.f64 = double(float(ctx.f11.f64 - ctx.f12.f64));
		// fmuls f9,f10,f0
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
		// fmuls f10,f6,f0
		ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
		// b 0x8223e818
	} else {
	loc_8223E800:
		// fneg f5,f13
		ctx.fpscr.disableFlushMode();
		ctx.f5.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// li r8,2
		ctx.r8.s64 = 2;
		// fsubs f4,f13,f12
		ctx.f4.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
		// fsubs f3,f5,f12
		ctx.f3.f64 = double(float(ctx.f5.f64 - ctx.f12.f64));
		// fmuls f10,f4,f0
		ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// fmuls f9,f3,f0
		ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	}
loc_8223E818:
	// fcmpu cr6,f10,f9
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8223e7c0
	if (ctx.f10.f64 > ctx.f9.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8223E820:
	// lfs f2,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// fcmpu cr6,f2,f8
	// bne cr6,0x8223e848
	if (ctx.f2.f64 == ctx.f8.f64) {
		// lfs f1,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		ctx.f1.f64 = double(temp.f32);
		// fabs f0,f1
		ctx.f0.u64 = ctx.f1.u64 & ~0x8000000000000000;
		// lfs f13,4(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// ble cr6,0x8223e904
		if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8223E904;
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8223E848:
	// lfs f0,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f13,f7,f0
	ctx.f13.f64 = double(float(ctx.f7.f64 / ctx.f0.f64));
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f8
	// lfs f0,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// ble cr6,0x8223e8b4
	if (ctx.f0.f64 > ctx.f8.f64) {
		// fsubs f11,f0,f12
		ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// clrlwi r11,r9,24
		ctx.r11.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// fmuls f11,f11,f13
		ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
		// beq cr6,0x8223e87c
		if (ctx.r11.u32 != 0) {
			// fcmpu cr6,f11,f9
			// bge cr6,0x8223e884
			if (ctx.f11.f64 >= ctx.f9.f64) goto loc_8223E884;
		}
	loc_8223E87C:
		// fmr f9,f11
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = ctx.f11.f64;
		// li r9,1
		ctx.r9.s64 = 1;
	loc_8223E884:
		// fneg f6,f0
		ctx.fpscr.disableFlushMode();
		ctx.f6.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// clrlwi r11,r10,24
		ctx.r11.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// fsubs f5,f6,f12
		ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f12.f64));
		// fmuls f0,f5,f13
		ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
		// beq cr6,0x8223e8a4
		if (ctx.r11.u32 != 0) {
			// fcmpu cr6,f0,f10
			// ble cr6,0x8223e904
			if (ctx.f0.f64 <= ctx.f10.f64) goto loc_8223E904;
		}
	loc_8223E8A4:
		// li r8,3
		ctx.r8.s64 = 3;
		// fmr f10,f0
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = ctx.f0.f64;
		// li r10,1
		ctx.r10.s64 = 1;
		// b 0x8223e904
	} else {
	loc_8223E8B4:
		// fsubs f4,f0,f12
		ctx.fpscr.disableFlushMode();
		ctx.f4.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// clrlwi r11,r10,24
		ctx.r11.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// fmuls f11,f4,f13
		ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// beq cr6,0x8223e8d0
		if (ctx.r11.u32 != 0) {
			// fcmpu cr6,f11,f10
			// ble cr6,0x8223e8dc
			if (ctx.f11.f64 <= ctx.f10.f64) goto loc_8223E8DC;
		}
	loc_8223E8D0:
		// li r8,4
		ctx.r8.s64 = 4;
		// fmr f10,f11
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = ctx.f11.f64;
		// li r10,1
		ctx.r10.s64 = 1;
	loc_8223E8DC:
		// fneg f3,f0
		ctx.fpscr.disableFlushMode();
		ctx.f3.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// clrlwi r11,r9,24
		ctx.r11.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// fsubs f2,f3,f12
		ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f12.f64));
		// fmuls f0,f2,f13
		ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
		// beq cr6,0x8223e8fc
		if (ctx.r11.u32 != 0) {
			// fcmpu cr6,f0,f9
			// bge cr6,0x8223e904
			if (ctx.f0.f64 >= ctx.f9.f64) goto loc_8223E904;
		}
	loc_8223E8FC:
		// fmr f9,f0
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = ctx.f0.f64;
		// li r9,1
		ctx.r9.s64 = 1;
	}
loc_8223E904:
	// fcmpu cr6,f10,f9
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8223e7c0
	if (ctx.f10.f64 > ctx.f9.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lfs f1,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f8
	// bne cr6,0x8223e934
	if (ctx.f1.f64 == ctx.f8.f64) {
		// lfs f0,8(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		ctx.f0.f64 = double(temp.f32);
		// fabs f0,f0
		ctx.f0.u64 = ctx.f0.u64 & ~0x8000000000000000;
		// lfs f13,8(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// ble cr6,0x8223e9e0
		if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8223E9E0;
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8223E934:
	// lfs f0,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f13,f7,f0
	ctx.f13.f64 = double(float(ctx.f7.f64 / ctx.f0.f64));
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f8
	// lfs f0,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// ble cr6,0x8223e998
	if (ctx.f0.f64 > ctx.f8.f64) {
		// fsubs f11,f0,f12
		ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// clrlwi r9,r9,24
		ctx.r9.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// fmuls f11,f11,f13
		ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
		// beq cr6,0x8223e968
		if (ctx.r9.u32 != 0) {
			// fcmpu cr6,f11,f9
			// bge cr6,0x8223e96c
			if (ctx.f11.f64 >= ctx.f9.f64) goto loc_8223E96C;
		}
	loc_8223E968:
		// fmr f9,f11
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = ctx.f11.f64;
	loc_8223E96C:
		// fneg f8,f0
		ctx.fpscr.disableFlushMode();
		ctx.f8.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// clrlwi r5,r10,24
		ctx.r5.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// fsubs f7,f8,f12
		ctx.f7.f64 = double(float(ctx.f8.f64 - ctx.f12.f64));
		// fmuls f0,f7,f13
		ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
		// beq cr6,0x8223e98c
		if (ctx.r5.u32 != 0) {
			// fcmpu cr6,f0,f10
			// ble cr6,0x8223e9e0
			if (ctx.f0.f64 <= ctx.f10.f64) goto loc_8223E9E0;
		}
	loc_8223E98C:
		// li r8,5
		ctx.r8.s64 = 5;
		// fmr f10,f0
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = ctx.f0.f64;
		// b 0x8223e9e0
	} else {
	loc_8223E998:
		// fsubs f6,f0,f12
		ctx.fpscr.disableFlushMode();
		ctx.f6.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// clrlwi r4,r10,24
		ctx.r4.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// fmuls f11,f6,f13
		ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
		// beq cr6,0x8223e9b4
		if (ctx.r4.u32 != 0) {
			// fcmpu cr6,f11,f10
			// ble cr6,0x8223e9bc
			if (ctx.f11.f64 <= ctx.f10.f64) goto loc_8223E9BC;
		}
	loc_8223E9B4:
		// li r8,6
		ctx.r8.s64 = 6;
		// fmr f10,f11
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = ctx.f11.f64;
	loc_8223E9BC:
		// fneg f5,f0
		ctx.fpscr.disableFlushMode();
		ctx.f5.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// clrlwi r3,r9,24
		ctx.r3.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// fsubs f4,f5,f12
		ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f12.f64));
		// fmuls f0,f4,f13
		ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// beq cr6,0x8223e9dc
		if (ctx.r3.u32 != 0) {
			// fcmpu cr6,f0,f9
			// bge cr6,0x8223e9e0
			if (ctx.f0.f64 >= ctx.f9.f64) goto loc_8223E9E0;
		}
	loc_8223E9DC:
		// fmr f9,f0
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = ctx.f0.f64;
	}
loc_8223E9E0:
	// fcmpu cr6,f10,f9
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8223e7c0
	if (ctx.f10.f64 > ctx.f9.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// cmpwi cr6,r8,0
	// blt cr6,0x8223e7c0
	if (ctx.r8.s32 < 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// cmplwi cr6,r6,0
	// beq cr6,0x8223e9fc
	if (ctx.r6.u32 != 0) {
		// stfs f10,0(r6)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	}
loc_8223E9FC:
	// cmplwi cr6,r7,0
	// beq cr6,0x8223ea08
	if (ctx.r7.u32 != 0) {
		// stfs f9,0(r7)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	}
loc_8223EA08:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBox_EA10"))) PPC_WEAK_FUNC(phBoundBox_EA10);
PPC_FUNC_IMPL(__imp__phBoundBox_EA10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// FRAME: size=144, manual
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// lwz r10,228(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// vaddfp v12,v0,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,236(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// vmulfp128 v13,v12,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vsubfp v10,v0,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vsubfp v9,v11,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223ea90
	phBoundBox_EA90_wrh(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBox_EA90_wrh"))) PPC_WEAK_FUNC(phBoundBox_EA90_wrh);
PPC_FUNC_IMPL(__imp__phBoundBox_EA90_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f898
	ctx.lr = 0x8223EA98;
	__savegprlr_28(ctx, base);
	// lis r11,-32164
	// li r29,-1
	var_r29 = (uint32_t)(-1);
	// li r31,-1
	var_r31 = (uint32_t)(-1);
	// subf r28,r3,r5
	var_r28 = (uint32_t)(ctx.r5.s64 - ctx.r3.s64);
	// subf r4,r3,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r3.s64;
	// lfs f6,22840(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22840);
	ctx.f6.f64 = double(temp.f32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r30,r11,-12024
	var_r30 = (uint32_t)(ctx.r11.s64 + -12024);  // lbl_8202D108 @ 0x8202d108
	// li r11,0
	ctx.r11.s64 = 0;
	// lfs f7,124(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 124);
	ctx.f7.f64 = double(temp.f32);
	// lfs f8,8(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmr f9,f7
	ctx.f9.f64 = ctx.f7.f64;
	// lfs f5,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundBox::vtable@+0x0 */;
	ctx.f5.f64 = double(temp.f32);
loc_8223EACC:
	// lfs f10,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundBox::vtable@+0x0 */;
	ctx.f10.f64 = double(temp.f32);
	// lfsx f11,r4,r3
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
	ctx.f11.f64 = double(temp.f32);
	// fabs f0,f10
	ctx.f0.u64 = ctx.f10.u64 & ~0x8000000000000000;
	// stfs f10,-48(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// fabs f13,f11
	ctx.f13.u64 = ctx.f11.u64 & ~0x8000000000000000;
	// stfs f11,-44(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// lfsx f12,r28,r3
	temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r3.u32);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// fcmpu cr6,f0,f8
	// lwz r5,-48(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lwz r30,-44(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + -44));
	// rlwinm r5,r5,1,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0x1;
	// rlwinm r30,r30,1,31,31
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 1) & 0x1);
	// eqv r5,r5,r30
	ctx.r5.u64 = ~(ctx.r5.u64 ^ var_r30);
	// clrlwi r5,r5,31
	ctx.r5.u64 = ctx.r5.u32 & 0x1;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// bge cr6,0x8223eb7c
	if (ctx.f0.f64 < ctx.f8.f64) {
		// cmplwi cr6,r5,0
		// bne cr6,0x8223ec70
		if (ctx.r5.u32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x8242f8e8
			__restgprlr_28(ctx, base);
			return;
		}
		// fneg f11,f13
		ctx.f11.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// fcmpu cr6,f0,f11
		// blt cr6,0x8223ec70
		if (ctx.f0.f64 < ctx.f11.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x8242f8e8
			__restgprlr_28(ctx, base);
			return;
		}
		// fdivs f13,f5,f13
		ctx.f13.f64 = double(float(ctx.f5.f64 / ctx.f13.f64));
		// fmuls f4,f13,f0
		ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// fneg f11,f4
		ctx.f11.u64 = ctx.f4.u64 ^ 0x8000000000000000;
		// fcmpu cr6,f11,f6
		// ble cr6,0x8223eb4c
		if (ctx.f11.f64 > ctx.f6.f64) {
			// fcmpu cr6,f10,f8
			// mr r29,r11
			var_r29 = ctx.r11.u32;
			// bgt cr6,0x8223eb48
			if (ctx.f10.f64 <= ctx.f8.f64) {
				// addi r29,r11,1
				var_r29 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82030001
			}
		loc_8223EB48:
			// fmr f6,f11
			ctx.fpscr.disableFlushMode();
			ctx.f6.f64 = ctx.f11.f64;
		}
	loc_8223EB4C:
		// fmsubs f3,f12,f7,f0
		ctx.fpscr.disableFlushMode();
		ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f0.f64));
		// fmuls f0,f3,f13
		ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
		// fcmpu cr6,f0,f9
		// bge cr6,0x8223ebc4
		if (ctx.f0.f64 >= ctx.f9.f64) goto loc_8223EBC4;
		// fcmpu cr6,f0,f5
		// bge cr6,0x8223ebc4
		if (ctx.f0.f64 >= ctx.f5.f64) goto loc_8223EBC4;
		// fcmpu cr6,f10,f8
		// addi r31,r11,1
		var_r31 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82030001
		// bgt cr6,0x8223eb74
		if (ctx.f10.f64 <= ctx.f8.f64) {
			// mr r31,r11
			var_r31 = ctx.r11.u32;
		}
	loc_8223EB74:
		// fmr f9,f0
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = ctx.f0.f64;
		// b 0x8223ebc4
		goto loc_8223EBC4;
	}
loc_8223EB7C:
	// cmplwi cr6,r5,0
	// beq cr6,0x8223ec40
	if (ctx.r5.u32 == 0) goto loc_8223EC40;
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	// bge cr6,0x8223ebc4
	if (ctx.f0.f64 >= ctx.f13.f64) goto loc_8223EBC4;
	// fmuls f2,f13,f9
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fcmpu cr6,f0,f2
	// bge cr6,0x8223ebc4
	if (ctx.f0.f64 >= ctx.f2.f64) goto loc_8223EBC4;
	// fcmpu cr6,f10,f8
	// ble cr6,0x8223eba8
	if (ctx.f10.f64 > ctx.f8.f64) {
		// mr r31,r11
		var_r31 = ctx.r11.u32;
		// b 0x8223ebc0
		goto loc_8223EBC0;
	}
loc_8223EBA8:
	// fcmpu cr6,f10,f8
	ctx.fpscr.disableFlushMode();
	// blt cr6,0x8223ebbc
	if (ctx.f10.f64 < ctx.f8.f64) goto loc_8223EBBC;
loc_8223EBB0:
	// mr r31,r11
	var_r31 = ctx.r11.u32;
	// fcmpu cr6,f11,f8
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8223ebc0
	if (ctx.f11.f64 > ctx.f8.f64) goto loc_8223EBC0;
loc_8223EBBC:
	// addi r31,r11,1
	var_r31 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82030001
loc_8223EBC0:
	// fdivs f9,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
loc_8223EBC4:
	// fcmpu cr6,f6,f9
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8223ec70
	if (ctx.f6.f64 > ctx.f9.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	}
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// cmpwi cr6,r11,6
	// blt cr6,0x8223eacc
	if (ctx.r11.s32 < 6) goto loc_8223EACC;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r29,0
	// blt cr6,0x8223ec78
	if ((int32_t)var_r29 < 0) goto loc_8223EC78;
	// lis r11,-32163
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r6,0
	// addi r11,r11,-18304
	ctx.r11.s64 = ctx.r11.s64 + -18304;
	// beq cr6,0x8223ec10
	if (ctx.r6.u32 != 0) {
		// rlwinm r4,r29,4,0,27
		ctx.r4.u64 = __builtin_rotateleft64(var_r29 | (var_r29 << 32), 4) & 0xFFFFFFF0;
		// stfs f6,0(r6)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// stw r29,0(r8)
		PPC_STORE_U32(ctx.r8.u32 + 0, var_r29);
		// lvx128 v0,r4,r11
		ea = (ctx.r4.u32 + ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223EC10:
	// cmpwi cr6,r31,0
	// blt cr6,0x8223eca8
	if ((int32_t)var_r31 < 0) {
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	}
	// li r3,2
	ctx.r3.s64 = 2;
	// cmplwi cr6,r9,0
	// beq cr6,0x8223eca8
	if (ctx.r9.u32 == 0) {
		// b 0x8242f8e8
		__restgprlr_28(ctx, base);
		return;
	}
	// rlwinm r8,r31,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stfs f9,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lvx128 v13,r8,r11
	ea = (ctx.r8.u32 + ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stw r31,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, var_r31);
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
loc_8223EC40:
	// fmsubs f0,f12,f7,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f0.f64));
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223ebc4
	if (ctx.f0.f64 >= ctx.f13.f64) goto loc_8223EBC4;
	// fmuls f1,f13,f9
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fcmpu cr6,f0,f1
	// bge cr6,0x8223ebc4
	if (ctx.f0.f64 >= ctx.f1.f64) goto loc_8223EBC4;
	// fcmpu cr6,f10,f8
	// bgt cr6,0x8223ebbc
	if (ctx.f10.f64 > ctx.f8.f64) goto loc_8223EBBC;
	// fcmpu cr6,f10,f8
	// bge cr6,0x8223ebb0
	if (ctx.f10.f64 >= ctx.f8.f64) goto loc_8223EBB0;
	// mr r31,r11
	var_r31 = ctx.r11.u32;
	// b 0x8223ebc0
	goto loc_8223EBC0;
loc_8223EC78:
	// cmpwi cr6,r31,0
	// blt cr6,0x8223eca8
	if ((int32_t)var_r31 >= 0) {
		// li r3,1
		ctx.r3.s64 = 1;
		// cmplwi cr6,r6,0
		// beq cr6,0x8223eca8
		if (ctx.r6.u32 == 0) {
			// b 0x8242f8e8
			__restgprlr_28(ctx, base);
			return;
		}
		// lis r11,-32163
		// stfs f9,0(r6)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// rlwinm r5,r31,4,0,27
		ctx.r5.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
		// stw r31,0(r8)
		PPC_STORE_U32(ctx.r8.u32 + 0, var_r31);
		// addi r11,r11,-18304
		ctx.r11.s64 = ctx.r11.s64 + -18304;
		// lvx128 v12,r5,r11
		ea = (ctx.r5.u32 + ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8223ECA8:
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__game_ECB0"))) PPC_WEAK_FUNC(game_ECB0);
PPC_FUNC_IMPL(__imp__game_ECB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,16(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f0,f9
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// lfs f13,48(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f12,32(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f6,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f5,f13,f7,f8
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + ctx.f8.f64));
	// fmadds f4,f12,f6,f5
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f5.f64));
	// fmadds f3,f11,f10,f4
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f4.f64));
	// stfs f3,0(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f2,20(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f2,f1
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f13,52(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,36(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 36);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f7,f13,f12,f0
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f0.f64));
	// fmadds f6,f11,f10,f7
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f7.f64));
	// fmadds f5,f9,f8,f6
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f6.f64));
	// stfs f5,4(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f4,24(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f1,56(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 56);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f9,f1,f0,f2
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f2.f64));
	// fmadds f8,f13,f12,f9
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fmadds f7,f11,f10,f8
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f8.f64));
	// stfs f7,8(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// lfs f6,28(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 28);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f6,f5
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f3,60(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 60);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,44(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f3,f2,f4
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f4.f64));
	// fmadds f10,f1,f0,f11
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fmadds f9,f13,f12,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f9,12(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_ED88"))) PPC_WEAK_FUNC(game_ED88);
PPC_FUNC_IMPL(__imp__game_ED88) {
	PPC_FUNC_PROLOGUE();
	double var_f21 = 0.0;
	double var_f16 = 0.0;
	double var_f20 = 0.0;
	double var_f24 = 0.0;
	double var_f15 = 0.0;
	double var_f25 = 0.0;
	double var_f19 = 0.0;
	double var_f18 = 0.0;
	double var_f17 = 0.0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	double var_f30 = 0.0;
	double var_f28 = 0.0;
	double var_f23 = 0.0;
	double var_f26 = 0.0;
	double var_f27 = 0.0;
	double var_f22 = 0.0;
	double var_f14 = 0.0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x824365e0
	__savefpr_14(ctx, base);
	// lfs f13,20(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f21,f13,f12
	var_f21 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmuls f16,f13,f10
	var_f16 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f20,f13,f6
	var_f20 = double(float(ctx.f13.f64 * ctx.f6.f64));
	// lfs f24,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	var_f24 = double(temp.f32);
	// lfs f8,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f15,f13,f8
	var_f15 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f25,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	var_f25 = double(temp.f32);
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f19,f25,f12
	var_f19 = double(float(var_f25 * ctx.f12.f64));
	// lfs f3,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f18,f10,f25
	var_f18 = double(float(ctx.f10.f64 * var_f25));
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f17,f8,f25
	var_f17 = double(float(ctx.f8.f64 * var_f25));
	// fmadds f21,f24,f11,f21
	var_f21 = double(float(var_f24 * ctx.f11.f64 + var_f21));
	// lfs f4,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f16,f24,f9,f16
	var_f16 = double(float(var_f24 * ctx.f9.f64 + var_f16));
	// lfs f2,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f20,f5,f24,f20
	var_f20 = double(float(ctx.f5.f64 * var_f24 + var_f20));
	// lfs f0,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	var_f31 = double(temp.f32);
	// lfs f29,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	var_f29 = double(temp.f32);
	// fmadds f24,f24,f7,f15
	var_f24 = double(float(var_f24 * ctx.f7.f64 + var_f15));
	// lfs f1,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	var_f30 = double(temp.f32);
	// fmadds f19,f0,f11,f19
	var_f19 = double(float(ctx.f0.f64 * ctx.f11.f64 + var_f19));
	// lfs f28,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	var_f28 = double(temp.f32);
	// fmadds f18,f9,f0,f18
	var_f18 = double(float(ctx.f9.f64 * ctx.f0.f64 + var_f18));
	// lfs f23,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	var_f23 = double(temp.f32);
	// fmadds f17,f7,f0,f17
	var_f17 = double(float(ctx.f7.f64 * ctx.f0.f64 + var_f17));
	// lfs f26,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	var_f26 = double(temp.f32);
	// fmadds f21,f3,f4,f21
	var_f21 = double(float(ctx.f3.f64 * ctx.f4.f64 + var_f21));
	// lfs f27,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	var_f27 = double(temp.f32);
	// fmadds f16,f3,f2,f16
	var_f16 = double(float(ctx.f3.f64 * ctx.f2.f64 + var_f16));
	// lfs f22,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	var_f22 = double(temp.f32);
	// fmadds f20,f3,f31,f20
	var_f20 = double(float(ctx.f3.f64 * var_f31 + var_f20));
	// lfs f13,60(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f3,f3,f1,f24
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f1.f64 + var_f24));
	// fmadds f19,f23,f4,f19
	var_f19 = double(float(var_f23 * ctx.f4.f64 + var_f19));
	// fmadds f18,f2,f23,f18
	var_f18 = double(float(ctx.f2.f64 * var_f23 + var_f18));
	// fmadds f17,f1,f23,f17
	var_f17 = double(float(ctx.f1.f64 * var_f23 + var_f17));
	// fmadds f21,f29,f30,f21
	var_f21 = double(float(var_f29 * var_f30 + var_f21));
	// stfs f21,-176(r1)
	temp.f32 = float(var_f21);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// lfs f21,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	var_f21 = double(temp.f32);
	// fmadds f24,f29,f28,f16
	var_f24 = double(float(var_f29 * var_f28 + var_f16));
	// stfs f24,-172(r1)
	temp.f32 = float(var_f24);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fmuls f16,f21,f12
	var_f16 = double(float(var_f21 * ctx.f12.f64));
	// fmuls f24,f21,f6
	var_f24 = double(float(var_f21 * ctx.f6.f64));
	// fmadds f20,f29,f26,f20
	var_f20 = double(float(var_f29 * var_f26 + var_f20));
	// stfs f20,-180(r1)
	temp.f32 = float(var_f20);
	PPC_STORE_U32(ctx.r1.u32 + -180, temp.u32);
	// lfs f20,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	var_f20 = double(temp.f32);
	// fmadds f3,f29,f27,f3
	ctx.f3.f64 = double(float(var_f29 * var_f27 + ctx.f3.f64));
	// fmuls f15,f21,f10
	var_f15 = double(float(var_f21 * ctx.f10.f64));
	// stfs f3,-168(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// fmuls f21,f21,f8
	var_f21 = double(float(var_f21 * ctx.f8.f64));
	// lfs f3,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f14,f20,f6
	var_f14 = double(float(var_f20 * ctx.f6.f64));
	// lfs f29,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	var_f29 = double(temp.f32);
	// fmuls f12,f20,f12
	ctx.f12.f64 = double(float(var_f20 * ctx.f12.f64));
	// fmadds f19,f22,f30,f19
	var_f19 = double(float(var_f22 * var_f30 + var_f19));
	// stfs f19,-192(r1)
	temp.f32 = float(var_f19);
	PPC_STORE_U32(ctx.r1.u32 + -192, temp.u32);
	// fmadds f19,f28,f22,f18
	var_f19 = double(float(var_f28 * var_f22 + var_f18));
	// stfs f19,-188(r1)
	temp.f32 = float(var_f19);
	PPC_STORE_U32(ctx.r1.u32 + -188, temp.u32);
	// fmadds f16,f3,f11,f16
	var_f16 = double(float(ctx.f3.f64 * ctx.f11.f64 + var_f16));
	// lfs f18,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	var_f18 = double(temp.f32);
	// fmadds f24,f3,f5,f24
	var_f24 = double(float(ctx.f3.f64 * ctx.f5.f64 + var_f24));
	// fmadds f19,f27,f22,f17
	var_f19 = double(float(var_f27 * var_f22 + var_f17));
	// stfs f19,-184(r1)
	temp.f32 = float(var_f19);
	PPC_STORE_U32(ctx.r1.u32 + -184, temp.u32);
	// lfs f19,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	var_f19 = double(temp.f32);
	// fmadds f15,f3,f9,f15
	var_f15 = double(float(ctx.f3.f64 * ctx.f9.f64 + var_f15));
	// lfs f17,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	var_f17 = double(temp.f32);
	// fmadds f3,f3,f7,f21
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f7.f64 + var_f21));
	// fmadds f21,f29,f5,f14
	var_f21 = double(float(var_f29 * ctx.f5.f64 + var_f14));
	// fmadds f14,f29,f11,f12
	var_f14 = double(float(var_f29 * ctx.f11.f64 + ctx.f12.f64));
	// fmadds f11,f19,f4,f16
	ctx.f11.f64 = double(float(var_f19 * ctx.f4.f64 + var_f16));
	// fmadds f12,f19,f31,f24
	ctx.f12.f64 = double(float(var_f19 * var_f31 + var_f24));
	// fmuls f10,f20,f10
	ctx.f10.f64 = double(float(var_f20 * ctx.f10.f64));
	// fmuls f8,f20,f8
	ctx.f8.f64 = double(float(var_f20 * ctx.f8.f64));
	// fmuls f6,f6,f25
	ctx.f6.f64 = double(float(ctx.f6.f64 * var_f25));
	// fmadds f21,f18,f31,f21
	var_f21 = double(float(var_f18 * var_f31 + var_f21));
	// fmadds f24,f19,f2,f15
	var_f24 = double(float(var_f19 * ctx.f2.f64 + var_f15));
	// fmadds f19,f19,f1,f3
	var_f19 = double(float(var_f19 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f12,f17,f26,f12
	ctx.f12.f64 = double(float(var_f17 * var_f26 + ctx.f12.f64));
	// fmadds f11,f17,f30,f11
	ctx.f11.f64 = double(float(var_f17 * var_f30 + ctx.f11.f64));
	// fmadds f10,f29,f9,f10
	ctx.f10.f64 = double(float(var_f29 * ctx.f9.f64 + ctx.f10.f64));
	// fmadds f9,f29,f7,f8
	ctx.f9.f64 = double(float(var_f29 * ctx.f7.f64 + ctx.f8.f64));
	// fmadds f8,f18,f4,f14
	ctx.f8.f64 = double(float(var_f18 * ctx.f4.f64 + var_f14));
	// fmadds f21,f13,f26,f21
	var_f21 = double(float(ctx.f13.f64 * var_f26 + var_f21));
	// fmadds f3,f17,f28,f24
	ctx.f3.f64 = double(float(var_f17 * var_f28 + var_f24));
	// fmadds f24,f17,f27,f19
	var_f24 = double(float(var_f17 * var_f27 + var_f19));
	// fmadds f7,f18,f2,f10
	ctx.f7.f64 = double(float(var_f18 * ctx.f2.f64 + ctx.f10.f64));
	// lfs f2,-192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -192);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f4,f18,f1,f9
	ctx.f4.f64 = double(float(var_f18 * ctx.f1.f64 + ctx.f9.f64));
	// lfs f1,-188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -188);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f10,f13,f30,f8
	ctx.f10.f64 = double(float(ctx.f13.f64 * var_f30 + ctx.f8.f64));
	// stfs f2,4(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// fmadds f8,f5,f0,f6
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f6.f64));
	// stfs f1,8(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// lfs f2,-176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -176);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,-172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -172);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f9,f13,f28,f7
	ctx.f9.f64 = double(float(ctx.f13.f64 * var_f28 + ctx.f7.f64));
	// lfs f7,-184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -184);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f0,f13,f27,f4
	ctx.f0.f64 = double(float(ctx.f13.f64 * var_f27 + ctx.f4.f64));
	// stfs f7,12(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// lfs f4,-180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -180);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f6,f31,f23,f8
	ctx.f6.f64 = double(float(var_f31 * var_f23 + ctx.f8.f64));
	// lfs f13,-168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -168);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f5,f26,f22,f6
	ctx.f5.f64 = double(float(var_f26 * var_f22 + ctx.f6.f64));
	// stfs f5,0(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f4,16(r3)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// stfs f2,20(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stfs f1,24(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f13,28(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// stfs f12,32(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// stfs f11,36(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stfs f3,40(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 40, temp.u32);
	// stfs f24,44(r3)
	temp.f32 = float(var_f24);
	PPC_STORE_U32(ctx.r3.u32 + 44, temp.u32);
	// stfs f21,48(r3)
	temp.f32 = float(var_f21);
	PPC_STORE_U32(ctx.r3.u32 + 48, temp.u32);
	// stfs f10,52(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// stfs f9,56(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 56, temp.u32);
	// stfs f0,60(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 60, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8243662c
	__restfpr_14(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr
	return;
}

__attribute__((alias("__imp__game_EFA8"))) PPC_WEAK_FUNC(game_EFA8);
PPC_FUNC_IMPL(__imp__game_EFA8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8223ecb0
	game_ECB0(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// addi r4,r29,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 16;
	// addi r3,r31,16
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 16;
	// bl 0x8223ecb0
	game_ECB0(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// addi r4,r29,32
	ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 32;
	// addi r3,r31,32
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 32;
	// bl 0x8223ecb0
	game_ECB0(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// addi r4,r29,48
	ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 48;
	// addi r3,r31,48
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 48;
	// bl 0x8223ecb0
	game_ECB0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__phBoundCapsule_F008_wrh"))) PPC_WEAK_FUNC(phBoundCapsule_F008_wrh);
PPC_FUNC_IMPL(__imp__phBoundCapsule_F008_wrh) {
	PPC_FUNC_PROLOGUE();
	double var_f24 = 0.0;
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	double var_f27 = 0.0;
	double var_f25 = 0.0;
	double var_f22 = 0.0;
	double var_f30 = 0.0;
	double var_f28 = 0.0;
	double var_f26 = 0.0;
	double var_f23 = 0.0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82436600
	__savefpr_22(ctx, base);
	// lfs f12,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f0,f12
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// lfs f13,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f10,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f24,f0,f12
	var_f24 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// lfs f8,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f29,f8,f11
	var_f29 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// lfs f5,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f31,f8,f11
	var_f31 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// lfs f7,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f27,f3,f5
	var_f27 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// lfs f6,60(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f25,f3,f11
	var_f25 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// lfs f9,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f22,f0,f5
	var_f22 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// lfs f4,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f30,f8,f13
	var_f30 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f1,f1,f10
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// fmuls f2,f2,f10
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// fmuls f28,f4,f5
	var_f28 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f26,f4,f11
	var_f26 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f24,f24,f9
	var_f24 = double(float(var_f24 * ctx.f9.f64));
	// fmuls f23,f0,f5
	var_f23 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fmadds f1,f29,f7,f1
	ctx.f1.f64 = double(float(var_f29 * ctx.f7.f64 + ctx.f1.f64));
	// fmadds f2,f31,f9,f2
	ctx.f2.f64 = double(float(var_f31 * ctx.f9.f64 + ctx.f2.f64));
	// fmuls f31,f8,f12
	var_f31 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// fmuls f29,f4,f11
	var_f29 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f11,f3,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmadds f1,f27,f6,f1
	ctx.f1.f64 = double(float(var_f27 * ctx.f6.f64 + ctx.f1.f64));
	// lfs f27,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	var_f27 = double(temp.f32);
	// fmadds f2,f28,f6,f2
	ctx.f2.f64 = double(float(var_f28 * ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f3,f13
	var_f28 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f29,f29,f7,f24
	var_f29 = double(float(var_f29 * ctx.f7.f64 + var_f24));
	// fnmsubs f1,f25,f10,f1
	ctx.f1.f64 = double(float(-(var_f25 * ctx.f10.f64 - ctx.f1.f64)));
	// fnmsubs f2,f26,f10,f2
	ctx.f2.f64 = double(float(-(var_f26 * ctx.f10.f64 - ctx.f2.f64)));
	// fmuls f26,f4,f12
	var_f26 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// fmadds f29,f28,f6,f29
	var_f29 = double(float(var_f28 * ctx.f6.f64 + var_f29));
	// lfs f28,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	var_f28 = double(temp.f32);
	// fnmsubs f1,f31,f6,f1
	ctx.f1.f64 = double(float(-(var_f31 * ctx.f6.f64 - ctx.f1.f64)));
	// lfs f31,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phBoundCapsule::flags@+0x4 */;
	var_f31 = double(temp.f32);
	// fnmsubs f2,f30,f6,f2
	ctx.f2.f64 = double(float(-(var_f30 * ctx.f6.f64 - ctx.f2.f64)));
	// lfs f30,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phBoundCapsule::vtable@+0x0 */;
	var_f30 = double(temp.f32);
	// fnmsubs f11,f11,f9,f29
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f9.f64 - var_f29)));
	// fnmsubs f1,f22,f7,f1
	ctx.f1.f64 = double(float(-(var_f22 * ctx.f7.f64 - ctx.f1.f64)));
	// fnmsubs f2,f23,f9,f2
	ctx.f2.f64 = double(float(-(var_f23 * ctx.f9.f64 - ctx.f2.f64)));
	// fmuls f1,f31,f1
	ctx.f1.f64 = double(float(var_f31 * ctx.f1.f64));
	// fmsubs f2,f30,f2,f1
	ctx.f2.f64 = double(float(var_f30 * ctx.f2.f64 - ctx.f1.f64));
	// fnmsubs f1,f26,f6,f11
	ctx.f1.f64 = double(float(-(var_f26 * ctx.f6.f64 - ctx.f11.f64)));
	// fmuls f11,f8,f12
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// fmuls f6,f4,f5
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f4,f4,f12
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// fmuls f12,f3,f5
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// fmuls f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fnmsubs f0,f0,f7,f1
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f7.f64 - ctx.f1.f64)));
	// fmuls f11,f11,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f1,f3,f13
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f2,f28,f0,f2
	ctx.f2.f64 = double(float(var_f28 * ctx.f0.f64 + ctx.f2.f64));
	// fmadds f6,f6,f7,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f7.f64 + ctx.f11.f64));
	// fmadds f5,f1,f10,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f10.f64 + ctx.f6.f64));
	// fnmsubs f3,f12,f9,f5
	ctx.f3.f64 = double(float(-(ctx.f12.f64 * ctx.f9.f64 - ctx.f5.f64)));
	// fnmsubs f1,f4,f10,f3
	ctx.f1.f64 = double(float(-(ctx.f4.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fnmsubs f0,f8,f7,f1
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f7.f64 - ctx.f1.f64)));
	// fnmsubs f1,f27,f0,f2
	ctx.f1.f64 = double(float(-(var_f27 * ctx.f0.f64 - ctx.f2.f64)));
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x8243664c
	__restfpr_22(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_F140_g"))) PPC_WEAK_FUNC(phBoundCapsule_F140_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_F140_g) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x8223f008
	phBoundCapsule_F008_wrh(ctx, base);
	// lfs f13,28(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32253
	// fmuls f4,f13,f12
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f8,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// lfs f11,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f8,f12
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f11
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// lfs f10,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f12,f5,f11
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// lfs f6,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f6.f64 = double(temp.f32);
	// fdivs f0,f0,f1
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f1.f64));
	// lfs f9,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f13,f6
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f6.f64));
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f11,f4,f10
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// fmadds f6,f3,f9,f11
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f9.f64 + ctx.f11.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f12,f10,f5
	ctx.f4.f64 = double(float(-(ctx.f12.f64 * ctx.f10.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f9,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f9.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lfs f13,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,16(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// lfs f8,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f4,f7,f8
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// lfs f6,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// lfs f12,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f2,f6,f8
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f10,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f6,f10
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f9,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f9.f64 = double(temp.f32);
	// lfs f11,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f7,f9
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// lfs f5,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f5,f9
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f13,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// fmuls f8,f4,f12
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// fmadds f7,f3,f11,f8
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f8.f64));
	// fmadds f4,f6,f13,f7
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f13.f64 + ctx.f7.f64));
	// fnmsubs f3,f5,f12,f4
	ctx.f3.f64 = double(float(-(ctx.f5.f64 * ctx.f12.f64 - ctx.f4.f64)));
	// fnmsubs f2,f2,f13,f3
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f13.f64 - ctx.f3.f64)));
	// fnmsubs f1,f1,f11,f2
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// fmuls f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f13,32(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 32, temp.u32);
	// lfs f13,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f13,f12,f6
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f5,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f7,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f9,f5,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f12,f4,f10
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// fmadds f8,f3,f11,f12
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f12.f64));
	// fmadds f6,f1,f7,f8
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f8.f64));
	// fnmsubs f5,f2,f7,f6
	ctx.f5.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f6.f64)));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f9,f10,f4
	ctx.f3.f64 = double(float(-(ctx.f9.f64 * ctx.f10.f64 - ctx.f4.f64)));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fneg f1,f2
	ctx.f1.u64 = ctx.f2.u64 ^ 0x8000000000000000;
	// stfs f1,48(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 48, temp.u32);
	// lfs f13,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// lfs f13,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,20(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// lfs f13,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,36(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 36, temp.u32);
	// lfs f13,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f2,f9,f13
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f9,f10
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f11,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f10
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// lfs f6,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f6.f64 = double(temp.f32);
	// lfs f8,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f9,f12,f6
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f10,f4,f11
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f8,f10
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f10.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f9,f8,f3
	ctx.f2.f64 = double(float(-(ctx.f9.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,52(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 52, temp.u32);
	// lfs f13,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,8(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// lfs f13,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,24(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 24, temp.u32);
	// lfs f13,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f10,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f3,f8,f10
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// lfs f11,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f7,f10
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f1,f7,f5
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// lfs f6,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f10,f4,f11
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f7,f3,f9,f10
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmadds f4,f1,f6,f7
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmuls f1,f12,f5
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// fnmsubs f3,f2,f11,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f4.f64)));
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fnmsubs f13,f2,f6,f3
	ctx.f13.f64 = double(float(-(ctx.f2.f64 * ctx.f6.f64 - ctx.f3.f64)));
	// fnmsubs f12,f1,f9,f13
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,40(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 40, temp.u32);
	// lfs f13,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,56(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 56, temp.u32);
	// lfs f13,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,12(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// lfs f13,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f9,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f6,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f6,f7
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f10,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f1,f4,f11
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f4,f3,f10,f1
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f1.f64));
	// fmuls f1,f8,f13
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f13,f12,f7
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// fmuls f3,f6,f9
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// fmadds f12,f2,f5,f4
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 + ctx.f4.f64));
	// fnmsubs f11,f3,f11,f12
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fnmsubs f9,f1,f5,f11
	ctx.f9.f64 = double(float(-(ctx.f1.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// fnmsubs f8,f13,f10,f9
	ctx.f8.f64 = double(float(-(ctx.f13.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,28(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 28, temp.u32);
	// lfs f13,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,44(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 44);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f13,f1
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f13,44(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 44, temp.u32);
	// lfs f13,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f12,f13
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f5,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f11,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f8,f12,f6
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f7,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f4,f11
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmadds f6,f3,f10,f9
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f5,f1,f7,f6
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fnmsubs f4,f13,f11,f5
	ctx.f4.f64 = double(float(-(ctx.f13.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f3,f2,f7,f4
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f2,f8,f10,f3
	ctx.f2.f64 = double(float(-(ctx.f8.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,60(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 60, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_F758"))) PPC_WEAK_FUNC(game_F758);
PPC_FUNC_IMPL(__imp__game_F758) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32253
	// lfs f0,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223f77c
	if (ctx.f0.f64 < ctx.f13.f64) {
		// fmr f10,f13
		ctx.f10.f64 = ctx.f13.f64;
		// b 0x8223f790
	} else {
	loc_8223F77C:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223f78c
		if (ctx.f0.f64 > ctx.f12.f64) {
			// fmr f10,f12
			ctx.f10.f64 = ctx.f12.f64;
			// b 0x8223f790
		} else {
		loc_8223F78C:
			// fmr f10,f0
			ctx.fpscr.disableFlushMode();
			ctx.f10.f64 = ctx.f0.f64;
		}
	}
loc_8223F790:
	// lfs f0,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223f7a4
	if (ctx.f0.f64 < ctx.f13.f64) {
		// fmr f11,f13
		ctx.f11.f64 = ctx.f13.f64;
		// b 0x8223f7b8
	} else {
	loc_8223F7A4:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223f7b4
		if (ctx.f0.f64 > ctx.f12.f64) {
			// fmr f11,f12
			ctx.f11.f64 = ctx.f12.f64;
			// b 0x8223f7b8
		} else {
		loc_8223F7B4:
			// fmr f11,f0
			ctx.fpscr.disableFlushMode();
			ctx.f11.f64 = ctx.f0.f64;
		}
	}
loc_8223F7B8:
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// blt cr6,0x8223f7d8
	if (ctx.f0.f64 >= ctx.f13.f64) {
		// fcmpu cr6,f0,f12
		// ble cr6,0x8223f7d4
		if (ctx.f0.f64 > ctx.f12.f64) {
			// fmr f13,f12
			ctx.f13.f64 = ctx.f12.f64;
			// b 0x8223f7d8
		} else {
		loc_8223F7D4:
			// fmr f13,f0
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = ctx.f0.f64;
		}
	}
loc_8223F7D8:
	// lis r11,-32248
	// lis r10,-1
	// lfs f0,-25632(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25632);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f12,f11,f0
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fctiwz f10,f13
	ctx.f10.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f10,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f10.u32);
	// fctiwz f9,f12
	ctx.f9.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// fctiwz f8,f11
	ctx.f8.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f11.f64));
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// clrlwi r7,r9,24
	ctx.r7.u64 = ctx.r9.u32 & 0xFF;
	// rlwimi r10,r7,8,16,23
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r7.u32, 8) & 0xFF00) | (ctx.r10.u64 & 0xFFFFFFFFFFFF00FF);
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// lwz r6,-16(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r5,r1,-16
	ctx.r5.s64 = ctx.r1.s64 + -16;
	// clrlwi r11,r6,24
	ctx.r11.u64 = ctx.r6.u32 & 0xFF;
	// or r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 | ctx.r11.u64;
	// stfiwx f8,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f8.u32);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// clrlwi r8,r9,24
	ctx.r8.u64 = ctx.r9.u32 & 0xFF;
	// rlwimi r8,r10,8,0,23
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r10.u32, 8) & 0xFFFFFF00) | (ctx.r8.u64 & 0xFFFFFFFF000000FF);
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// blr
	return;
}

__attribute__((alias("__imp__util_F840"))) PPC_WEAK_FUNC(util_F840);
PPC_FUNC_IMPL(__imp__util_F840) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32253
	// lfs f0,12(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223f864
	if (ctx.f0.f64 < ctx.f13.f64) {
		// fmr f9,f13
		ctx.f9.f64 = ctx.f13.f64;
		// b 0x8223f878
	} else {
	loc_8223F864:
		// fcmpu cr6,f0,f11
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223f874
		if (ctx.f0.f64 > ctx.f11.f64) {
			// fmr f9,f11
			ctx.f9.f64 = ctx.f11.f64;
			// b 0x8223f878
		} else {
		loc_8223F874:
			// fmr f9,f0
			ctx.fpscr.disableFlushMode();
			ctx.f9.f64 = ctx.f0.f64;
		}
	}
loc_8223F878:
	// lfs f0,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223f88c
	if (ctx.f0.f64 < ctx.f13.f64) {
		// fmr f10,f13
		ctx.f10.f64 = ctx.f13.f64;
		// b 0x8223f8a0
	} else {
	loc_8223F88C:
		// fcmpu cr6,f0,f11
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223f89c
		if (ctx.f0.f64 > ctx.f11.f64) {
			// fmr f10,f11
			ctx.f10.f64 = ctx.f11.f64;
			// b 0x8223f8a0
		} else {
		loc_8223F89C:
			// fmr f10,f0
			ctx.fpscr.disableFlushMode();
			ctx.f10.f64 = ctx.f0.f64;
		}
	}
loc_8223F8A0:
	// lfs f0,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// bge cr6,0x8223f8b4
	if (ctx.f0.f64 < ctx.f13.f64) {
		// fmr f12,f13
		ctx.f12.f64 = ctx.f13.f64;
		// b 0x8223f8c8
	} else {
	loc_8223F8B4:
		// fcmpu cr6,f0,f11
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8223f8c4
		if (ctx.f0.f64 > ctx.f11.f64) {
			// fmr f12,f11
			ctx.f12.f64 = ctx.f11.f64;
			// b 0x8223f8c8
		} else {
		loc_8223F8C4:
			// fmr f12,f0
			ctx.fpscr.disableFlushMode();
			ctx.f12.f64 = ctx.f0.f64;
		}
	}
loc_8223F8C8:
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// blt cr6,0x8223f8e8
	if (ctx.f0.f64 >= ctx.f13.f64) {
		// fcmpu cr6,f0,f11
		// ble cr6,0x8223f8e4
		if (ctx.f0.f64 > ctx.f11.f64) {
			// fmr f13,f11
			ctx.f13.f64 = ctx.f11.f64;
			// b 0x8223f8e8
		} else {
		loc_8223F8E4:
			// fmr f13,f0
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = ctx.f0.f64;
		}
	}
loc_8223F8E8:
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25632(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25632);  /* glob:lbl_82079BE0 @ 0x82079be0 */
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// fmuls f11,f9,f0
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f8,f12,f0
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fctiwz f6,f11
	ctx.f6.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f11.f64));
	// stfiwx f6,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f6.u32);
	// fctiwz f5,f9
	ctx.f5.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
	// fctiwz f4,f8
	ctx.f4.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f8.f64));
	// fctiwz f3,f7
	ctx.f3.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f7.f64));
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// clrlwi r8,r10,24
	ctx.r8.u64 = ctx.r10.u32 & 0xFF;
	// stfiwx f5,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f5.u32);
	// lwz r7,-16(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// clrlwi r5,r7,24
	ctx.r5.u64 = ctx.r7.u32 & 0xFF;
	// rlwimi r5,r8,8,16,23
	ctx.r5.u64 = (__builtin_rotateleft32(ctx.r8.u32, 8) & 0xFF00) | (ctx.r5.u64 & 0xFFFFFFFFFFFF00FF);
	// stfiwx f4,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f4.u32);
	// clrlwi r4,r5,16
	ctx.r4.u64 = ctx.r5.u32 & 0xFFFF;
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// clrlwi r9,r11,24
	ctx.r9.u64 = ctx.r11.u32 & 0xFF;
	// rlwimi r9,r4,8,0,23
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r4.u32, 8) & 0xFFFFFF00) | (ctx.r9.u64 & 0xFFFFFFFF000000FF);
	// stfiwx f3,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f3.u32);
	// lwz r8,-16(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// clrlwi r7,r8,24
	ctx.r7.u64 = ctx.r8.u32 & 0xFF;
	// rlwimi r7,r9,8,0,23
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r9.u32, 8) & 0xFFFFFF00) | (ctx.r7.u64 & 0xFFFFFFFF000000FF);
	// stw r7,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_F968_p46"))) PPC_WEAK_FUNC(lvlLevelMgr_F968_p46);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_F968_p46) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32253
	// lfs f12,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-9752(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -9752);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25800(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25800);  /* glob:lbl_82079B38 @ 0x82079b38 */
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fnmsubs f10,f11,f0,f0
	ctx.f10.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f0.f64)));
	// fctiwz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f10.f64));
	// stfiwx f9,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f9.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r11,0
	// bge cr6,0x8223f9a4
	if (ctx.r11.s32 < 0) {
		// li r9,0
		ctx.r9.s64 = 0;
		// b 0x8223f9b4
	} else {
	loc_8223F9A4:
		// cmpwi cr6,r11,2047
		// li r9,2047
		ctx.r9.s64 = 2047;
		// bgt cr6,0x8223f9b4
		if (ctx.r11.s32 > 2047) goto loc_8223F9B4;
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8223F9B4:
	// lfs f8,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fnmsubs f6,f7,f0,f0
	ctx.f6.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f0.f64)));
	// fctiwz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f5.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r11,0
	// bge cr6,0x8223f9e0
	if (ctx.r11.s32 < 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x8223f9ec
	} else {
	loc_8223F9E0:
		// cmpwi cr6,r11,2047
		// ble cr6,0x8223f9ec
		if (ctx.r11.s32 <= 2047) goto loc_8223F9EC;
		// li r11,2047
		ctx.r11.s64 = 2047;
	}
loc_8223F9EC:
	// lfs f4,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// lis r10,-32248
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// rlwinm r7,r11,11,0,20
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 11) & 0xFFFFF800;
	// or r11,r7,r9
	ctx.r11.u64 = ctx.r7.u64 | ctx.r9.u64;
	// lfs f0,-25804(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25804);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f2,f3,f0,f0
	ctx.f2.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f0.f64)));
	// fctiwz f1,f2
	ctx.f1.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f2.f64));
	// stfiwx f1,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f1.u32);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r10,0
	// bge cr6,0x8223fa30
	if (ctx.r10.s32 < 0) {
		// li r10,0
		ctx.r10.s64 = 0;
		// rlwinm r6,r10,22,0,9
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 22) & 0xFFC00000;
		// or r3,r6,r11
		ctx.r3.u64 = ctx.r6.u64 | ctx.r11.u64;
		// blr
		return;
	}
loc_8223FA30:
	// cmpwi cr6,r10,1023
	// ble cr6,0x8223fa48
	if (ctx.r10.s32 > 1023) {
		// li r10,1023
		ctx.r10.s64 = 1023;
		// rlwinm r5,r10,22,0,9
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 22) & 0xFFC00000;
		// or r3,r5,r11
		ctx.r3.u64 = ctx.r5.u64 | ctx.r11.u64;
		// blr
		return;
	}
loc_8223FA48:
	// rlwinm r4,r10,22,0,9
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 22) & 0xFFC00000;
	// or r3,r4,r11
	ctx.r3.u64 = ctx.r4.u64 | ctx.r11.u64;
	// blr
	return;
}

__attribute__((alias("__imp__ph_ctor_FA58"))) PPC_WEAK_FUNC(ph_ctor_FA58);
PPC_FUNC_IMPL(__imp__ph_ctor_FA58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r10,-32253
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// addi r10,r10,-12016
	ctx.r10.s64 = ctx.r10.s64 + -12016;
	// lis r8,-32253
	// li r5,-1
	// addi r8,r8,9220
	ctx.r8.s64 = ctx.r8.s64 + 9220;
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// addi r9,r10,7276
	ctx.r9.s64 = ctx.r10.s64 + 7276;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// lis r9,-32251
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// addi r7,r9,-3236
	ctx.r7.s64 = ctx.r9.s64 + -3236;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// lis r9,-32251
	// stw r10,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r10.u32);
	// stw r10,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r10.u32);
	// addi r6,r9,-3064
	ctx.r6.s64 = ctx.r9.s64 + -3064;
	// stw r10,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, ctx.r10.u32);
	// addi r9,r11,48
	ctx.r9.s64 = ctx.r11.s64 + 48;
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stfs f0,32(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stfs f0,36(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 36, temp.u32);
	// stfs f13,40(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 40, temp.u32);
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stw r8,96(r3)
	PPC_STORE_U32(ctx.r3.u32 + 96, ctx.r8.u32);
	// stfs f0,100(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 100, temp.u32);
	// stw r10,104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 104, ctx.r10.u32);
	// stw r5,108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 108, ctx.r5.u32);
	// stw r7,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r7.u32);
	// stw r6,96(r3)
	PPC_STORE_U32(ctx.r3.u32 + 96, ctx.r6.u32);
	// stb r10,112(r3)
	PPC_STORE_U8(ctx.r3.u32 + 112, ctx.r10.u8);
	// stb r4,113(r3)
	PPC_STORE_U8(ctx.r3.u32 + 113, ctx.r4.u8);
	// stb r10,114(r3)
	PPC_STORE_U8(ctx.r3.u32 + 114, ctx.r10.u8);
	// stw r10,116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 116, ctx.r10.u32);
	// stw r10,120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 120, ctx.r10.u32);
	// stw r10,124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 124, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__lvlTableTop_vfn_34"))) PPC_WEAK_FUNC(lvlTableTop_vfn_34);
PPC_FUNC_IMPL(__imp__lvlTableTop_vfn_34) {
	PPC_FUNC_PROLOGUE();
	// lbz r3,113(r3)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 113);
	// blr
	return;
}

__attribute__((alias("__imp__lvlTableTop_vfn_0"))) PPC_WEAK_FUNC(lvlTableTop_vfn_0);
PPC_FUNC_IMPL(__imp__lvlTableTop_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x823d90a0
	game_90A0(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223fb58
	if (ctx.r11.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8223FB58:
	// blr
	return;
}

__attribute__((alias("__imp__ph_ctor_FB70"))) PPC_WEAK_FUNC(ph_ctor_FB70);
PPC_FUNC_IMPL(__imp__ph_ctor_FB70) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lis r11,-32253
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,-1
	// lis r6,-32160
	// lfs f0,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// addi r10,r11,31540
	ctx.r10.s64 = ctx.r11.s64 + 31540;
	// lis r11,-32251
	// stw r3,25956(r6)
	PPC_STORE_U32(ctx.r6.u32 + 25956, ctx.r3.u32);
	// addi r9,r11,-3388
	ctx.r9.s64 = ctx.r11.s64 + -3388;
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r7,r11,-3256
	ctx.r7.s64 = ctx.r11.s64 + -3256;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r11.u32);
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// lis r10,-32163
	// stfs f0,24(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// addi r10,r10,-18128
	ctx.r10.s64 = ctx.r10.s64 + -18128;
	// stfs f0,32(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// stfs f0,40(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 40, temp.u32);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// stw r4,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r4.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r11.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r11.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r11.u32);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r11.u32);
	// stb r11,56(r3)
	PPC_STORE_U8(ctx.r3.u32 + 56, ctx.r11.u8);
	// stb r11,57(r3)
	PPC_STORE_U8(ctx.r3.u32 + 57, ctx.r11.u8);
	// stb r5,58(r3)
	PPC_STORE_U8(ctx.r3.u32 + 58, ctx.r5.u8);
	// stb r5,59(r3)
	PPC_STORE_U8(ctx.r3.u32 + 59, ctx.r5.u8);
	// stb r11,60(r3)
	PPC_STORE_U8(ctx.r3.u32 + 60, ctx.r11.u8);
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r11.u32);
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r11.u32);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, ctx.r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r11.u32);
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, ctx.r11.u32);
	// stw r11,88(r3)
	PPC_STORE_U32(ctx.r3.u32 + 88, ctx.r11.u32);
	// stw r11,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, ctx.r11.u32);
	// stb r11,96(r3)
	PPC_STORE_U8(ctx.r3.u32 + 96, ctx.r11.u8);
	// stb r11,97(r3)
	PPC_STORE_U8(ctx.r3.u32 + 97, ctx.r11.u8);
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplwi cr6,r10,0
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// bne cr6,0x8223fc40
	if (ctx.r10.u32 == 0) {
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	}
loc_8223FC40:
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x8223fc50
	if (ctx.r9.u32 != 0) {
		// stb r11,58(r3)
		PPC_STORE_U8(ctx.r3.u32 + 58, ctx.r11.u8);
	}
loc_8223FC50:
	// lis r10,-32163
	// addi r10,r10,-18108
	ctx.r10.s64 = ctx.r10.s64 + -18108;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplwi cr6,r10,0
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// bne cr6,0x8223fc6c
	if (ctx.r10.u32 == 0) {
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	}
loc_8223FC6C:
	// clrlwi r6,r10,24
	ctx.r6.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8223fc7c
	if (ctx.r6.u32 != 0) {
		// stb r11,59(r3)
		PPC_STORE_U8(ctx.r3.u32 + 59, ctx.r11.u8);
	}
loc_8223FC7C:
	// lis r10,-32163
	// addi r10,r10,-18088
	ctx.r10.s64 = ctx.r10.s64 + -18088;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplwi cr6,r10,0
	// beq cr6,0x8223fc94
	if (ctx.r10.u32 != 0) {
		// mr r11,r5
		ctx.r11.u64 = ctx.r5.u64;
	}
loc_8223FC94:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223fca4
	if (ctx.r11.u32 != 0) {
		// stb r5,60(r3)
		PPC_STORE_U8(ctx.r3.u32 + 60, ctx.r5.u8);
	}
loc_8223FCA4:
	// lis r9,-32161
	// lis r7,-32161
	// li r4,128
	ctx.r4.s64 = 128;
	// addi r7,r7,14952
	ctx.r7.s64 = ctx.r7.s64 + 14952;
	// lis r6,-32161
	// lwz r11,14436(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 14436);
	// lis r5,-32161
	// addi r6,r6,14440
	ctx.r6.s64 = ctx.r6.s64 + 14440;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r5,14696
	ctx.r5.s64 = ctx.r5.s64 + 14696;
	// li r31,514
	var_r31 = 514;
	// stbx r4,r11,r7
	PPC_STORE_U8(ctx.r11.u32 + ctx.r7.u32, ctx.r4.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stwx r8,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r8.u32);
	// stwx r31,r10,r5
	PPC_STORE_U32(ctx.r10.u32 + ctx.r5.u32, var_r31);
	// stw r11,14436(r9)
	PPC_STORE_U32(ctx.r9.u32 + 14436, ctx.r11.u32);
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_0"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_0);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32251
	// lis r10,-32251
	// addi r30,r31,16
	var_r30 = (uint32_t)(var_r31 + 16);
	// addi r11,r11,-3388
	ctx.r11.s64 = ctx.r11.s64 + -3388;
	// addi r10,r10,-3256
	ctx.r10.s64 = ctx.r10.s64 + -3256;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r10.u32);
	// bl 0x8225e8c0
	msgEventHandler_E8C0_g(ctx, base);
	// lis r11,-32254
	// lis r10,-32253
	// addi r11,r11,31540
	ctx.r11.s64 = ctx.r11.s64 + 31540;
	// addi r10,r10,13196
	ctx.r10.s64 = ctx.r10.s64 + 13196;
	// lis r8,-32160
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
	// stw r9,25956(r8)
	PPC_STORE_U32(ctx.r8.u32 + 25956, ctx.r9.u32);
	// bl 0x821a9420
	atSingleton_9420(ctx, base);
	// clrlwi r11,r29,31
	ctx.r11.u64 = var_r29 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x8223fd6c
	if (ctx.r11.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8223FD6C:
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_20"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_20);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_20) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32164
	ctx.r11.s64 = -2107899904;
	// lwz r11,22596(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 22596);
	// cmplw cr6,r4,r11
	// bne cr6,0x8223fd90
	if (ctx.r4.u32 == ctx.r11.u32) {
		// li r3,1
		ctx.r3.s64 = 1;
		// blr
		return;
	}
loc_8223FD90:
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// lwz r11,-27696(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -27696);
	// cmplw cr6,r4,r11
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x8223fda8
	if (ctx.r4.u32 != ctx.r11.u32) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8223FDA8:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_22"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_22);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_22) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r3,r11,-4176
	ctx.r3.s64 = ctx.r11.s64 + -4176;
	// blr
	return;
}

__attribute__((alias("__imp__cmOperatorCtor_FDC0_w"))) PPC_WEAK_FUNC(cmOperatorCtor_FDC0_w);
PPC_FUNC_IMPL(__imp__cmOperatorCtor_FDC0_w) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,100
	ctx.r4.s64 = 100;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x8223fe10
	if (ctx.r3.u32 != 0) {
		// bl 0x8223fb70
		ph_ctor_FB70(ctx, base);
		// blr
		return;
	}
loc_8223FE10:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_21"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_21);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_21) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32163
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r30,r11,-20588
	var_r30 = (uint32_t)(ctx.r11.s64 + -20588);  // lbl_825CAF94 @ 0x825caf94
	// lis r11,-32251
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r4,r11,-4164
	ctx.r4.s64 = ctx.r11.s64 + -4164;
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// addi r5,r31,24
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 24;
	// bl 0x821a8f58
	game_8F58(ctx, base);
	// lis r11,-32251
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r4,r11,-4148
	ctx.r4.s64 = ctx.r11.s64 + -4148;
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// addi r5,r31,32
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 32;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x821a8f58
	game_8F58(ctx, base);
	// lis r11,-32251
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r4,r11,-4132
	ctx.r4.s64 = ctx.r11.s64 + -4132;
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// addi r5,r31,40
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 40;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x821a8f58
	game_8F58(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_1"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_1);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// lhz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,2076
	// beq cr6,0x8223fed4
	if (ctx.r10.u32 != 2076) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8223FED4:
	// clrlwi r9,r11,24
	ctx.r9.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// bne cr6,0x8223fff4
	if (ctx.r9.u32 == 0) {
		// cmplwi cr6,r10,18455
		// li r11,1
		ctx.r11.s64 = 1;
		// beq cr6,0x8223fef0
		if (ctx.r10.u32 != 18455) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223FEF0:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r6,0
		// bne cr6,0x8223fff4
		if (ctx.r6.u32 != 0) goto loc_8223FFF4;
		// cmplwi cr6,r10,18459
		// li r11,1
		ctx.r11.s64 = 1;
		// beq cr6,0x8223ff0c
		if (ctx.r10.u32 != 18459) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8223FF0C:
		// clrlwi r4,r11,24
		ctx.r4.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// bne cr6,0x8223ffec
		if (ctx.r4.u32 == 0) {
			// cmplwi cr6,r10,18456
			// li r11,1
			ctx.r11.s64 = 1;
			// beq cr6,0x8223ff28
			if (ctx.r10.u32 != 18456) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8223FF28:
			// clrlwi r11,r11,24
			ctx.r11.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r11,0
			// bne cr6,0x8223ffec
			if (ctx.r11.u32 != 0) goto loc_8223FFEC;
			// cmplwi cr6,r10,2049
			// li r11,1
			ctx.r11.s64 = 1;
			// beq cr6,0x8223ff44
			if (ctx.r10.u32 != 2049) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8223FF44:
			// clrlwi r8,r11,24
			ctx.r8.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r8,0
			// bne cr6,0x8223ffa8
			if (ctx.r8.u32 == 0) {
				// cmplwi cr6,r10,2065
				// li r11,1
				ctx.r11.s64 = 1;
				// beq cr6,0x8223ff60
				if (ctx.r10.u32 != 2065) {
					// li r11,0
					ctx.r11.s64 = 0;
				}
			loc_8223FF60:
				// clrlwi r5,r11,24
				ctx.r5.u64 = ctx.r11.u32 & 0xFF;
				// cmplwi cr6,r5,0
				// bne cr6,0x8223ffa8
				if (ctx.r5.u32 != 0) goto loc_8223FFA8;
				// cmplwi cr6,r10,2050
				// li r11,1
				ctx.r11.s64 = 1;
				// beq cr6,0x8223ff7c
				if (ctx.r10.u32 != 2050) {
					// li r11,0
					ctx.r11.s64 = 0;
				}
			loc_8223FF7C:
				// clrlwi r3,r11,24
				ctx.r3.u64 = ctx.r11.u32 & 0xFF;
				// cmplwi cr6,r3,0
				// beq cr6,0x82240000
				if (ctx.r3.u32 == 0) {
					// blr
					return;
				}
				// lwz r3,60(r7)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
				// bl 0x821a9ec0
				xmlTree_9EC0_g(ctx, base);
				// addi r3,r7,-16
				ctx.r3.s64 = ctx.r7.s64 + -16;
				// bl 0x82240538
				rage_0538(ctx, base);
				// blr
				return;
			}
		loc_8223FFA8:
			// lwz r3,-4(r7)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
			// cmplwi cr6,r3,0
			// beq cr6,0x8223ffc4
		while (ctx.r3.u32 != 0) {
			loc_8223FFB4:
				// bl 0x821a9ec0
				xmlTree_9EC0_g(ctx, base);
				// lwz r3,-4(r7)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + -4);
				// cmplwi cr6,r3,0
				// bne cr6,0x8223ffb4
		}
		loc_8223FFC4:
			// addi r4,r7,-16
			ctx.r4.s64 = ctx.r7.s64 + -16;
			// lwz r3,60(r7)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
			// bl 0x821a9f08
			xmlTree_9F08_g(ctx, base);
			// mr r3,r4
			ctx.r3.u64 = ctx.r4.u64;
			// lwz r4,60(r7)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
			// bl 0x822405a0
			rage_05A0(ctx, base);
			// blr
			return;
		}
	loc_8223FFEC:
		// li r4,0
		ctx.r4.s64 = 0;
		// b 0x8223fff8
	} else {
	loc_8223FFF4:
		// li r4,1
		ctx.r4.s64 = 1;
	}
loc_8223FFF8:
	// addi r3,r7,-16
	ctx.r3.s64 = ctx.r7.s64 + -16;
	// bl 0x822404b8
	lvlLevelMgr_04B8_h(ctx, base);
loc_82240000:
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_23"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_23);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_23) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r28,0(r13)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
	// li r29,4
	var_r29 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,20
	ctx.r4.s64 = 20;
	// lwzx r3,r29,r28
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + var_r28);
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// stw r3,48(r31)
	PPC_STORE_U32(var_r31 + 48, ctx.r3.u32);
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r29,r28
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + var_r28);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,20
	ctx.r4.s64 = 20;
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// stw r3,52(r31)
	PPC_STORE_U32(var_r31 + 52, ctx.r3.u32);
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r29,r28
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + var_r28);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,20
	ctx.r4.s64 = 20;
	// lwz r6,4(r7)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// lis r27,-32161
	var_r27 = (uint32_t)(-2107703296);
	// beq cr6,0x822400b8
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// lwz r5,-17888(r27)
		ctx.r5.u64 = PPC_LOAD_U32(var_r27 + -17888);
		// addi r4,r11,-4116
		ctx.r4.s64 = ctx.r11.s64 + -4116;
		// bl 0x8215db70
		grc_DB70(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// b 0x822400bc
	} else {
	loc_822400B8:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822400BC:
	// lis r10,-32251
	// stw r11,80(r31)
	PPC_STORE_U32(var_r31 + 80, ctx.r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// addi r4,r10,-4100
	ctx.r4.s64 = ctx.r10.s64 + -4100;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// addi r30,r10,32376
	var_r30 = (uint32_t)(ctx.r10.s64 + 32376);  // lbl_82617E78 @ 0x82617e78
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82151e58
	atSingleton_1E58_g(ctx, base);
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r29,r28
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + var_r28);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,20
	ctx.r4.s64 = 20;
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82240120
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// lwz r5,-17888(r27)
		ctx.r5.u64 = PPC_LOAD_U32(var_r27 + -17888);
		// addi r4,r11,-4084
		ctx.r4.s64 = ctx.r11.s64 + -4084;
		// bl 0x8215db70
		grc_DB70(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// b 0x82240124
	} else {
	loc_82240120:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82240124:
	// lis r10,-32251
	// stw r11,84(r31)
	PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// addi r4,r10,-4064
	ctx.r4.s64 = ctx.r10.s64 + -4064;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82151e58
	atSingleton_1E58_g(ctx, base);
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r29,r28
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + var_r28);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,20
	ctx.r4.s64 = 20;
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82240180
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// lwz r5,-17888(r27)
		ctx.r5.u64 = PPC_LOAD_U32(var_r27 + -17888);
		// addi r4,r11,-4048
		ctx.r4.s64 = ctx.r11.s64 + -4048;
		// bl 0x8215db70
		grc_DB70(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// b 0x82240184
	} else {
	loc_82240180:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82240184:
	// lis r10,-32251
	// stw r11,88(r31)
	PPC_STORE_U32(var_r31 + 88, ctx.r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// addi r4,r10,-4028
	ctx.r4.s64 = ctx.r10.s64 + -4028;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82151e58
	atSingleton_1E58_g(ctx, base);
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r29,r28
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + var_r28);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,20
	ctx.r4.s64 = 20;
	// lwz r6,4(r7)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x822401e0
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// lwz r5,-17888(r27)
		ctx.r5.u64 = PPC_LOAD_U32(var_r27 + -17888);
		// addi r4,r11,-4016
		ctx.r4.s64 = ctx.r11.s64 + -4016;
		// bl 0x8215db70
		grc_DB70(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// b 0x822401e4
	} else {
	loc_822401E0:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822401E4:
	// lis r10,-32251
	// stw r11,92(r31)
	PPC_STORE_U32(var_r31 + 92, ctx.r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// addi r4,r10,-3996
	ctx.r4.s64 = ctx.r10.s64 + -3996;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82151e58
	atSingleton_1E58_g(ctx, base);
	// lis r11,-32160
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,32
	ctx.r8.s64 = 32;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r3,25604(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25604);
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// lwz r6,-28652(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + -28652);  /* glob:lbl_825C9014 @ 0x825c9014 */
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// lwz r7,-28648(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + -28648);  /* glob:lbl_825C9018 @ 0x825c9018 */
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// addi r4,r11,-21252
	ctx.r4.s64 = ctx.r11.s64 + -21252;
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// lwz r29,26256(r11)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 26256));  /* glob:lbl_82606690 @ 0x82606690 */
	// lwz r10,24(r11)
	// bctrl
	VCALL(ctx.r3.u32, 6, ctx, base);  // vtable slot 6 (byte +24)
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32249
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r10,-21236
	ctx.r4.s64 = ctx.r10.s64 + -21236;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// stw r11,0(r29)
	PPC_STORE_U32(var_r29 + 0, ctx.r11.u32);
	// bl 0x82151e58
	atSingleton_1E58_g(ctx, base);
	// lis r11,-32248
	// lfs f0,24(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 24);
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f31,-23916(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23916);
	var_f31 = double(temp.f32);
	// fmuls f13,f0,f31
	ctx.f13.f64 = double(float(ctx.f0.f64 * var_f31));
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x820f8fb0
	game_8FB0(ctx, base);
	// lfs f11,32(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmuls f10,f11,f31
	ctx.f10.f64 = double(float(ctx.f11.f64 * var_f31));
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r3,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r3.u32);
	// fctiwz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f10.f64));
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x820f8fb0
	game_8FB0(ctx, base);
	// lfs f8,40(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f8.f64 = double(temp.f32);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// fmuls f7,f8,f31
	ctx.f7.f64 = double(float(ctx.f8.f64 * var_f31));
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r3,36(r31)
	PPC_STORE_U32(var_r31 + 36, ctx.r3.u32);
	// fctiwz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f7.f64));
	// stfiwx f6,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f6.u32);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x820f8fb0
	game_8FB0(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,44(r31)
	PPC_STORE_U32(var_r31 + 44, ctx.r11.u32);
	// bl 0x82240620
	ke_0620(ctx, base);
	// lwz r6,72(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 72);
	// cmplwi cr6,r6,0
	// bne cr6,0x8224036c
	if (ctx.r6.u32 == 0) {
		// li r3,128
		ctx.r3.s64 = 128;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmplwi cr6,r30,0
		// beq cr6,0x82240324
		if (var_r30 != 0) {
			// bl 0x8223fa58
			ph_ctor_FA58(ctx, base);
			// lis r11,-32251
			// lis r10,-32251
			// addi r11,r11,-3580
			ctx.r11.s64 = ctx.r11.s64 + -3580;
			// addi r10,r10,-3408
			ctx.r10.s64 = ctx.r10.s64 + -3408;
			// stw r11,0(r30)
			PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
			// stw r10,96(r30)
			PPC_STORE_U32(var_r30 + 96, ctx.r10.u32);
			// b 0x82240328
		} else {
		loc_82240324:
			// li r30,0
			var_r30 = 0;
		}
	loc_82240328:
		// lis r11,-32251
		// stw r30,72(r31)
		PPC_STORE_U32(var_r31 + 72, var_r30);
		// addi r3,r11,-2692
		ctx.r3.s64 = ctx.r11.s64 + -2692;
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// lwz r3,80(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 80);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// stw r29,80(r30)
		PPC_STORE_U32(var_r30 + 80, var_r29);
		// li r5,1
		ctx.r5.s64 = 1;
		// lwz r4,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stb r5,112(r30)
		PPC_STORE_U8(var_r30 + 112, ctx.r5.u8);
		// lwz r11,92(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 23, ctx, base);  // pattern-B slot 23 (byte +92)
		// lwz r3,72(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 72);
		// bl 0x82241ec8
		lvlLevelMgr_1EC8_wrh(ctx, base);
	}
loc_8224036C:
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_vfn_24"))) PPC_WEAK_FUNC(lvlLevelMgr_vfn_24);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_vfn_24) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r29,0
	var_r29 = 0;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 80);
	// stw r29,12(r31)
	PPC_STORE_U32(var_r31 + 12, var_r29);
	// cmplwi cr6,r3,0
	// beq cr6,0x822403a4
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r29,80(r31)
		PPC_STORE_U32(var_r31 + 80, var_r29);
	}
loc_822403A4:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 84);
	// cmplwi cr6,r3,0
	// beq cr6,0x822403b8
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r29,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r29);
	}
loc_822403B8:
	// lwz r3,88(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 88);
	// cmplwi cr6,r3,0
	// beq cr6,0x822403cc
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r29,88(r31)
		PPC_STORE_U32(var_r31 + 88, var_r29);
	}
loc_822403CC:
	// lwz r3,92(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 92);
	// cmplwi cr6,r3,0
	// beq cr6,0x822403e0
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r29,92(r31)
		PPC_STORE_U32(var_r31 + 92, var_r29);
	}
loc_822403E0:
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 72);
	// cmplwi cr6,r3,0
	// beq cr6,0x822403f0
	if (ctx.r3.u32 != 0) {
		// bl 0x823d9510
		game_9510(ctx, base);
	}
loc_822403F0:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r3,0
	// beq cr6,0x82240400
	if (ctx.r3.u32 != 0) {
		// bl 0x823d9510
		game_9510(ctx, base);
	}
loc_82240400:
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
	// cmplwi cr6,r3,0
	// beq cr6,0x82240410
	if (ctx.r3.u32 != 0) {
		// bl 0x822419c0
		game_19C0(ctx, base);
	}
loc_82240410:
	// lwz r3,76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 76);
	// cmplwi cr6,r3,0
	// beq cr6,0x82240444
	if (ctx.r3.u32 != 0) {
		// bl 0x822419c0
		game_19C0(ctx, base);
		// lwz r3,76(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 76);
		// cmplwi cr6,r3,0
		// beq cr6,0x82240440
		if (ctx.r3.u32 != 0) {
			// lwz r11,0(r3)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r10,0(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 0, ctx, base);  // pattern-B slot 0 (byte +0)
		}
	loc_82240440:
		// stw r29,76(r31)
		PPC_STORE_U32(var_r31 + 76, var_r29);
	}
loc_82240444:
	// lwz r30,44(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 44));
	// lis r28,-8191
	var_r28 = (uint32_t)(-536805376);
	// cmplwi cr6,r30,0
	// beq cr6,0x8224046c
	if (var_r30 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f8ee8
		game_8EE8(ctx, base);
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// stw r29,44(r31)
		PPC_STORE_U32(var_r31 + 44, var_r29);
	}
loc_8224046C:
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 36);
	// cmplwi cr6,r9,0
	// beq cr6,0x8224047c
	if (ctx.r9.u32 != 0) {
		// stw r29,36(r31)
		PPC_STORE_U32(var_r31 + 36, var_r29);
	}
loc_8224047C:
	// lwz r30,28(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 28));
	// cmplwi cr6,r30,0
	// beq cr6,0x822404a0
	if (var_r30 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f8ee8
		game_8EE8(ctx, base);
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// stw r29,28(r31)
		PPC_STORE_U32(var_r31 + 28, var_r29);
	}
loc_822404A0:
	// stw r29,72(r31)
	PPC_STORE_U32(var_r31 + 72, var_r29);
	// stw r29,68(r31)
	PPC_STORE_U32(var_r31 + 68, var_r29);
	// stw r29,64(r31)
	PPC_STORE_U32(var_r31 + 64, var_r29);
	// stw r29,76(r31)
	PPC_STORE_U32(var_r31 + 76, var_r29);
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_04B8_h"))) PPC_WEAK_FUNC(lvlLevelMgr_04B8_h);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_04B8_h) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// lis r11,-32161
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lwz r11,-21712(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -21712);
	// lbz r10,576(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 576);
	// lis r11,-32142
	ctx.r11.s64 = -2106458112;
	// cmplwi cr6,r10,0
	// lwz r3,-23792(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23792);  /* glob:lbl_8271A310 @ 0x8271a310 */
	// beq cr6,0x822404f0
	if (ctx.r10.u32 != 0) {
		// lwz r4,32(r3)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
		// b 0x822404f4
	} else {
	loc_822404F0:
		// lwz r4,20(r9)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	}
loc_822404F4:
	// cmpwi cr6,r4,-1
	// ble cr6,0x82240528
	if (ctx.r4.s32 > -1) {
		// cmplwi cr6,r10,0
		// beq cr6,0x8224050c
		if (ctx.r10.u32 != 0) {
			// lwz r8,76(r9)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 76);
			// b 0x82240510
		} else {
		loc_8224050C:
			// lwz r8,64(r9)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
		}
	loc_82240510:
		// cmplwi cr6,r8,0
		// beq cr6,0x82240528
		if (ctx.r8.u32 == 0) {
			// blr
			return;
		}
		// bl 0x821dc2a0
		pg_C2A0_g(ctx, base);
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// mr r3,r8
		ctx.r3.u64 = ctx.r8.u64;
		// bl 0x82241a28
		lvlLevelMgr_1A28_fw(ctx, base);
	}
loc_82240528:
	// blr
	return;
}

__attribute__((alias("__imp__rage_0538"))) PPC_WEAK_FUNC(rage_0538);
PPC_FUNC_IMPL(__imp__rage_0538) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r30,-32161
	var_r30 = (uint32_t)(-2107703296);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 80);
	// lwz r4,-17888(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + -17888);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// lwz r4,-17888(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + -17888);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 84);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// lwz r4,-17888(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + -17888);
	// lwz r3,88(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 88);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// lwz r4,-17888(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + -17888);
	// lwz r3,92(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 92);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__rage_05A0"))) PPC_WEAK_FUNC(rage_05A0);
PPC_FUNC_IMPL(__imp__rage_05A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32160
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r11,25540(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25540);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r10,0
	// ble cr6,0x822405d8
	if (ctx.r10.s32 > 0) {
		// lwz r4,96(r11)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 96);
		// b 0x822405dc
	} else {
	loc_822405D8:
		// lwz r4,100(r11)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 100);
	}
loc_822405DC:
	// lwz r3,92(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 92);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// lwz r4,24(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 24);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 80);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// lwz r4,28(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 28);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 84);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// lwz r4,32(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 32);
	// lwz r3,88(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 88);
	// bl 0x8215dae8
	rage_DAE8(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__ke_0620"))) PPC_WEAK_FUNC(ke_0620);
PPC_FUNC_IMPL(__imp__ke_0620) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=560, savegprlr_25
	// lis r11,-32142
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,-23792(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23792);
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r4,r11,-1
	ctx.r4.s64 = ctx.r11.s64 + -1;
	// bl 0x821dc2a0
	pg_C2A0_g(ctx, base);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r3,r11,-3916
	ctx.r3.s64 = ctx.r11.s64 + -3916;
	// lwz r4,16(r28)
	ctx.r4.u64 = PPC_LOAD_U32(var_r28 + 16);
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r4,r11,-4240
	ctx.r4.s64 = ctx.r11.s64 + -4240;
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// addi r25,r11,128
	var_r25 = (uint32_t)(ctx.r11.s64 + 128);  // lbl_825D0080 @ 0x825d0080
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// bl 0x822e2e60
	atSingleton_2E60_g(ctx, base);
	// lwz r30,76(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 76));
	// lwz r29,20(r28)
	var_r29 = (uint32_t)(PPC_LOAD_U32(var_r28 + 20));
	// li r27,0
	var_r27 = 0;
	// cmplwi cr6,r30,0
	// beq cr6,0x822406b8
	if (var_r30 != 0) {
		// lis r11,-32251
		ctx.r11.s64 = -2113601536;
		// addi r3,r11,-3824
		ctx.r3.s64 = ctx.r11.s64 + -3824;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x821a9ec0
		xmlTree_9EC0_g(ctx, base);
		// lwz r3,76(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 76);
		// li r4,0
		ctx.r4.s64 = 0;
		// lwz r10,0(r11)
		// bctrl
		DTOR(ctx.r3.u32, ctx, base);  // vtable slot 0 (destructor)
		// stw r27,76(r31)
		PPC_STORE_U32(var_r31 + 76, var_r27);
	}
loc_822406B8:
	// lis r11,-32251
	// mr r7,r29
	ctx.r7.u64 = var_r29;
	// addi r6,r11,-4260
	ctx.r6.s64 = ctx.r11.s64 + -4260;
	// lis r11,-32253
	// li r4,256
	ctx.r4.s64 = 256;
	// addi r5,r11,-2544
	ctx.r5.s64 = ctx.r11.s64 + -2544;
	// addi r3,r1,224
	ctx.r3.s64 = ctx.r1.s64 + 224;
	// bl 0x820c0b40
	_snprintf(ctx, base);
	// lis r11,-32160
	// lfs f13,32(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 36);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// lwz r11,25848(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25848);
	// addi r26,r11,144
	var_r26 = (uint32_t)(ctx.r11.s64 + 144);  // addr:0x82600090
	// lis r11,-32248
	// mr r6,r26
	ctx.r6.u64 = var_r26;
	// lfs f0,-23916(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23916);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f11.u32);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x820f9208
	rage_9208(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x82240754
	if (var_r30 != 0) {
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// lwz r6,116(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
		// addi r3,r1,128
		ctx.r3.s64 = ctx.r1.s64 + 128;
		// lwz r5,112(r1)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		// bl 0x82188190
		ke_8190(ctx, base);
		// addi r4,r1,128
		ctx.r4.s64 = ctx.r1.s64 + 128;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82241c00
		game_1C00(ctx, base);
		// addi r3,r1,128
		ctx.r3.s64 = ctx.r1.s64 + 128;
		// bl 0x82188220
		ke_8220(ctx, base);
		// b 0x82240758
	} else {
	loc_82240754:
		// mr r30,r27
		var_r30 = (uint32_t)(var_r27);
	}
loc_82240758:
	// stw r30,76(r31)
	PPC_STORE_U32(var_r31 + 76, var_r30);
	// lwz r8,16(r30)
	ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 16);
	// cmpwi cr6,r8,0
	// ble cr6,0x82240798
	if (ctx.r8.s32 > 0) {
		// mr r29,r27
		var_r29 = (uint32_t)(var_r27);
	loc_8224076C:
		// lwz r11,20(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r7,r29,r11
		ctx.r7.u64 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
		// cmplwi cr6,r7,0
		// beq cr6,0x82240784
		if (ctx.r7.u32 != 0) {
			// rotlwi r3,r7,0
			ctx.r3.u64 = ctx.r7.u32;
			// bl 0x823d95f8
			game_95F8(ctx, base);
		}
	loc_82240784:
		// lwz r6,16(r30)
		ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r27,r27,1
		var_r27 = (uint32_t)(var_r27 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpw cr6,r27,r6
		// blt cr6,0x8224076c
		if ((int32_t)var_r27 < ctx.r6.s32) goto loc_8224076C;
	}
loc_82240798:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82241110
	game_1110(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// lfs f31,52(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r28 + 52);
	var_f31 = double(temp.f32);
	// li r4,192
	ctx.r4.s64 = 192;
	// li r3,6146
	ctx.r3.s64 = 6146;
	// bl 0x8225e6e0
	pg_E6E0(ctx, base);
	// lis r11,-32252
	// li r4,192
	ctx.r4.s64 = 192;
	// stfs f31,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r5,r11,-14712
	ctx.r5.s64 = ctx.r11.s64 + -14712;
	// lwz r6,112(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r3,6147
	ctx.r3.s64 = 6147;
	// bl 0x8225e480
	pg_E480(ctx, base);
	// lis r11,-32251
	// stw r26,5920(r28)
	PPC_STORE_U32(var_r28 + 5920, var_r26);
	// addi r3,r11,-3896
	ctx.r3.s64 = ctx.r11.s64 + -3896;
	// lwz r11,1536(r25)
	ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 1536);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,1536(r25)
	PPC_STORE_U32(var_r25 + 1536, ctx.r11.u32);
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
	return;
}

__attribute__((alias("__imp__pg_0800_g"))) PPC_WEAK_FUNC(pg_0800_g);
PPC_FUNC_IMPL(__imp__pg_0800_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82240860
	rage_0860(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x82240858
while (ctx.r10.u32 == 0) {
	loc_82240820:
		// li r5,1
		ctx.r5.s64 = 1;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82240930
		ref_net_0930(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
	loc_82240834:
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82240ba8
		pg_0BA8(ctx, base);
		// clrlwi r11,r3,24
		ctx.r11.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x82240834
		if (ctx.r11.u32 == 0) goto loc_82240834;
		// clrlwi r10,r29,24
		ctx.r10.u64 = var_r29 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x82240820
}
loc_82240858:
	return;
}

__attribute__((alias("__imp__rage_0860"))) PPC_WEAK_FUNC(rage_0860);
PPC_FUNC_IMPL(__imp__rage_0860) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// lis r11,-32142
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r3,-23792(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23792);
	// bl 0x821dc2a0
	pg_C2A0_g(ctx, base);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmpw cr6,r30,r11
	// bne cr6,0x822408d8
	if ((int32_t)var_r30 == ctx.r11.s32) {
		// lwz r10,12(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 12);
		// cmplwi cr6,r10,0
		// bne cr6,0x822408bc
		if (ctx.r10.u32 == 0) {
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwz r3,64(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
			// bl 0x821a9f08
			xmlTree_9F08_g(ctx, base);
			// lwz r3,68(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
			// bl 0x821a9f08
			xmlTree_9F08_g(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r4,64(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 64);
			// bl 0x822405a0
			rage_05A0(ctx, base);
		}
	loc_822408BC:
		// lis r11,-32251
		// lwz r4,16(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 16);
		// addi r3,r11,-3872
		ctx.r3.s64 = ctx.r11.s64 + -3872;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_822408D8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82240538
	rage_0538(ctx, base);
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
	// cmplwi cr6,r3,0
	// beq cr6,0x822408f0
	if (ctx.r3.u32 != 0) {
		// bl 0x822419c0
		game_19C0(ctx, base);
	}
loc_822408F0:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r3,0
	// beq cr6,0x82240900
	if (ctx.r3.u32 != 0) {
		// bl 0x823d9510
		game_9510(ctx, base);
	}
loc_82240900:
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,20(r31)
	PPC_STORE_U32(var_r31 + 20, var_r30);
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r3,r11,-3916
	ctx.r3.s64 = ctx.r11.s64 + -3916;
	// stb r10,96(r31)
	PPC_STORE_U8(var_r31 + 96, ctx.r10.u8);
	// stb r10,97(r31)
	PPC_STORE_U8(var_r31 + 97, ctx.r10.u8);
	// lwz r4,16(r29)
	ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 16);
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	return;
}

__attribute__((alias("__imp__ref_net_0930"))) PPC_WEAK_FUNC(ref_net_0930);
PPC_FUNC_IMPL(__imp__ref_net_0930) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	// FRAME: size=416, savegprlr_26
	// lis r11,-32251
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r4,r11,-4240
	ctx.r4.s64 = ctx.r11.s64 + -4240;
	// lis r11,-32163
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r30,r11,128
	var_r30 = (uint32_t)(ctx.r11.s64 + 128);  // lbl_825D0080 @ 0x825d0080
	// lis r11,-32160
	// lwz r27,5920(r29)
	var_r27 = (uint32_t)(PPC_LOAD_U32(var_r29 + 5920));
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// li r28,0
	var_r28 = 0;
	// lwz r11,25848(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25848);
	// addi r26,r11,144
	var_r26 = (uint32_t)(ctx.r11.s64 + 144);  // addr:0x82600090
	// bl 0x822e2e60
	atSingleton_2E60_g(ctx, base);
	// lbz r11,96(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 96);
	// cmplwi cr6,r11,0
	// bne cr6,0x82240a40
	if (ctx.r11.u32 == 0) {
		// li r10,1
		ctx.r10.s64 = 1;
		// stb r10,96(r31)
		PPC_STORE_U8(var_r31 + 96, ctx.r10.u8);
		// lwz r27,20(r29)
		var_r27 = (uint32_t)(PPC_LOAD_U32(var_r29 + 20));
		// lwz r29,64(r31)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 64));
		// cmplwi cr6,r29,0
		// beq cr6,0x822409c4
		if (var_r29 != 0) {
			// lis r11,-32251
			ctx.r11.s64 = -2113601536;
			// addi r3,r11,-3844
			ctx.r3.s64 = ctx.r11.s64 + -3844;
			// bl 0x8240e6d0
			nop_8240E6D0(ctx, base);
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x821a9ec0
			xmlTree_9EC0_g(ctx, base);
			// lwz r3,64(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r8,0(r9)
			// bctrl
			DTOR(ctx.r3.u32, ctx, base);  // vtable slot 0 (destructor)
			// stw r28,64(r31)
			PPC_STORE_U32(var_r31 + 64, var_r28);
		}
	loc_822409C4:
		// lis r11,-32251
		// mr r7,r27
		ctx.r7.u64 = var_r27;
		// addi r6,r11,-4260
		ctx.r6.s64 = ctx.r11.s64 + -4260;
		// lis r11,-32253
		// li r4,256
		ctx.r4.s64 = 256;
		// addi r5,r11,-2544
		ctx.r5.s64 = ctx.r11.s64 + -2544;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// bl 0x820c0b40
		_snprintf(ctx, base);
		// lis r11,-32251
		// stb r28,56(r31)
		PPC_STORE_U8(var_r31 + 56, (uint8_t)var_r28);
		// addi r3,r11,-3792
		ctx.r3.s64 = ctx.r11.s64 + -3792;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// lis r11,-32248
		// lfs f13,24(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 24);
		ctx.f13.f64 = double(temp.f32);
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// li r7,0
		ctx.r7.s64 = 0;
		// lwz r5,28(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 28);
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// lwz r4,48(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 48);
		// lfs f0,-23916(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23916);  /* glob:0x8204a294 */
		ctx.f0.f64 = double(temp.f32);
		// fmuls f12,f13,f0
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// fctiwz f11,f12
		ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
		// stfiwx f11,0,r6
		PPC_STORE_U32(ctx.r6.u32, ctx.f11.u32);
		// lwz r6,80(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// bl 0x820f8c38
		pg_8C38_g(ctx, base);
		// lwz r11,1536(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 1536);
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,1536(r30)
		PPC_STORE_U32(var_r30 + 1536, ctx.r11.u32);
		return;
	}
loc_82240A40:
	// lbz r5,97(r31)
	ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 97);
	// cmplwi cr6,r5,0
	// bne cr6,0x82240a78
	if (ctx.r5.u32 == 0) {
		// mr r6,r26
		ctx.r6.u64 = var_r26;
		// mr r5,r27
		ctx.r5.u64 = var_r27;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82240a90
		ref_net_0A90(ctx, base);
		// lwz r11,1536(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 1536);
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,1536(r30)
		PPC_STORE_U32(var_r30 + 1536, ctx.r11.u32);
		return;
	}
loc_82240A78:
	// lwz r11,1536(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 1536);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,1536(r30)
	PPC_STORE_U32(var_r30 + 1536, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__ref_net_0A90"))) PPC_WEAK_FUNC(ref_net_0A90);
PPC_FUNC_IMPL(__imp__ref_net_0A90) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=400, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r11,1
	ctx.r11.s64 = 1;
	// stb r11,97(r31)
	PPC_STORE_U8(var_r31 + 97, ctx.r11.u8);
	// lwz r28,24(r4)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 24));
	// cmplwi cr6,r28,0
	// beq cr6,0x82240b8c
	if (var_r28 != 0) {
		// mr r11,r28
		ctx.r11.u64 = var_r28;
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	loc_82240ABC:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplwi cr6,r9,0
		// bne cr6,0x82240abc
		if (ctx.r9.u32 != 0) goto loc_82240ABC;
		// subf r11,r10,r11
		ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
		// addi r8,r11,-1
		ctx.r8.s64 = ctx.r11.s64 + -1;
		// rotlwi r11,r8,0
		ctx.r11.u64 = ctx.r8.u32;
		// cmplwi cr6,r11,0
		// beq cr6,0x82240b8c
		if (ctx.r11.u32 == 0) {
			// lis r11,-32251
			// lwz r4,20(r4)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
			// addi r3,r11,-3740
			ctx.r3.s64 = ctx.r11.s64 + -3740;
			// bl 0x8240e6d0
			nop_8240E6D0(ctx, base);
			return;
		}
		// lwz r29,68(r31)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 68));
		// li r30,0
		var_r30 = 0;
		// cmplwi cr6,r29,0
		// beq cr6,0x82240b20
		if (var_r29 != 0) {
			// lis r11,-32251
			ctx.r11.s64 = -2113601536;
			// addi r3,r11,-3760
			ctx.r3.s64 = ctx.r11.s64 + -3760;
			// bl 0x8240e6d0
			nop_8240E6D0(ctx, base);
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x821a9ec0
			xmlTree_9EC0_g(ctx, base);
			// lwz r3,68(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r6,0(r7)
			// bctrl
			DTOR(ctx.r3.u32, ctx, base);  // vtable slot 0 (destructor)
			// stw r30,68(r31)
			PPC_STORE_U32(var_r31 + 68, var_r30);
		}
	loc_82240B20:
		// lis r11,-32251
		// mr r7,r28
		ctx.r7.u64 = var_r28;
		// addi r6,r11,-4280
		ctx.r6.s64 = ctx.r11.s64 + -4280;
		// lis r11,-32253
		// li r4,256
		ctx.r4.s64 = 256;
		// addi r5,r11,-2544
		ctx.r5.s64 = ctx.r11.s64 + -2544;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// bl 0x820c0b40
		_snprintf(ctx, base);
		// lis r11,-32251
		// stb r30,57(r31)
		PPC_STORE_U8(var_r31 + 57, (uint8_t)var_r30);
		// addi r3,r11,-3680
		ctx.r3.s64 = ctx.r11.s64 + -3680;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// lis r11,-32248
		// lfs f13,40(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 40);
		ctx.f13.f64 = double(temp.f32);
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// li r7,0
		ctx.r7.s64 = 0;
		// lwz r5,44(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 44);
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// lwz r4,52(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 52);
		// lfs f0,-23916(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23916);  /* glob:0x8204a294 */
		ctx.f0.f64 = double(temp.f32);
		// fmuls f12,f13,f0
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// fctiwz f11,f12
		ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
		// stfiwx f11,0,r10
		PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
		// lwz r6,80(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// bl 0x820f8c38
		pg_8C38_g(ctx, base);
		return;
	}
loc_82240B8C:
	// lis r11,-32251
	// lwz r4,20(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// addi r3,r11,-3740
	ctx.r3.s64 = ctx.r11.s64 + -3740;
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
	return;
}

__attribute__((alias("__imp__pg_0BA8"))) PPC_WEAK_FUNC(pg_0BA8);
PPC_FUNC_IMPL(__imp__pg_0BA8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=176, savegprlr_28
	// lis r11,-32160
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r30,1
	var_r30 = 1;
	// lwz r11,25848(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25848);
	// addi r29,r11,144
	var_r29 = (uint32_t)(ctx.r11.s64 + 144);  // addr:0x82600090
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
	// cmpwi cr6,r11,-1
	// ble cr6,0x82240c0c
	if (ctx.r11.s32 > -1) {
		// lwz r10,64(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 64);
		// cmplwi cr6,r10,0
		// beq cr6,0x82240c0c
		if (ctx.r10.u32 == 0) goto loc_82240C0C;
		// lwz r9,68(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 68);
		// cmplwi cr6,r9,0
		// beq cr6,0x82240c0c
		if (ctx.r9.u32 == 0) goto loc_82240C0C;
		// lbz r8,56(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 56);
		// cmplwi cr6,r8,0
		// beq cr6,0x82240c0c
		if (ctx.r8.u32 == 0) goto loc_82240C0C;
		// lbz r7,57(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 57);
		// li r10,1
		ctx.r10.s64 = 1;
		// cmplwi cr6,r7,0
		// bne cr6,0x82240c10
		if (ctx.r7.u32 != 0) goto loc_82240C10;
	}
loc_82240C0C:
	// li r10,0
	ctx.r10.s64 = 0;
loc_82240C10:
	// clrlwi r28,r10,24
	var_r28 = (uint32_t)(ctx.r10.u32 & 0xFF);
	// cmpwi cr6,r11,-1
	// beq cr6,0x82240c78
	if (ctx.r11.s32 != -1) {
		// lbz r6,96(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 96);
		// cmplwi cr6,r6,0
		// beq cr6,0x82240c4c
		if (ctx.r6.u32 != 0) {
			// lbz r5,56(r31)
			ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 56);
			// cmplwi cr6,r5,0
			// bne cr6,0x82240c4c
			if (ctx.r5.u32 != 0) goto loc_82240C4C;
			// li r5,1
			ctx.r5.s64 = 1;
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82240d48
			ke_0D48(ctx, base);
			// lbz r30,56(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 56));
			// b 0x82240c78
		} else {
		loc_82240C4C:
			// lbz r4,97(r31)
			ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 97);
			// cmplwi cr6,r4,0
			// beq cr6,0x82240c78
			if (ctx.r4.u32 == 0) goto loc_82240C78;
			// lbz r3,57(r31)
			ctx.r3.u64 = PPC_LOAD_U8(var_r31 + 57);
			// cmplwi cr6,r3,0
			// bne cr6,0x82240c78
			if (ctx.r3.u32 != 0) goto loc_82240C78;
			// li r5,1
			ctx.r5.s64 = 1;
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82240e58
			ke_0E58(ctx, base);
			// lbz r30,57(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 57));
		}
	}
loc_82240C78:
	// clrlwi r11,r28,24
	ctx.r11.u64 = var_r28 & 0xFF;
	// cmplwi cr6,r11,0
	// bne cr6,0x82240d34
	if (ctx.r11.u32 == 0) {
		// lwz r10,20(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
		// cmpwi cr6,r10,-1
		// ble cr6,0x82240cc4
		if (ctx.r10.s32 > -1) {
			// lwz r9,64(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 64);
			// cmplwi cr6,r9,0
			// beq cr6,0x82240cc4
			if (ctx.r9.u32 == 0) goto loc_82240CC4;
			// lwz r8,68(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 68);
			// cmplwi cr6,r8,0
			// beq cr6,0x82240cc4
			if (ctx.r8.u32 == 0) goto loc_82240CC4;
			// lbz r7,56(r31)
			ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 56);
			// cmplwi cr6,r7,0
			// beq cr6,0x82240cc4
			if (ctx.r7.u32 == 0) goto loc_82240CC4;
			// lbz r6,57(r31)
			ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 57);
			// li r11,1
			ctx.r11.s64 = 1;
			// cmplwi cr6,r6,0
			// bne cr6,0x82240cc8
			if (ctx.r6.u32 != 0) goto loc_82240CC8;
		}
	loc_82240CC4:
		// li r11,0
		ctx.r11.s64 = 0;
	loc_82240CC8:
		// clrlwi r4,r11,24
		ctx.r4.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// beq cr6,0x82240d34
		if (ctx.r4.u32 == 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82241110
		game_1110(ctx, base);
		// lis r11,-32142
		// lwz r4,20(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lwz r3,-23792(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23792);
		// bl 0x821dc2a0
		pg_C2A0_g(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,192
		ctx.r4.s64 = 192;
		// li r3,6146
		ctx.r3.s64 = 6146;
		// lfs f31,52(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 52);
		var_f31 = double(temp.f32);
		// bl 0x8225e6e0
		pg_E6E0(ctx, base);
		// stfs f31,112(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// lis r11,-32252
		// li r4,192
		ctx.r4.s64 = 192;
		// addi r5,r11,-14712
		ctx.r5.s64 = ctx.r11.s64 + -14712;
		// li r3,6147
		ctx.r3.s64 = 6147;
		// lwz r6,112(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		// bl 0x8225e480
		pg_E480(ctx, base);
		// lis r11,-32251
		// stw r29,5920(r31)
		PPC_STORE_U32(var_r31 + 5920, var_r29);
		// addi r3,r11,-3896
		ctx.r3.s64 = ctx.r11.s64 + -3896;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
	}
loc_82240D34:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__ke_0D48"))) PPC_WEAK_FUNC(ke_0D48);
PPC_FUNC_IMPL(__imp__ke_0D48) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=208, savegprlr_28
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// lwz r11,64(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 64);
	// cmplwi cr6,r11,0
	// beq cr6,0x82240d70
	if (ctx.r11.u32 != 0) {
		// lbz r10,56(r28)
		ctx.r10.u64 = PPC_LOAD_U8(var_r28 + 56);
		// cmplwi cr6,r10,0
		// beq cr6,0x82240de8
		if (ctx.r10.u32 == 0) goto loc_82240DE8;
	}
loc_82240D70:
	// lwz r31,48(r28)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 48));
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x820f9178
	pg_9178_g(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x82240dd8
	if (ctx.r9.u32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// addi r31,r11,32
		var_r31 = (uint32_t)(ctx.r11.s64 + 32);  // addr:0x82050020
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// stw r31,64(r28)
		PPC_STORE_U32(var_r28 + 64, var_r31);
		// lwz r10,8(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// lwz r6,12(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		// addi r5,r10,32
		ctx.r5.s64 = ctx.r10.s64 + 32;
		// bl 0x82188190
		ke_8190(ctx, base);
		// cmplwi cr6,r31,0
		// beq cr6,0x82240dc8
		if (var_r31 != 0) {
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82241c00
			game_1C00(ctx, base);
		}
	loc_82240DC8:
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x82188220
		ke_8220(ctx, base);
		// li r11,1
		ctx.r11.s64 = 1;
		// b 0x82240ddc
	} else {
	loc_82240DD8:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82240DDC:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82240e4c
	if (ctx.r8.u32 != 0) {
	loc_82240DE8:
		// bl 0x820f8b00
		pgStreamer_Drain(ctx, base);
		// lis r11,-32251
		ctx.r11.s64 = -2113601536;
		// addi r3,r11,-3648
		ctx.r3.s64 = ctx.r11.s64 + -3648;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// lwz r30,64(r28)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r28 + 64));
		// li r29,0
		var_r29 = 0;
		// lwz r7,16(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 16);
		// cmpwi cr6,r7,0
		// ble cr6,0x82240e3c
		if (ctx.r7.s32 > 0) {
			// li r31,0
			var_r31 = 0;
		loc_82240E10:
			// lwz r11,20(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
			// lwzx r6,r11,r31
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
			// cmplwi cr6,r6,0
			// beq cr6,0x82240e28
			if (ctx.r6.u32 != 0) {
				// rotlwi r3,r6,0
				ctx.r3.u64 = ctx.r6.u32;
				// bl 0x823d95f8
				game_95F8(ctx, base);
			}
		loc_82240E28:
			// lwz r5,16(r30)
			ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 16);
			// addi r29,r29,1
			var_r29 = (uint32_t)(var_r29 + 1);
			// addi r31,r31,4
			var_r31 = (uint32_t)(var_r31 + 4);
			// cmpw cr6,r29,r5
			// blt cr6,0x82240e10
			if ((int32_t)var_r29 < ctx.r5.s32) goto loc_82240E10;
		}
	loc_82240E3C:
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x82240f40
		game_0F40(ctx, base);
		// li r4,1
		ctx.r4.s64 = 1;
		// stb r4,56(r28)
		PPC_STORE_U8(var_r28 + 56, ctx.r4.u8);
	}
loc_82240E4C:
	return;
}

__attribute__((alias("__imp__ke_0E58"))) PPC_WEAK_FUNC(ke_0E58);
PPC_FUNC_IMPL(__imp__ke_0E58) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=192, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lwz r11,68(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 68);
	// cmplwi cr6,r11,0
	// beq cr6,0x82240e88
	if (ctx.r11.u32 != 0) {
		// lbz r10,57(r30)
		ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 57);
		// cmplwi cr6,r10,0
		// beq cr6,0x82240f00
		if (ctx.r10.u32 == 0) goto loc_82240F00;
	}
loc_82240E88:
	// lwz r31,52(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 52));
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x820f9178
	pg_9178_g(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x82240ef0
	if (ctx.r9.u32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// addi r31,r11,32
		var_r31 = (uint32_t)(ctx.r11.s64 + 32);  // addr:0x82050020
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// stw r31,68(r30)
		PPC_STORE_U32(var_r30 + 68, var_r31);
		// lwz r10,8(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// lwz r6,12(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		// addi r5,r10,32
		ctx.r5.s64 = ctx.r10.s64 + 32;
		// bl 0x82188190
		ke_8190(ctx, base);
		// cmplwi cr6,r31,0
		// beq cr6,0x82240ee0
		if (var_r31 != 0) {
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82243a68
			ke_3A68(ctx, base);
		}
	loc_82240EE0:
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x82188220
		ke_8220(ctx, base);
		// li r11,1
		ctx.r11.s64 = 1;
		// b 0x82240ef4
	} else {
	loc_82240EF0:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82240EF4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82240f24
	if (ctx.r8.u32 != 0) {
	loc_82240F00:
		// lis r11,-32251
		ctx.r11.s64 = -2113601536;
		// addi r3,r11,-3616
		ctx.r3.s64 = ctx.r11.s64 + -3616;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// lwz r3,68(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 68);
		// bl 0x823d95f8
		game_95F8(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82241030
		game_1030(ctx, base);
		// li r7,1
		ctx.r7.s64 = 1;
		// stb r7,57(r30)
		PPC_STORE_U8(var_r30 + 57, ctx.r7.u8);
	}
loc_82240F24:
	// blr
	return;
}

__attribute__((alias("__imp__game_0F40"))) PPC_WEAK_FUNC(game_0F40);
PPC_FUNC_IMPL(__imp__game_0F40) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r3,64(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 64);
	// bl 0x821a9f08
	xmlTree_9F08_g(ctx, base);
	// lis r11,-32163
	// li r29,0
	var_r29 = 0;
	// addi r11,r11,-18128
	ctx.r11.s64 = ctx.r11.s64 + -18128;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r10,0
	// li r10,1
	ctx.r10.s64 = 1;
	// bne cr6,0x82240f78
	if (ctx.r10.u32 == 0) {
		// mr r10,r29
		ctx.r10.u64 = var_r29;
	}
loc_82240F78:
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// stb r7,58(r4)
	PPC_STORE_U8(ctx.r4.u32 + 58, ctx.r7.u8);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x82240f9c
	if (ctx.r11.u32 == 0) {
		// mr r11,r29
		ctx.r11.u64 = var_r29;
	}
loc_82240F9C:
	// clrlwi r5,r11,24
	ctx.r5.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r5,0
	// beq cr6,0x82240fac
	if (ctx.r5.u32 != 0) {
		// stb r29,58(r4)
		PPC_STORE_U8(ctx.r4.u32 + 58, (uint8_t)var_r29);
	}
loc_82240FAC:
	// lwz r30,64(r4)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 64));
	// cmplwi cr6,r30,0
	// beq cr6,0x82241024
	if (var_r30 != 0) {
		// lbz r11,58(r4)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 58);
		// lbz r3,44(r30)
		ctx.r3.u64 = PPC_LOAD_U8(var_r30 + 44);
		// xor r10,r3,r11
		ctx.r10.u64 = ctx.r3.u64 ^ ctx.r11.u64;
		// clrlwi r8,r10,24
		ctx.r8.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r8,0
		// beq cr6,0x82241024
		if (ctx.r8.u32 == 0) {
			return;
		}
		// lwz r7,16(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 16);
		// stb r11,44(r30)
		PPC_STORE_U8(var_r30 + 44, ctx.r11.u8);
		// cmpwi cr6,r7,0
		// ble cr6,0x82241024
		if (ctx.r7.s32 <= 0) {
			return;
		}
		// mr r31,r29
		var_r31 = (uint32_t)(var_r29);
	loc_82240FE4:
		// lwz r11,20(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r6,r31,r11
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
		// cmplwi cr6,r6,0
		// beq cr6,0x82241010
		if (ctx.r6.u32 != 0) {
			// lbz r5,44(r30)
			ctx.r5.u64 = PPC_LOAD_U8(var_r30 + 44);
			// rotlwi r3,r6,0
			ctx.r3.u64 = ctx.r6.u32;
			// cmplwi cr6,r5,0
			// beq cr6,0x8224100c
			if (ctx.r5.u32 != 0) {
				// bl 0x823d95f8
				game_95F8(ctx, base);
				// b 0x82241010
			} else {
			loc_8224100C:
				// bl 0x823d9510
				game_9510(ctx, base);
			}
		}
	loc_82241010:
		// lwz r4,16(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r4
		// blt cr6,0x82240fe4
		if ((int32_t)var_r29 < ctx.r4.s32) goto loc_82240FE4;
	}
loc_82241024:
	return;
}

__attribute__((alias("__imp__game_1030"))) PPC_WEAK_FUNC(game_1030);
PPC_FUNC_IMPL(__imp__game_1030) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// bl 0x82243580
	net_3580(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// bl 0x821a9f08
	xmlTree_9F08_g(ctx, base);
	// lis r11,-32163
	// addi r11,r11,-18108
	ctx.r11.s64 = ctx.r11.s64 + -18108;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r10,0
	// li r10,1
	ctx.r10.s64 = 1;
	// bne cr6,0x82241074
	if (ctx.r10.u32 == 0) {
		// li r10,0
		ctx.r10.s64 = 0;
	}
loc_82241074:
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// stb r7,59(r31)
	PPC_STORE_U8(var_r31 + 59, ctx.r7.u8);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x82241098
	if (ctx.r11.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82241098:
	// clrlwi r5,r11,24
	ctx.r5.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r5,0
	// beq cr6,0x822410ac
	if (ctx.r5.u32 != 0) {
		// li r4,0
		ctx.r4.s64 = 0;
		// stb r4,59(r31)
		PPC_STORE_U8(var_r31 + 59, ctx.r4.u8);
	}
loc_822410AC:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r3,0
	// beq cr6,0x822410d8
	if (ctx.r3.u32 != 0) {
		// lwz r11,0(r3)
  // [ph4a] vtable load collapsed
		// lbz r4,59(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 59);
		// lwz r10,140(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 35, ctx, base);  // pattern-B slot 35 (byte +140)
		// lwz r9,68(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 68);
		// lbz r8,60(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 60);
		// stb r8,232(r9)
		PPC_STORE_U8(ctx.r9.u32 + 232, ctx.r8.u8);
	}
loc_822410D8:
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 72);
	// cmplwi cr6,r3,0
	// beq cr6,0x822410f8
	if (ctx.r3.u32 != 0) {
		// lwz r7,0(r3)
  // [ph4a] vtable load collapsed
		// lbz r4,59(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 59);
		// lwz r6,140(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 35, ctx, base);  // pattern-B slot 35 (byte +140)
	}
loc_822410F8:
	// blr
	return;
}

__attribute__((alias("__imp__game_1110"))) PPC_WEAK_FUNC(game_1110);
PPC_FUNC_IMPL(__imp__game_1110) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r30);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lis r11,-32158
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r5,r11,-25552
	ctx.r5.s64 = ctx.r11.s64 + -25552;
	// li r10,50
	ctx.r10.s64 = 50;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82241130:
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x82241130
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82241130;
	// lis r11,-32161
	ctx.r11.s64 = -2107703296;
	// lwz r30,-21712(r11)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + -21712));  /* glob:g_loop_obj_ptr @ 0x825eab30 */
	// lbz r11,576(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 576);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241158
	if (ctx.r11.u32 != 0) {
		// lwz r11,76(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
		// b 0x8224115c
	} else {
	loc_82241158:
		// lwz r11,64(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 64);
	}
loc_8224115C:
	// cmplwi cr6,r11,0
	// beq cr6,0x822411d8
	if (ctx.r11.u32 != 0) {
		// addi r4,r11,48
		ctx.r4.s64 = ctx.r11.s64 + 48;
		// li r31,0
		var_r31 = 0;
		// lhz r11,4(r4)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 4);
		// cmpwi cr6,r11,0
		// ble cr6,0x822411d8
		if (ctx.r11.s32 <= 0) goto loc_822411D8;
		// li r8,0
		ctx.r8.s64 = 0;
	loc_8224117C:
		// lwz r6,0(r4)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		// li r10,0
		ctx.r10.s64 = 0;
		// mr r11,r5
		ctx.r11.u64 = ctx.r5.u64;
		// lwzx r7,r8,r6
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	loc_8224118C:
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r9,r7
		// beq cr6,0x822411c4
		if (ctx.r9.u32 == ctx.r7.u32) goto loc_822411C4;
		// cmplwi cr6,r9,0
		// beq cr6,0x822411b8
		if (ctx.r9.u32 != 0) {
			// addi r11,r11,4
			ctx.r11.s64 = ctx.r11.s64 + 4;
			// addi r9,r5,200
			ctx.r9.s64 = ctx.r5.s64 + 200;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// cmpw cr6,r11,r9
			// blt cr6,0x8224118c
			if (ctx.r11.s32 < ctx.r9.s32) goto loc_8224118C;
			// b 0x822411c4
		} else {
		loc_822411B8:
			// lwzx r7,r8,r6
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
			// rlwinm r6,r10,2,0,29
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// stwx r7,r6,r5
			PPC_STORE_U32(ctx.r6.u32 + ctx.r5.u32, ctx.r7.u32);
		}
	loc_822411C4:
		// lhz r11,4(r4)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 4);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// cmpw cr6,r31,r11
		// blt cr6,0x8224117c
		if ((int32_t)var_r31 < ctx.r11.s32) goto loc_8224117C;
	}
loc_822411D8:
	// lbz r4,576(r30)
	ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 576);
	// cmplwi cr6,r4,0
	// bne cr6,0x82241274
	if (ctx.r4.u32 == 0) {
		// lwz r11,68(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 68);
		// cmplwi cr6,r11,0
		// beq cr6,0x82241274
		if (ctx.r11.u32 == 0) {
			// ld r30,-16(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// ld r31,-8(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
			// blr
			return;
		}
		// lwz r3,124(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
		// lwz r11,96(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
		// cmplwi cr6,r11,0
		// beq cr6,0x82241274
		if (ctx.r11.u32 == 0) {
			// ld r30,-16(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// ld r31,-8(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
			// blr
			return;
		}
		// addi r4,r11,192
		ctx.r4.s64 = ctx.r11.s64 + 192;
		// li r3,0
		ctx.r3.s64 = 0;
		// lhz r11,4(r4)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 4);
		// cmpwi cr6,r11,0
		// ble cr6,0x82241274
		if (ctx.r11.s32 <= 0) {
			// ld r30,-16(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// ld r31,-8(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
			// blr
			return;
		}
		// li r8,0
		ctx.r8.s64 = 0;
	loc_82241218:
		// lwz r6,0(r4)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		// li r10,0
		ctx.r10.s64 = 0;
		// mr r11,r5
		ctx.r11.u64 = ctx.r5.u64;
		// lwzx r7,r8,r6
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	loc_82241228:
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r9,r7
		// beq cr6,0x82241260
		if (ctx.r9.u32 == ctx.r7.u32) goto loc_82241260;
		// cmplwi cr6,r9,0
		// beq cr6,0x82241254
		if (ctx.r9.u32 != 0) {
			// addi r11,r11,4
			ctx.r11.s64 = ctx.r11.s64 + 4;
			// addi r9,r5,200
			ctx.r9.s64 = ctx.r5.s64 + 200;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// cmpw cr6,r11,r9
			// blt cr6,0x82241228
			if (ctx.r11.s32 < ctx.r9.s32) goto loc_82241228;
			// b 0x82241260
		} else {
		loc_82241254:
			// lwzx r7,r8,r6
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
			// rlwinm r6,r10,2,0,29
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// stwx r7,r6,r5
			PPC_STORE_U32(ctx.r6.u32 + ctx.r5.u32, ctx.r7.u32);
		}
	loc_82241260:
		// lhz r11,4(r4)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 4);
		// addi r3,r3,1
		ctx.r3.s64 = ctx.r3.s64 + 1;
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// cmpw cr6,r3,r11
		// blt cr6,0x82241218
		if (ctx.r3.s32 < ctx.r11.s32) goto loc_82241218;
	}
loc_82241274:
	// ld r30,-16(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_rtti_F348_0"))) PPC_WEAK_FUNC(lvlLevelMgr_rtti_F348_0);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_rtti_F348_0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x8223fcf0
	lvlLevelMgr_vfn_0(ctx, base);
	return;
}

__attribute__((alias("__imp__lvlLevelPiece_vfn_0"))) PPC_WEAK_FUNC(lvlLevelPiece_vfn_0);
PPC_FUNC_IMPL(__imp__lvlLevelPiece_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32251
	// lis r10,-32251
	// addi r11,r11,-3012
	ctx.r11.s64 = ctx.r11.s64 + -3012;
	// addi r10,r10,-2840
	ctx.r10.s64 = ctx.r10.s64 + -2840;
	// lwz r3,128(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 128);
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// cmplwi cr6,r3,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,96(r31)
	PPC_STORE_U32(var_r31 + 96, ctx.r10.u32);
	// beq cr6,0x822412d0
	if (ctx.r3.u32 != 0) {
		// li r4,1
		ctx.r4.s64 = 1;
		// bl 0x820c5658
		rage_5658(ctx, base);
	}
loc_822412D0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x823d90a0
	game_90A0(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x822412f0
	if (ctx.r11.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_822412F0:
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelPiece_vfn_27"))) PPC_WEAK_FUNC(lvlLevelPiece_vfn_27);
PPC_FUNC_IMPL(__imp__lvlLevelPiece_vfn_27) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=176, savegprlr_25
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// lwz r11,128(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 128);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241468
	if (ctx.r11.u32 != 0) {
		// lwz r10,124(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 124);
		// li r28,0
		var_r28 = 0;
		// mr r25,r28
		var_r25 = (uint32_t)(var_r28);
		// lwz r9,96(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 96);
		// lhz r11,208(r9)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 208);
		// cmpwi cr6,r11,0
		// ble cr6,0x82241458
		if (ctx.r11.s32 > 0) {
			// lis r10,-32248
			// lis r11,-32251
			// mr r27,r28
			var_r27 = (uint32_t)(var_r28);
			// addi r26,r11,-836
			var_r26 = (uint32_t)(ctx.r11.s64 + -836);  // lbl_8204FCBC @ 0x8204fcbc
			// lfs f31,-25672(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25672);
			var_f31 = double(temp.f32);
		loc_82241358:
			// lwz r8,124(r29)
			ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 124);
			// lwz r7,96(r8)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 96);
			// lwz r6,204(r7)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 204);
			// lwzx r30,r6,r27
			var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r6.u32 + var_r27));
			// lwz r31,0(r30)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
			// cmplwi cr6,r31,0
			// beq cr6,0x8224143c
			if (var_r31 != 0) {
				// li r7,0
				ctx.r7.s64 = 0;
				// stw r26,80(r1)
				PPC_STORE_U32(ctx.r1.u32 + 80, var_r26);
				// li r6,0
				ctx.r6.s64 = 0;
				// stw r28,84(r1)
				PPC_STORE_U32(ctx.r1.u32 + 84, var_r28);
				// li r5,1
				ctx.r5.s64 = 1;
				// stw r28,88(r1)
				PPC_STORE_U32(ctx.r1.u32 + 88, var_r28);
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// sth r28,92(r1)
				PPC_STORE_U16(ctx.r1.u32 + 92, (uint16_t)var_r28);
				// addi r3,r1,80
				ctx.r3.s64 = ctx.r1.s64 + 80;
				// sth r28,94(r1)
				PPC_STORE_U16(ctx.r1.u32 + 94, (uint16_t)var_r28);
				// bl 0x8224b3f8
				atSingleton_B3F8_g(ctx, base);
				// lwz r5,4(r31)
				ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
				// lfs f0,12(r31)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(var_r31 + 12);
				ctx.f0.f64 = double(temp.f32);
				// li r11,1
				ctx.r11.s64 = 1;
				// rlwinm r4,r5,0,27,27
				ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x10;
				// lfs f13,24(r30)
				temp.u32 = PPC_LOAD_U32(var_r30 + 24);
				ctx.f13.f64 = double(temp.f32);
				// fdivs f13,f13,f0
				ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
				// cmplwi cr6,r4,0
				// bne cr6,0x822413c4
				if (ctx.r4.u32 == 0) {
					// mr r11,r28
					ctx.r11.u64 = var_r28;
				}
			loc_822413C4:
				// clrlwi r11,r11,24
				ctx.r11.u64 = ctx.r11.u32 & 0xFF;
				// fmuls f1,f0,f13
				ctx.fpscr.disableFlushMode();
				ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// cmplwi cr6,r11,0
				// beq cr6,0x822413f0
				if (ctx.r11.u32 != 0) {
					// li r8,1
					ctx.r8.s64 = 1;
					// fmr f2,f31
					ctx.f2.f64 = var_f31;
					// li r7,0
					ctx.r7.s64 = 0;
					// addi r6,r1,80
					ctx.r6.s64 = ctx.r1.s64 + 80;
					// bl 0x82248af8
					LocomotionStateAnim_8AF8_g(ctx, base);
					// b 0x82241400
				} else {
				loc_822413F0:
					// li r7,1
					ctx.r7.s64 = 1;
					// li r6,0
					ctx.r6.s64 = 0;
					// addi r5,r1,80
					ctx.r5.s64 = ctx.r1.s64 + 80;
					// bl 0x822488e0
					LocomotionStateAnim_88E0_g(ctx, base);
				}
			loc_82241400:
				// li r8,1
				ctx.r8.s64 = 1;
				// lbz r7,13(r30)
				ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 13);
				// addi r3,r1,80
				ctx.r3.s64 = ctx.r1.s64 + 80;
				// lwz r6,8(r30)
				ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 8);
				// lwz r5,4(r30)
				ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
				// lwz r4,128(r29)
				ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 128);
				// bl 0x8224c550
				LocomotionStateAnim_C550(ctx, base);
				// addi r3,r1,80
				ctx.r3.s64 = ctx.r1.s64 + 80;
				// stw r26,80(r1)
				PPC_STORE_U32(ctx.r1.u32 + 80, var_r26);
				// bl 0x8224ac28
				atSingleton_AC28_g(ctx, base);
				// lhz r9,94(r1)
				ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
				// cmplwi cr6,r9,0
				// beq cr6,0x8224143c
				if (ctx.r9.u32 == 0) goto loc_8224143C;
				// lwz r3,88(r1)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
				// bl 0x820c00c0
				rage_free_00C0(ctx, base);
			}
		loc_8224143C:
			// lwz r8,124(r29)
			ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 124);
			// addi r25,r25,1
			var_r25 = (uint32_t)(var_r25 + 1);
			// addi r27,r27,4
			var_r27 = (uint32_t)(var_r27 + 4);
			// lwz r7,96(r8)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 96);
			// lhz r11,208(r7)
			ctx.r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 208);
			// cmpw cr6,r25,r11
			// blt cr6,0x82241358
			if ((int32_t)var_r25 < ctx.r11.s32) goto loc_82241358;
		}
	loc_82241458:
		// lwz r3,128(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 128);
		// bl 0x8214c8f8
		LocomotionStateAnim_C8F8_g(ctx, base);
		// li r6,2
		ctx.r6.s64 = 2;
		// stw r6,108(r29)
		PPC_STORE_U32(var_r29 + 108, ctx.r6.u32);
	}
loc_82241468:
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_0"))) PPC_WEAK_FUNC(lvlLevel_vfn_0);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x822414c8
	rage_14C8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x822414b0
	if (ctx.r11.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_822414B0:
	// blr
	return;
}

__attribute__((alias("__imp__rage_14C8"))) PPC_WEAK_FUNC(rage_14C8);
PPC_FUNC_IMPL(__imp__rage_14C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-2820
	ctx.r11.s64 = ctx.r11.s64 + -2820;
	// li r28,0
	var_r28 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* rage_GameObject::vtable@+0x0 */ ctx.r11.u32);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241568
	if (ctx.r11.u32 != 0) {
		// lwz r10,16(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 16);
		// mr r29,r28
		var_r29 = (uint32_t)(var_r28);
		// cmpwi cr6,r10,0
		// ble cr6,0x82241568
		if (ctx.r10.s32 <= 0) goto loc_82241568;
		// mr r30,r28
		var_r30 = (uint32_t)(var_r28);
	loc_82241508:
		// lwz r9,20(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lwzx r11,r30,r9
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
		// lwz r3,124(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
		// cmplwi cr6,r3,0
		// beq cr6,0x82241530
		if (ctx.r3.u32 != 0) {
			// lwz r8,0(r3)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r7,8(r8)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
			// bctrl
			PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
		}
	loc_82241530:
		// lwz r6,20(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lwzx r3,r30,r6
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + ctx.r6.u32);
		// cmplwi cr6,r3,0
		// beq cr6,0x82241554
		if (ctx.r3.u32 != 0) {
			// lwz r5,0(r3)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r11,0(r5)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
			// bctrl
			PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
		}
	loc_82241554:
		// lwz r10,16(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r29,r10
		// blt cr6,0x82241508
		if ((int32_t)var_r29 < ctx.r10.s32) goto loc_82241508;
	}
loc_82241568:
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 24);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224157c
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r28,24(r31)
		PPC_STORE_U32(var_r31 + 24, var_r28);
	}
loc_8224157C:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 28);
	// cmplwi cr6,r3,0
	// beq cr6,0x82241590
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r28,28(r31)
		PPC_STORE_U32(var_r31 + 28, var_r28);
	}
loc_82241590:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 32);
	// cmplwi cr6,r3,0
	// beq cr6,0x822415a4
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r28,32(r31)
		PPC_STORE_U32(var_r31 + 32, var_r28);
	}
loc_822415A4:
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 36);
	// cmplwi cr6,r3,0
	// beq cr6,0x822415b8
	if (ctx.r3.u32 != 0) {
		// bl 0x820c2e18
		rage_2E18(ctx, base);
		// stw r28,36(r31)
		PPC_STORE_U32(var_r31 + 36, var_r28);
	}
loc_822415B8:
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 40);
	// cmplwi cr6,r3,0
	// beq cr6,0x822415dc
	if (ctx.r3.u32 != 0) {
		// lwz r9,0(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r8,0(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
		// stw r28,40(r31)
		PPC_STORE_U32(var_r31 + 40, var_r28);
	}
loc_822415DC:
	// lhz r7,54(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 54);
	// cmplwi cr6,r7,0
	// beq cr6,0x822415f0
	if (ctx.r7.u32 != 0) {
		// lwz r3,48(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 48);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_822415F0:
	// lis r11,-32253
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,13196
	ctx.r11.s64 = ctx.r11.s64 + 13196;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* rage_GameObject::vtable@+0x0 */ ctx.r11.u32);
	// bl 0x821a9420
	atSingleton_9420(ctx, base);
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_23"))) PPC_WEAK_FUNC(lvlLevel_vfn_23);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_23) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r29,0
	var_r29 = 0;
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
	// cmpwi cr6,r11,0
	// ble cr6,0x82241670
	if (ctx.r11.s32 > 0) {
		// li r31,0
		var_r31 = 0;
	loc_82241634:
		// lwz r10,20(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r9,r10,r31
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
		// cmplwi cr6,r9,0
		// beq cr6,0x8224165c
		if (ctx.r9.u32 != 0) {
			// rotlwi r8,r10,0
			ctx.r8.u64 = ctx.r10.u32;
			// lwzx r3,r8,r31
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
			// lwz r6,92(r7)
			// bctrl
			VCALL(ctx.r3.u32, 23, ctx, base);  // vtable slot 23 (byte +92)
		}
	loc_8224165C:
		// lwz r5,16(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r5
		// blt cr6,0x82241634
		if ((int32_t)var_r29 < ctx.r5.s32) goto loc_82241634;
	}
loc_82241670:
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_24"))) PPC_WEAK_FUNC(lvlLevel_vfn_24);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_24) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r29,0
	var_r29 = 0;
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
	// cmpwi cr6,r11,0
	// ble cr6,0x822416d8
	if (ctx.r11.s32 > 0) {
		// li r31,0
		var_r31 = 0;
	loc_8224169C:
		// lwz r10,20(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r9,r10,r31
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
		// cmplwi cr6,r9,0
		// beq cr6,0x822416c4
		if (ctx.r9.u32 != 0) {
			// rotlwi r8,r10,0
			ctx.r8.u64 = ctx.r10.u32;
			// lwzx r3,r8,r31
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
			// lwz r6,96(r7)
			// bctrl
			VCALL(ctx.r3.u32, 24, ctx, base);  // vtable slot 24 (byte +96)
		}
	loc_822416C4:
		// lwz r5,16(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r5
		// blt cr6,0x8224169c
		if ((int32_t)var_r29 < ctx.r5.s32) goto loc_8224169C;
	}
loc_822416D8:
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_25"))) PPC_WEAK_FUNC(lvlLevel_vfn_25);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_25) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// lis r11,-32163
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r11,r11,-18128
	ctx.r11.s64 = ctx.r11.s64 + -18128;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x8224170c
	if (ctx.r11.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224170C:
	// clrlwi r10,r11,24
	ctx.r10.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r10,0
	// bne cr6,0x82241768
	if (ctx.r10.u32 == 0) {
		// lwz r9,16(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 16);
		// li r29,0
		var_r29 = 0;
		// cmpwi cr6,r9,0
		// ble cr6,0x82241768
		if (ctx.r9.s32 <= 0) {
			return;
		}
		// li r31,0
		var_r31 = 0;
	loc_8224172C:
		// lwz r8,20(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r7,r8,r31
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
		// cmplwi cr6,r7,0
		// beq cr6,0x82241754
		if (ctx.r7.u32 != 0) {
			// rotlwi r6,r8,0
			ctx.r6.u64 = ctx.r8.u32;
			// lwzx r3,r6,r31
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r31);
			// lwz r4,100(r5)
			// bctrl
			VCALL(ctx.r3.u32, 25, ctx, base);  // vtable slot 25 (byte +100)
		}
	loc_82241754:
		// lwz r3,16(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r3
		// blt cr6,0x8224172c
		if ((int32_t)var_r29 < ctx.r3.s32) goto loc_8224172C;
	}
loc_82241768:
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_26"))) PPC_WEAK_FUNC(lvlLevel_vfn_26);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_26) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r29,0
	var_r29 = 0;
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
	// cmpwi cr6,r11,0
	// ble cr6,0x822417d0
	if (ctx.r11.s32 > 0) {
		// li r31,0
		var_r31 = 0;
	loc_82241794:
		// lwz r10,20(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r9,r10,r31
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
		// cmplwi cr6,r9,0
		// beq cr6,0x822417bc
		if (ctx.r9.u32 != 0) {
			// rotlwi r8,r10,0
			ctx.r8.u64 = ctx.r10.u32;
			// lwzx r3,r8,r31
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
			// lwz r6,104(r7)
			// bctrl
			VCALL(ctx.r3.u32, 26, ctx, base);  // vtable slot 26 (byte +104)
		}
	loc_822417BC:
		// lwz r5,16(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r5
		// blt cr6,0x82241794
		if ((int32_t)var_r29 < ctx.r5.s32) goto loc_82241794;
	}
loc_822417D0:
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_27"))) PPC_WEAK_FUNC(lvlLevel_vfn_27);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_27) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lbz r11,44(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 44);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241844
	if (ctx.r11.u32 != 0) {
		// lwz r10,16(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 16);
		// li r29,0
		var_r29 = 0;
		// cmpwi cr6,r10,0
		// ble cr6,0x82241844
		if (ctx.r10.s32 <= 0) {
			return;
		}
		// li r31,0
		var_r31 = 0;
	loc_82241808:
		// lwz r9,20(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r8,r9,r31
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r31);
		// cmplwi cr6,r8,0
		// beq cr6,0x82241830
		if (ctx.r8.u32 != 0) {
			// rotlwi r7,r9,0
			ctx.r7.u64 = ctx.r9.u32;
			// lwzx r3,r7,r31
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r31);
			// lwz r5,108(r6)
			// bctrl
			VCALL(ctx.r3.u32, 27, ctx, base);  // vtable slot 27 (byte +108)
		}
	loc_82241830:
		// lwz r4,16(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r4
		// blt cr6,0x82241808
		if ((int32_t)var_r29 < ctx.r4.s32) goto loc_82241808;
	}
loc_82241844:
	return;
}

__attribute__((alias("__imp__lvlLevel_vfn_29"))) PPC_WEAK_FUNC(lvlLevel_vfn_29);
PPC_FUNC_IMPL(__imp__lvlLevel_vfn_29) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lbz r11,44(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 44);
	// cmplwi cr6,r11,0
	// bne cr6,0x82241920
	if (ctx.r11.u32 == 0) {
		// lis r11,-32163
		// addi r11,r11,-18068
		ctx.r11.s64 = ctx.r11.s64 + -18068;
		// lwz r11,4(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// cmplwi cr6,r11,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x82241888
		if (ctx.r11.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82241888:
		// clrlwi r9,r11,24
		ctx.r9.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x822419b4
		if (ctx.r9.u32 == 0) {
			return;
		}
		// lwz r8,20(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwz r7,0(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// cmplwi cr6,r7,0
		// beq cr6,0x822418bc
		if (ctx.r7.u32 != 0) {
			// rotlwi r6,r8,0
			ctx.r6.u64 = ctx.r8.u32;
			// lwz r3,0(r6)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
			// lwz r4,116(r5)
			// bctrl
			VCALL(ctx.r3.u32, 29, ctx, base);  // vtable slot 29 (byte +116)
		}
	loc_822418BC:
		// lwz r3,16(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 16);
		// cmpwi cr6,r3,8
		// blt cr6,0x822419b4
		if (ctx.r3.s32 < 8) {
			return;
		}
		// lwz r11,20(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwz r10,32(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
		// cmplwi cr6,r10,0
		// beq cr6,0x822418f0
		if (ctx.r10.u32 != 0) {
			// rotlwi r9,r11,0
			ctx.r9.u64 = ctx.r11.u32;
			// lwz r3,32(r9)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
			// lwz r7,116(r8)
			// bctrl
			VCALL(ctx.r3.u32, 29, ctx, base);  // vtable slot 29 (byte +116)
		}
	loc_822418F0:
		// lwz r6,20(r30)
		ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwz r5,36(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 36);
		// cmplwi cr6,r5,0
		// beq cr6,0x822419b4
		if (ctx.r5.u32 == 0) {
			return;
		}
		// rotlwi r4,r6,0
		ctx.r4.u64 = ctx.r6.u32;
		// lwz r3,36(r4)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
		// lwz r10,116(r11)
		// bctrl
		VCALL(ctx.r3.u32, 29, ctx, base);  // vtable slot 29 (byte +116)
		return;
	}
loc_82241920:
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,10
	// blt cr6,0x82241934
	if (ctx.r11.s32 >= 10) {
		// li r11,10
		ctx.r11.s64 = 10;
	}
loc_82241934:
	// mr r29,r11
	var_r29 = ctx.r11.u32;
	// cmpwi cr6,r11,0
	// blt cr6,0x8224197c
	if (ctx.r11.s32 >= 0) {
		// rlwinm r31,r11,2,0,29
		var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
	loc_82241944:
		// lwz r9,20(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r8,r31,r9
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
		// cmplwi cr6,r8,0
		// beq cr6,0x8224196c
		if (ctx.r8.u32 != 0) {
			// rotlwi r7,r9,0
			ctx.r7.u64 = ctx.r9.u32;
			// lwzx r3,r31,r7
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + ctx.r7.u32);
			// lwz r5,116(r6)
			// bctrl
			VCALL(ctx.r3.u32, 29, ctx, base);  // vtable slot 29 (byte +116)
		}
	loc_8224196C:
		// addi r29,r29,-1
		var_r29 = (uint32_t)(var_r29 + -1);
		// addi r31,r31,-4
		var_r31 = (uint32_t)(var_r31 + -4);
		// cmpwi cr6,r29,0
		// bge cr6,0x82241944
		if ((int32_t)var_r29 >= 0) goto loc_82241944;
	}
loc_8224197C:
	// lwz r4,16(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 16);
	// cmpwi cr6,r4,11
	// ble cr6,0x822419b4
	if (ctx.r4.s32 > 11) {
		// lwz r3,20(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwz r11,44(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
		// cmplwi cr6,r11,0
		// beq cr6,0x822419b4
		if (ctx.r11.u32 == 0) {
			return;
		}
		// rotlwi r10,r3,0
		ctx.r10.u64 = ctx.r3.u32;
		// lwz r11,44(r10)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 44);
		// addi r3,r11,96
		ctx.r3.s64 = ctx.r11.s64 + 96;
		// lwz r8,4(r9)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	}
loc_822419B4:
	return;
}

__attribute__((alias("__imp__game_19C0"))) PPC_WEAK_FUNC(game_19C0);
PPC_FUNC_IMPL(__imp__game_19C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lbz r11,44(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 44);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241a1c
	if (ctx.r11.u32 != 0) {
		// lwz r10,16(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 16);
		// li r29,0
		var_r29 = 0;
		// cmpwi cr6,r10,0
		// ble cr6,0x82241a1c
		if (ctx.r10.s32 <= 0) {
			return;
		}
		// li r31,0
		var_r31 = 0;
	loc_822419F0:
		// lwz r11,20(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r9,r11,r31
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
		// cmplwi cr6,r9,0
		// beq cr6,0x82241a08
		if (ctx.r9.u32 != 0) {
			// rotlwi r3,r9,0
			ctx.r3.u64 = ctx.r9.u32;
			// bl 0x823d9510
			game_9510(ctx, base);
		}
	loc_82241A08:
		// lwz r8,16(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r8
		// blt cr6,0x822419f0
		if ((int32_t)var_r29 < ctx.r8.s32) goto loc_822419F0;
	}
loc_82241A1C:
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_1A28_fw"))) PPC_WEAK_FUNC(lvlLevelMgr_1A28_fw);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_1A28_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	// FRAME: size=176, savegprlr_21
	// mr r23,r3
	var_r23 = ctx.r3.u32;
	// mr r24,r4
	var_r24 = ctx.r4.u32;
	// mr r26,r5
	var_r26 = ctx.r5.u32;
	// li r22,0
	var_r22 = 0;
	// li r30,0
	var_r30 = 0;
	// lwz r11,16(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 16);
	// cmpwi cr6,r11,0
	// ble cr6,0x82241b10
	if (ctx.r11.s32 > 0) {
		// lis r25,-32142
		var_r25 = (uint32_t)(-2106458112);
		// li r29,0
		var_r29 = 0;
		// li r21,1
		var_r21 = 1;
		// lwz r31,-23784(r25)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r25 + -23784));
	loc_82241A64:
		// lwz r11,28(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
		// mr r7,r26
		ctx.r7.u64 = var_r26;
		// lwz r10,12(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 12);
		// li r6,1
		ctx.r6.s64 = 1;
		// addi r9,r11,1
		ctx.r9.s64 = ctx.r11.s64 + 1;
		// lwz r8,5760(r24)
		ctx.r8.u64 = PPC_LOAD_U32(var_r24 + 5760);
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// rlwinm r4,r9,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r28,r29,r8
		var_r28 = (uint32_t)(PPC_LOAD_U32(var_r29 + ctx.r8.u32));
		// lwzx r3,r4,r10
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
		// lwz r11,8(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r27,32(r11)
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 32));
		// mr r4,r27
		ctx.r4.u64 = var_r27;
		// bl 0x821dc2f8
		lvlLevelMgr_C2F8_2hr(ctx, base);
		// clrlwi r10,r3,24
		ctx.r10.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x82241ab4
		if (ctx.r10.u32 != 0) {
			// slw r9,r21,r30
			ctx.r9.u64 = (uint8_t)var_r30 & 0x20 ? 0 : (var_r21 << ((uint8_t)var_r30 & 0x3F));
			// or r22,r9,r22
			var_r22 = (uint32_t)(ctx.r9.u64 | var_r22);
		}
	loc_82241AB4:
		// lwz r11,20(r23)
		ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 20);
		// lwzx r8,r29,r11
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
		// cmplwi cr6,r8,0
		// beq cr6,0x82241afc
		if (ctx.r8.u32 != 0) {
			// mr r7,r26
			ctx.r7.u64 = var_r26;
			// li r6,1
			ctx.r6.s64 = 1;
			// mr r5,r30
			ctx.r5.u64 = var_r30;
			// mr r4,r27
			ctx.r4.u64 = var_r27;
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// rotlwi r31,r8,0
			var_r31 = (uint32_t)(ctx.r8.u32);
			// bl 0x821dc2f8
			lvlLevelMgr_C2F8_2hr(ctx, base);
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// mr r4,r3
			ctx.r4.u64 = ctx.r3.u64;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,140(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 35, ctx, base);  // pattern-B slot 35 (byte +140)
			// lwz r31,-23784(r25)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r25 + -23784));
		}
	loc_82241AFC:
		// lwz r5,16(r23)
		ctx.r5.u64 = PPC_LOAD_U32(var_r23 + 16);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpw cr6,r30,r5
		// blt cr6,0x82241a64
		if ((int32_t)var_r30 < ctx.r5.s32) goto loc_82241A64;
	}
loc_82241B10:
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// lwz r11,26248(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26248);
	// stw r22,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r22);
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// lwz r11,26252(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26252);
	// stw r22,92(r11)
	PPC_STORE_U32(ctx.r11.u32 + 92, var_r22);
	return;
}

__attribute__((alias("__imp__game_1B30"))) PPC_WEAK_FUNC(game_1B30);
PPC_FUNC_IMPL(__imp__game_1B30) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x823d99a0
	game_99A0(ctx, base);
	// lis r11,-32251
	// lis r10,-32251
	// addi r11,r11,-3012
	ctx.r11.s64 = ctx.r11.s64 + -3012;
	// addi r10,r10,-2840
	ctx.r10.s64 = ctx.r10.s64 + -2840;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,96(r31)
	PPC_STORE_U32(var_r31 + 96, ctx.r10.u32);
	// lwz r11,128(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 128);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241bb0
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwz r9,76(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// divwu r11,r8,r9
		ctx.r11.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// addi r7,r11,2
		ctx.r7.s64 = ctx.r11.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r6,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r30);
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r3,0
		// stw r3,128(r31)
		PPC_STORE_U32(var_r31 + 128, ctx.r3.u32);
		// beq cr6,0x82241bb0
		if (ctx.r3.u32 == 0) goto loc_82241BB0;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x8214c388
		crAnimDofFloat_C388_g(ctx, base);
	}
loc_82241BB0:
	// lwz r11,132(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 132);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x82241be8
	if (ctx.r11.u32 != 0) {
		// lwz r5,4(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwz r4,76(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r10,r5,r11
		ctx.r10.s64 = ctx.r11.s64 - ctx.r5.s64;
		// twllei r4,0
		if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
		// divwu r10,r10,r4
		ctx.r10.u32 = ctx.r4.u32 ? ctx.r10.u32 / ctx.r4.u32 : 0;
		// addi r9,r10,2
		ctx.r9.s64 = ctx.r10.s64 + 2;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r8,r30
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r30);
		// add r7,r10,r11
		ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r7,132(r31)
		PPC_STORE_U32(var_r31 + 132, ctx.r7.u32);
	}
loc_82241BE8:
	// blr
	return;
}

__attribute__((alias("__imp__game_1C00"))) PPC_WEAK_FUNC(game_1C00);
PPC_FUNC_IMPL(__imp__game_1C00) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// bl 0x8212b3b0
	ph_ctor_B3B0(ctx, base);
	// lis r11,-32251
	// addi r11,r11,-2820
	ctx.r11.s64 = ctx.r11.s64 + -2820;
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// lwz r11,48(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 48);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241c58
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lwz r9,76(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r8,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r10,2
		ctx.r7.s64 = ctx.r10.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r6,r31
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r31);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,48(r30)
		PPC_STORE_U32(var_r30 + 48, ctx.r5.u32);
	}
loc_82241C58:
	// lis r29,-32160
	var_r29 = (uint32_t)(-2107637760);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r30,24
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 24;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r3,25604(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 25604);
	// lwz r10,40(r11)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// lwz r3,25604(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 25604);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r30,28
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 28;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r8,40(r9)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// lwz r3,25604(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 25604);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r30,32
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 32;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r11,40(r7)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// lwz r3,25604(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 25604);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r30,36
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 36;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r9,40(r10)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// lis r11,-32160
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lwz r3,25956(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25956);
	// bl 0x822405a0
	rage_05A0(ctx, base);
	// lwz r11,20(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241d20
	if (ctx.r11.u32 != 0) {
		// lwz r8,4(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lwz r7,76(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 76);
		// subf r6,r8,r11
		ctx.r6.s64 = ctx.r11.s64 - ctx.r8.s64;
		// twllei r7,0
		if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
		// divwu r10,r6,r7
		ctx.r10.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
		// addi r5,r10,2
		ctx.r5.s64 = ctx.r10.s64 + 2;
		// rlwinm r4,r5,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r4,r31
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + var_r31);
		// add r3,r10,r11
		ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r3,20(r30)
		PPC_STORE_U32(var_r30 + 20, ctx.r3.u32);
	}
loc_82241D20:
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
	// li r28,0
	var_r28 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x82241d90
	if (ctx.r11.s32 > 0) {
		// li r29,0
		var_r29 = 0;
	loc_82241D34:
		// lwz r10,20(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 20);
		// lwzx r11,r10,r29
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r29);
		// cmplwi cr6,r11,0
		// beq cr6,0x82241d7c
		if (ctx.r11.u32 != 0) {
			// lwz r9,4(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
			// lwz r8,76(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 76);
			// subf r7,r9,r11
			ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
			// twllei r8,0
			if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
			// divwu r9,r7,r8
			ctx.r9.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
			// addi r6,r9,2
			ctx.r6.s64 = ctx.r9.s64 + 2;
			// rlwinm r5,r6,2,0,29
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r9,r5,r31
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r31);
			// add r3,r9,r11
			ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
			// cmplwi cr6,r3,0
			// stwx r3,r10,r29
			PPC_STORE_U32(ctx.r10.u32 + var_r29, ctx.r3.u32);
			// beq cr6,0x82241d7c
			if (ctx.r3.u32 == 0) goto loc_82241D7C;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// bl 0x82241b30
			game_1B30(ctx, base);
		}
	loc_82241D7C:
		// lwz r4,16(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpw cr6,r28,r4
		// blt cr6,0x82241d34
		if ((int32_t)var_r28 < ctx.r4.s32) goto loc_82241D34;
	}
loc_82241D90:
	// lwz r11,40(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 40);
	// cmplwi cr6,r11,0
	// beq cr6,0x82241e1c
	if (ctx.r11.u32 != 0) {
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lwz r10,76(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 76);
		// subf r9,r3,r11
		ctx.r9.s64 = ctx.r11.s64 - ctx.r3.s64;
		// twllei r10,0
		if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
		// divwu r10,r9,r10
		ctx.r10.u32 = ctx.r10.u32 ? ctx.r9.u32 / ctx.r10.u32 : 0;
		// addi r8,r10,2
		ctx.r8.s64 = ctx.r10.s64 + 2;
		// rlwinm r7,r8,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r7,r31
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r31);
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// cmplwi cr6,r11,0
		// stw r11,40(r30)
		PPC_STORE_U32(var_r30 + 40, ctx.r11.u32);
		// beq cr6,0x82241e1c
		if (ctx.r11.u32 == 0) goto loc_82241E1C;
		// lis r10,-32249
		// addi r10,r10,-20344
		ctx.r10.s64 = ctx.r10.s64 + -20344;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// lwz r10,4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// cmplwi cr6,r10,0
		// beq cr6,0x82241e1c
		if (ctx.r10.u32 == 0) goto loc_82241E1C;
		// lwz r6,4(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lwz r5,76(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 76);
		// subf r4,r6,r10
		ctx.r4.s64 = ctx.r10.s64 - ctx.r6.s64;
		// twllei r5,0
		if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
		// divwu r9,r4,r5
		ctx.r9.u32 = ctx.r5.u32 ? ctx.r4.u32 / ctx.r5.u32 : 0;
		// addi r3,r9,2
		ctx.r3.s64 = ctx.r9.s64 + 2;
		// rlwinm r9,r3,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r9,r9,r31
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r31);
		// add r3,r9,r10
		ctx.r3.u64 = ctx.r9.u64 + ctx.r10.u64;
		// cmplwi cr6,r3,0
		// stw r3,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r3.u32);
		// beq cr6,0x82241e1c
		if (ctx.r3.u32 == 0) goto loc_82241E1C;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x8237f2b0
		game_F2B0(ctx, base);
	}
loc_82241E1C:
	// lhz r11,52(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 52);
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r7,0
	// blt cr6,0x82241e78
	if (ctx.r7.s32 >= 0) {
		// rlwinm r10,r7,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82241E30:
		// lwz r9,48(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 48);
		// lwzx r11,r10,r9
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
		// cmplwi cr6,r11,0
		// beq cr6,0x82241e68
		if (ctx.r11.u32 != 0) {
			// lwz r8,4(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 4);
			// lwz r6,76(r31)
			ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 76);
			// subf r5,r8,r11
			ctx.r5.s64 = ctx.r11.s64 - ctx.r8.s64;
			// twllei r6,0
			if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
			// divwu r8,r5,r6
			ctx.r8.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
			// addi r4,r8,2
			ctx.r4.s64 = ctx.r8.s64 + 2;
			// rlwinm r3,r4,2,0,29
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r8,r3,r31
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r31);
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
			// stwx r11,r10,r9
			PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r11.u32);
		}
	loc_82241E68:
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// addi r10,r10,-4
		ctx.r10.s64 = ctx.r10.s64 + -4;
		// cmpwi cr6,r7,0
		// bge cr6,0x82241e30
		if (ctx.r7.s32 >= 0) goto loc_82241E30;
	}
loc_82241E78:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__lvlTableTop_vfn_39"))) PPC_WEAK_FUNC(lvlTableTop_vfn_39);
PPC_FUNC_IMPL(__imp__lvlTableTop_vfn_39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32160
	// lwz r4,116(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 116);
	// lwz r3,25960(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25960);
	// bl 0x8227f810
	pongBallInstance_F810_p46(ctx, base);
	// stw r3,124(r31)
	PPC_STORE_U32(var_r31 + 124, ctx.r3.u32);
	// blr
	return;
}

__attribute__((alias("__imp__lvlLevelMgr_1EC8_wrh"))) PPC_WEAK_FUNC(lvlLevelMgr_1EC8_wrh);
PPC_FUNC_IMPL(__imp__lvlLevelMgr_1EC8_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32163
	// lwz r10,116(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	// li r9,4
	ctx.r9.s64 = 4;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f31,-30228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -30228);
	var_f31 = double(temp.f32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r31,12(r11)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 12));
	// stw r9,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r9.u32);
	// stw r8,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r8.u32);
	// lbz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
	// cmplwi cr6,r7,9
	// bne cr6,0x82241f7c
	if (ctx.r7.u32 == 9) {
		// lwz r6,112(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 112);
		// lwz r30,0(r6)
		var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r6.u32 + 0));
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r11,44(r5)
		// bctrl
		VCALL(ctx.r3.u32, 11, ctx, base);  // vtable slot 11 (byte +44)
		// addi r10,r30,32
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 32;
		// addi r9,r30,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
		// lfs f13,36(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 36);
		ctx.f13.f64 = double(temp.f32);
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// addi r11,r31,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v0,v13,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lfs f12,4(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// fsubs f11,f12,f13
		ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f10,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f10.f64 = double(temp.f32);
		// fsubs f31,f11,f10
		var_f31 = double(float(ctx.f11.f64 - ctx.f10.f64));
		// b 0x82241fac
	} else {
	loc_82241F7C:
		// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r6,44(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 11, ctx, base);  // pattern-B slot 11 (byte +44)
		// addi r11,r31,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
		// addi r5,r31,32
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 32;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v13,v13,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v13,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_82241FAC:
	// lfs f9,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lis r11,-32142
	// fsubs f8,f9,f31
	ctx.f8.f64 = double(float(ctx.f9.f64 - var_f31));
	// stfs f8,84(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lfs f0,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,-23804(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23804);
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// stfs f31,25784(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 25784, temp.u32);  /* glob:lbl_826064B8 @ 0x826064b8 */
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__lvlTable_vfn_0"))) PPC_WEAK_FUNC(lvlTable_vfn_0);
PPC_FUNC_IMPL(__imp__lvlTable_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x82242050
	rage_2050(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x82242038
	if (ctx.r11.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82242038:
	// blr
	return;
}

__attribute__((alias("__imp__rage_2050"))) PPC_WEAK_FUNC(rage_2050);
PPC_FUNC_IMPL(__imp__rage_2050) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32251
	// lis r10,-32251
	// addi r11,r11,-2524
	ctx.r11.s64 = ctx.r11.s64 + -2524;
	// addi r10,r10,-2348
	ctx.r10.s64 = ctx.r10.s64 + -2348;
	// lwz r3,264(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 264);
	// cmplwi cr6,r3,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* rage_GameObject::vtable@+0x0 */ ctx.r11.u32);
	// stw r10,96(r31)
	PPC_STORE_U32(var_r31 + 96, ctx.r10.u32);
	// beq cr6,0x8224209c
	if (ctx.r3.u32 != 0) {
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	}
loc_8224209C:
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 256);
	// cmplwi cr6,r3,0
	// beq cr6,0x822420bc
	if (ctx.r3.u32 != 0) {
		// lwz r9,0(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r8,0(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
	}
loc_822420BC:
	// lwz r3,252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 252);
	// cmplwi cr6,r3,0
	// beq cr6,0x822420dc
	if (ctx.r3.u32 != 0) {
		// lwz r7,0(r3)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r6,0(r7)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
	}
loc_822420DC:
	// lwz r3,260(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 260);
	// cmplwi cr6,r3,0
	// beq cr6,0x822420fc
	if (ctx.r3.u32 != 0) {
		// lwz r5,0(r3)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r11,0(r5)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	}
loc_822420FC:
	// lwz r3,228(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 228);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224211c
	if (ctx.r3.u32 != 0) {
		// lwz r10,0(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r9,0(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	}
loc_8224211C:
	// lwz r3,224(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 224);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224213c
	if (ctx.r3.u32 != 0) {
		// lwz r8,0(r3)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r7,0(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
	}
loc_8224213C:
	// lwz r3,220(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 220);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224215c
	if (ctx.r3.u32 != 0) {
		// lwz r6,0(r3)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r5,0(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r5.u32);
	}
loc_8224215C:
	// lhz r4,250(r31)
	ctx.r4.u64 = PPC_LOAD_U16(var_r31 + 250);
	// cmplwi cr6,r4,0
	// beq cr6,0x82242170
	if (ctx.r4.u32 != 0) {
		// lwz r3,244(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 244);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_82242170:
	// lhz r3,242(r31)
	ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 242);
	// cmplwi cr6,r3,0
	// beq cr6,0x82242184
	if (ctx.r3.u32 != 0) {
		// lwz r3,236(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 236);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_82242184:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x823d90a0
	game_90A0(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__lvlTable_vfn_27"))) PPC_WEAK_FUNC(lvlTable_vfn_27);
PPC_FUNC_IMPL(__imp__lvlTable_vfn_27) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	double var_f21 = 0.0;
	double var_f30 = 0.0;
	double var_f22 = 0.0;
	double var_f31 = 0.0;
	double var_f25 = 0.0;
	double var_f29 = 0.0;
	double var_f23 = 0.0;
	double var_f24 = 0.0;
	double var_f26 = 0.0;
	double var_f28 = 0.0;
	double var_f27 = 0.0;
	double var_f20 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f868
	ctx.lr = 0x822421A8;
	__savegprlr_16(ctx, base);
	// addi r12,r1,-136
	ctx.r12.s64 = ctx.r1.s64 + -136;
	// bl 0x824365f8
	__savefpr_20(ctx, base);
	// li r12,-256
	ctx.r12.s64 = -256;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-608(r1)
	ea = -608 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r20,-32142
	var_r20 = (uint32_t)(-2106458112);
	// lis r11,6
	ctx.r11.s64 = 393216;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// ori r22,r11,36080
	var_r22 = (uint32_t)(ctx.r11.u64 | 36080);
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r28,-23828(r20)
	var_r28 = (uint32_t)(PPC_LOAD_U32(var_r20 + -23828));
	// add r27,r28,r22
	var_r27 = (uint32_t)(var_r28 + var_r22);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 0);
	// cmpwi cr6,r10,1
	// beq cr6,0x822421e8
	if (ctx.r10.s32 != 1) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822421E8:
	// lis r10,-32161
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// cmplwi cr6,r8,0
	// addi r19,r11,-20808
	var_r19 = (uint32_t)(ctx.r11.s64 + -20808);  // lbl_825CAEB8 @ 0x825caeb8
	// lwz r30,-21712(r10)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + -21712));
	// beq cr6,0x82242218
	if (ctx.r8.u32 != 0) {
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x821ccbd0
		util_CBD0(ctx, base);
		// lfs f0,520(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 520);
		ctx.f0.f64 = double(temp.f32);
		// fdivs f21,f1,f0
		var_f21 = double(float(ctx.f1.f64 / ctx.f0.f64));
		// b 0x8224221c
	} else {
	loc_82242218:
		// lfs f21,8(r19)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		var_f21 = double(temp.f32);
	}
loc_8224221C:
	// lfs f13,164(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 164);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f21,f13
	ctx.f12.f64 = double(float(var_f21 + ctx.f13.f64));
	// stfs f12,164(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 164, temp.u32);
	// lbz r7,495(r30)
	ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 495);
	// cmplwi cr6,r7,0
	// bne cr6,0x8224224c
	if (ctx.r7.u32 == 0) {
		// lis r11,-32142
		ctx.r11.s64 = -2106458112;
		// lwz r11,-23740(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23740);
		// lbz r6,24(r11)
		ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 24);
		// li r11,0
		ctx.r11.s64 = 0;
		// cmplwi cr6,r6,0
		// beq cr6,0x82242250
		if (ctx.r6.u32 == 0) goto loc_82242250;
	}
loc_8224224C:
	// li r11,1
	ctx.r11.s64 = 1;
loc_82242250:
	// clrlwi r4,r11,24
	ctx.r4.u64 = ctx.r11.u32 & 0xFF;
	// lis r11,-32253
	// lis r16,-32142
	var_r16 = (uint32_t)(-2106458112);
	// addi r17,r11,-15132
	var_r17 = (uint32_t)(ctx.r11.s64 + -15132);  // lbl_8202C4E4 @ 0x8202c4e4
	// cmplwi cr6,r4,0
	// lwz r30,-23804(r16)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r16 + -23804));
	// lfs f30,3116(r17)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r17 + 3116);
	var_f30 = double(temp.f32);
	// lfs f22,3204(r17)
	temp.u32 = PPC_LOAD_U32(var_r17 + 3204);
	var_f22 = double(temp.f32);
	// bne cr6,0x822423b4
	if (ctx.r4.u32 == 0) {
		// lfs f11,160(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 160);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f11,f30
		// bge cr6,0x82242288
		if (ctx.f11.f64 < var_f30) {
			// stfs f30,160(r31)
			temp.f32 = float(var_f30);
			PPC_STORE_U32(var_r31 + 160, temp.u32);
			// stfs f30,164(r31)
			temp.f32 = float(var_f30);
			PPC_STORE_U32(var_r31 + 164, temp.u32);
		}
	loc_82242288:
		// lfs f0,160(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 160);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f30
		// ble cr6,0x822422ac
		if (ctx.f0.f64 > var_f30) {
			// lwz r3,0(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
			// lfs f10,116(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
			ctx.f10.f64 = double(temp.f32);
			// fadds f13,f10,f0
			ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
			// fmuls f9,f13,f13
			ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
			// fnmsubs f8,f9,f21,f0
			ctx.f8.f64 = double(float(-(ctx.f9.f64 * var_f21 - ctx.f0.f64)));
			// stfs f8,160(r31)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r31 + 160, temp.u32);
		}
	loc_822422AC:
		// lwz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
		// lis r29,-32142
		var_r29 = (uint32_t)(-2106458112);
		// cntlzw r10,r11
		ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
		// rlwinm r7,r10,27,31,31
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
		// cmplwi cr6,r7,0
		// beq cr6,0x82242350
		if (ctx.r7.u32 != 0) {
			// lwz r6,0(r30)
			ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 0);
			// lfs f0,160(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 160);
			ctx.f0.f64 = double(temp.f32);
			// lfs f7,108(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 108);
			ctx.f7.f64 = double(temp.f32);
			// fcmpu cr6,f0,f7
			// beq cr6,0x82242350
			if (ctx.f0.f64 != ctx.f7.f64) {
				// mr r3,r28
				ctx.r3.u64 = var_r28;
				// lfs f6,164(r31)
				temp.u32 = PPC_LOAD_U32(var_r31 + 164);
				ctx.f6.f64 = double(temp.f32);
				// stfs f6,136(r1)
				temp.f32 = float(ctx.f6.f64);
				PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
				// stfs f0,132(r1)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
				// bl 0x821cdc88
				game_DC88(ctx, base);
				// clrlwi r5,r3,24
				ctx.r5.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r5,0
				// beq cr6,0x8224231c
				if (ctx.r5.u32 != 0) {
				// mr r3,r28
				ctx.r3.u64 = var_r28;
				// bl 0x821cded8
				pongLerpQueue_DED8_g(ctx, base);
				// lis r11,-32142
				// fadds f13,f1,f22
				ctx.fpscr.disableFlushMode();
				ctx.f13.f64 = double(float(ctx.f1.f64 + var_f22));
				// lwz r11,-23832(r11)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23832);
				// lfs f0,52(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
				ctx.f0.f64 = double(temp.f32);
				// fsubs f5,f0,f13
				ctx.f5.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
				// fsel f0,f5,f0,f13
				ctx.f0.f64 = ctx.f5.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
				// b 0x82242328
				} else {
				loc_8224231C:
				// lis r11,-32142
				ctx.r11.s64 = -2106458112;
				// lwz r11,-23832(r11)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23832);
				// lfs f0,52(r11)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
				ctx.f0.f64 = double(temp.f32);
				}
				loc_82242328:
				// addi r4,r1,128
				ctx.r4.s64 = ctx.r1.s64 + 128;
				// stfs f0,128(r1)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
				// mr r3,r28
				ctx.r3.u64 = var_r28;
				// bl 0x821cd248
				game_D248(ctx, base);
				// lfs f4,160(r31)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(var_r31 + 160);
				ctx.f4.f64 = double(temp.f32);
				// fcmpu cr6,f4,f30
			} else {
				if (ctx.f4.f64 != var_f30) {
					// li r10,1
					ctx.r10.s64 = 1;
					// stb r10,-22540(r29)
					PPC_STORE_U8(var_r29 + -22540, ctx.r10.u8);
					// b 0x82242354
					} else {
				}
			}
		loc_82242350:
			// lbz r10,-22540(r29)
			ctx.r10.u64 = PPC_LOAD_U8(var_r29 + -22540);
		}
	loc_82242354:
		// lwz r4,0(r27)
		ctx.r4.u64 = PPC_LOAD_U32(var_r27 + 0);
		// li r11,1
		ctx.r11.s64 = 1;
		// cmpwi cr6,r4,1
		// beq cr6,0x82242368
		if (ctx.r4.s32 != 1) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82242368:
		// clrlwi r11,r11,24
		ctx.r11.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x822423b4
		if (ctx.r11.u32 == 0) goto loc_822423B4;
		// lis r9,1
		ctx.r9.s64 = 65536;
		// lis r7,1
		ctx.r7.s64 = 65536;
		// ori r8,r9,4964
		ctx.r8.u64 = ctx.r9.u64 | 4964;
		// ori r6,r7,4960
		ctx.r6.u64 = ctx.r7.u64 | 4960;
		// clrlwi r5,r10,24
		ctx.r5.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// lfsx f3,r28,r8
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r8.u32);
		ctx.f3.f64 = double(temp.f32);
		// stfs f3,164(r31)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(var_r31 + 164, temp.u32);
		// lfsx f0,r28,r6
		temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r6.u32);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,160(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 160, temp.u32);
		// beq cr6,0x822423b4
		if (ctx.r5.u32 == 0) goto loc_822423B4;
		// fcmpu cr6,f0,f30
		// ble cr6,0x822423b4
		if (ctx.f0.f64 <= var_f30) goto loc_822423B4;
		// li r11,0
		ctx.r11.s64 = 0;
		// stfs f30,164(r31)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(var_r31 + 164, temp.u32);
		// stb r11,-22540(r29)
		PPC_STORE_U8(var_r29 + -22540, ctx.r11.u8);
	}
loc_822423B4:
	// lwz r4,172(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 172);
	// cmpwi cr6,r4,-1
	// beq cr6,0x82243288
	if (ctx.r4.s32 != -1) {
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r8,r1,208
		ctx.r8.s64 = ctx.r1.s64 + 208;
		// addi r7,r1,224
		ctx.r7.s64 = ctx.r1.s64 + 224;
		// lfs f11,164(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 164);
		ctx.f11.f64 = double(temp.f32);
		// addi r10,r11,48
		ctx.r10.s64 = ctx.r11.s64 + 48;
		// lfs f31,3108(r17)
		temp.u32 = PPC_LOAD_U32(var_r17 + 3108);
		var_f31 = double(temp.f32);
		// addi r9,r11,64
		ctx.r9.s64 = ctx.r11.s64 + 64;
		// addi r3,r31,128
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 128;
		// lfs f2,104(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 104);
		ctx.f2.f64 = double(temp.f32);
		// addi r5,r1,128
		ctx.r5.s64 = ctx.r1.s64 + 128;
		// fmuls f0,f2,f11
		ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,208
		ctx.r4.s64 = ctx.r1.s64 + 208;
		// stvx v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,224(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
		ctx.f13.f64 = double(temp.f32);
		// lfs f1,208(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
		ctx.f1.f64 = double(temp.f32);
		// lfs f10,212(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
		ctx.f10.f64 = double(temp.f32);
		// fadds f12,f13,f1
		ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f1.f64));
		// lfs f9,228(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
		ctx.f9.f64 = double(temp.f32);
		// fadds f8,f10,f9
		ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// lfs f13,3112(r17)
		temp.u32 = PPC_LOAD_U32(var_r17 + 3112);
		ctx.f13.f64 = double(temp.f32);
		// lfs f7,216(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
		ctx.f7.f64 = double(temp.f32);
		// fcmpu cr6,f0,f31
		ctx.cr6.compare(ctx.f0.f64, var_f31);
		// stfs f7,264(r1)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
		// lvx128 v11,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r1,224
		ctx.r3.s64 = ctx.r1.s64 + 224;
		// fmuls f25,f12,f13
		var_f25 = double(float(ctx.f12.f64 * ctx.f13.f64));
		// stfs f25,256(r1)
		temp.f32 = float(var_f25);
		PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
		// fmuls f6,f8,f13
		ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
		// stfs f6,260(r1)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
		// addi r6,r1,256
		ctx.r6.s64 = ctx.r1.s64 + 256;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v10,v11,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v9,v13,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v8,v12,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stfs f30,136(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
		// stvx v9,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// ble cr6,0x8224246c
		if (ctx.cr6.gt) {
			// fmr f0,f31
			ctx.f0.f64 = var_f31;
		}
	loc_8224246C:
		// stfs f30,112(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// addi r11,r1,128
		ctx.r11.s64 = ctx.r1.s64 + 128;
		// stfs f30,116(r1)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
		// addi r8,r1,128
		ctx.r8.s64 = ctx.r1.s64 + 128;
		// stfs f30,120(r1)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// stfs f0,80(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// lfs f5,208(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
		ctx.f5.f64 = double(temp.f32);
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32248
		// lfs f4,224(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
		ctx.f4.f64 = double(temp.f32);
		// lvx128 v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// fsubs f0,f5,f4
		ctx.f0.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
		// vsubfp v13,v7,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvx128 v6,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// lfs f3,212(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
		ctx.f3.f64 = double(temp.f32);
		// lfs f2,228(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
		ctx.f2.f64 = double(temp.f32);
		// fsubs f12,f3,f2
		ctx.f12.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
		// stfs f0,240(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 240, temp.u32);
		// stfs f12,244(r1)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
		// fmuls f0,f0,f13
		ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
		// vmaddfp v5,v13,v12,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// fmuls f13,f12,f13
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
		// stvx v5,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f12,128(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f12.f64 = double(temp.f32);
		// fsubs f1,f0,f12
		ctx.f1.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// fadds f12,f0,f12
		ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
		// lfs f0,132(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
		ctx.f0.f64 = double(temp.f32);
		// fsubs f10,f13,f0
		ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// fadds f9,f13,f0
		ctx.f9.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
		// fdivs f8,f31,f1
		ctx.f8.f64 = double(float(var_f31 / ctx.f1.f64));
		// fdivs f7,f31,f12
		ctx.f7.f64 = double(float(var_f31 / ctx.f12.f64));
		// fdivs f6,f31,f10
		ctx.f6.f64 = double(float(var_f31 / ctx.f10.f64));
		// fdivs f5,f31,f9
		ctx.f5.f64 = double(float(var_f31 / ctx.f9.f64));
		// fabs f4,f8
		ctx.f4.u64 = ctx.f8.u64 & ~0x8000000000000000;
		// stfs f4,272(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
		// fabs f3,f7
		ctx.f3.u64 = ctx.f7.u64 & ~0x8000000000000000;
		// stfs f3,280(r1)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
		// fabs f2,f6
		ctx.f2.u64 = ctx.f6.u64 & ~0x8000000000000000;
		// stfs f2,276(r1)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
		// fabs f1,f5
		ctx.f1.u64 = ctx.f5.u64 & ~0x8000000000000000;
		// stfs f1,284(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 284, temp.u32);
		// lwz r7,0(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lfs f0,112(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 112);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f13,f0,f11
		ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
		// lfs f0,-25808(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25808);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f1,f13,f0
		ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// bl 0x824301d8
		phBoundCapsule_01D8_g(ctx, base);
		// frsp f0,f1
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = double(float(ctx.f1.f64));
		// lfs f12,160(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 160);
		ctx.f12.f64 = double(temp.f32);
		// lis r11,-32163
		// lwz r6,124(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 124);
		// lwz r4,172(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 172);
		// addi r18,r11,-25472
		var_r18 = (uint32_t)(ctx.r11.s64 + -25472);  // lbl_825C9C80 @ 0x825c9c80
		// fmuls f11,f12,f0
		ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
		// lfs f0,-21304(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + -21304);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f10,f11,f0
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// lfs f0,-28128(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + -28128);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f29,f10,f0
		var_f29 = double(float(ctx.f10.f64 * ctx.f0.f64));
		// stfs f29,212(r31)
		temp.f32 = float(var_f29);
		PPC_STORE_U32(var_r31 + 212, temp.u32);
		// lwz r5,96(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 96);
		// lwz r30,4(r5)
		var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + 4));
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// fmr f1,f29
		ctx.f1.f64 = var_f29;
		// bl 0x820f2490
		hudFlashBase_2490_g(ctx, base);
		// addi r5,r1,256
		ctx.r5.s64 = ctx.r1.s64 + 256;
		// lwz r4,176(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 176);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f2568
		hudFlashBase_2568_fw(ctx, base);
		// stfs f21,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f21);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// lwz r4,180(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 180);
		// addi r11,r31,144
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 144;
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lvx128 v3,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v3,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
		// lvx128 v4,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v2,v4,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// stvx v2,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x820f2568
		hudFlashBase_2568_fw(ctx, base);
		// addi r5,r1,128
		ctx.r5.s64 = ctx.r1.s64 + 128;
		// lwz r4,184(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 184);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f2568
		hudFlashBase_2568_fw(ctx, base);
		// addi r5,r1,208
		ctx.r5.s64 = ctx.r1.s64 + 208;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,188(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 188);
		// bl 0x820f2568
		hudFlashBase_2568_fw(ctx, base);
		// addi r5,r1,224
		ctx.r5.s64 = ctx.r1.s64 + 224;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,192(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 192);
		// bl 0x820f2568
		hudFlashBase_2568_fw(ctx, base);
		// addi r5,r1,272
		ctx.r5.s64 = ctx.r1.s64 + 272;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,196(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 196);
		// bl 0x820f24f8
		lvlTable_24F8_h(ctx, base);
		// addi r5,r1,280
		ctx.r5.s64 = ctx.r1.s64 + 280;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,200(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 200);
		// bl 0x820f24f8
		lvlTable_24F8_h(ctx, base);
		// addi r5,r1,240
		ctx.r5.s64 = ctx.r1.s64 + 240;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,204(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 204);
		// bl 0x820f24f8
		lvlTable_24F8_h(ctx, base);
		// lwz r11,-23804(r16)
		ctx.r11.u64 = PPC_LOAD_U32(var_r16 + -23804);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,208(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 208);
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r5,r11,32
		ctx.r5.s64 = ctx.r11.s64 + 32;
		// bl 0x820f2568
		hudFlashBase_2568_fw(ctx, base);
		// lwz r30,-23828(r20)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r20 + -23828));
		// li r11,1
		ctx.r11.s64 = 1;
		// add r29,r30,r22
		var_r29 = (uint32_t)(var_r30 + var_r22);
		// lwz r8,0(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 0);
		// cmpwi cr6,r8,1
		// beq cr6,0x82242650
		if (ctx.r8.s32 != 1) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82242650:
		// lis r23,-32142
		var_r23 = (uint32_t)(-2106458112);
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// lis r5,-30584
		ctx.r5.s64 = -2004353024;
		// cmplwi cr6,r6,0
		// ori r25,r5,34953
		var_r25 = (uint32_t)(ctx.r5.u64 | 34953);
		// lwz r28,-23816(r23)
		var_r28 = (uint32_t)(PPC_LOAD_U32(var_r23 + -23816));
		// beq cr6,0x82242768
		if (ctx.r6.u32 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x821cd8f0
			pongLerpQueue_D8F0(ctx, base);
			// cmpwi cr6,r3,1
			// ble cr6,0x82242750
			if (ctx.r3.s32 > 1) {
				// lis r11,-32248
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// lfs f0,-23672(r11)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23672);
				ctx.f0.f64 = double(temp.f32);
				// stfs f0,80(r1)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
				// addi r4,r1,80
				ctx.r4.s64 = ctx.r1.s64 + 80;
				// lvx128 v1,r0,r4
				ea = (ctx.r4.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vspltw128 v127,v1,0
				simde_mm_store_si128((simde__m128i*)ctx.v127.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.u32), 0xFF));
				// bl 0x821cd8f0
				pongLerpQueue_D8F0(ctx, base);
				// lis r10,0
				ctx.r10.s64 = 0;
				// addi r11,r30,16416
				ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16416;
				// ori r10,r10,43204
				ctx.r10.u64 = ctx.r10.u64 | 43204;
				// lis r7,-28254
				// mr r9,r10
				ctx.r9.u64 = ctx.r10.u64;
				// mr r8,r10
				ctx.r8.u64 = ctx.r10.u64;
				// ori r10,r7,46021
				ctx.r10.u64 = ctx.r7.u64 | 46021;
				// mr r6,r10
				ctx.r6.u64 = ctx.r10.u64;
				// lwzx r9,r11,r9
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
				// mr r5,r10
				ctx.r5.u64 = ctx.r10.u64;
				// lwzx r10,r11,r8
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
				// add r9,r9,r3
				ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
				// add r10,r10,r3
				ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
				// addi r9,r9,1
				ctx.r9.s64 = ctx.r9.s64 + 1;
				// mulhw r8,r10,r5
				ctx.r8.s64 = (int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32)) >> 32;
				// mulhw r7,r9,r6
				ctx.r7.s64 = (int64_t(ctx.r9.s32) * int64_t(ctx.r6.s32)) >> 32;
				// add r4,r7,r9
				ctx.r4.u64 = ctx.r7.u64 + ctx.r9.u64;
				// add r3,r8,r10
				ctx.r3.u64 = ctx.r8.u64 + ctx.r10.u64;
				// srawi r8,r4,9
				ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1FF) != 0);
				ctx.r8.s64 = ctx.r4.s32 >> 9;
				// srawi r7,r3,9
				ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1FF) != 0);
				ctx.r7.s64 = ctx.r3.s32 >> 9;
				// rlwinm r5,r8,1,31,31
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
				// rlwinm r6,r7,1,31,31
				ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
				// add r8,r8,r5
				ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
				// add r7,r7,r6
				ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
				// mulli r6,r8,900
				ctx.r6.s64 = static_cast<int64_t>(ctx.r8.u64 * static_cast<uint64_t>(900));
				// mulli r5,r7,900
				ctx.r5.s64 = static_cast<int64_t>(ctx.r7.u64 * static_cast<uint64_t>(900));
				// subf r9,r6,r9
				ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
				// subf r10,r5,r10
				ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
				// rlwinm r7,r9,1,0,30
				ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
				// rlwinm r8,r10,1,0,30
				ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
				// add r4,r9,r7
				ctx.r4.u64 = ctx.r9.u64 + ctx.r7.u64;
				// add r3,r10,r8
				ctx.r3.u64 = ctx.r10.u64 + ctx.r8.u64;
				// rlwinm r10,r4,4,0,27
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
				// rlwinm r9,r3,4,0,27
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
				// add r10,r10,r11
				ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
				// add r11,r9,r11
				ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
				// addi r10,r10,16
				ctx.r10.s64 = ctx.r10.s64 + 16;
				// addi r9,r11,16
				ctx.r9.s64 = ctx.r11.s64 + 16;
				// addi r8,r1,96
				ctx.r8.s64 = ctx.r1.s64 + 96;
				// lvx128 v0,r0,r10
				ea = (ctx.r10.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lvx128 v13,r0,r9
				ea = (ctx.r9.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vsubfp v0,v13,v0
				ctx.fpscr.enableFlushMode();
				simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
				// vmulfp128 v0,v0,v127
				simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32)));
				// stvx v0,r0,r8
				ea = (ctx.r8.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// b 0x822427d8
				goto loc_822427D8;
			}
		loc_82242750:
			// addi r7,r1,96
			ctx.r7.s64 = ctx.r1.s64 + 96;
			// addi r6,r1,96
			ctx.r6.s64 = ctx.r1.s64 + 96;
			// lvx128 v0,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vxor v0,v0,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
			// stvx v0,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// b 0x822427d8
		} else {
		loc_82242768:
			// lwz r11,20(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 20);
			// lwz r5,9640(r11)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 9640);
			// cmpwi cr6,r5,0
			// ble cr6,0x822427c4
			if (ctx.r5.s32 > 0) {
				// addi r10,r11,32
				ctx.r10.s64 = ctx.r11.s64 + 32;
				// addi r6,r1,96
				ctx.r6.s64 = ctx.r1.s64 + 96;
				// lwz r11,9604(r10)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 9604);
				// addi r11,r11,2
				ctx.r11.s64 = ctx.r11.s64 + 2;
				// mulhw r9,r11,r25
				ctx.r9.s64 = (int64_t(ctx.r11.s32) * int64_t((int32_t)var_r25)) >> 32;
				// add r4,r9,r11
				ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
				// srawi r9,r4,6
				ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3F) != 0);
				ctx.r9.s64 = ctx.r4.s32 >> 6;
				// rlwinm r8,r9,1,31,31
				ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
				// add r3,r9,r8
				ctx.r3.u64 = ctx.r9.u64 + ctx.r8.u64;
				// mulli r9,r3,120
				ctx.r9.s64 = static_cast<int64_t>(ctx.r3.u64 * static_cast<uint64_t>(120));
				// subf r11,r9,r11
				ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
				// rlwinm r9,r11,2,0,29
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
				// add r8,r11,r9
				ctx.r8.u64 = ctx.r11.u64 + ctx.r9.u64;
				// rlwinm r11,r8,4,0,27
				ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
				// add r11,r11,r10
				ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
				// addi r7,r11,32
				ctx.r7.s64 = ctx.r11.s64 + 32;
				// lvx128 v0,r0,r7
				ea = (ctx.r7.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx v0,r0,r6
				ea = (ctx.r6.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// b 0x822427d8
			} else {
			loc_822427C4:
				// addi r5,r1,96
				ctx.r5.s64 = ctx.r1.s64 + 96;
				// addi r4,r1,96
				ctx.r4.s64 = ctx.r1.s64 + 96;
				// lvx128 v0,r0,r5
				ea = (ctx.r5.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vxor v0,v0,v0
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
				// stvx v0,r0,r4
				ea = (ctx.r4.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			}
		}
	loc_822427D8:
		// vmsum3fp128 v12,v0,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lis r11,-32248
		// lwz r3,220(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 220);
		// lfs f23,3232(r17)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(var_r17 + 3232);
		var_f23 = double(temp.f32);
		// lis r21,-32142
		var_r21 = (uint32_t)(-2106458112);
		// cmplwi cr6,r3,0
		ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
		// lfd f0,-23680(r11)
		ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -23680);
		// lis r11,-32158
		// addi r11,r11,-25616
		ctx.r11.s64 = ctx.r11.s64 + -25616;
		// lvx128 v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32160
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// vrsqrtefp v11,v12
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v12.f32))));
		// lvx128 v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32163
		// addi r11,r11,-18480
		ctx.r11.s64 = ctx.r11.s64 + -18480;
		// lvx128 v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32163
		// vcmpeqfp v0,v11,v10
		simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
		// addi r11,r11,-18496
		ctx.r11.s64 = ctx.r11.s64 + -18496;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// vsel v0,v11,v9,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
		// vmulfp128 v11,v0,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v10,v8,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vnmsubfp v13,v12,v11,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v10,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v31,v12,v0
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v31,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32253
		// lfs f13,80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,-15080(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -15080);
		ctx.f12.f64 = double(temp.f32);
		// lis r11,-32248
		// fsubs f9,f13,f12
		ctx.f9.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
		// lfd f20,-25848(r11)
		ctx.f20.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
		// fsel f24,f9,f0,f13
		var_f24 = ctx.f9.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
		// beq cr6,0x82242d14
		if (!(ctx.cr6.eq)) {
			// lbz r10,232(r31)
			ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 232);
			// cmplwi cr6,r10,0
			// beq cr6,0x82242d14
			if (ctx.r10.u32 == 0) goto loc_82242D14;
			// lwz r11,-23820(r21)
			ctx.r11.u64 = PPC_LOAD_U32(var_r21 + -23820);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// cmplwi cr6,r11,0
			// bne cr6,0x82242890
			if (ctx.r11.u32 == 0) {
				// fmr f26,f30
				var_f26 = var_f30;
				// b 0x8224289c
			} else {
			loc_82242890:
				// lwz r9,4(r11)
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				// lwz r10,12(r9)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
				// lfs f26,8(r10)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
				var_f26 = double(temp.f32);
			}
		loc_8224289C:
			// cmplwi cr6,r11,0
			// beq cr6,0x82243288
			if (ctx.r11.u32 == 0) {
				// addi r1,r1,608
				ctx.r1.s64 = ctx.r1.s64 + 608;
				// li r0,-256
				ctx.r0.s64 = -256;
				// lvx128 v127,r1,r0
				ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// addi r12,r1,-136
				ctx.r12.s64 = ctx.r1.s64 + -136;
				// bl 0x82436644
				__restfpr_20(ctx, base);
				// b 0x8242f8b8
				__restgprlr_16(ctx, base);
				return;
			}
			// addi r26,r11,16
			var_r26 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82080010
			// addi r7,r1,144
			ctx.r7.s64 = ctx.r1.s64 + 144;
			// addi r8,r26,16
			ctx.r8.s64 = (int64_t)(int32_t)var_r26 + 16;
			// addi r6,r26,32
			ctx.r6.s64 = (int64_t)(int32_t)var_r26 + 32;
			// addi r5,r26,48
			ctx.r5.s64 = (int64_t)(int32_t)var_r26 + 48;
			// lvx128 v0,r0,r26
			ea = (var_r26) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v0,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r1,160
			ctx.r4.s64 = ctx.r1.s64 + 160;
			// lvx128 v30,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v30,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r3,r1,176
			ctx.r3.s64 = ctx.r1.s64 + 176;
			// lvx128 v29,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v29,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r1,192
			ctx.r11.s64 = ctx.r1.s64 + 192;
			// lvx128 v28,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v28,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r10,0(r29)
			ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
			// li r11,1
			ctx.r11.s64 = 1;
			// cmpwi cr6,r10,1
			// beq cr6,0x822428f8
			if (ctx.r10.s32 != 1) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_822428F8:
			// clrlwi r8,r11,24
			ctx.r8.u64 = ctx.r11.u32 & 0xFF;
			// lis r7,0
			ctx.r7.s64 = 0;
			// cmplwi cr6,r8,0
			// ori r24,r7,59648
			var_r24 = (uint32_t)(ctx.r7.u64 | 59648);
			// beq cr6,0x8224291c
			if (ctx.r8.u32 != 0) {
				// addi r6,r1,192
				ctx.r6.s64 = ctx.r1.s64 + 192;
				// lvx128 v27,r30,r24
				ea = (var_r30 + var_r24) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx v27,r0,r6
				ea = (ctx.r6.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// b 0x82242974
			} else {
			loc_8224291C:
				// lwz r11,20(r28)
				ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 20);
				// lwz r5,9640(r11)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 9640);
				// cmpwi cr6,r5,0
				// ble cr6,0x82242974
				if (ctx.r5.s32 <= 0) goto loc_82242974;
				// addi r10,r11,32
				ctx.r10.s64 = ctx.r11.s64 + 32;
				// addi r6,r1,192
				ctx.r6.s64 = ctx.r1.s64 + 192;
				// lwz r11,9604(r10)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 9604);
				// addi r11,r11,2
				ctx.r11.s64 = ctx.r11.s64 + 2;
				// mulhw r9,r11,r25
				ctx.r9.s64 = (int64_t(ctx.r11.s32) * int64_t((int32_t)var_r25)) >> 32;
				// add r4,r9,r11
				ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
				// srawi r9,r4,6
				ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3F) != 0);
				ctx.r9.s64 = ctx.r4.s32 >> 6;
				// rlwinm r8,r9,1,31,31
				ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
				// add r3,r9,r8
				ctx.r3.u64 = ctx.r9.u64 + ctx.r8.u64;
				// mulli r9,r3,120
				ctx.r9.s64 = static_cast<int64_t>(ctx.r3.u64 * static_cast<uint64_t>(120));
				// subf r11,r9,r11
				ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
				// rlwinm r9,r11,2,0,29
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
				// add r8,r11,r9
				ctx.r8.u64 = ctx.r11.u64 + ctx.r9.u64;
				// rlwinm r11,r8,4,0,27
				ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
				// add r11,r11,r10
				ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
				// addi r7,r11,16
				ctx.r7.s64 = ctx.r11.s64 + 16;
				// lvx128 v26,r0,r7
				ea = (ctx.r7.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx v26,r0,r6
				ea = (ctx.r6.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			}
		loc_82242974:
			// lis r11,-32248
			// addi r28,r31,236
			var_r28 = (uint32_t)(var_r31 + 236);
			// lfs f0,-25600(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25600);
			ctx.f0.f64 = double(temp.f32);
			// lis r11,-32248
			// fmuls f8,f24,f0
			ctx.f8.f64 = double(float(var_f24 * ctx.f0.f64));
			// lfs f0,-25828(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25828);
			ctx.f0.f64 = double(temp.f32);
			// lwz r11,0(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// fmuls f28,f8,f0
			var_f28 = double(float(ctx.f8.f64 * ctx.f0.f64));
			// addi r11,r11,16
			ctx.r11.s64 = ctx.r11.s64 + 16;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// addi r3,r11,32
			ctx.r3.s64 = ctx.r11.s64 + 32;
			// stvx v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r1,160
			ctx.r4.s64 = ctx.r1.s64 + 160;
			// addi r11,r11,48
			ctx.r11.s64 = ctx.r11.s64 + 48;
			// lvx128 v25,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v25,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r10,r1,176
			ctx.r10.s64 = ctx.r1.s64 + 176;
			// lvx128 v24,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v24,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,192
			ctx.r9.s64 = ctx.r1.s64 + 192;
			// lvx128 v23,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v23,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lis r11,-32160
			ctx.r11.s64 = -2107637760;
			// lwz r11,25968(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25968);
			// lfs f7,140(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 140);
			ctx.f7.f64 = double(temp.f32);
			// fcmpu cr6,f7,f31
			// bgt cr6,0x82242a28
			if (ctx.f7.f64 <= var_f31) {
				// lwz r11,0(r28)
				ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
				// fadds f6,f28,f23
				ctx.f6.f64 = double(float(var_f28 + var_f23));
				// lwz r8,0(r11)
				ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
				// lwz r7,4(r8)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
				// fmuls f0,f6,f26
				ctx.f0.f64 = double(float(ctx.f6.f64 * var_f26));
				// stfs f0,80(r1)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
				// addi r6,r1,80
				ctx.r6.s64 = ctx.r1.s64 + 80;
				// lwz r3,12(r7)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
				// lvx128 v22,r0,r6
				ea = (ctx.r6.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vspltw v0,v22,0
				simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v22.u32), 0xFF));
				// addi r4,r3,112
				ctx.r4.s64 = ctx.r3.s64 + 112;
				// lwz r5,0(r3)
  // [ph4a] vtable load collapsed
				// stfs f0,8(r3)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
				// stvx v0,r0,r4
				ea = (ctx.r4.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lwz r11,148(r5)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(ctx.r3.u32, 37, ctx, base);  // pattern-B slot 37 (byte +148)
			}
		loc_82242A28:
			// lwz r11,0(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r9,4(r10)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
			// lwz r3,12(r9)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
			// lwz r7,4(r8)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// fadds f5,f28,f31
			ctx.fpscr.disableFlushMode();
			ctx.f5.f64 = double(float(var_f28 + var_f31));
			// addi r27,r31,244
			var_r27 = (uint32_t)(var_r31 + 244);
			// lfs f4,196(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
			ctx.f4.f64 = double(temp.f32);
			// addi r6,r1,144
			ctx.r6.s64 = ctx.r1.s64 + 144;
			// addi r4,r1,144
			ctx.r4.s64 = ctx.r1.s64 + 144;
			// lwz r11,0(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
			// lvx128 v21,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fmadds f3,f5,f26,f4
			ctx.f3.f64 = double(float(ctx.f5.f64 * var_f26 + ctx.f4.f64));
			// stfs f3,196(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// addi r11,r11,160
			ctx.r11.s64 = ctx.r11.s64 + 160;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// addi r10,r11,32
			ctx.r10.s64 = ctx.r11.s64 + 32;
			// addi r9,r11,48
			ctx.r9.s64 = ctx.r11.s64 + 48;
			// stvx v21,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r3,r1,160
			ctx.r3.s64 = ctx.r1.s64 + 160;
			// lvx128 v20,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v20,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,176
			ctx.r8.s64 = ctx.r1.s64 + 176;
			// lvx128 v19,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v19,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r7,r1,192
			ctx.r7.s64 = ctx.r1.s64 + 192;
			// lvx128 v18,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v18,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,0(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
			// addi r6,r1,144
			ctx.r6.s64 = ctx.r1.s64 + 144;
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lvx128 v17,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r11,32
			ctx.r11.s64 = ctx.r11.s64 + 32;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// addi r10,r11,32
			ctx.r10.s64 = ctx.r11.s64 + 32;
			// addi r9,r11,48
			ctx.r9.s64 = ctx.r11.s64 + 48;
			// stvx v17,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r3,r1,160
			ctx.r3.s64 = ctx.r1.s64 + 160;
			// lvx128 v16,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v16,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,176
			ctx.r8.s64 = ctx.r1.s64 + 176;
			// lvx128 v15,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v15,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r7,r1,192
			ctx.r7.s64 = ctx.r1.s64 + 192;
			// lvx128 v14,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v14,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,0(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
			// lwz r6,0(r11)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r5,4(r6)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
			// lwz r3,12(r5)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
			// lwz r10,12(r11)
			// bctrl
			VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
			// lwz r9,220(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 220);
			// lwz r3,16(r9)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
			// lbz r8,44(r3)
			ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 44);
			// cmplwi cr6,r8,0
			// beq cr6,0x82242b2c
			if (ctx.r8.u32 != 0) {
				// bl 0x823d8a20
				lvlTable_8A20_h(ctx, base);
			}
		loc_82242B2C:
			// lwz r7,220(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 220);
			// lwz r6,8(r7)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
			// lwz r5,8(r6)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
			// lwz r29,8(r5)
			var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + 8));
			// cmpwi cr6,r29,0
			// ble cr6,0x82242bf8
			if ((int32_t)var_r29 > 0) {
				// lis r11,-32248
				// lfs f27,2828(r17)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(var_r17 + 2828);
				var_f27 = double(temp.f32);
				// li r30,0
				var_r30 = 0;
				// lfs f28,-23684(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23684);
				var_f28 = double(temp.f32);
			loc_82242B54:
				// lwz r4,220(r31)
				ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 220);
				// addi r9,r1,80
				ctx.r9.s64 = ctx.r1.s64 + 80;
				// lfs f2,240(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
				ctx.f2.f64 = double(temp.f32);
				// fmuls f1,f2,f28
				ctx.f1.f64 = double(float(ctx.f2.f64 * var_f28));
				// lwz r3,8(r4)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
				// lwz r11,8(r3)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
				// lwz r10,20(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
				// lvx128 v127,r10,r30
				ea = (ctx.r10.u32 + var_r30) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx128 v127,r0,r9
				ea = (ctx.r9.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lfs f0,80(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				ctx.f0.f64 = double(temp.f32);
				// fsubs f13,f0,f25
				ctx.f13.f64 = double(float(ctx.f0.f64 - var_f25));
				// fdivs f12,f13,f1
				ctx.f12.f64 = double(float(ctx.f13.f64 / ctx.f1.f64));
				// fsubs f11,f31,f12
				ctx.f11.f64 = double(float(var_f31 - ctx.f12.f64));
				// fabs f0,f11
				ctx.f0.u64 = ctx.f11.u64 & ~0x8000000000000000;
				// fmuls f1,f0,f27
				ctx.f1.f64 = double(float(ctx.f0.f64 * var_f27));
				// bl 0x824301d8
				phBoundCapsule_01D8_g(ctx, base);
				// frsp f0,f1
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = double(float(ctx.f1.f64));
				// lwz r8,220(r31)
				ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 220);
				// stfs f30,112(r1)
				temp.f32 = float(var_f30);
				PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
				// addi r29,r29,-1
				var_r29 = (uint32_t)(var_r29 + -1);
				// stfs f30,116(r1)
				temp.f32 = float(var_f30);
				PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
				// cmplwi cr6,r29,0
				ctx.cr6.compare<uint32_t>(var_r29, 0, ctx.xer);
				// lwz r7,8(r8)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
				// fmuls f10,f0,f29
				ctx.f10.f64 = double(float(ctx.f0.f64 * var_f29));
				// lwz r11,8(r7)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
				// lwz r5,36(r11)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
				// fmuls f9,f10,f22
				ctx.f9.f64 = double(float(ctx.f10.f64 * var_f22));
				// stfs f9,120(r1)
				temp.f32 = float(ctx.f9.f64);
				PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
				// addi r6,r1,112
				ctx.r6.s64 = ctx.r1.s64 + 112;
				// lvx128 v63,r0,r6
				ea = (ctx.r6.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// vaddfp128 v0,v127,v63
				ctx.fpscr.enableFlushModeUnconditional();
				simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v63.f32)));
				// stvx128 v0,r5,r30
				ea = (ctx.r5.u32 + var_r30) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lwz r4,28(r11)
				ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
				// stvx128 v0,r4,r30
				ea = (ctx.r4.u32 + var_r30) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lwz r3,220(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 220);
				// lwz r11,8(r3)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
				// lwz r10,8(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
				// lwz r9,36(r10)
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
				// stvx128 v0,r9,r30
				ea = (ctx.r9.u32 + var_r30) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// addi r30,r30,16
				var_r30 = (uint32_t)(var_r30 + 16);
				// bne cr6,0x82242b54
				if (!ctx.cr6.eq) goto loc_82242B54;
			}
		loc_82242BF8:
			// lwz r10,-23828(r20)
			ctx.r10.u64 = PPC_LOAD_U32(var_r20 + -23828);
			// addi r7,r26,48
			ctx.r7.s64 = (int64_t)(int32_t)var_r26 + 48;
			// addi r6,r1,336
			ctx.r6.s64 = ctx.r1.s64 + 336;
			// li r11,1
			ctx.r11.s64 = 1;
			// lwzx r8,r10,r22
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r22);
			// lvx128 v62,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx128 v62,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// cmpwi cr6,r8,1
			// beq cr6,0x82242c20
			if (ctx.r8.s32 != 1) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_82242C20:
			// clrlwi r4,r11,24
			ctx.r4.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r4,0
			// beq cr6,0x82242c3c
			if (ctx.r4.u32 != 0) {
				// addi r3,r1,336
				ctx.r3.s64 = ctx.r1.s64 + 336;
				// lvx128 v61,r10,r24
				ea = (ctx.r10.u32 + var_r24) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx128 v61,r0,r3
				ea = (ctx.r3.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// b 0x82242c98
			} else {
			loc_82242C3C:
				// lwz r11,-23816(r23)
				ctx.r11.u64 = PPC_LOAD_U32(var_r23 + -23816);
				// lwz r11,20(r11)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
				// lwz r10,9640(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 9640);
				// cmpwi cr6,r10,0
				// ble cr6,0x82242c98
				if (ctx.r10.s32 <= 0) goto loc_82242C98;
				// addi r10,r11,32
				ctx.r10.s64 = ctx.r11.s64 + 32;
				// addi r4,r1,336
				ctx.r4.s64 = ctx.r1.s64 + 336;
				// lwz r11,9604(r10)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 9604);
				// addi r11,r11,2
				ctx.r11.s64 = ctx.r11.s64 + 2;
				// mulhw r9,r11,r25
				ctx.r9.s64 = (int64_t(ctx.r11.s32) * int64_t((int32_t)var_r25)) >> 32;
				// add r9,r9,r11
				ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
				// srawi r9,r9,6
				ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3F) != 0);
				ctx.r9.s64 = ctx.r9.s32 >> 6;
				// rlwinm r8,r9,1,31,31
				ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
				// add r8,r9,r8
				ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
				// mulli r7,r8,120
				ctx.r7.s64 = static_cast<int64_t>(ctx.r8.u64 * static_cast<uint64_t>(120));
				// subf r11,r7,r11
				ctx.r11.s64 = ctx.r11.s64 - ctx.r7.s64;
				// rlwinm r9,r11,2,0,29
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
				// add r6,r11,r9
				ctx.r6.u64 = ctx.r11.u64 + ctx.r9.u64;
				// rlwinm r11,r6,4,0,27
				ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
				// add r11,r11,r10
				ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
				// addi r5,r11,16
				ctx.r5.s64 = ctx.r11.s64 + 16;
				// lvx128 v60,r0,r5
				ea = (ctx.r5.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v60.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx128 v60,r0,r4
				ea = (ctx.r4.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v60.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			}
		loc_82242C98:
			// lfs f0,84(r19)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r19 + 84);
			ctx.f0.f64 = double(temp.f32);
			// lwz r3,220(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 220);
			// lfs f8,344(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
			ctx.f8.f64 = double(temp.f32);
			// fsubs f7,f0,f21
			ctx.f7.f64 = double(float(ctx.f0.f64 - var_f21));
			// fabs f0,f8
			ctx.f0.u64 = ctx.f8.u64 & ~0x8000000000000000;
			// mr r6,r27
			ctx.r6.u64 = var_r27;
			// mr r5,r28
			ctx.r5.u64 = var_r28;
			// lwz r11,8(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			// lwz r3,8(r11)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			// fsubs f6,f0,f31
			ctx.f6.f64 = double(float(ctx.f0.f64 - var_f31));
			// fsel f0,f6,f20,f0
			ctx.f0.f64 = ctx.f6.f64 >= 0.0 ? var_f20 : ctx.f0.f64;
			// fmadds f1,f7,f0,f21
			ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + var_f21));
			// bl 0x82341a50
			lvlTable_1A50(ctx, base);
			// lwz r11,0(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
			// stfs f26,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(var_f26);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// addi r9,r1,80
			ctx.r9.s64 = ctx.r1.s64 + 80;
			// lwz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lvx128 v59,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v59.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw128 v0,v59,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v59.u32), 0xFF));
			// lwz r8,4(r10)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
			// lwz r3,12(r8)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
			// addi r6,r3,112
			ctx.r6.s64 = ctx.r3.s64 + 112;
			// lwz r7,0(r3)
  // [ph4a] vtable load collapsed
			// stfs f26,8(r3)
			temp.f32 = float(var_f26);
			PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
			// stvx v0,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r5,148(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 37, ctx, base);  // pattern-B slot 37 (byte +148)
			// lwz r4,220(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 220);
			// lwz r3,16(r4)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
			// bl 0x823d8910
			grc_8910(ctx, base);
		}
	loc_82242D14:
		// lis r11,-32248
		// lwz r3,224(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 224);
		// cmplwi cr6,r3,0
		// lfs f29,-24968(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24968);
		var_f29 = double(temp.f32);
		// beq cr6,0x82242f28
		if (ctx.r3.u32 != 0) {
			// lbz r11,232(r31)
			ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 232);
			// cmplwi cr6,r11,0
			// beq cr6,0x82242f28
			if (ctx.r11.u32 == 0) goto loc_82242F28;
			// lwz r11,-23820(r21)
			ctx.r11.u64 = PPC_LOAD_U32(var_r21 + -23820);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// cmplwi cr6,r11,0
			// bne cr6,0x82242d4c
			if (ctx.r11.u32 == 0) {
				// fmr f31,f30
				var_f31 = var_f30;
				// b 0x82242d58
			} else {
			loc_82242D4C:
				// lwz r10,4(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				// lwz r10,12(r10)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
				// lfs f31,8(r10)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
				var_f31 = double(temp.f32);
			}
		loc_82242D58:
			// cmplwi cr6,r11,0
			// beq cr6,0x82243288
			if (ctx.r11.u32 == 0) {
				// addi r1,r1,608
				ctx.r1.s64 = ctx.r1.s64 + 608;
				// li r0,-256
				ctx.r0.s64 = -256;
				// lvx128 v127,r1,r0
				ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// addi r12,r1,-136
				ctx.r12.s64 = ctx.r1.s64 + -136;
				// bl 0x82436644
				__restfpr_20(ctx, base);
				// b 0x8242f8b8
				__restgprlr_16(ctx, base);
				return;
			}
			// addi r29,r31,236
			var_r29 = (uint32_t)(var_r31 + 236);
			// fmadds f5,f24,f29,f23
			ctx.fpscr.disableFlushMode();
			ctx.f5.f64 = double(float(var_f24 * var_f29 + var_f23));
			// addi r30,r11,16
			var_r30 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82080010
			// addi r28,r31,244
			var_r28 = (uint32_t)(var_r31 + 244);
			// addi r9,r30,16
			ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
			// addi r8,r30,32
			ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 32;
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// addi r7,r30,48
			ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 48;
			// lvx128 v0,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v13,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v12,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lvx128 v11,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fmuls f0,f5,f31
			ctx.f0.f64 = double(float(ctx.f5.f64 * var_f31));
			// stfs f0,80(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// addi r11,r11,16
			ctx.r11.s64 = ctx.r11.s64 + 16;
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// addi r4,r11,32
			ctx.r4.s64 = ctx.r11.s64 + 32;
			// addi r3,r11,48
			ctx.r3.s64 = ctx.r11.s64 + 48;
			// stvx v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v58,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v58.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw128 v10,v58,0
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v58.u32), 0xFF));
			// stvx v12,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v11,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,0(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// addi r11,r11,160
			ctx.r11.s64 = ctx.r11.s64 + 160;
			// addi r10,r11,16
			ctx.r10.s64 = ctx.r11.s64 + 16;
			// addi r9,r11,32
			ctx.r9.s64 = ctx.r11.s64 + 32;
			// addi r8,r11,48
			ctx.r8.s64 = ctx.r11.s64 + 48;
			// stvx v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v12,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v11,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r7,r1,96
			ctx.r7.s64 = ctx.r1.s64 + 96;
			// lfs f13,8(r19)
			temp.u32 = PPC_LOAD_U32(var_r19 + 8);
			ctx.f13.f64 = double(temp.f32);
			// lwz r11,0(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
			// stfs f13,80(r1)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// lvx128 v8,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lvx128 v7,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v9,v7,0
			simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
			// addi r11,r11,32
			ctx.r11.s64 = ctx.r11.s64 + 32;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// addi r4,r11,32
			ctx.r4.s64 = ctx.r11.s64 + 32;
			// vnmsubfp v11,v8,v9,v11
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v11.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v11.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// addi r3,r11,48
			ctx.r3.s64 = ctx.r11.s64 + 48;
			// stvx v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v12,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v11,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r10,4(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// lwz r3,12(r10)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
			// addi r8,r3,112
			ctx.r8.s64 = ctx.r3.s64 + 112;
			// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
			// stfs f0,8(r3)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
			// stvx v10,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r7,148(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 37, ctx, base);  // pattern-B slot 37 (byte +148)
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r6,0(r11)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r5,4(r6)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
			// lwz r3,12(r5)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
			// lwz r10,4(r11)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// lwz r9,0(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r8,4(r9)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
			// lwz r3,12(r8)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
			// lwz r6,12(r7)
			// bctrl
			VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
			// lwz r5,224(r31)
			ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 224);
			// lwz r3,16(r5)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
			// lbz r4,44(r3)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 44);
			// cmplwi cr6,r4,0
			// beq cr6,0x82242ec4
			if (ctx.r4.u32 != 0) {
				// bl 0x823d8a20
				lvlTable_8A20_h(ctx, base);
			}
		loc_82242EC4:
			// lwz r3,224(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 224);
			// mr r6,r28
			ctx.r6.u64 = var_r28;
			// mr r5,r29
			ctx.r5.u64 = var_r29;
			// fmr f1,f21
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f21;
			// lwz r11,8(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			// lwz r3,8(r11)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			// bl 0x82341a50
			lvlTable_1A50(ctx, base);
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// stfs f31,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// addi r9,r1,80
			ctx.r9.s64 = ctx.r1.s64 + 80;
			// lwz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lvx128 v6,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v0,v6,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
			// lwz r8,4(r10)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
			// lwz r3,12(r8)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
			// addi r6,r3,112
			ctx.r6.s64 = ctx.r3.s64 + 112;
			// lwz r7,0(r3)
  // [ph4a] vtable load collapsed
			// stfs f31,8(r3)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
			// stvx v0,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r5,148(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 37, ctx, base);  // pattern-B slot 37 (byte +148)
			// lwz r4,224(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 224);
			// lwz r3,16(r4)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
			// bl 0x823d8910
			grc_8910(ctx, base);
		}
	loc_82242F28:
		// lwz r3,228(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 228);
		// cmplwi cr6,r3,0
		// beq cr6,0x82243288
		if (ctx.r3.u32 == 0) {
			// addi r1,r1,608
			ctx.r1.s64 = ctx.r1.s64 + 608;
			// li r0,-256
			ctx.r0.s64 = -256;
			// lvx128 v127,r1,r0
			ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r12,r1,-136
			ctx.r12.s64 = ctx.r1.s64 + -136;
			// bl 0x82436644
			__restfpr_20(ctx, base);
			// b 0x8242f8b8
			__restgprlr_16(ctx, base);
			return;
		}
		// lbz r11,232(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 232);
		// cmplwi cr6,r11,0
		// beq cr6,0x82243288
		if (ctx.r11.u32 == 0) {
			// addi r1,r1,608
			ctx.r1.s64 = ctx.r1.s64 + 608;
			// li r0,-256
			ctx.r0.s64 = -256;
			// lvx128 v127,r1,r0
			ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r12,r1,-136
			ctx.r12.s64 = ctx.r1.s64 + -136;
			// bl 0x82436644
			__restfpr_20(ctx, base);
			// b 0x8242f8b8
			__restgprlr_16(ctx, base);
			return;
		}
		// lwz r11,-23820(r21)
		ctx.r11.u64 = PPC_LOAD_U32(var_r21 + -23820);
		// lwz r27,0(r11)
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 0));
		// cmplwi cr6,r27,0
		// bne cr6,0x82242f58
		if (var_r27 == 0) {
			// fmr f31,f30
			ctx.fpscr.disableFlushMode();
			var_f31 = var_f30;
			// b 0x82242f64
		} else {
		loc_82242F58:
			// lwz r10,4(r27)
			ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 4);
			// lwz r11,12(r10)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
			// lfs f31,8(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			var_f31 = double(temp.f32);
		}
	loc_82242F64:
		// cmplwi cr6,r27,0
		// beq cr6,0x82243288
		if (var_r27 == 0) {
			// addi r1,r1,608
			ctx.r1.s64 = ctx.r1.s64 + 608;
			// li r0,-256
			ctx.r0.s64 = -256;
			// lvx128 v127,r1,r0
			ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r12,r1,-136
			ctx.r12.s64 = ctx.r1.s64 + -136;
			// bl 0x82436644
			__restfpr_20(ctx, base);
			// b 0x8242f8b8
			__restgprlr_16(ctx, base);
			return;
		}
		// addi r29,r31,236
		var_r29 = (uint32_t)(var_r31 + 236);
		// fmadds f4,f24,f29,f23
		ctx.fpscr.disableFlushMode();
		ctx.f4.f64 = double(float(var_f24 * var_f29 + var_f23));
		// addi r30,r27,16
		var_r30 = (uint32_t)(var_r27 + 16);
		// addi r28,r31,244
		var_r28 = (uint32_t)(var_r31 + 244);
		// addi r9,r30,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r8,r30,32
		ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 32;
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// addi r7,r30,48
		ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 48;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lvx128 v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// fmuls f0,f4,f31
		ctx.f0.f64 = double(float(ctx.f4.f64 * var_f31));
		// stfs f0,80(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// addi r4,r11,32
		ctx.r4.s64 = ctx.r11.s64 + 32;
		// addi r3,r11,48
		ctx.r3.s64 = ctx.r11.s64 + 48;
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v5,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v10,v5,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
		// stvx v12,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r11,r11,160
		ctx.r11.s64 = ctx.r11.s64 + 160;
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r9,r11,32
		ctx.r9.s64 = ctx.r11.s64 + 32;
		// addi r8,r11,48
		ctx.r8.s64 = ctx.r11.s64 + 48;
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// lfs f13,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f13.f64 = double(temp.f32);
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// stfs f13,80(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// lvx128 v4,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lvx128 v3,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v9,v3,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
		// addi r11,r11,32
		ctx.r11.s64 = ctx.r11.s64 + 32;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// addi r4,r11,32
		ctx.r4.s64 = ctx.r11.s64 + 32;
		// vnmsubfp v11,v4,v9,v11
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v11.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// addi r3,r11,48
		ctx.r3.s64 = ctx.r11.s64 + 48;
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r10,4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r3,12(r10)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
		// addi r8,r3,112
		ctx.r8.s64 = ctx.r3.s64 + 112;
		// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
		// stfs f0,8(r3)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
		// stvx v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r7,148(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 37, ctx, base);  // pattern-B slot 37 (byte +148)
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r6,0(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r5,4(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
		// lwz r3,12(r5)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
		// lwz r10,4(r11)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r8,4(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
		// lwz r3,12(r8)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
		// lwz r6,12(r7)
		// bctrl
		VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
		// lwz r5,228(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 228);
		// lwz r3,16(r5)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
		// lbz r4,44(r3)
		ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 44);
		// cmplwi cr6,r4,0
		// beq cr6,0x822430d0
		if (ctx.r4.u32 != 0) {
			// bl 0x823d8a20
			lvlTable_8A20_h(ctx, base);
		}
	loc_822430D0:
		// lwz r3,228(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 228);
		// mr r6,r28
		ctx.r6.u64 = var_r28;
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// fmr f1,f21
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f21;
		// lwz r11,8(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// lwz r3,8(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// bl 0x82341a50
		lvlTable_1A50(ctx, base);
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// stfs f31,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lvx128 v2,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v2,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
		// lwz r8,4(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// lwz r3,12(r8)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
		// addi r6,r3,112
		ctx.r6.s64 = ctx.r3.s64 + 112;
		// lwz r7,0(r3)
  // [ph4a] vtable load collapsed
		// stfs f31,8(r3)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r5,148(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 37, ctx, base);  // pattern-B slot 37 (byte +148)
		// lis r11,-32160
		// lhz r4,8(r27)
		ctx.r4.u64 = PPC_LOAD_U16(var_r27 + 8);
		// addi r10,r30,48
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 48;
		// addi r6,r1,336
		ctx.r6.s64 = ctx.r1.s64 + 336;
		// lwz r11,26084(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26084);
		// lvx128 v1,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v1,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r3,28(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
		// lbzx r11,r3,r4
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + ctx.r4.u32);
		// clrlwi r9,r11,29
		ctx.r9.u64 = ctx.r11.u32 & 0x7;
		// cntlzw r8,r9
		ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
		// rlwinm r3,r8,27,31,31
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
		// cmplwi cr6,r3,0
		// beq cr6,0x8224327c
		if (ctx.r3.u32 != 0) {
			// lfs f0,0(r18)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r18 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f3,344(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
			ctx.f3.f64 = double(temp.f32);
			// fcmpu cr6,f3,f0
			// bge cr6,0x8224327c
			if (ctx.f3.f64 >= ctx.f0.f64) goto loc_8224327C;
			// lis r11,-32161
			ctx.r11.s64 = -2107703296;
			// lwz r11,23504(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23504);
			// lwz r10,4(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// lwz r9,28(r10)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
			// lbzx r8,r9,r4
			ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
			// clrlwi r7,r8,29
			ctx.r7.u64 = ctx.r8.u32 & 0x7;
			// cntlzw r6,r7
			ctx.r6.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
			// rlwinm r10,r6,27,31,31
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
			// cmplwi cr6,r10,0
			// beq cr6,0x822431b0
			if (ctx.r10.u32 != 0) {
				// lwz r3,4(r11)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				// lwz r8,16(r9)
				// bctrl
				VCALL(ctx.r3.u32, 4, ctx, base);  // vtable slot 4 (byte +16)
				// b 0x822431b4
			} else {
			loc_822431B0:
				// li r3,0
				ctx.r3.s64 = 0;
			}
		loc_822431B4:
			// addi r7,r3,224
			ctx.r7.s64 = ctx.r3.s64 + 224;
			// addi r6,r1,96
			ctx.r6.s64 = ctx.r1.s64 + 96;
			// lvx128 v0,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v0,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f2,104(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
			ctx.f2.f64 = double(temp.f32);
			// fcmpu cr6,f2,f30
			// ble cr6,0x8224327c
			if (ctx.f2.f64 <= var_f30) goto loc_8224327C;
			// lfs f1,336(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
			ctx.f1.f64 = double(temp.f32);
			// lis r11,-32248
			// fabs f0,f1
			ctx.f0.u64 = ctx.f1.u64 & ~0x8000000000000000;
			// lfs f13,-23688(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23688);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f0,f13
			// ble cr6,0x8224327c
			if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8224327C;
			// lwz r11,-23820(r21)
			ctx.r11.u64 = PPC_LOAD_U32(var_r21 + -23820);
			// lwz r10,-23804(r16)
			ctx.r10.u64 = PPC_LOAD_U32(var_r16 + -23804);
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lfs f0,20(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
			ctx.f0.f64 = double(temp.f32);
			// cmplwi cr6,r11,0
			// beq cr6,0x8224320c
			if (ctx.r11.u32 != 0) {
				// lwz r5,4(r11)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				// lwz r11,12(r5)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
				// lfs f30,8(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
				var_f30 = double(temp.f32);
			}
		loc_8224320C:
			// fadds f0,f0,f30
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(ctx.f0.f64 + var_f30));
			// lfs f13,340(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f13,f0
			// bso cr6,0x82243220
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82243218, "bso");
			// ble cr6,0x8224327c
			if (ctx.f13.f64 <= ctx.f0.f64) goto loc_8224327C;
		loc_82243220:
			// lis r11,-32253
			// lfs f12,100(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
			ctx.f12.f64 = double(temp.f32);
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// addi r4,r1,96
			ctx.r4.s64 = ctx.r1.s64 + 96;
			// lfs f0,-12312(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12312);
			ctx.f0.f64 = double(temp.f32);
			// lis r11,-32248
			// fsubs f11,f12,f0
			ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// lfd f0,-25168(r11)
			ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25168);
			// lis r11,-32164
			ctx.r11.s64 = -2107899904;
			// lfs f13,18736(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 18736);  /* glob:lbl_825C4930 @ 0x825c4930 */
			ctx.f13.f64 = double(temp.f32);
			// stfs f13,112(r1)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
			// fsel f0,f11,f0,f20
			ctx.f0.f64 = ctx.f11.f64 >= 0.0 ? ctx.f0.f64 : var_f20;
			// lfs f13,0(r17)
			temp.u32 = PPC_LOAD_U32(var_r17 + 0);
			ctx.f13.f64 = double(temp.f32);
			// stfs f13,120(r1)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
			// stfs f0,116(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
			// addi r11,r1,112
			ctx.r11.s64 = ctx.r1.s64 + 112;
			// lvx128 v31,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmulfp128 v30,v0,v31
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v31.f32)));
			// stvx v30,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r8,160(r9)
			// bctrl
			VCALL(ctx.r3.u32, 40, ctx, base);  // vtable slot 40 (byte +160)
		}
	loc_8224327C:
		// lwz r7,228(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 228);
		// lwz r3,16(r7)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
		// bl 0x823d8910
		grc_8910(ctx, base);
	}
loc_82243288:
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// li r0,-256
	ctx.r0.s64 = -256;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r12,r1,-136
	ctx.r12.s64 = ctx.r1.s64 + -136;
	// bl 0x82436644
	__restfpr_20(ctx, base);
	// b 0x8242f8b8
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__lvlTable_rtti_F6D4_0"))) PPC_WEAK_FUNC(lvlTable_rtti_F6D4_0);
PPC_FUNC_IMPL(__imp__lvlTable_rtti_F6D4_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// cmplwi cr6,r11,0
	// beq cr6,0x8224339c
	if (ctx.r11.u32 != 0) {
		// cmpwi cr6,r5,6
		// beq cr6,0x822432e8
		if (ctx.r5.s32 != 6) {
			// rotlwi r11,r11,0
			ctx.r11.u64 = ctx.r11.u32;
			// addi r4,r3,-80
			ctx.r4.s64 = ctx.r3.s64 + -80;
			// mr r3,r11
			ctx.r3.u64 = ctx.r11.u64;
			// lwz r9,24(r10)
			// bctrl
			VCALL(ctx.r3.u32, 6, ctx, base);  // vtable slot 6 (byte +24)
			return;
		}
	loc_822432E8:
		// lis r11,-32161
		ctx.r11.s64 = -2107703296;
		// addi r30,r11,-17448
		var_r30 = (uint32_t)(ctx.r11.s64 + -17448);  // lbl_825EBBD8 @ 0x825ebbd8
		// li r11,7
		ctx.r11.s64 = 7;
		// stw r11,36(r30)
		PPC_STORE_U32(var_r30 + 36, ctx.r11.u32);
		// lis r11,-32161
		ctx.r11.s64 = -2107703296;
		// addi r29,r11,-17536
		var_r29 = (uint32_t)(ctx.r11.s64 + -17536);  // lbl_825EBB80 @ 0x825ebb80
		// lwz r11,36(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 36);
		// cmplwi cr6,r11,7
		// lis r11,-32161
		ctx.r11.s64 = -2107703296;
		// addi r31,r11,-17636
		var_r31 = (uint32_t)(ctx.r11.s64 + -17636);  // lbl_825EBB1C @ 0x825ebb1c
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// bne cr6,0x82243320
		if (ctx.r11.u32 == 7) {
			// rlwinm r11,r11,0,23,21
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFDFF;
			// b 0x82243324
		} else {
		loc_82243320:
			// ori r11,r11,512
			ctx.r11.u64 = ctx.r11.u64 | 512;
		}
	loc_82243324:
		// li r10,0
		ctx.r10.s64 = 0;
		// stw r10,28(r30)
		PPC_STORE_U32(var_r30 + 28, ctx.r10.u32);
		// lwz r10,28(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 28);
		// cmplwi cr6,r10,0
		// bne cr6,0x82243340
		if (ctx.r10.u32 == 0) {
			// rlwinm r11,r11,0,25,23
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFF7F;
			// b 0x82243344
		} else {
		loc_82243340:
			// ori r11,r11,128
			ctx.r11.u64 = ctx.r11.u64 | 128;
		}
	loc_82243344:
		// addi r3,r3,-96
		ctx.r3.s64 = ctx.r3.s64 + -96;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// lwz r7,164(r8)
		// bctrl
		VCALL(ctx.r3.u32, 41, ctx, base);  // vtable slot 41 (byte +164)
		// li r11,1
		ctx.r11.s64 = 1;
		// stw r11,28(r30)
		PPC_STORE_U32(var_r30 + 28, ctx.r11.u32);
		// lwz r11,28(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 28);
		// cmplwi cr6,r11,1
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// rlwinm r10,r11,0,25,23
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFF7F;
		// beq cr6,0x8224337c
		if (ctx.r11.u32 != 1) {
			// ori r10,r11,128
			ctx.r10.u64 = ctx.r11.u64 | 128;
		}
	loc_8224337C:
		// li r11,15
		ctx.r11.s64 = 15;
		// stw r11,36(r30)
		PPC_STORE_U32(var_r30 + 36, ctx.r11.u32);
		// lwz r11,36(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 36);
		// cmplwi cr6,r11,15
		// rlwinm r11,r10,0,23,21
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFDFF;
		// beq cr6,0x82243398
		if (ctx.r11.u32 != 15) {
			// ori r11,r10,512
			ctx.r11.u64 = ctx.r10.u64 | 512;
		}
	loc_82243398:
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	}
loc_8224339C:
	return;
}

__attribute__((alias("__imp__lvlTable_vfn_41"))) PPC_WEAK_FUNC(lvlTable_vfn_41);
PPC_FUNC_IMPL(__imp__lvlTable_vfn_41) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,232(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 232);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224346c
	if (ctx.r11.u32 != 0) {
		// lwz r11,224(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 224);
		// cmplwi cr6,r11,0
		// beq cr6,0x822433f8
		if (ctx.r11.u32 != 0) {
			// lwz r3,4(r11)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// li r7,0
			ctx.r7.s64 = 0;
			// li r6,0
			ctx.r6.s64 = 0;
			// li r5,6
			ctx.r5.s64 = 6;
			// addi r4,r31,16
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
			// lwz r9,20(r10)
			// bctrl
			VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		}
	loc_822433F8:
		// lwz r11,228(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 228);
		// cmplwi cr6,r11,0
		// beq cr6,0x82243428
		if (ctx.r11.u32 != 0) {
			// lwz r3,4(r11)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// li r7,0
			ctx.r7.s64 = 0;
			// li r6,0
			ctx.r6.s64 = 0;
			// li r5,6
			ctx.r5.s64 = 6;
			// addi r4,r31,16
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
			// lwz r11,20(r8)
			// bctrl
			VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		}
	loc_82243428:
		// lwz r11,220(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 220);
		// cmplwi cr6,r11,0
		// beq cr6,0x82243488
		if (ctx.r11.u32 == 0) {
			// blr
			return;
		}
		// lwz r3,4(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// li r7,0
		ctx.r7.s64 = 0;
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,6
		ctx.r5.s64 = 6;
		// addi r4,r31,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lwz r9,20(r10)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// blr
		return;
	}
loc_8224346C:
	// lwz r3,124(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 124);
	// li r5,6
	ctx.r5.s64 = 6;
	// addi r4,r31,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
	// lwz r7,24(r8)
	// bctrl
	VCALL(ctx.r3.u32, 6, ctx, base);  // vtable slot 6 (byte +24)
loc_82243488:
	// blr
	return;
}

__attribute__((alias("__imp__pongShadowMap_34A0_w"))) PPC_WEAK_FUNC(pongShadowMap_34A0_w);
PPC_FUNC_IMPL(__imp__pongShadowMap_34A0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,220(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 220);
	// cmplwi cr6,r11,0
	// beq cr6,0x822434f0
	if (ctx.r11.u32 != 0) {
		// lbz r10,232(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 232);
		// cmplwi cr6,r10,0
		// beq cr6,0x822434f0
		if (ctx.r10.u32 == 0) goto loc_822434F0;
		// lwz r3,4(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// li r7,0
		ctx.r7.s64 = 0;
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,6
		ctx.r5.s64 = 6;
		// addi r4,r31,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lwz r8,20(r9)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	}
loc_822434F0:
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 224);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224352c
	if (ctx.r11.u32 != 0) {
		// lbz r7,232(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 232);
		// cmplwi cr6,r7,0
		// beq cr6,0x8224352c
		if (ctx.r7.u32 == 0) goto loc_8224352C;
		// lwz r3,4(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// li r7,0
		ctx.r7.s64 = 0;
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,6
		ctx.r5.s64 = 6;
		// addi r4,r31,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lwz r10,20(r11)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	}
loc_8224352C:
	// lwz r11,228(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 228);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243568
	if (ctx.r11.u32 != 0) {
		// lbz r9,232(r31)
		ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 232);
		// cmplwi cr6,r9,0
		// beq cr6,0x82243568
		if (ctx.r9.u32 == 0) {
			// blr
			return;
		}
		// lwz r3,4(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// li r7,0
		ctx.r7.s64 = 0;
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,6
		ctx.r5.s64 = 6;
		// addi r4,r31,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lwz r11,20(r8)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	}
loc_82243568:
	// blr
	return;
}

__attribute__((alias("__imp__net_3580"))) PPC_WEAK_FUNC(net_3580);
PPC_FUNC_IMPL(__imp__net_3580) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32251
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r4,r11,-2680
	ctx.r4.s64 = ctx.r11.s64 + -2680;
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r10,116(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 116);
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r9.u32);
	// stw r8,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r8.u32);
	// lwz r7,124(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 124);
	// lwz r6,96(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 96);
	// lwz r30,4(r6)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r6.u32 + 4));
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2664
	ctx.r4.s64 = ctx.r10.s64 + -2664;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,172(r31)
	PPC_STORE_U32(var_r31 + 172, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2648
	ctx.r4.s64 = ctx.r10.s64 + -2648;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,176(r31)
	PPC_STORE_U32(var_r31 + 176, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2632
	ctx.r4.s64 = ctx.r10.s64 + -2632;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,180(r31)
	PPC_STORE_U32(var_r31 + 180, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2616
	ctx.r4.s64 = ctx.r10.s64 + -2616;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,184(r31)
	PPC_STORE_U32(var_r31 + 184, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2600
	ctx.r4.s64 = ctx.r10.s64 + -2600;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,188(r31)
	PPC_STORE_U32(var_r31 + 188, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2584
	ctx.r4.s64 = ctx.r10.s64 + -2584;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,192(r31)
	PPC_STORE_U32(var_r31 + 192, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2568
	ctx.r4.s64 = ctx.r10.s64 + -2568;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,196(r31)
	PPC_STORE_U32(var_r31 + 196, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r10,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-2552
	ctx.r4.s64 = ctx.r10.s64 + -2552;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r11,200(r31)
	PPC_STORE_U32(var_r31 + 200, ctx.r11.u32);
	// bl 0x820f2340
	net_2340(ctx, base);
	// lis r11,-32251
	// stw r3,204(r31)
	PPC_STORE_U32(var_r31 + 204, ctx.r3.u32);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r11,-2544
	ctx.r4.s64 = ctx.r11.s64 + -2544;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x820f2340
	net_2340(ctx, base);
	// stw r3,208(r31)
	PPC_STORE_U32(var_r31 + 208, ctx.r3.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_36E8"))) PPC_WEAK_FUNC(game_36E8);
PPC_FUNC_IMPL(__imp__game_36E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=208, savegprlr_27
	// lis r11,-32248
	// fmr f29,f1
	var_f29 = ctx.f1.f64;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lfs f13,-25744(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25744);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r28,r11,-11908
	var_r28 = (uint32_t)(ctx.r11.s64 + -11908);  // lbl_8202D17C @ 0x8202d17c
	// fcmpu cr6,f29,f13
	// lfs f31,-108(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + -108);
	var_f31 = double(temp.f32);
	// bge cr6,0x82243748
	if (var_f29 < ctx.f13.f64) {
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfs f0,-24680(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24680);  /* glob:lbl_82079F98 @ 0x82079f98 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f29,f0
		// ble cr6,0x82243748
		if (var_f29 <= ctx.f0.f64) goto loc_82243748;
		// fcmpu cr6,f29,f31
		// bge cr6,0x82243744
		if (var_f29 < var_f31) {
			// fmr f29,f0
			var_f29 = ctx.f0.f64;
			// b 0x82243748
		} else {
		loc_82243744:
			// fmr f29,f13
			ctx.fpscr.disableFlushMode();
			var_f29 = ctx.f13.f64;
		}
	}
loc_82243748:
	// lis r11,-32142
	// lfs f11,160(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 160);
	ctx.f11.f64 = double(temp.f32);
	// lwz r30,-23804(r11)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + -23804));  /* glob:lbl_8207A304 @ 0x8207a304 */
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
	// lfs f13,108(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// addi r8,r11,-32648
	ctx.r8.s64 = ctx.r11.s64 + -32648;
	// lfs f0,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fcmpu cr6,f11,f12
	// bgt cr6,0x82243a50
	if (ctx.f11.f64 <= ctx.f12.f64) {
		// lis r9,-32158
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r31,128
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 128;
		// addi r9,r9,-25344
		ctx.r9.s64 = ctx.r9.s64 + -25344;
		// addi r10,r31,144
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 144;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r9,0(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lfs f10,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,148(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 148);
		ctx.f9.f64 = double(temp.f32);
		// lfs f8,152(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 152);
		ctx.f8.f64 = double(temp.f32);
		// lvx128 v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f7,80(r9)
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 80);
		ctx.f7.f64 = double(temp.f32);
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// fmuls f6,f7,f10
		ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
		// stfs f6,0(r10)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// lwz r7,0(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lfs f5,84(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 84);
		ctx.f5.f64 = double(temp.f32);
		// fmuls f4,f5,f9
		ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
		// stfs f4,148(r31)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(var_r31 + 148, temp.u32);
		// lwz r6,0(r30)
		ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lfs f3,88(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 88);
		ctx.f3.f64 = double(temp.f32);
		// fmuls f2,f3,f8
		ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
		// stfs f2,152(r31)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(var_r31 + 152, temp.u32);
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r4,r11,64
		ctx.r4.s64 = ctx.r11.s64 + 64;
		// addi r5,r11,48
		ctx.r5.s64 = ctx.r11.s64 + 48;
		// lfs f1,32(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 32);
		ctx.f1.f64 = double(temp.f32);
		// lfs f5,96(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 96);
		ctx.f5.f64 = double(temp.f32);
		// lvx128 v12,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// lfs f13,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// lfs f10,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f10.f64 = double(temp.f32);
		// lfs f0,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,120(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
		// lfs f0,-112(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + -112);
		ctx.f0.f64 = double(temp.f32);
		// lfs f4,100(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 100);
		ctx.f4.f64 = double(temp.f32);
		// lfs f12,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f12.f64 = double(temp.f32);
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// lfs f9,100(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		ctx.f9.f64 = double(temp.f32);
		// fadds f11,f12,f13
		ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
		// fadds f8,f10,f9
		ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// fmuls f7,f11,f0
		ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// stfs f7,112(r1)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// fmuls f6,f8,f0
		ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
		// stfs f6,116(r1)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
		// addi r7,r1,112
		ctx.r7.s64 = ctx.r1.s64 + 112;
		// lfs f7,-116(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + -116);
		ctx.f7.f64 = double(temp.f32);
		// addi r4,r1,112
		ctx.r4.s64 = ctx.r1.s64 + 112;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v9,v12,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v10,v13,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v8,v11,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v9,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v10,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// fadds f3,f1,f13
		ctx.f3.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
		// lfs f8,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f8.f64 = double(temp.f32);
		// stvx v8,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f10,112(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f10.f64 = double(temp.f32);
		// fsubs f2,f3,f10
		ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f10.f64));
		// lfs f11,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f13,f13,f11
		ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
		// lfs f6,100(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		ctx.f6.f64 = double(temp.f32);
		// fsubs f12,f8,f6
		ctx.f12.f64 = double(float(ctx.f8.f64 - ctx.f6.f64));
		// fmuls f1,f13,f0
		ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// fmuls f0,f12,f0
		ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
		// fdivs f13,f5,f1
		ctx.f13.f64 = double(float(ctx.f5.f64 / ctx.f1.f64));
		// fdivs f12,f4,f0
		ctx.f12.f64 = double(float(ctx.f4.f64 / ctx.f0.f64));
		// fabs f13,f13
		ctx.f13.u64 = ctx.f13.u64 & ~0x8000000000000000;
		// fabs f9,f12
		ctx.f9.u64 = ctx.f12.u64 & ~0x8000000000000000;
		// fmuls f0,f2,f13
		ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
		// fcmpu cr6,f0,f31
		// bso cr6,0x822438bc
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x822438B4, "bso");
		// bge cr6,0x822438c4
		if (ctx.f0.f64 < var_f31) {
		loc_822438BC:
			// fmr f0,f31
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = var_f31;
			// b 0x822438d4
		} else {
		loc_822438C4:
			// fcmpu cr6,f0,f7
			ctx.fpscr.disableFlushMode();
			// bso cr6,0x822438d0
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x822438C8, "bso");
			// ble cr6,0x822438d4
			if (ctx.f0.f64 <= ctx.f7.f64) goto loc_822438D4;
		loc_822438D0:
			// fmr f0,f7
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = ctx.f7.f64;
		}
	loc_822438D4:
		// lfs f5,40(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
		ctx.f5.f64 = double(temp.f32);
		// fsubs f4,f11,f5
		ctx.f4.f64 = double(float(ctx.f11.f64 - ctx.f5.f64));
		// fsubs f3,f4,f10
		ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f10.f64));
		// fmuls f2,f3,f13
		ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
		// fneg f13,f2
		ctx.f13.u64 = ctx.f2.u64 ^ 0x8000000000000000;
		// fcmpu cr6,f13,f31
		// bso cr6,0x822438f4
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x822438EC, "bso");
		// bge cr6,0x822438fc
		if (ctx.f13.f64 < var_f31) {
		loc_822438F4:
			// fmr f13,f31
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = var_f31;
			// b 0x8224390c
		} else {
		loc_822438FC:
			// fcmpu cr6,f13,f7
			ctx.fpscr.disableFlushMode();
			// bso cr6,0x82243908
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82243900, "bso");
			// ble cr6,0x8224390c
			if (ctx.f13.f64 <= ctx.f7.f64) goto loc_8224390C;
		loc_82243908:
			// fmr f13,f7
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = ctx.f7.f64;
		}
	loc_8224390C:
		// lfs f1,36(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 36);
		ctx.f1.f64 = double(temp.f32);
		// fmuls f13,f0,f13
		ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
		// fadds f0,f1,f8
		ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
		// lfs f12,116(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
		ctx.f12.f64 = double(temp.f32);
		// fsubs f11,f0,f12
		ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// fmuls f0,f11,f9
		ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
		// fcmpu cr6,f0,f31
		// bso cr6,0x82243930
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82243928, "bso");
		// bge cr6,0x82243938
		if (ctx.f0.f64 < var_f31) {
		loc_82243930:
			// fmr f0,f31
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = var_f31;
			// b 0x82243948
		} else {
		loc_82243938:
			// fcmpu cr6,f0,f7
			ctx.fpscr.disableFlushMode();
			// bso cr6,0x82243944
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8224393C, "bso");
			// ble cr6,0x82243948
			if (ctx.f0.f64 <= ctx.f7.f64) goto loc_82243948;
		loc_82243944:
			// fmr f0,f7
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = ctx.f7.f64;
		}
	loc_82243948:
		// lfs f10,44(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 44);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f13,f13,f0
		ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// fsubs f8,f6,f10
		ctx.f8.f64 = double(float(ctx.f6.f64 - ctx.f10.f64));
		// fsubs f6,f8,f12
		ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f12.f64));
		// fmuls f5,f6,f9
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
		// fneg f0,f5
		ctx.f0.u64 = ctx.f5.u64 ^ 0x8000000000000000;
		// fcmpu cr6,f0,f31
		// bso cr6,0x8224396c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82243964, "bso");
		// bge cr6,0x82243974
		if (ctx.f0.f64 < var_f31) {
		loc_8224396C:
			// fmr f0,f31
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = var_f31;
			// b 0x82243984
		} else {
		loc_82243974:
			// fcmpu cr6,f0,f7
			ctx.fpscr.disableFlushMode();
			// bso cr6,0x82243980
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82243978, "bso");
			// ble cr6,0x82243984
			if (ctx.f0.f64 <= ctx.f7.f64) goto loc_82243984;
		loc_82243980:
			// fmr f0,f7
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = ctx.f7.f64;
		}
	loc_82243984:
		// fmuls f4,f13,f0
		ctx.fpscr.disableFlushMode();
		ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// stfs f4,112(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// lvx128 v6,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32142
		// lfs f0,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// stfs f31,88(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		// lvx128 v7,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// lwz r27,-23828(r11)
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + -23828));
		// lis r11,-32142
		// addis r29,r27,7
		var_r29 = (uint32_t)(var_r27 + 458752);
		// vmulfp128 v5,v6,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r29,r29,-29456
		var_r29 = (uint32_t)(var_r29 + -29456);
		// lwz r11,-23832(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -23832);
		// stvx v5,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stfs f31,164(r31)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r31 + 164, temp.u32);
		// lwz r10,0(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lfs f3,52(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
		ctx.f3.f64 = double(temp.f32);
		// fadds f2,f3,f29
		ctx.f2.f64 = double(float(ctx.f3.f64 + var_f29));
		// cntlzw r9,r10
		ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
		// lfs f1,160(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 160);
		ctx.f1.f64 = double(temp.f32);
		// rlwinm r6,r9,27,31,31
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
		// stfs f1,84(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// cmplwi cr6,r6,0
		// fnmsubs f30,f29,f0,f2
		var_f30 = double(float(-(var_f29 * ctx.f0.f64 - ctx.f2.f64)));
		// stfs f30,80(r1)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// beq cr6,0x82243a00
		if (ctx.r6.u32 != 0) {
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x821cd248
			game_D248(ctx, base);
		}
	loc_82243A00:
		// lwz r5,0(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lfs f0,164(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 164);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,88(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		// lfs f13,0(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + 0);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f13,f29,f13,f30
		ctx.f13.f64 = double(float(var_f29 * ctx.f13.f64 + var_f30));
		// stfs f13,80(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// lfs f0,108(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 108);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,160(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 160, temp.u32);
		// lwz r4,0(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 0);
		// stfs f0,84(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// cntlzw r3,r4
		ctx.r3.u64 = ctx.r4.u32 == 0 ? 32 : __builtin_clz(ctx.r4.u32);
		// rlwinm r9,r3,27,31,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 27) & 0x1;
		// cmplwi cr6,r9,0
		// beq cr6,0x82243a44
		if (ctx.r9.u32 != 0) {
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x821cd248
			game_D248(ctx, base);
		}
	loc_82243A44:
		// li r8,1
		ctx.r8.s64 = 1;
		// stfs f31,212(r31)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r31 + 212, temp.u32);
		// stb r8,216(r31)
		PPC_STORE_U8(var_r31 + 216, ctx.r8.u8);
	}
loc_82243A50:
	return;
}

__attribute__((alias("__imp__ke_3A68"))) PPC_WEAK_FUNC(ke_3A68);
PPC_FUNC_IMPL(__imp__ke_3A68) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x823d99a0
	game_99A0(ctx, base);
	// lis r11,-32251
	// lis r10,-32251
	// addi r11,r11,-2524
	ctx.r11.s64 = ctx.r11.s64 + -2524;
	// addi r10,r10,-2348
	ctx.r10.s64 = ctx.r10.s64 + -2348;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,96(r31)
	PPC_STORE_U32(var_r31 + 96, ctx.r10.u32);
	// lwz r11,236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 236);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243acc
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwz r9,76(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r8,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r10,2
		ctx.r7.s64 = ctx.r10.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r6,r30
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r30);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,236(r31)
		PPC_STORE_U32(var_r31 + 236, ctx.r5.u32);
	}
loc_82243ACC:
	// lwz r11,244(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 244);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243b00
	if (ctx.r11.u32 != 0) {
		// lwz r4,4(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwz r3,76(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r10,r4,r11
		ctx.r10.s64 = ctx.r11.s64 - ctx.r4.s64;
		// twllei r3,0
		if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
		// divwu r10,r10,r3
		ctx.r10.u32 = ctx.r3.u32 ? ctx.r10.u32 / ctx.r3.u32 : 0;
		// addi r9,r10,2
		ctx.r9.s64 = ctx.r10.s64 + 2;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r8,r30
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r30);
		// add r7,r10,r11
		ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r7,244(r31)
		PPC_STORE_U32(var_r31 + 244, ctx.r7.u32);
	}
loc_82243B00:
	// lwz r11,220(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 220);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243b48
	if (ctx.r11.u32 != 0) {
		// lwz r6,4(r30)
		ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 4);
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// lwz r5,76(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r4,r6,r11
		ctx.r4.s64 = ctx.r11.s64 - ctx.r6.s64;
		// twllei r5,0
		if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
		// divwu r11,r4,r5
		ctx.r11.u32 = ctx.r5.u32 ? ctx.r4.u32 / ctx.r5.u32 : 0;
		// addi r3,r11,2
		ctx.r3.s64 = ctx.r11.s64 + 2;
		// rlwinm r11,r3,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r11,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r30);
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r3,0
		// stw r3,220(r31)
		PPC_STORE_U32(var_r31 + 220, ctx.r3.u32);
		// beq cr6,0x82243b48
		if (ctx.r3.u32 == 0) goto loc_82243B48;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x823d7e30
		ke_7E30(ctx, base);
	}
loc_82243B48:
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 224);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243b90
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwz r9,76(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// divwu r11,r8,r9
		ctx.r11.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// addi r7,r11,2
		ctx.r7.s64 = ctx.r11.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r6,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r30);
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r3,0
		// stw r3,224(r31)
		PPC_STORE_U32(var_r31 + 224, ctx.r3.u32);
		// beq cr6,0x82243b90
		if (ctx.r3.u32 == 0) goto loc_82243B90;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x823d7e30
		ke_7E30(ctx, base);
	}
loc_82243B90:
	// lwz r11,228(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 228);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243bd8
	if (ctx.r11.u32 != 0) {
		// lwz r5,4(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// lwz r4,76(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r3,r5,r11
		ctx.r3.s64 = ctx.r11.s64 - ctx.r5.s64;
		// twllei r4,0
		if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
		// divwu r11,r3,r4
		ctx.r11.u32 = ctx.r4.u32 ? ctx.r3.u32 / ctx.r4.u32 : 0;
		// addi r11,r11,2
		ctx.r11.s64 = ctx.r11.s64 + 2;
		// rlwinm r9,r11,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r9,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r30);
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r3,0
		// stw r3,228(r31)
		PPC_STORE_U32(var_r31 + 228, ctx.r3.u32);
		// beq cr6,0x82243bd8
		if (ctx.r3.u32 == 0) goto loc_82243BD8;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x823d7e30
		ke_7E30(ctx, base);
	}
loc_82243BD8:
	// lwz r11,252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 252);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243c20
	if (ctx.r11.u32 != 0) {
		// lwz r8,4(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// lwz r7,76(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r6,r8,r11
		ctx.r6.s64 = ctx.r11.s64 - ctx.r8.s64;
		// twllei r7,0
		if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
		// divwu r11,r6,r7
		ctx.r11.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
		// addi r5,r11,2
		ctx.r5.s64 = ctx.r11.s64 + 2;
		// rlwinm r4,r5,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r4,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + var_r30);
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r3,0
		// stw r3,252(r31)
		PPC_STORE_U32(var_r31 + 252, ctx.r3.u32);
		// beq cr6,0x82243c20
		if (ctx.r3.u32 == 0) goto loc_82243C20;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x822cd1b0
		ph_ctor_D1B0(ctx, base);
	}
loc_82243C20:
	// lwz r11,256(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 256);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243c74
	if (ctx.r11.u32 != 0) {
		// lwz r3,4(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4);
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// lwz r9,76(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r8,r3,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r3.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r11,r8,r9
		ctx.r11.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r11,2
		ctx.r7.s64 = ctx.r11.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r6,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r30);
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r11,0
		// stw r11,256(r31)
		PPC_STORE_U32(var_r31 + 256, ctx.r11.u32);
		// beq cr6,0x82243c74
		if (ctx.r11.u32 == 0) goto loc_82243C74;
		// lis r10,-32250
		// addi r3,r11,4
		ctx.r3.s64 = ctx.r11.s64 + 4;
		// addi r10,r10,-26340
		ctx.r10.s64 = ctx.r10.s64 + -26340;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// bl 0x82121b00
		ke_1B00(ctx, base);
	}
loc_82243C74:
	// lwz r11,260(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 260);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243cc0
	if (ctx.r11.u32 != 0) {
		// lwz r5,4(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// lwz r4,76(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 76);
		// subf r3,r5,r11
		ctx.r3.s64 = ctx.r11.s64 - ctx.r5.s64;
		// twllei r4,0
		if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
		// divwu r11,r3,r4
		ctx.r11.u32 = ctx.r4.u32 ? ctx.r3.u32 / ctx.r4.u32 : 0;
		// addi r11,r11,2
		ctx.r11.s64 = ctx.r11.s64 + 2;
		// rlwinm r9,r11,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r9,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r30);
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r11,0
		// stw r11,260(r31)
		PPC_STORE_U32(var_r31 + 260, ctx.r11.u32);
		// beq cr6,0x82243cc0
		if (ctx.r11.u32 == 0) goto loc_82243CC0;
		// lis r10,-32250
		// addi r10,r10,-31356
		ctx.r10.s64 = ctx.r10.s64 + -31356;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	}
loc_82243CC0:
	// addi r29,r31,264
	var_r29 = (uint32_t)(var_r31 + 264);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x82243da0
	SinglesNetworkClient_3DA0_w(ctx, base);
	// lwz r8,244(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 244);
	// lwz r7,252(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 252);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// lwz r6,236(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 236);
	// lwz r5,256(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 256);
	// stw r5,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r5.u32);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
	// cmplwi cr6,r3,0
	// beq cr6,0x82243d38
	if (ctx.r3.u32 != 0) {
		// lwz r4,256(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 256);
		// cmplwi cr6,r4,0
		// beq cr6,0x82243d38
		if (ctx.r4.u32 == 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			return;
		}
		// lwz r11,252(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 252);
		// cmplwi cr6,r11,0
		// beq cr6,0x82243d38
		if (ctx.r11.u32 == 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			return;
		}
		// lwz r4,260(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 260);
		// bl 0x822c9e50
		ph_9E50(ctx, base);
		// lwz r4,0(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r3,256(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 256);
		// bl 0x822c9030
		pongCreatureInst_9030_g(ctx, base);
		// lwz r11,256(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 256);
		// lwz r3,252(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 252);
		// cmplwi cr6,r11,0
		// stw r11,16(r3)
		PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r11.u32);
		// beq cr6,0x82243d38
		if (ctx.r11.u32 == 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			return;
		}
		// bl 0x822ca9b8
		game_A9B8(ctx, base);
	}
loc_82243D38:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__crAnimDofFloat_3D48_2hr"))) PPC_WEAK_FUNC(crAnimDofFloat_3D48_2hr);
PPC_FUNC_IMPL(__imp__crAnimDofFloat_3D48_2hr) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,76(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// rotlwi r10,r11,0
	ctx.r10.u64 = ctx.r11.u32;
	// divwu r11,r8,r9
	ctx.r11.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// addi r7,r11,2
	ctx.r7.s64 = ctx.r11.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r6,r3
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r3.u32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lis r10,-32250
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// addi r10,r10,-26340
	ctx.r10.s64 = ctx.r10.s64 + -26340;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// b 0x82121b00
	ke_1B00(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82243D9C"))) PPC_WEAK_FUNC(sub_82243D9C);
PPC_FUNC_IMPL(__imp__sub_82243D9C) {
	PPC_FUNC_PROLOGUE();
	// blr
	return;
}

__attribute__((alias("__imp__SinglesNetworkClient_3DA0_w"))) PPC_WEAK_FUNC(SinglesNetworkClient_3DA0_w);
PPC_FUNC_IMPL(__imp__SinglesNetworkClient_3DA0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplwi cr6,r11,0
	// beq cr6,0x82243e0c
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* SinglesNetworkClient::flags@+0x4 */;
		// lwz r9,76(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// rotlwi r10,r11,0
		ctx.r10.u64 = ctx.r11.u32;
		// divwu r11,r8,r9
		ctx.r11.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// addi r7,r11,2
		ctx.r7.s64 = ctx.r11.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r6,r3
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r3.u32);
		// add r31,r11,r10
		var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// cmplwi cr6,r31,0
		// stw r31,0(r4)
		PPC_STORE_U32(ctx.r4.u32 + 0, var_r31);
		// beq cr6,0x82243e0c
		if (var_r31 == 0) {
			// blr
			return;
		}
		// lis r11,-32250
		// addi r3,r31,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
		// addi r11,r11,-25780
		ctx.r11.s64 = ctx.r11.s64 + -25780;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* SinglesNetworkClient::vtable@+0x0 */ ctx.r11.u32);
		// bl 0x82121b00
		ke_1B00(ctx, base);
		// addi r3,r31,12
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 12;
		// bl 0x823d8108
		SinglesNetworkClient_8108_gen(ctx, base);
	}
loc_82243E0C:
	// blr
	return;
}

__attribute__((alias("__imp__grcDisplay_create_3E20"))) PPC_WEAK_FUNC(grcDisplay_create_3E20);
PPC_FUNC_IMPL(__imp__grcDisplay_create_3E20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32254
	// li r10,-1
	// addi r9,r11,31824
	ctx.r9.s64 = ctx.r11.s64 + 31824;
	// lis r11,-32253
	// li r7,2
	ctx.r7.s64 = 2;
	// li r6,0
	ctx.r6.s64 = 0;
	// sth r10,40(r3)
	PPC_STORE_U16(ctx.r3.u32 + 40, ctx.r10.u16);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// lfs f11,-12312(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12312);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32251
	// stfs f11,16(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// sth r10,42(r3)
	PPC_STORE_U16(ctx.r3.u32 + 42, ctx.r10.u16);
	// addi r8,r11,-2292
	ctx.r8.s64 = ctx.r11.s64 + -2292;
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// lis r11,-32253
	// sth r10,52(r3)
	PPC_STORE_U16(ctx.r3.u32 + 52, ctx.r10.u16);
	// sth r10,54(r3)
	PPC_STORE_U16(ctx.r3.u32 + 54, ctx.r10.u16);
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// sth r6,56(r3)
	PPC_STORE_U16(ctx.r3.u32 + 56, ctx.r6.u16);
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32163
	// stfs f13,12(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f12,20(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stfs f0,24(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f12,28(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// lfs f10,-32640(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32640);
	ctx.f10.f64 = double(temp.f32);
	// stfs f0,32(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// stfs f0,36(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stfs f10,44(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 44, temp.u32);
	// stfs f13,48(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 48, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__pongSurface_vfn_1"))) PPC_WEAK_FUNC(pongSurface_vfn_1);
PPC_FUNC_IMPL(__imp__pongSurface_vfn_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// bl 0x82410cb8
	ph_0CB8(ctx, base);
	// lis r11,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r11,-2336
	ctx.r4.s64 = ctx.r11.s64 + -2336;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822e59c8
	ph_59C8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r29,r11,-2396
	var_r29 = (uint32_t)(ctx.r11.s64 + -2396);  // lbl_8202F6A4 @ 0x8202f6a4
	// beq cr6,0x82243f1c
	if (ctx.r11.u32 != 0) {
		// li r5,1
		ctx.r5.s64 = 1;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x822e5908
		ph_5908(ctx, base);
		// clrlwi r10,r3,24
		ctx.r10.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// bne cr6,0x82243f1c
		if (ctx.r10.u32 != 0) goto loc_82243F1C;
		// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r8,16(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 4, ctx, base);  // pattern-B slot 4 (byte +16)
		// sth r3,52(r30)
		PPC_STORE_U16(var_r30 + 52, ctx.r3.u16);
	}
loc_82243F1C:
	// lis r11,-32251
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r11,-2328
	ctx.r4.s64 = ctx.r11.s64 + -2328;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822e59c8
	ph_59C8(ctx, base);
	// clrlwi r6,r3,24
	ctx.r6.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x82243f70
	if (ctx.r6.u32 != 0) {
		// li r5,1
		ctx.r5.s64 = 1;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x822e5908
		ph_5908(ctx, base);
		// clrlwi r5,r3,24
		ctx.r5.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// bne cr6,0x82243f70
		if (ctx.r5.u32 != 0) {
			return;
		}
		// lwz r4,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r11,16(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 4, ctx, base);  // pattern-B slot 4 (byte +16)
		// sth r3,54(r30)
		PPC_STORE_U16(var_r30 + 54, ctx.r3.u16);
	}
loc_82243F70:
	return;
}

__attribute__((alias("__imp__pongSurface_vfn_2"))) PPC_WEAK_FUNC(pongSurface_vfn_2);
PPC_FUNC_IMPL(__imp__pongSurface_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// bl 0x82410eb0
	pongSurface_0EB0_fw(ctx, base);
	// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
	// lis r11,-32251
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r4,r11,-2320
	ctx.r4.s64 = ctx.r11.s64 + -2320;
	// lwz r9,148(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 37, ctx, base);  // pattern-B slot 37 (byte +148)
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// lhz r7,52(r30)
	ctx.r7.u64 = PPC_LOAD_U16(var_r30 + 52);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// lwz r6,140(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 35, ctx, base);  // pattern-B slot 35 (byte +140)
	// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
	// lis r11,-32253
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r29,r11,-2532
	var_r29 = (uint32_t)(ctx.r11.s64 + -2532);  // lbl_8202F61C @ 0x8202f61c
	// lwz r11,148(r5)
  // [ph4a] slot load collapsed
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bctrl
	VCALL(var_r31, 37, ctx, base);  // pattern-B slot 37 (byte +148)
	// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
	// lis r11,-32251
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r4,r11,-2308
	ctx.r4.s64 = ctx.r11.s64 + -2308;
	// lwz r9,148(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 37, ctx, base);  // pattern-B slot 37 (byte +148)
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// lhz r7,54(r30)
	ctx.r7.u64 = PPC_LOAD_U16(var_r30 + 54);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// lwz r6,140(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 35, ctx, base);  // pattern-B slot 35 (byte +140)
	// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// lwz r11,148(r5)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 37, ctx, base);  // pattern-B slot 37 (byte +148)
	return;
}

__attribute__((alias("__imp__crAnimChannel_vfn_0"))) PPC_WEAK_FUNC(crAnimChannel_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannel_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// cmplwi cr6,r10,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82244074
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82244074:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x82244174
	if (var_r30 != 0) {
		// lis r11,-32251
		// lis r10,-32251
		// addi r11,r11,-2268
		ctx.r11.s64 = ctx.r11.s64 + -2268;
		// addi r10,r10,-2196
		ctx.r10.s64 = ctx.r10.s64 + -2196;
		// addi r31,r30,8
		var_r31 = (uint32_t)(var_r30 + 8);
		// stw r11,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
		// lbz r7,4(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 4);
		// stb r7,4(r30)
		PPC_STORE_U8(var_r30 + 4, ctx.r7.u8);
		// lbz r6,5(r29)
		ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 5);
		// stb r6,5(r30)
		PPC_STORE_U8(var_r30 + 5, ctx.r6.u8);
		// lhz r5,6(r29)
		ctx.r5.u64 = PPC_LOAD_U16(var_r29 + 6);
		// stw r10,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r10.u32);
		// sth r5,6(r30)
		PPC_STORE_U16(var_r30 + 6, ctx.r5.u16);
		// lhz r11,12(r29)
		ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
		// cmplwi cr6,r10,0
		// sth r11,4(r31)
		PPC_STORE_U16(var_r31 + 4, ctx.r11.u16);
		// sth r11,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
		// beq cr6,0x82244128
		if (ctx.r10.u32 != 0) {
			// rlwinm r3,r10,2,0,29
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// bl 0x820dec88
			xe_EC88(ctx, base);
			// stw r3,0(r31)
			PPC_STORE_U32(var_r31 + 0, ctx.r3.u32);
			// b 0x82244130
		} else {
		loc_82244128:
			// li r4,0
			ctx.r4.s64 = 0;
			// stw r4,0(r31)
			PPC_STORE_U32(var_r31 + 0, ctx.r4.u32);
		}
	loc_82244130:
		// lhz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 4);
		// cmplwi cr6,r3,0
		// beq cr6,0x82244168
		if (ctx.r3.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		loc_82244140:
			// lwz r8,8(r29)
			ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 8);
			// rlwinm r10,r11,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// lwz r7,0(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0);
			// addi r9,r11,1
			ctx.r9.s64 = ctx.r11.s64 + 1;
			// clrlwi r11,r9,16
			ctx.r11.u64 = ctx.r9.u32 & 0xFFFF;
			// lfsx f0,r8,r10
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
			ctx.f0.f64 = double(temp.f32);
			// stfsx f0,r10,r7
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r10.u32 + ctx.r7.u32, temp.u32);
			// lhz r6,4(r31)
			ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 4);
			// cmplw cr6,r11,r6
			// blt cr6,0x82244140
			if (ctx.r11.u32 < ctx.r6.u32) goto loc_82244140;
		}
	loc_82244168:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		return;
	}
loc_82244174:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-2196
	ctx.r11.s64 = ctx.r11.s64 + -2196;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x822441bc
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_822441BC:
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x822441e0
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_822441E0:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x82244288
	if (var_r31 != 0) {
		// lis r11,-32251
		// lis r10,-32251
		// addi r11,r11,-2268
		ctx.r11.s64 = ctx.r11.s64 + -2268;
		// addi r10,r10,-2124
		ctx.r10.s64 = ctx.r10.s64 + -2124;
		// addi r4,r30,8
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 8;
		// addi r3,r31,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// lbz r7,4(r30)
		ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 4);
		// stb r7,4(r31)
		PPC_STORE_U8(var_r31 + 4, ctx.r7.u8);
		// lbz r6,5(r30)
		ctx.r6.u64 = PPC_LOAD_U8(var_r30 + 5);
		// stb r6,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r6.u8);
		// lhz r5,6(r30)
		ctx.r5.u64 = PPC_LOAD_U16(var_r30 + 6);
		// stw r10,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
		// sth r5,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r5.u16);
		// bl 0x82247e78
		xe_7E78(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// b 0x8224428c
	} else {
	loc_82244288:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8224428C:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-2124
	ctx.r11.s64 = ctx.r11.s64 + -2124;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x822442e4
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_822442E4:
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82244308
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82244308:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawQuaternion_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelRawQuaternion_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelRawQuaternion_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82244380
	if (ctx.r3.u32 != 0) {
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x82244410
		pongNetMessageHolder_4410_w(ctx, base);
		// blr
		return;
	}
loc_82244380:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawQuaternion_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelRawQuaternion_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelRawQuaternion_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-2052
	ctx.r11.s64 = ctx.r11.s64 + -2052;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x822443d4
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_822443D4:
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x822443f8
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_822443F8:
	// blr
	return;
}

__attribute__((alias("__imp__pongNetMessageHolder_4410_w"))) PPC_WEAK_FUNC(pongNetMessageHolder_4410_w);
PPC_FUNC_IMPL(__imp__pongNetMessageHolder_4410_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// lis r11,-32251
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lis r10,-32251
	// addi r31,r29,8
	var_r31 = (uint32_t)(var_r29 + 8);
	// addi r10,r10,-2052
	ctx.r10.s64 = ctx.r10.s64 + -2052;
	// stw r11,0(r29)
	PPC_STORE_U32(var_r29 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r11.u32);
	// lbz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 4);
	// stb r11,4(r29)
	PPC_STORE_U8(var_r29 + 4, ctx.r11.u8);
	// lbz r9,5(r30)
	ctx.r9.u64 = PPC_LOAD_U8(var_r30 + 5);
	// stb r9,5(r29)
	PPC_STORE_U8(var_r29 + 5, ctx.r9.u8);
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(var_r30 + 6);
	// stw r10,0(r29)
	PPC_STORE_U32(var_r29 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r10.u32);
	// sth r8,6(r29)
	PPC_STORE_U16(var_r29 + 6, ctx.r8.u16);
	// lhz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 12);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// cmplwi cr6,r10,0
	// sth r11,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r11.u16);
	// sth r11,6(r31)
	PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
	// beq cr6,0x82244494
	if (ctx.r10.u32 != 0) {
		// lis r7,4095
		ctx.r7.s64 = 268369920;
		// rlwinm r3,r10,4,0,27
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
		// ori r6,r7,65535
		ctx.r6.u64 = ctx.r7.u64 | 65535;
		// cmplw cr6,r10,r6
		// ble cr6,0x82244488
		if (ctx.r10.u32 > ctx.r6.u32) {
			// li r3,-1
		}
	loc_82244488:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// stw r3,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r3.u32);
		// b 0x8224449c
	} else {
	loc_82244494:
		// li r5,0
		ctx.r5.s64 = 0;
		// stw r5,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r5.u32);
	}
loc_8224449C:
	// lhz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U16(var_r31 + 4);
	// cmplwi cr6,r4,0
	// beq cr6,0x822444e8
	if (ctx.r4.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_822444AC:
		// lwz r8,8(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 8);
		// rlwinm r10,r11,4,0,27
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
		// lwz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* pongNetMessageHolder::vtable@+0x0 */;
		// addi r3,r11,1
		ctx.r3.s64 = ctx.r11.s64 + 1;
		// add r8,r8,r10
		ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
		// add r9,r9,r10
		ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
		// mr r10,r8
		ctx.r10.u64 = ctx.r8.u64;
		// clrlwi r11,r3,16
		ctx.r11.u64 = ctx.r3.u32 & 0xFFFF;
		// ld r7,0(r10)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
		// std r7,0(r9)
		PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r7.u64);
		// ld r6,8(r10)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
		// std r6,8(r9)
		PPC_STORE_U64(ctx.r9.u32 + 8, ctx.r6.u64);
		// lhz r5,4(r31)
		ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 4);
		// cmplw cr6,r11,r5
		// blt cr6,0x822444ac
		if (ctx.r11.u32 < ctx.r5.u32) goto loc_822444AC;
	}
loc_822444E8:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawInt_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelRawInt_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelRawInt_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x82244588
	if (var_r31 != 0) {
		// lis r11,-32251
		// lis r10,-32251
		// addi r11,r11,-2268
		ctx.r11.s64 = ctx.r11.s64 + -2268;
		// addi r10,r10,-1980
		ctx.r10.s64 = ctx.r10.s64 + -1980;
		// addi r4,r30,8
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 8;
		// addi r3,r31,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// lbz r7,4(r30)
		ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 4);
		// stb r7,4(r31)
		PPC_STORE_U8(var_r31 + 4, ctx.r7.u8);
		// lbz r6,5(r30)
		ctx.r6.u64 = PPC_LOAD_U8(var_r30 + 5);
		// stb r6,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r6.u8);
		// lhz r5,6(r30)
		ctx.r5.u64 = PPC_LOAD_U16(var_r30 + 6);
		// stw r10,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
		// sth r5,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r5.u16);
		// bl 0x820fd100
		xe_D100(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// b 0x8224458c
	} else {
	loc_82244588:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8224458C:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawInt_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelRawInt_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelRawInt_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-1980
	ctx.r11.s64 = ctx.r11.s64 + -1980;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x822445e4
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_822445E4:
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82244608
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82244608:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x82244704
	if (var_r31 != 0) {
		// lis r11,-32251
		// lis r10,-32251
		// addi r11,r11,-2268
		ctx.r11.s64 = ctx.r11.s64 + -2268;
		// addi r10,r10,-1908
		ctx.r10.s64 = ctx.r10.s64 + -1908;
		// addi r30,r31,8
		var_r30 = (uint32_t)(var_r31 + 8);
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// lbz r7,4(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 4);
		// stb r7,4(r31)
		PPC_STORE_U8(var_r31 + 4, ctx.r7.u8);
		// lbz r6,5(r29)
		ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 5);
		// stb r6,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r6.u8);
		// lhz r5,6(r29)
		ctx.r5.u64 = PPC_LOAD_U16(var_r29 + 6);
		// stw r10,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
		// sth r5,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r5.u16);
		// lhz r11,12(r29)
		ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
		// mr r3,r11
		ctx.r3.u64 = ctx.r11.u64;
		// cmplwi cr6,r3,0
		// sth r11,4(r30)
		PPC_STORE_U16(var_r30 + 4, ctx.r11.u16);
		// sth r11,6(r30)
		PPC_STORE_U16(var_r30 + 6, ctx.r11.u16);
		// beq cr6,0x822446bc
		if (ctx.r3.u32 != 0) {
			// bl 0x820dec88
			xe_EC88(ctx, base);
			// stw r3,0(r30)
			PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
			// b 0x822446c4
		} else {
		loc_822446BC:
			// li r4,0
			ctx.r4.s64 = 0;
			// stw r4,0(r30)
			PPC_STORE_U32(var_r30 + 0, ctx.r4.u32);
		}
	loc_822446C4:
		// lhz r3,4(r30)
		ctx.r3.u64 = PPC_LOAD_U16(var_r30 + 4);
		// cmplwi cr6,r3,0
		// beq cr6,0x822446f8
		if (ctx.r3.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		loc_822446D4:
			// lwz r9,8(r29)
			ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 8);
			// addi r10,r11,1
			ctx.r10.s64 = ctx.r11.s64 + 1;
			// lwz r8,0(r30)
			ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 0);
			// lbzx r7,r9,r11
			ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r11.u32);
			// stbx r7,r11,r8
			PPC_STORE_U8(ctx.r11.u32 + ctx.r8.u32, ctx.r7.u8);
			// clrlwi r11,r10,16
			ctx.r11.u64 = ctx.r10.u32 & 0xFFFF;
			// lhz r6,4(r30)
			ctx.r6.u64 = PPC_LOAD_U16(var_r30 + 4);
			// cmplw cr6,r11,r6
			// blt cr6,0x822446d4
			if (ctx.r11.u32 < ctx.r6.u32) goto loc_822446D4;
		}
	loc_822446F8:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		return;
	}
loc_82244704:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-1908
	ctx.r11.s64 = ctx.r11.s64 + -1908;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224474c
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224474C:
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82244770
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82244770:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticFloat_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelStaticFloat_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticFloat_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,12
	ctx.r4.s64 = 12;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82244818
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// lis r10,-32251
		// addi r11,r11,-2268
		ctx.r11.s64 = ctx.r11.s64 + -2268;
		// addi r10,r10,-1836
		ctx.r10.s64 = ctx.r10.s64 + -1836;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// lbz r7,4(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
		// stb r7,4(r3)
		PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r7.u8);
		// lbz r6,5(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 5);
		// stb r6,5(r3)
		PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r6.u8);
		// lhz r5,6(r31)
		ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
		// stw r10,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
		// sth r5,6(r3)
		PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r5.u16);
		// lfs f0,8(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 8);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,8(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
		// blr
		return;
	}
loc_82244818:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticQuaternion_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelStaticQuaternion_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticQuaternion_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,32
	ctx.r4.s64 = 32;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x822448e0
	if (ctx.r3.u32 != 0) {
		// lis r9,-32251
		// lis r8,-32251
		// addi r9,r9,-2268
		ctx.r9.s64 = ctx.r9.s64 + -2268;
		// addi r8,r8,-1764
		ctx.r8.s64 = ctx.r8.s64 + -1764;
		// addi r11,r31,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
		// addi r10,r3,16
		ctx.r10.s64 = ctx.r3.s64 + 16;
		// stw r9,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
		// lbz r7,4(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 4);
		// stb r7,4(r3)
		PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r7.u8);
		// lbz r6,5(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 5);
		// stb r6,5(r3)
		PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r6.u8);
		// lhz r5,6(r31)
		ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
		// stw r8,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
		// sth r5,6(r3)
		PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r5.u16);
		// lwz r4,0(r11)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// stw r4,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
		// lwz r9,4(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r9,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
		// lwz r8,8(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// stw r8,8(r10)
		PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
		// lwz r7,12(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		// stw r7,12(r10)
		PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r7.u32);
		// blr
		return;
	}
loc_822448E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,24
	ctx.r4.s64 = 24;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x82244998
	if (var_r31 != 0) {
		// lis r11,-32251
		// lis r10,-32251
		// addi r11,r11,-2268
		ctx.r11.s64 = ctx.r11.s64 + -2268;
		// addi r10,r10,-1692
		ctx.r10.s64 = ctx.r10.s64 + -1692;
		// addi r4,r30,8
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 8;
		// addi r3,r31,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// lbz r7,4(r30)
		ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 4);
		// stb r7,4(r31)
		PPC_STORE_U8(var_r31 + 4, ctx.r7.u8);
		// lbz r6,5(r30)
		ctx.r6.u64 = PPC_LOAD_U8(var_r30 + 5);
		// stb r6,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r6.u8);
		// lhz r5,6(r30)
		ctx.r5.u64 = PPC_LOAD_U16(var_r30 + 6);
		// stw r10,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
		// sth r5,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r5.u16);
		// bl 0x82247f28
		pongNetMessageHolder_7F28_w(ctx, base);
		// lfs f0,16(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 16);
		ctx.f0.f64 = double(temp.f32);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stfs f0,16(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 16, temp.u32);
		// lfs f13,20(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 20);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,20(r31)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r31 + 20, temp.u32);
		// b 0x8224499c
	} else {
	loc_82244998:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8224499C:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-1692
	ctx.r11.s64 = ctx.r11.s64 + -1692;
	// addi r3,r31,8
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 8;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82247d00
	atSingleton_7D00_h(ctx, base);
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82244a0c
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82244A0C:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_2"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,24
	ctx.r4.s64 = 24;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82244a88
	if (ctx.r3.u32 != 0) {
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x82244b18
		pongNetMessageHolder_4B18_w(ctx, base);
		// blr
		return;
	}
loc_82244A88:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_0"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-1620
	ctx.r11.s64 = ctx.r11.s64 + -1620;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x82244adc
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_82244ADC:
	// lis r11,-32251
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82244b00
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82244B00:
	// blr
	return;
}

__attribute__((alias("__imp__pongNetMessageHolder_4B18_w"))) PPC_WEAK_FUNC(pongNetMessageHolder_4B18_w);
PPC_FUNC_IMPL(__imp__pongNetMessageHolder_4B18_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=112, savegprlr_29
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-2268
	ctx.r11.s64 = ctx.r11.s64 + -2268;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lis r10,-32251
	// addi r29,r31,8
	var_r29 = (uint32_t)(var_r31 + 8);
	// addi r10,r10,-1620
	ctx.r10.s64 = ctx.r10.s64 + -1620;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r11.u32);
	// lbz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 4);
	// stb r11,4(r31)
	PPC_STORE_U8(var_r31 + 4, ctx.r11.u8);
	// lbz r9,5(r30)
	ctx.r9.u64 = PPC_LOAD_U8(var_r30 + 5);
	// stb r9,5(r31)
	PPC_STORE_U8(var_r31 + 5, ctx.r9.u8);
	// lhz r8,6(r30)
	ctx.r8.u64 = PPC_LOAD_U16(var_r30 + 6);
	// stw r10,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r10.u32);
	// sth r8,6(r31)
	PPC_STORE_U16(var_r31 + 6, ctx.r8.u16);
	// lhz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 12);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// cmplwi cr6,r10,0
	// sth r11,4(r29)
	PPC_STORE_U16(var_r29 + 4, ctx.r11.u16);
	// sth r11,6(r29)
	PPC_STORE_U16(var_r29 + 6, ctx.r11.u16);
	// beq cr6,0x82244b88
	if (ctx.r10.u32 != 0) {
		// rlwinm r3,r10,1,0,30
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// stw r3,0(r29)
		PPC_STORE_U32(var_r29 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r3.u32);
		// b 0x82244b90
	} else {
	loc_82244B88:
		// li r7,0
		ctx.r7.s64 = 0;
		// stw r7,0(r29)
		PPC_STORE_U32(var_r29 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r7.u32);
	}
loc_82244B90:
	// lhz r6,4(r29)
	ctx.r6.u64 = PPC_LOAD_U16(var_r29 + 4);
	// cmplwi cr6,r6,0
	// beq cr6,0x82244bc8
	if (ctx.r6.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_82244BA0:
		// lwz r4,8(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 8);
		// rlwinm r10,r11,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
		// lwz r3,0(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0)/* pongNetMessageHolder::vtable@+0x0 */;
		// addi r5,r11,1
		ctx.r5.s64 = ctx.r11.s64 + 1;
		// clrlwi r11,r5,16
		ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
		// lhzx r9,r4,r10
		ctx.r9.u64 = PPC_LOAD_U16(ctx.r4.u32 + ctx.r10.u32);
		// sthx r9,r3,r10
		PPC_STORE_U16(ctx.r3.u32 + ctx.r10.u32, ctx.r9.u16);
		// lhz r8,4(r29)
		ctx.r8.u64 = PPC_LOAD_U16(var_r29 + 4);
		// cmplw cr6,r11,r8
		// blt cr6,0x82244ba0
		if (ctx.r11.u32 < ctx.r8.u32) goto loc_82244BA0;
	}
loc_82244BC8:
	// lfs f0,16(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 16);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stfs f0,16(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// lfs f13,20(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 20);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,20(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_4BE8_p46"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_4BE8_p46);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_4BE8_p46) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// cmplwi cr6,r5,3
	// bgt cr6,0x82244c78
	if (ctx.r5.u32 > 3) goto loc_82244C78;
	// lis r12,-32220
	// addi r12,r12,19464
	ctx.r12.s64 = ctx.r12.s64 + 19464;
	// rlwinm r0,r5,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r5.u64) {
	case 0:
		// lfs f0,0(r4)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r6)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// blr
		return;
	case 1:
		// lfs f13,0(r4)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,4(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// fmadds f11,f13,f1,f12
		ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f1.f64 + ctx.f12.f64));
		// stfs f11,0(r6)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// blr
		return;
	case 2:
		goto loc_82244C38;
	case 3:
		goto loc_82244C54;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82244C18:
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr
	return;
loc_82244C38:
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f7,f10,f1,f9
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f9.f64));
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f6,f7,f1,f8
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 + ctx.f8.f64));
	// stfs f6,0(r6)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr
	return;
loc_82244C54:
	// lfs f5,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f0,f5,f1,f4
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f4.f64));
	// lfs f3,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f13,f0,f1,f3
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f12,f13,f1,f2
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f1.f64 + ctx.f2.f64));
	// stfs f12,0(r6)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// blr
	return;
loc_82244C78:
	// lfs f11,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// cmpwi cr6,r5,0
	// stfs f11,0(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r11,r4,4
	ctx.r11.s64 = ctx.r4.s64 + 4;
	// beqlr cr6
	if (ctx.r5.s32 == 0) return;
loc_82244C8C:
	// lfs f10,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// fmuls f0,f10,f1
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// cmpwi cr6,r5,0
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// stfs f8,0(r6)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// bne cr6,0x82244c8c
	if (ctx.r5.s32 != 0) goto loc_82244C8C;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_4CB8_h"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_4CB8_h);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_4CB8_h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// cmpwi cr6,r4,0
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// bge cr6,0x82244cd0
	if (ctx.r4.s32 < 0) {
		// li r4,0
		ctx.r4.s64 = 0;
		// b 0x82244cdc
	} else {
	loc_82244CD0:
		// cmpw cr6,r4,r11
		// ble cr6,0x82244cdc
		if (ctx.r4.s32 <= ctx.r11.s32) goto loc_82244CDC;
		// mr r4,r11
		ctx.r4.u64 = ctx.r11.u64;
	}
loc_82244CDC:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f13,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32248
	// lfs f12,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lhzx r11,r11,r10
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r10.u32);
	// lfs f0,-25524(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25524);
	ctx.f0.f64 = double(temp.f32);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f13,f9
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmadds f1,f8,f0,f12
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f12.f64));
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_4D18_w"))) PPC_WEAK_FUNC(atSingleton_4D18_w);
PPC_FUNC_IMPL(__imp__atSingleton_4D18_w) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82244d30
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244D30:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr
	return;
}

__attribute__((alias("__imp__grc_4D38"))) PPC_WEAK_FUNC(grc_4D38);
PPC_FUNC_IMPL(__imp__grc_4D38) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82244d5c
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244D5C:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r8,0
	// beq cr6,0x82244d84
	if (ctx.r8.u32 != 0) {
		// li r5,1
		ctx.r5.s64 = 1;
		// bl 0x822e3c68
		grc_3C68(ctx, base);
		// blr
		return;
	}
loc_82244D84:
	// lhz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 0);
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// rotlwi r7,r11,8
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r11.u32, 8);
	// rlwinm r6,r11,24,8,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 24) & 0xFFFFFF;
	// clrlwi r11,r7,16
	ctx.r11.u64 = ctx.r7.u32 & 0xFFFF;
	// or r10,r11,r6
	ctx.r10.u64 = ctx.r11.u64 | ctx.r6.u64;
	// sth r10,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r10.u16);
	// bl 0x822e39b0
	util_39B0(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__grc_4DB8"))) PPC_WEAK_FUNC(grc_4DB8);
PPC_FUNC_IMPL(__imp__grc_4DB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82244dec
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244DEC:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// beq cr6,0x82244e0c
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244e10
	} else {
	loc_82244E0C:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244E10:
	// lhz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 0);
	// addi r4,r30,4
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 4;
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x82244e2c
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244E2C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// beq cr6,0x82244e48
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244e4c
	} else {
	loc_82244E48:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244E4C:
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 0);
	// addi r4,r30,8
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 8;
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82244e68
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244E68:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// beq cr6,0x82244e84
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244e88
	} else {
	loc_82244E84:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244E88:
	// blr
	return;
}

__attribute__((alias("__imp__grc_4EA0"))) PPC_WEAK_FUNC(grc_4EA0);
PPC_FUNC_IMPL(__imp__grc_4EA0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82244ed4
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244ED4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// beq cr6,0x82244ef4
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244ef8
	} else {
	loc_82244EF4:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244EF8:
	// lhz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 0);
	// addi r4,r30,4
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 4;
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x82244f14
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244F14:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// beq cr6,0x82244f30
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244f34
	} else {
	loc_82244F30:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244F34:
	// lhz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 0);
	// addi r4,r30,8
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 8;
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82244f50
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244F50:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// beq cr6,0x82244f6c
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244f70
	} else {
	loc_82244F6C:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244F70:
	// lhz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 0);
	// addi r4,r30,12
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 12;
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x82244f8c
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82244F8C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// beq cr6,0x82244fa8
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82244fac
	} else {
	loc_82244FA8:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82244FAC:
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_4FC8_wrh"))) PPC_WEAK_FUNC(atSingleton_4FC8_wrh);
PPC_FUNC_IMPL(__imp__atSingleton_4FC8_wrh) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,9
	// bgt cr6,0x82245264
	if (ctx.r11.u32 > 9) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r12,-32220
	// addi r12,r12,20472
	ctx.r12.s64 = ctx.r12.s64 + 20472;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	case 1:
		goto loc_82245020;
	case 2:
		goto loc_82245064;
	case 3:
		goto loc_822450A8;
	case 4:
		goto loc_82245174;
	case 5:
		goto loc_822451A8;
	case 6:
		goto loc_822451EC;
	case 7:
		goto loc_822450EC;
	case 8:
		goto loc_82245130;
	case 9:
		goto loc_82245230;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82245020:
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r10,r11,-2196
	ctx.r10.s64 = ctx.r11.s64 + -2196;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r9,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r9.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_82245064:
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r8,2
	ctx.r8.s64 = 2;
	// addi r10,r11,-2124
	ctx.r10.s64 = ctx.r11.s64 + -2124;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r8,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r8.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_822450A8:
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r7,3
	ctx.r7.s64 = 3;
	// addi r10,r11,-2052
	ctx.r10.s64 = ctx.r11.s64 + -2052;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r7,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r7.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_822450EC:
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r6,7
	ctx.r6.s64 = 7;
	// addi r10,r11,-1980
	ctx.r10.s64 = ctx.r11.s64 + -1980;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r6,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r6.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_82245130:
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r5,8
	ctx.r5.s64 = 8;
	// addi r10,r11,-1908
	ctx.r10.s64 = ctx.r11.s64 + -1908;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r5,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r5.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_82245174:
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r11,r11,-1836
	ctx.r11.s64 = ctx.r11.s64 + -1836;
	// stb r4,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r4.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
loc_822451A8:
	// li r3,24
	ctx.r3.s64 = 24;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r9,5
	ctx.r9.s64 = 5;
	// addi r10,r11,-1692
	ctx.r10.s64 = ctx.r11.s64 + -1692;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r9,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r9.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_822451EC:
	// li r3,24
	ctx.r3.s64 = 24;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r8,6
	ctx.r8.s64 = 6;
	// addi r10,r11,-1620
	ctx.r10.s64 = ctx.r11.s64 + -1620;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r8,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r8.u8);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sth r11,12(r3)
	PPC_STORE_U16(ctx.r3.u32 + 12, ctx.r11.u16);
	// sth r11,14(r3)
	PPC_STORE_U16(ctx.r3.u32 + 14, ctx.r11.u16);
	// blr
	return;
loc_82245230:
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82245264
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r11,-32251
	// li r7,9
	ctx.r7.s64 = 9;
	// addi r11,r11,-1764
	ctx.r11.s64 = ctx.r11.s64 + -1764;
	// stb r7,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r7.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_5278_p46"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_5278_p46);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_5278_p46) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,5(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r11,8
	// bgtlr cr6
	if (ctx.r11.u32 > 8) return;
	// lis r12,-32220
	// addi r12,r12,21152
	ctx.r12.s64 = ctx.r12.s64 + 21152;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		goto loc_822452C4;
	case 1:
		goto loc_82245314;
	case 2:
		goto loc_82245364;
	case 3:
		// lis r11,-32251
		// addi r11,r11,-1836
		ctx.r11.s64 = ctx.r11.s64 + -1836;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// blr
		return;
	case 4:
		// b 0x82246740
		// FATAL: unresolved function 0x82246740 (no CallTarget in FunctionNode)
		REX_FATAL("Unresolved call from 0x82245464 to 0x82246740");
		return;
	case 5:
		goto loc_82245468;
	case 6:
		goto loc_822453B4;
	case 7:
		goto loc_82245404;
	case 8:
		// lis r11,-32251
		// addi r11,r11,-1764
		ctx.r11.s64 = ctx.r11.s64 + -1764;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// blr
		return;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_822452C4:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r10,r11,-2196
	ctx.r10.s64 = ctx.r11.s64 + -2196;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
loc_82245314:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r10,r11,-2124
	ctx.r10.s64 = ctx.r11.s64 + -2124;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
loc_82245364:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r10,r11,-2052
	ctx.r10.s64 = ctx.r11.s64 + -2052;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
loc_822453B4:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r10,r11,-1980
	ctx.r10.s64 = ctx.r11.s64 + -1980;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
loc_82245404:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r10,r11,-1908
	ctx.r10.s64 = ctx.r11.s64 + -1908;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
loc_82245468:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r10,r11,-1620
	ctx.r10.s64 = ctx.r11.s64 + -1620;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stb r11,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r11.u8);
	// stb r11,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r11.u8);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-2196
	ctx.r11.s64 = ctx.r11.s64 + -2196;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_5"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_5);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x82431910
	phBoundOTGrid_1910_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f13.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// lfs f13,-25528(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25528);  /* glob:lbl_82079C48 @ 0x82079c48 */
	ctx.f13.f64 = double(temp.f32);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fsubs f0,f31,f10
	ctx.f0.f64 = double(float(var_f31 - ctx.f10.f64));
	// fcmpu cr6,f0,f13
	// ble cr6,0x822455a0
	if (ctx.f0.f64 > ctx.f13.f64) {
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
	loc_82245588:
		// lhz r10,12(r31)
		ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 12);
		// cmpwi cr6,r11,0
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// bge cr6,0x82245618
		if (ctx.r11.s32 >= 0) goto loc_82245618;
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x82245624
		goto loc_82245624;
	}
loc_822455A0:
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// lfs f13,-11928(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -11928);  /* glob:lbl_8202D168 @ 0x8202d168 */
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x82245588
	if (ctx.f0.f64 <= ctx.f13.f64) goto loc_82245588;
	// lhz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U16(var_r31 + 12);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// addi r8,r9,-1
	ctx.r8.s64 = ctx.r9.s64 + -1;
	// cmpwi cr6,r10,0
	// bge cr6,0x822455cc
	if (ctx.r10.s32 < 0) {
		// li r10,0
		ctx.r10.s64 = 0;
		// b 0x822455d8
	} else {
	loc_822455CC:
		// cmpw cr6,r10,r8
		// ble cr6,0x822455d8
		if (ctx.r10.s32 <= ctx.r8.s32) goto loc_822455D8;
		// mr r10,r8
		ctx.r10.u64 = ctx.r8.u64;
	}
loc_822455D8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r11,0
	// bge cr6,0x822455f0
	if (ctx.r11.s32 < 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x822455fc
	} else {
	loc_822455F0:
		// cmpw cr6,r11,r8
		// ble cr6,0x822455fc
		if (ctx.r11.s32 <= ctx.r8.s32) goto loc_822455FC;
		// mr r11,r8
		ctx.r11.u64 = ctx.r8.u64;
	}
loc_822455FC:
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f9,r10,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f13,r8,r9
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
	// fmadds f7,f8,f0,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f7,0(r30)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// b 0x82245634
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
loc_82245618:
	// cmpw cr6,r11,r10
	// ble cr6,0x82245624
	if (ctx.r11.s32 > ctx.r10.s32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82245624:
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f6,r6,r7
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,0(r30)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
loc_82245634:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_8"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_8);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_27
	// addi r31,r3,8
	var_r31 = (uint32_t)(ctx.r3.s64 + 8);
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// li r29,0
	var_r29 = 0;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r29,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r29);
	// sth r29,4(r31)
	PPC_STORE_U16(var_r31 + 4, (uint16_t)var_r29);
	// sth r29,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r29);
	// bl 0x8240a250
	xe_A250(ctx, base);
	// addi r4,r27,1
	ctx.r4.s64 = (int64_t)(int32_t)var_r27 + 1;
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// cmpwi cr6,r28,4
	// blt cr6,0x82245714
	if ((int32_t)var_r28 >= 4) {
		// addi r11,r28,-4
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 + -4;
		// rlwinm r10,r4,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r9,r11,30,2,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
		// mr r11,r29
		ctx.r11.u64 = var_r29;
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// rlwinm r5,r9,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_822456B8:
		// lwz r6,0(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lfs f0,0(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 0);
		ctx.f0.f64 = double(temp.f32);
		// add r8,r10,r30
		ctx.r8.u64 = ctx.r10.u64 + var_r30;
		// addi r7,r11,12
		ctx.r7.s64 = ctx.r11.s64 + 12;
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// stfsx f0,r11,r6
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r11.u32 + ctx.r6.u32, temp.u32);
		// lwz r6,0(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lfs f13,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// add r8,r10,r8
		ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
		// add r3,r11,r6
		ctx.r3.u64 = ctx.r11.u64 + ctx.r6.u64;
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// cmplwi cr6,r9,0
		ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
		// stfs f13,4(r3)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		// lwz r6,0(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lfs f12,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f12.f64 = double(temp.f32);
		// add r8,r10,r8
		ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
		// add r6,r7,r6
		ctx.r6.u64 = ctx.r7.u64 + ctx.r6.u64;
		// add r30,r10,r8
		var_r30 = (uint32_t)(ctx.r10.u64 + ctx.r8.u64);
		// stfs f12,-4(r6)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r6.u32 + -4, temp.u32);
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lfs f11,0(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// stfsx f11,r7,r3
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r7.u32 + ctx.r3.u32, temp.u32);
		// bne cr6,0x822456b8
		if (!ctx.cr6.eq) goto loc_822456B8;
	}
loc_82245714:
	// cmpw cr6,r5,r28
	// bge cr6,0x82245748
	if (ctx.r5.s32 < (int32_t)var_r28) {
		// rlwinm r9,r4,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r10,r5,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r11,r5,r28
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 - ctx.r5.s64;
	loc_82245728:
		// lwz r8,0(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// lfs f10,0(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 0);
		ctx.f10.f64 = double(temp.f32);
		// add r30,r9,r30
		var_r30 = (uint32_t)(ctx.r9.u64 + var_r30);
		// cmplwi cr6,r11,0
		// stfsx f10,r10,r8
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, temp.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bne cr6,0x82245728
		if (ctx.r11.u32 != 0) goto loc_82245728;
	}
loc_82245748:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lhz r11,12(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82245798
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82245798:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x822457b0
	if (ctx.r7.u32 != 0) {
		// addi r3,r29,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 8;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x8240a250
		xe_A250(ctx, base);
	}
loc_822457B0:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// cmplwi cr6,r4,0
	// beq cr6,0x82245818
	if (ctx.r4.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_822457C0:
		// lhz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 0);
		// rlwinm r11,r30,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
		// lwz r10,8(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 8);
		// clrlwi r9,r3,31
		ctx.r9.u64 = ctx.r3.u32 & 0x1;
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// cmplwi cr6,r9,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x822457e4
		if (ctx.r9.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_822457E4:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
		// li r5,1
		ctx.r5.s64 = 1;
		// cmplwi cr6,r7,0
		// beq cr6,0x82245800
		if (ctx.r7.u32 != 0) {
			// bl 0x822e3cd8
			grc_3CD8(ctx, base);
			// b 0x82245804
		} else {
		loc_82245800:
			// bl 0x822e3dc0
			util_3DC0(ctx, base);
		}
	loc_82245804:
		// addi r6,r30,1
		ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 1;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r30,r6,16
		var_r30 = (uint32_t)(ctx.r6.u32 & 0xFFFF);
		// cmplw cr6,r30,r4
		// blt cr6,0x822457c0
		if (var_r30 < ctx.r4.u32) goto loc_822457C0;
	}
loc_82245818:
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-2124
	ctx.r11.s64 = ctx.r11.s64 + -2124;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_3"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_3);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_3) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x82431910
	phBoundOTGrid_1910_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f13.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// lfs f13,-25528(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25528);  /* glob:lbl_82079C48 @ 0x82079c48 */
	ctx.f13.f64 = double(temp.f32);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fsubs f0,f31,f10
	ctx.f0.f64 = double(float(var_f31 - ctx.f10.f64));
	// fcmpu cr6,f0,f13
	// ble cr6,0x822458f8
	if (ctx.f0.f64 > ctx.f13.f64) {
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
	loc_822458E0:
		// lhz r10,12(r31)
		ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 12);
		// cmpwi cr6,r11,0
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// bge cr6,0x822459a4
		if (ctx.r11.s32 >= 0) goto loc_822459A4;
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x822459b0
		goto loc_822459B0;
	}
loc_822458F8:
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// lfs f13,-11928(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -11928);  /* glob:lbl_8202D168 @ 0x8202d168 */
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x822458e0
	if (ctx.f0.f64 <= ctx.f13.f64) goto loc_822458E0;
	// lhz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 12);
	// cmpwi cr6,r11,0
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// bge cr6,0x82245920
	if (ctx.r11.s32 < 0) {
		// li r10,0
		ctx.r10.s64 = 0;
		// b 0x82245930
	} else {
	loc_82245920:
		// cmpw cr6,r11,r8
		// mr r10,r8
		ctx.r10.u64 = ctx.r8.u64;
		// bgt cr6,0x82245930
		if (ctx.r11.s32 > ctx.r8.s32) goto loc_82245930;
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	}
loc_82245930:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x82245950
	if (ctx.r11.s32 < 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x8224595c
	} else {
	loc_82245950:
		// cmpw cr6,r11,r8
		// ble cr6,0x8224595c
		if (ctx.r11.s32 <= ctx.r8.s32) goto loc_8224595C;
		// mr r11,r8
		ctx.r11.u64 = ctx.r8.u64;
	}
loc_8224595C:
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
	// fmadds f7,f8,f0,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f7,0(r30)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f6,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmadds f4,f5,f0,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f4,4(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	// lfs f13,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f3,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
	// fmadds f1,f2,f0,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f1,8(r30)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// b 0x822459c0
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
loc_822459A4:
	// cmpw cr6,r11,r10
	// ble cr6,0x822459b0
	if (ctx.r11.s32 > ctx.r10.s32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_822459B0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
	// rlwinm r8,r11,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lvx128 v0,r8,r9
	ea = (ctx.r8.u32 + ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_822459C0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_9"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_9);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t ea{};
	// FRAME: size=128, savegprlr_28
	// addi r31,r3,8
	var_r31 = (uint32_t)(ctx.r3.s64 + 8);
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// li r28,0
	var_r28 = 0;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r28,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r28);
	// sth r28,4(r31)
	PPC_STORE_U16(var_r31 + 4, (uint16_t)var_r28);
	// sth r28,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r28);
	// bl 0x82247c20
	xe_7C20(ctx, base);
	// cmpwi cr6,r29,0
	// ble cr6,0x82245a4c
	if ((int32_t)var_r29 > 0) {
		// mr r10,r28
		ctx.r10.u64 = var_r28;
		// mr r11,r29
		ctx.r11.u64 = var_r29;
	loc_82245A2C:
		// lwz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r30,16
		var_r30 = (uint32_t)(var_r30 + 16);
		// cmplwi cr6,r11,0
		// stvx128 v0,r10,r9
		ea = (ctx.r10.u32 + ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r10,16
		ctx.r10.s64 = ctx.r10.s64 + 16;
		// bne cr6,0x82245a2c
		if (ctx.r11.u32 != 0) goto loc_82245A2C;
	}
loc_82245A4C:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lhz r11,12(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82245a98
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82245A98:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82245ab0
	if (ctx.r7.u32 != 0) {
		// addi r3,r29,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 8;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x82247c20
		xe_7C20(ctx, base);
	}
loc_82245AB0:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// cmplwi cr6,r4,0
	// beq cr6,0x82245ae8
	if (ctx.r4.u32 != 0) {
		// li r31,0
		var_r31 = 0;
	loc_82245AC0:
		// lwz r10,8(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 8);
		// rlwinm r11,r31,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// bl 0x82244db8
		grc_4DB8(ctx, base);
		// addi r3,r31,1
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 1;
		// lhz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r31,r3,16
		var_r31 = (uint32_t)(ctx.r3.u32 & 0xFFFF);
		// cmplw cr6,r31,r10
		// blt cr6,0x82245ac0
		if (var_r31 < ctx.r10.u32) goto loc_82245AC0;
	}
loc_82245AE8:
	return;
}

__attribute__((alias("__imp__crAnimChannelRawQuaternion_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelRawQuaternion_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelRawQuaternion_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-2052
	ctx.r11.s64 = ctx.r11.s64 + -2052;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawQuaternion_vfn_4"))) PPC_WEAK_FUNC(crAnimChannelRawQuaternion_vfn_4);
PPC_FUNC_IMPL(__imp__crAnimChannelRawQuaternion_vfn_4) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x82431910
	phBoundOTGrid_1910_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f13.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// lfs f13,-25528(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25528);  /* glob:lbl_82079C48 @ 0x82079c48 */
	ctx.f13.f64 = double(temp.f32);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fsubs f0,f31,f10
	ctx.f0.f64 = double(float(var_f31 - ctx.f10.f64));
	// fcmpu cr6,f0,f13
	// ble cr6,0x82245bc8
	if (ctx.f0.f64 > ctx.f13.f64) {
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
	loc_82245BB0:
		// lhz r10,12(r31)
		ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 12);
		// cmpwi cr6,r11,0
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// bge cr6,0x82245c90
		if (ctx.r11.s32 >= 0) goto loc_82245C90;
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x82245c9c
		goto loc_82245C9C;
	}
loc_82245BC8:
	// lis r10,-32253
	ctx.r10.s64 = -2113732608;
	// lfs f13,-11928(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -11928);  /* glob:lbl_8202D168 @ 0x8202d168 */
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x82245bb0
	if (ctx.f0.f64 <= ctx.f13.f64) goto loc_82245BB0;
	// lhz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 12);
	// cmpwi cr6,r11,0
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// bge cr6,0x82245bf0
	if (ctx.r11.s32 < 0) {
		// li r10,0
		ctx.r10.s64 = 0;
		// b 0x82245c00
	} else {
	loc_82245BF0:
		// cmpw cr6,r11,r8
		// mr r10,r8
		ctx.r10.u64 = ctx.r8.u64;
		// bgt cr6,0x82245c00
		if (ctx.r11.s32 > ctx.r8.s32) goto loc_82245C00;
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	}
loc_82245C00:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,0
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// bge cr6,0x82245c20
	if (ctx.r11.s32 < 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// b 0x82245c2c
	} else {
	loc_82245C20:
		// cmpw cr6,r11,r8
		// ble cr6,0x82245c2c
		if (ctx.r11.s32 <= ctx.r8.s32) goto loc_82245C2C;
		// mr r11,r8
		ctx.r11.u64 = ctx.r8.u64;
	}
loc_82245C2C:
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lfs f13,12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lfs f9,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
	// fmadds f7,f8,f0,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f7,12(r30)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f6,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmadds f4,f5,f0,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f4,0(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f3,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
	// fmadds f1,f2,f0,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f1,4(r30)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	// lfs f13,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// fmadds f10,f11,f0,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f10,8(r30)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// bl 0x820c42d8
	util_42D8(ctx, base);
	// b 0x82245cb8
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
loc_82245C90:
	// cmpw cr6,r11,r10
	// ble cr6,0x82245c9c
	if (ctx.r11.s32 > ctx.r10.s32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82245C9C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// std r9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.r9.u64);
	// ld r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// std r8,8(r30)
	PPC_STORE_U64(var_r30 + 8, ctx.r8.u64);
loc_82245CB8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawQuaternion_vfn_10"))) PPC_WEAK_FUNC(crAnimChannelRawQuaternion_vfn_10);
PPC_FUNC_IMPL(__imp__crAnimChannelRawQuaternion_vfn_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_28
	// addi r31,r3,8
	var_r31 = (uint32_t)(ctx.r3.s64 + 8);
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// li r30,0
	var_r30 = 0;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r30,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r30);
	// sth r30,4(r31)
	PPC_STORE_U16(var_r31 + 4, (uint16_t)var_r30);
	// sth r30,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r30);
	// bl 0x82247c20
	xe_7C20(ctx, base);
	// lis r11,-32253
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// cmpwi cr6,r28,4
	// lfs f10,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f10.f64 = double(temp.f32);
	// blt cr6,0x82245f6c
	if ((int32_t)var_r28 >= 4) {
		// addi r4,r28,-3
		ctx.r4.s64 = (int64_t)(int32_t)var_r28 + -3;
		// mr r8,r30
		ctx.r8.u64 = var_r30;
		// addi r5,r29,32
		ctx.r5.s64 = (int64_t)(int32_t)var_r29 + 32;
	loc_82245D34:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmpwi cr6,r10,0
		// ld r9,0(r29)
		ctx.r9.u64 = PPC_LOAD_U64(var_r29 + 0);
		// add r11,r8,r11
		ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
		// std r9,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
		// ld r7,8(r29)
		ctx.r7.u64 = PPC_LOAD_U64(var_r29 + 8);
		// std r7,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r7.u64);
		// ble cr6,0x82245db4
		if (ctx.r10.s32 > 0) {
			// lwz r11,0(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
			// lfs f9,-8(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
			ctx.f9.f64 = double(temp.f32);
			// lfs f13,8(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// fmuls f5,f9,f13
			ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// lfs f8,-12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12);
			ctx.f8.f64 = double(temp.f32);
			// lfs f12,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// lfs f7,-4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
			ctx.f7.f64 = double(temp.f32);
			// lfs f11,12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			ctx.f11.f64 = double(temp.f32);
			// lfs f0,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f6,-16(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16);
			ctx.f6.f64 = double(temp.f32);
			// fmadds f4,f8,f12,f5
			ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f5.f64));
			// fmadds f3,f7,f11,f4
			ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f4.f64));
			// fmadds f2,f0,f6,f3
			ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f6.f64 + ctx.f3.f64));
			// fcmpu cr6,f2,f10
			// bge cr6,0x82245db4
			if (ctx.f2.f64 >= ctx.f10.f64) goto loc_82245DB4;
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f1,0(r11)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
			// fneg f0,f12
			ctx.f0.u64 = ctx.f12.u64 ^ 0x8000000000000000;
			// stfs f0,4(r11)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
			// fneg f13,f13
			ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
			// stfs f13,8(r11)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
			// fneg f12,f11
			ctx.f12.u64 = ctx.f11.u64 ^ 0x8000000000000000;
			// stfs f12,12(r11)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
		}
	loc_82245DB4:
		// lwz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r11,r5,-16
		ctx.r11.s64 = ctx.r5.s64 + -16;
		// addic. r6,r10,1
		ctx.xer.ca = ctx.r10.u32 > 4294967294;
		ctx.r6.s64 = ctx.r10.s64 + 1;
		// add r9,r8,r9
		ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// ld r3,0(r11)
		ctx.r3.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
		// std r3,0(r9)
		PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r3.u64);
		// ld r11,8(r11)
		ctx.r11.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
		// std r11,8(r9)
		PPC_STORE_U64(ctx.r9.u32 + 8, ctx.r11.u64);
		// ble 0x82245e40
		if (ctx.r6.s32 > 0) {
			// lwz r11,0(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
			// addi r9,r11,16
			ctx.r9.s64 = ctx.r11.s64 + 16;
			// lfs f9,8(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			ctx.f9.f64 = double(temp.f32);
			// lfs f8,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f8.f64 = double(temp.f32);
			// lfs f13,8(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// fmuls f5,f9,f13
			ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// lfs f12,4(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// lfs f7,12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			ctx.f7.f64 = double(temp.f32);
			// lfs f11,12(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
			ctx.f11.f64 = double(temp.f32);
			// lfs f0,0(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f6,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f6.f64 = double(temp.f32);
			// fmadds f4,f8,f12,f5
			ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f5.f64));
			// fmadds f3,f7,f11,f4
			ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f4.f64));
			// fmadds f2,f0,f6,f3
			ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f6.f64 + ctx.f3.f64));
			// fcmpu cr6,f2,f10
			// bge cr6,0x82245e40
			if (ctx.f2.f64 >= ctx.f10.f64) goto loc_82245E40;
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f1,0(r9)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
			// fneg f0,f12
			ctx.f0.u64 = ctx.f12.u64 ^ 0x8000000000000000;
			// stfs f0,4(r9)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
			// fneg f13,f13
			ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
			// stfs f13,8(r9)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
			// fneg f12,f11
			ctx.f12.u64 = ctx.f11.u64 ^ 0x8000000000000000;
			// stfs f12,12(r9)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
		}
	loc_82245E40:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r9,r8,32
		ctx.r9.s64 = ctx.r8.s64 + 32;
		// ld r7,0(r5)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r5.u32 + 0);
		// cmpwi cr6,r10,-2
		// add r11,r9,r11
		ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
		// std r7,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r7.u64);
		// ld r6,8(r5)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r5.u32 + 8);
		// std r6,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r6.u64);
		// ble cr6,0x82245ec8
		if (ctx.r10.s32 > -2) {
			// lwz r7,0(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0);
			// add r11,r7,r9
			ctx.r11.u64 = ctx.r7.u64 + ctx.r9.u64;
			// add r7,r8,r7
			ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
			// lfs f13,8(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// lfs f9,24(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 24);
			ctx.f9.f64 = double(temp.f32);
			// fmuls f5,f9,f13
			ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// lfs f8,20(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 20);
			ctx.f8.f64 = double(temp.f32);
			// lfs f12,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// lfs f7,28(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 28);
			ctx.f7.f64 = double(temp.f32);
			// lfs f11,12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			ctx.f11.f64 = double(temp.f32);
			// lfs f0,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f6,16(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 16);
			ctx.f6.f64 = double(temp.f32);
			// fmadds f4,f8,f12,f5
			ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f5.f64));
			// fmadds f3,f7,f11,f4
			ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f4.f64));
			// fmadds f2,f0,f6,f3
			ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f6.f64 + ctx.f3.f64));
			// fcmpu cr6,f2,f10
			// bge cr6,0x82245ec8
			if (ctx.f2.f64 >= ctx.f10.f64) goto loc_82245EC8;
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f1,0(r11)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
			// fneg f0,f12
			ctx.f0.u64 = ctx.f12.u64 ^ 0x8000000000000000;
			// stfs f0,4(r11)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
			// fneg f13,f13
			ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
			// stfs f13,8(r11)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
			// fneg f12,f11
			ctx.f12.u64 = ctx.f11.u64 ^ 0x8000000000000000;
			// stfs f12,12(r11)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
		}
	loc_82245EC8:
		// addi r11,r5,16
		ctx.r11.s64 = ctx.r5.s64 + 16;
		// lwz r7,0(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r6,r9,16
		ctx.r6.s64 = ctx.r9.s64 + 16;
		// addic. r3,r10,3
		ctx.xer.ca = ctx.r10.u32 > 4294967292;
		ctx.r3.s64 = ctx.r10.s64 + 3;
		// add r7,r6,r7
		ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
		// ld r3,0(r11)
		ctx.r3.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
		// std r3,0(r7)
		PPC_STORE_U64(ctx.r7.u32 + 0, ctx.r3.u64);
		// ld r11,8(r11)
		ctx.r11.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
		// std r11,8(r7)
		PPC_STORE_U64(ctx.r7.u32 + 8, ctx.r11.u64);
		// ble 0x82245f54
		if (ctx.r3.s32 > 0) {
			// lwz r7,0(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0);
			// add r11,r7,r6
			ctx.r11.u64 = ctx.r7.u64 + ctx.r6.u64;
			// add r9,r7,r9
			ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
			// lfs f13,8(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// lfs f9,8(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
			ctx.f9.f64 = double(temp.f32);
			// fmuls f5,f9,f13
			ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// lfs f8,4(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
			ctx.f8.f64 = double(temp.f32);
			// lfs f12,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// lfs f7,12(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
			ctx.f7.f64 = double(temp.f32);
			// lfs f11,12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			ctx.f11.f64 = double(temp.f32);
			// lfs f0,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f6,0(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
			ctx.f6.f64 = double(temp.f32);
			// fmadds f4,f8,f12,f5
			ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f5.f64));
			// fmadds f3,f7,f11,f4
			ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f4.f64));
			// fmadds f2,f0,f6,f3
			ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f6.f64 + ctx.f3.f64));
			// fcmpu cr6,f2,f10
			// bge cr6,0x82245f54
			if (ctx.f2.f64 >= ctx.f10.f64) goto loc_82245F54;
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f1,0(r11)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
			// fneg f0,f12
			ctx.f0.u64 = ctx.f12.u64 ^ 0x8000000000000000;
			// stfs f0,4(r11)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
			// fneg f13,f13
			ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
			// stfs f13,8(r11)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
			// fneg f12,f11
			ctx.f12.u64 = ctx.f11.u64 ^ 0x8000000000000000;
			// stfs f12,12(r11)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
		}
	loc_82245F54:
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// addi r29,r29,64
		var_r29 = (uint32_t)(var_r29 + 64);
		// addi r5,r5,64
		ctx.r5.s64 = ctx.r5.s64 + 64;
		// addi r8,r8,64
		ctx.r8.s64 = ctx.r8.s64 + 64;
		// cmpw cr6,r10,r4
		// blt cr6,0x82245d34
		if (ctx.r10.s32 < ctx.r4.s32) goto loc_82245D34;
	}
loc_82245F6C:
	// cmpw cr6,r10,r28
	// bge cr6,0x8224600c
	if (ctx.r10.s32 < (int32_t)var_r28) {
		// rlwinm r9,r10,4,0,27
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	loc_82245F78:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmpwi cr6,r10,0
		// ld r8,0(r29)
		ctx.r8.u64 = PPC_LOAD_U64(var_r29 + 0);
		// add r11,r9,r11
		ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
		// std r8,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r8.u64);
		// ld r7,8(r29)
		ctx.r7.u64 = PPC_LOAD_U64(var_r29 + 8);
		// std r7,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r7.u64);
		// ble cr6,0x82245ff8
		if (ctx.r10.s32 > 0) {
			// lwz r11,0(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
			// add r11,r9,r11
			ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
			// lfs f9,-8(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
			ctx.f9.f64 = double(temp.f32);
			// lfs f13,8(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// fmuls f5,f9,f13
			ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// lfs f8,-12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12);
			ctx.f8.f64 = double(temp.f32);
			// lfs f12,4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// lfs f7,-4(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
			ctx.f7.f64 = double(temp.f32);
			// lfs f11,12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			ctx.f11.f64 = double(temp.f32);
			// lfs f0,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// lfs f6,-16(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16);
			ctx.f6.f64 = double(temp.f32);
			// fmadds f4,f8,f12,f5
			ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f5.f64));
			// fmadds f3,f7,f11,f4
			ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f4.f64));
			// fmadds f2,f0,f6,f3
			ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f6.f64 + ctx.f3.f64));
			// fcmpu cr6,f2,f10
			// bge cr6,0x82245ff8
			if (ctx.f2.f64 >= ctx.f10.f64) goto loc_82245FF8;
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f1,0(r11)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
			// fneg f0,f12
			ctx.f0.u64 = ctx.f12.u64 ^ 0x8000000000000000;
			// stfs f0,4(r11)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
			// fneg f13,f13
			ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
			// stfs f13,8(r11)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
			// fneg f12,f11
			ctx.f12.u64 = ctx.f11.u64 ^ 0x8000000000000000;
			// stfs f12,12(r11)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
		}
	loc_82245FF8:
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// addi r29,r29,16
		var_r29 = (uint32_t)(var_r29 + 16);
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// cmpw cr6,r10,r28
		// blt cr6,0x82245f78
		if (ctx.r10.s32 < (int32_t)var_r28) goto loc_82245F78;
	}
loc_8224600C:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawVector3_vfn_13"))) PPC_WEAK_FUNC(crAnimChannelRawVector3_vfn_13);
PPC_FUNC_IMPL(__imp__crAnimChannelRawVector3_vfn_13) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r3,r11,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawQuaternion_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelRawQuaternion_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelRawQuaternion_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lhz r11,12(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82246068
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82246068:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82246080
	if (ctx.r7.u32 != 0) {
		// addi r3,r29,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 8;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x82247c20
		xe_7C20(ctx, base);
	}
loc_82246080:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// cmplwi cr6,r4,0
	// beq cr6,0x822460b8
	if (ctx.r4.u32 != 0) {
		// li r31,0
		var_r31 = 0;
	loc_82246090:
		// lwz r10,8(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 8);
		// rlwinm r11,r31,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// bl 0x82244ea0
		grc_4EA0(ctx, base);
		// addi r3,r31,1
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 1;
		// lhz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r31,r3,16
		var_r31 = (uint32_t)(ctx.r3.u32 & 0xFFFF);
		// cmplw cr6,r31,r10
		// blt cr6,0x82246090
		if (var_r31 < ctx.r10.u32) goto loc_82246090;
	}
loc_822460B8:
	return;
}

__attribute__((alias("__imp__crAnimChannelRawInt_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelRawInt_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelRawInt_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-1980
	ctx.r11.s64 = ctx.r11.s64 + -1980;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawInt_vfn_6"))) PPC_WEAK_FUNC(crAnimChannelRawInt_vfn_6);
PPC_FUNC_IMPL(__imp__crAnimChannelRawInt_vfn_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32253
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f0,-12020(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12020);
	ctx.f0.f64 = double(temp.f32);
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// fadds f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r10,0
	// bge cr6,0x8224615c
	if (ctx.r10.s32 < 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// lwz r9,8(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// rlwinm r8,r11,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r7,r8,r9
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
		// stw r7,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r7.u32);
		// blr
		return;
	}
loc_8224615C:
	// cmpw cr6,r10,r11
	// bgt cr6,0x82246168
	if (ctx.r10.s32 <= ctx.r11.s32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82246168:
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r8,r9
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// stw r7,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawInt_vfn_11"))) PPC_WEAK_FUNC(crAnimChannelRawInt_vfn_11);
PPC_FUNC_IMPL(__imp__crAnimChannelRawInt_vfn_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_28
	// addi r31,r3,8
	var_r31 = (uint32_t)(ctx.r3.s64 + 8);
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// li r28,0
	var_r28 = 0;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r28,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r28);
	// sth r28,4(r31)
	PPC_STORE_U16(var_r31 + 4, (uint16_t)var_r28);
	// sth r28,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r28);
	// bl 0x8240a250
	xe_A250(ctx, base);
	// cmpwi cr6,r29,0
	// ble cr6,0x822461ec
	if ((int32_t)var_r29 > 0) {
		// mr r10,r28
		ctx.r10.u64 = var_r28;
		// mr r11,r29
		ctx.r11.u64 = var_r29;
	loc_822461CC:
		// lwz r9,0(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// lwz r8,0(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r11,0
		// stwx r9,r10,r8
		PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bne cr6,0x822461cc
		if (ctx.r11.u32 != 0) goto loc_822461CC;
	}
loc_822461EC:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawFloat_vfn_13"))) PPC_WEAK_FUNC(crAnimChannelRawFloat_vfn_13);
PPC_FUNC_IMPL(__imp__crAnimChannelRawFloat_vfn_13) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-1908
	ctx.r11.s64 = ctx.r11.s64 + -1908;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_7"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_7);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_7) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f0,-12020(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12020);  /* glob:lbl_8202D10C @ 0x8202d10c */
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// fadds f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f13.u32);
	// lwz r9,-16(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r9,0
	// bge cr6,0x82246288
	if (ctx.r9.s32 < 0) {
		// li r9,0
		ctx.r9.s64 = 0;
	}
loc_82246288:
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// rlwinm r10,r9,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFF;
	// cmpw cr6,r10,r11
	// bge cr6,0x8224629c
	if (ctx.r10.s32 < ctx.r11.s32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8224629C:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbzx r8,r10,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r11.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// and r7,r8,r9
	ctx.r7.u64 = ctx.r8.u64 & ctx.r9.u64;
	// clrlwi r6,r7,29
	ctx.r6.u64 = ctx.r7.u32 & 0x7;
	// cmplwi cr6,r6,0
	// bne cr6,0x822462bc
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822462BC:
	// stb r11,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_12"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_12);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	// FRAME: size=144, savegprlr_26
	// addi r31,r3,8
	var_r31 = (uint32_t)(ctx.r3.s64 + 8);
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// li r30,0
	var_r30 = 0;
	// clrlwi r11,r29,29
	ctx.r11.u64 = var_r29 & 0x7;
	// li r28,1
	var_r28 = 1;
	// cmplwi cr6,r11,0
	// mr r11,r28
	ctx.r11.u64 = var_r28;
	// stw r30,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r30);
	// sth r30,4(r31)
	PPC_STORE_U16(var_r31 + 4, (uint16_t)var_r30);
	// sth r30,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r30);
	// bne cr6,0x82246310
	if (ctx.r11.u32 == 0) {
		// mr r11,r30
		ctx.r11.u64 = var_r30;
	}
loc_82246310:
	// rlwinm r10,r29,29,3,31
	ctx.r10.u64 = __builtin_rotateleft64(var_r29 | (var_r29 << 32), 29) & 0x1FFFFFFF;
	// add r26,r10,r11
	var_r26 = (uint32_t)(ctx.r10.u64 + ctx.r11.u64);
	// cmpwi cr6,r26,0
	// ble cr6,0x82246490
	if ((int32_t)var_r26 > 0) {
		// mr r4,r26
		ctx.r4.u64 = var_r26;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82247c98
		xe_7C98(ctx, base);
		// mr r8,r30
		ctx.r8.u64 = var_r30;
		// mr r11,r30
		ctx.r11.u64 = var_r30;
		// cmpwi cr6,r26,0
		// ble cr6,0x82246490
		if ((int32_t)var_r26 <= 0) {
			// li r3,1
			ctx.r3.s64 = 1;
			return;
		}
		// addi r7,r29,-1
		ctx.r7.s64 = (int64_t)(int32_t)var_r29 + -1;
	loc_82246340:
		// li r9,2
		ctx.r9.s64 = 2;
	loc_82246344:
		// lbzx r10,r8,r27
		ctx.r10.u64 = PPC_LOAD_U8(ctx.r8.u32 + var_r27);
		// cmplwi cr6,r10,0
		// lwz r10,0(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
		// beq cr6,0x8224636c
		if (ctx.r10.u32 != 0) {
			// addi r6,r9,-2
			ctx.r6.s64 = ctx.r9.s64 + -2;
			// lbzx r4,r11,r10
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
			// slw r5,r28,r6
			ctx.r5.u64 = ctx.r6.u8 & 0x20 ? 0 : (var_r28 << (ctx.r6.u8 & 0x3F));
			// or r3,r5,r4
			ctx.r3.u64 = ctx.r5.u64 | ctx.r4.u64;
			// stbx r3,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r3.u8);
			// b 0x82246380
		} else {
		loc_8224636C:
			// addi r5,r9,-2
			ctx.r5.s64 = ctx.r9.s64 + -2;
			// lbzx r3,r11,r10
			ctx.r3.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
			// slw r4,r28,r5
			ctx.r4.u64 = ctx.r5.u8 & 0x20 ? 0 : (var_r28 << (ctx.r5.u8 & 0x3F));
			// andc r6,r3,r4
			ctx.r6.u64 = ctx.r3.u64 & ~ctx.r4.u64;
			// stbx r6,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r6.u8);
		}
	loc_82246380:
		// addi r10,r8,1
		ctx.r10.s64 = ctx.r8.s64 + 1;
		// cmpw cr6,r10,r7
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
		// blt cr6,0x82246394
		if (ctx.r10.s32 >= ctx.r7.s32) {
			// mr r8,r7
			ctx.r8.u64 = ctx.r7.u64;
		}
	loc_82246394:
		// lbzx r5,r8,r27
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + var_r27);
		// lwz r10,0(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmplwi cr6,r5,0
		// beq cr6,0x822463bc
		if (ctx.r5.u32 != 0) {
			// addi r4,r9,-1
			ctx.r4.s64 = ctx.r9.s64 + -1;
			// lbzx r6,r11,r10
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
			// slw r3,r28,r4
			ctx.r3.u64 = ctx.r4.u8 & 0x20 ? 0 : (var_r28 << (ctx.r4.u8 & 0x3F));
			// or r5,r3,r6
			ctx.r5.u64 = ctx.r3.u64 | ctx.r6.u64;
			// stbx r5,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r5.u8);
			// b 0x822463d0
		} else {
		loc_822463BC:
			// addi r3,r9,-1
			ctx.r3.s64 = ctx.r9.s64 + -1;
			// lbzx r5,r11,r10
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
			// slw r6,r28,r3
			ctx.r6.u64 = ctx.r3.u8 & 0x20 ? 0 : (var_r28 << (ctx.r3.u8 & 0x3F));
			// andc r4,r5,r6
			ctx.r4.u64 = ctx.r5.u64 & ~ctx.r6.u64;
			// stbx r4,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r4.u8);
		}
	loc_822463D0:
		// addi r10,r8,1
		ctx.r10.s64 = ctx.r8.s64 + 1;
		// cmpw cr6,r10,r7
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
		// blt cr6,0x822463e4
		if (ctx.r10.s32 >= ctx.r7.s32) {
			// mr r8,r7
			ctx.r8.u64 = ctx.r7.u64;
		}
	loc_822463E4:
		// lwz r10,0(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
		// slw r6,r28,r9
		ctx.r6.u64 = ctx.r9.u8 & 0x20 ? 0 : (var_r28 << (ctx.r9.u8 & 0x3F));
		// lbzx r3,r8,r27
		ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + var_r27);
		// cmplwi cr6,r3,0
		// lbzx r5,r11,r10
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
		// beq cr6,0x82246408
		if (ctx.r3.u32 != 0) {
			// or r4,r6,r5
			ctx.r4.u64 = ctx.r6.u64 | ctx.r5.u64;
			// stbx r4,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r4.u8);
			// b 0x82246410
		} else {
		loc_82246408:
			// andc r4,r5,r6
			ctx.r4.u64 = ctx.r5.u64 & ~ctx.r6.u64;
			// stbx r4,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r4.u8);
		}
	loc_82246410:
		// addi r10,r8,1
		ctx.r10.s64 = ctx.r8.s64 + 1;
		// cmpw cr6,r10,r7
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
		// blt cr6,0x82246424
		if (ctx.r10.s32 >= ctx.r7.s32) {
			// mr r8,r7
			ctx.r8.u64 = ctx.r7.u64;
		}
	loc_82246424:
		// lbzx r3,r8,r27
		ctx.r3.u64 = PPC_LOAD_U8(ctx.r8.u32 + var_r27);
		// lwz r10,0(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmplwi cr6,r3,0
		// beq cr6,0x8224644c
		if (ctx.r3.u32 != 0) {
			// addi r6,r9,1
			ctx.r6.s64 = ctx.r9.s64 + 1;
			// lbzx r4,r11,r10
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
			// slw r5,r28,r6
			ctx.r5.u64 = ctx.r6.u8 & 0x20 ? 0 : (var_r28 << (ctx.r6.u8 & 0x3F));
			// or r3,r5,r4
			ctx.r3.u64 = ctx.r5.u64 | ctx.r4.u64;
			// stbx r3,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r3.u8);
			// b 0x82246460
		} else {
		loc_8224644C:
			// addi r5,r9,1
			ctx.r5.s64 = ctx.r9.s64 + 1;
			// lbzx r3,r11,r10
			ctx.r3.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r10.u32);
			// slw r4,r28,r5
			ctx.r4.u64 = ctx.r5.u8 & 0x20 ? 0 : (var_r28 << (ctx.r5.u8 & 0x3F));
			// andc r6,r3,r4
			ctx.r6.u64 = ctx.r3.u64 & ~ctx.r4.u64;
			// stbx r6,r11,r10
			PPC_STORE_U8(ctx.r11.u32 + ctx.r10.u32, ctx.r6.u8);
		}
	loc_82246460:
		// addi r10,r8,1
		ctx.r10.s64 = ctx.r8.s64 + 1;
		// cmpw cr6,r10,r7
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
		// blt cr6,0x82246474
		if (ctx.r10.s32 >= ctx.r7.s32) {
			// mr r8,r7
			ctx.r8.u64 = ctx.r7.u64;
		}
	loc_82246474:
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// addi r5,r9,-2
		ctx.r5.s64 = ctx.r9.s64 + -2;
		// cmpwi cr6,r5,8
		// blt cr6,0x82246344
		if (ctx.r5.s32 < 8) goto loc_82246344;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmpw cr6,r11,r26
		// blt cr6,0x82246340
		if (ctx.r11.s32 < (int32_t)var_r26) goto loc_82246340;
	}
loc_82246490:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_13"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_13);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_13) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelRawBool_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelRawBool_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelRawBool_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lhz r11,12(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x822464f0
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822464F0:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82246508
	if (ctx.r7.u32 != 0) {
		// addi r3,r29,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 8;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x82247c98
		xe_7C98(ctx, base);
	}
loc_82246508:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// cmplwi cr6,r4,0
	// beq cr6,0x8224656c
	if (ctx.r4.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_82246518:
		// lhz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 0);
		// lwz r11,8(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 8);
		// clrlwi r10,r3,31
		ctx.r10.u64 = ctx.r3.u32 & 0x1;
		// add r4,r30,r11
		ctx.r4.u64 = var_r30 + ctx.r11.u64;
		// cmplwi cr6,r10,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x82246538
		if (ctx.r10.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82246538:
		// clrlwi r8,r11,24
		ctx.r8.u64 = ctx.r11.u32 & 0xFF;
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
		// li r5,1
		ctx.r5.s64 = 1;
		// cmplwi cr6,r8,0
		// beq cr6,0x82246554
		if (ctx.r8.u32 != 0) {
			// bl 0x822e3828
			rage_obj_bind_3828(ctx, base);
			// b 0x82246558
		} else {
		loc_82246554:
			// bl 0x822e39b0
			util_39B0(ctx, base);
		}
	loc_82246558:
		// addi r7,r30,1
		ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 1;
		// lhz r5,80(r1)
		ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r30,r7,16
		var_r30 = (uint32_t)(ctx.r7.u32 & 0xFFFF);
		// cmplw cr6,r30,r5
		// blt cr6,0x82246518
		if (var_r30 < ctx.r5.u32) goto loc_82246518;
	}
loc_8224656C:
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticFloat_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelStaticFloat_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticFloat_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// addi r11,r11,-1836
	ctx.r11.s64 = ctx.r11.s64 + -1836;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticFloat_vfn_5"))) PPC_WEAK_FUNC(crAnimChannelStaticFloat_vfn_5);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticFloat_vfn_5) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticFloat_vfn_8"))) PPC_WEAK_FUNC(crAnimChannelStaticFloat_vfn_8);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticFloat_vfn_8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 1;
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r5,1
	// add r10,r9,r4
	ctx.r10.u64 = ctx.r9.u64 + ctx.r4.u64;
	// ble cr6,0x822465ec
	if (ctx.r5.s32 > 1) {
		// fmr f13,f0
		ctx.f13.f64 = ctx.f0.f64;
		// fmuls f12,f1,f1
		ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	loc_822465C8:
		// lfs f11,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f0,f13,f11
		ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
		// fmuls f10,f0,f0
		ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
		// fcmpu cr6,f10,f12
		// bgt cr6,0x822465f4
		if (ctx.f10.f64 > ctx.f12.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// add r10,r9,r10
		ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
		// cmpw cr6,r11,r5
		// blt cr6,0x822465c8
		if (ctx.r11.s32 < ctx.r5.s32) goto loc_822465C8;
	}
loc_822465EC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticFloat_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelStaticFloat_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticFloat_vfn_14) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// addi r4,r3,8
	ctx.r4.s64 = ctx.r3.s64 + 8;
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82246620
	if (ctx.r9.u32 == 0) {
		// li r10,0
		ctx.r10.s64 = 0;
	}
loc_82246620:
	// clrlwi r7,r10,24
	ctx.r7.u64 = ctx.r10.u32 & 0xFF;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r7,0
	// beq cr6,0x82246638
	if (ctx.r7.u32 != 0) {
		// b 0x822e3cd8
		grc_3CD8(ctx, base);
		return;
	}
loc_82246638:
	// b 0x822e3dc0
	util_3DC0(ctx, base);
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticQuaternion_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelStaticQuaternion_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticQuaternion_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// addi r11,r11,-1764
	ctx.r11.s64 = ctx.r11.s64 + -1764;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticQuaternion_vfn_4"))) PPC_WEAK_FUNC(crAnimChannelStaticQuaternion_vfn_4);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticQuaternion_vfn_4) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// std r10,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r10.u64);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// std r9,8(r5)
	PPC_STORE_U64(ctx.r5.u32 + 8, ctx.r9.u64);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticQuaternion_vfn_10"))) PPC_WEAK_FUNC(crAnimChannelStaticQuaternion_vfn_10);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticQuaternion_vfn_10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addi r11,r4,16
	ctx.r11.s64 = ctx.r4.s64 + 16;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpwi cr6,r5,1
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r8,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, ctx.r8.u64);
	// ld r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r7,24(r3)
	PPC_STORE_U64(ctx.r3.u32 + 24, ctx.r7.u64);
	// ble cr6,0x82246710
	if (ctx.r5.s32 > 1) {
		// lfs f7,28(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
		ctx.f7.f64 = double(temp.f32);
		// fmuls f5,f1,f1
		ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
		// lfs f6,24(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
		ctx.f6.f64 = double(temp.f32);
		// lfs f4,20(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
		ctx.f4.f64 = double(temp.f32);
		// lfs f3,16(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
		ctx.f3.f64 = double(temp.f32);
	loc_822466A8:
		// lfs f0,12(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fsubs f0,f7,f0
		ctx.f0.f64 = double(float(ctx.f7.f64 - ctx.f0.f64));
		// lfs f12,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// fsubs f13,f6,f13
		ctx.f13.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
		// lfs f11,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f12,f4,f12
		ctx.f12.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
		// fsubs f11,f3,f11
		ctx.f11.f64 = double(float(ctx.f3.f64 - ctx.f11.f64));
		// fmuls f0,f0,f0
		ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
		// fmuls f13,f13,f13
		ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
		// fmuls f12,f12,f12
		ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
		// fmuls f11,f11,f11
		ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
		// fsubs f10,f13,f0
		ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// fsubs f9,f11,f12
		ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f12.f64));
		// fsel f8,f10,f13,f0
		ctx.f8.f64 = ctx.f10.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
		// fsel f0,f10,f13,f0
		ctx.f0.f64 = ctx.f10.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
		// fsel f13,f9,f11,f12
		ctx.f13.f64 = ctx.f9.f64 >= 0.0 ? ctx.f11.f64 : ctx.f12.f64;
		// fsel f12,f9,f11,f12
		ctx.f12.f64 = ctx.f9.f64 >= 0.0 ? ctx.f11.f64 : ctx.f12.f64;
		// fsubs f10,f12,f0
		ctx.f10.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
		// fsel f0,f10,f13,f8
		ctx.f0.f64 = ctx.f10.f64 >= 0.0 ? ctx.f13.f64 : ctx.f8.f64;
		// fcmpu cr6,f0,f5
		// bgt cr6,0x82246718
		if (ctx.f0.f64 > ctx.f5.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// cmpw cr6,r10,r5
		// blt cr6,0x822466a8
		if (ctx.r10.s32 < ctx.r5.s32) goto loc_822466A8;
	}
loc_82246710:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelStaticQuaternion_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelStaticQuaternion_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelStaticQuaternion_vfn_14) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r11,16
	ctx.r4.s64 = ctx.r11.s64 + 16;
	// b 0x82244ea0
	grc_4EA0(ctx, base);
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// b 0x82246740
	goto loc_82246740;
loc_82246740:
	// lis r11,-32251
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r11,r11,-1692
	ctx.r11.s64 = ctx.r11.s64 + -1692;
	// stb r8,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r8.u8);
	// stb r8,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r8.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224678c
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r4)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r9,76(r4)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
		// subf r7,r10,r11
		ctx.r7.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r7,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r7.u32 / ctx.r9.u32 : 0;
		// addi r6,r10,2
		ctx.r6.s64 = ctx.r10.s64 + 2;
		// rlwinm r5,r6,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r5,r4
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r11,8(r3)
		PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	}
loc_8224678C:
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// cmpwi cr6,r11,0
	// blelr cr6
	if (ctx.r11.s32 <= 0) return;
loc_8224679C:
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// add r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
	// cmplwi cr6,r11,0
	// beq cr6,0x822467e0
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// cmplwi cr6,r10,0
		// beq cr6,0x822467e0
		if (ctx.r10.u32 == 0) goto loc_822467E0;
		// lwz r9,4(r4)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r6,76(r4)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
		// subf r5,r9,r10
		ctx.r5.s64 = ctx.r10.s64 - ctx.r9.s64;
		// twllei r6,0
		if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
		// divwu r9,r5,r6
		ctx.r9.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
		// addi r9,r9,2
		ctx.r9.s64 = ctx.r9.s64 + 2;
		// rlwinm r6,r9,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r9,r6,r4
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
		// add r5,r9,r10
		ctx.r5.u64 = ctx.r9.u64 + ctx.r10.u64;
		// stw r5,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
	}
loc_822467E0:
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// cmpw cr6,r7,r11
	// blt cr6,0x8224679c
	if (ctx.r7.s32 < ctx.r11.s32) goto loc_8224679C;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_5"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_5);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// fneg f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// lis r11,-32248
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// lfd f0,-25856(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25856);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lhz r6,12(r31)
	ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 12);
	// addi r7,r6,-1
	ctx.r7.s64 = ctx.r6.s64 + -1;
	// fsel f0,f13,f0,f1
	ctx.f0.f64 = ctx.f13.f64 >= 0.0 ? ctx.f0.f64 : ctx.f1.f64;
	// cmpwi cr6,r7,0
	// fctiwz f12,f0
	ctx.f12.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f12,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f12.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r5,r11,1
	ctx.r5.s64 = ctx.r11.s64 + 1;
	// ble cr6,0x82246874
	if (ctx.r7.s32 > 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
	loc_82246854:
		// lhz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
		// cmpw cr6,r8,r5
		// bge cr6,0x822468f4
		if (ctx.r8.s32 >= ctx.r5.s32) goto loc_822468F4;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// mr r9,r8
		ctx.r9.u64 = ctx.r8.u64;
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// cmpw cr6,r10,r7
		// blt cr6,0x82246854
		if (ctx.r10.s32 < ctx.r7.s32) goto loc_82246854;
	}
loc_82246874:
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
	// rlwinm r11,r6,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lhz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + -8);
	// lbz r5,-6(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + -6);
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// lwz r4,-4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// lfd f4,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// lfd f1,80(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f1
	ctx.f13.f64 = double(ctx.f1.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// fsel f0,f12,f13,f0
	ctx.f0.f64 = ctx.f12.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
	// fsubs f1,f0,f2
	ctx.f1.f64 = double(float(ctx.f0.f64 - ctx.f2.f64));
	// bl 0x82244be8
	crAnimChannelRawFloat_4BE8_p46(ctx, base);
	// lfs f11,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f11,f10,f9
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f9.f64));
	// stfs f8,0(r30)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
loc_822468DC:
	// blr
	return;
loc_822468F4:
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f11,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fsubs f1,f0,f9
	ctx.f1.f64 = double(float(ctx.f0.f64 - ctx.f9.f64));
	// bl 0x82244be8
	crAnimChannelRawFloat_4BE8_p46(ctx, base);
	// lfs f8,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,20(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 20);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f8,f7,f6
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f6.f64));
	// stfs f5,0(r30)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// b 0x822468dc
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_8"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_8);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=208, savegprlr_21
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// fmr f30,f1
	var_f30 = ctx.f1.f64;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// cmpwi cr6,r31,3
	// bge cr6,0x82246978
	if ((int32_t)var_r31 < 3) {
	loc_82246964:
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_82246978:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r8,r11,-12024
	ctx.r8.s64 = ctx.r11.s64 + -12024;
	// lfs f0,8(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f30,f0
	// beq cr6,0x82246964
	if (var_f30 == ctx.f0.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
	// lis r11,-32248
	// addi r28,r6,1
	var_r28 = (uint32_t)(ctx.r6.s64 + 1);  // addr:0x825f0001
	// mr r10,r29
	ctx.r10.u64 = var_r29;
	// cmpwi cr6,r31,0
	// lfs f12,-25792(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25792);  /* glob:0x82029b40 */
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25896(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);  /* glob:lbl_82079AD8 @ 0x82079ad8 */
	ctx.f13.f64 = double(temp.f32);
	// ble cr6,0x822469d8
	if ((int32_t)var_r31 > 0) {
		// rlwinm r9,r28,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC;
		// mr r11,r31
		ctx.r11.u64 = var_r31;
	loc_822469B4:
		// lfs f0,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// fsubs f11,f12,f0
		ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
		// add r10,r9,r10
		ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
		// fsubs f10,f13,f0
		ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// cmplwi cr6,r11,0
		// fsel f12,f11,f12,f0
		ctx.f12.f64 = ctx.f11.f64 >= 0.0 ? ctx.f12.f64 : ctx.f0.f64;
		// fsel f13,f10,f0,f13
		ctx.f13.f64 = ctx.f10.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
		// bne cr6,0x822469b4
		if (ctx.r11.u32 != 0) goto loc_822469B4;
	}
loc_822469D8:
	// fsubs f0,f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// lis r11,16383
	ctx.r11.s64 = 1073676288;
	// stfs f13,20(r27)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r27 + 20, temp.u32);
	// li r25,-1
	var_r25 = (uint32_t)(-1);
	// ori r10,r11,65535
	ctx.r10.u64 = ctx.r11.u64 | 65535;
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,16(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r27 + 16, temp.u32);
	// rlwinm r30,r31,2,0,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC);
	// cmplw cr6,r31,r10
	// fdivs f31,f13,f0
	var_f31 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// ble cr6,0x82246a08
	if (var_r31 > ctx.r10.u32) {
		// mr r30,r25
		var_r30 = (uint32_t)(var_r25);
	}
loc_82246A08:
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r22,0(r13)
	var_r22 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
	// li r23,4
	var_r23 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lwzx r3,r23,r22
	ctx.r3.u64 = PPC_LOAD_U32(var_r23 + var_r22);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r21,r3
	var_r21 = ctx.r3.u32;
	// mr r9,r29
	ctx.r9.u64 = var_r29;
	// cmpwi cr6,r31,0
	// ble cr6,0x82246a74
	if ((int32_t)var_r31 > 0) {
		// rlwinm r8,r28,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC;
		// mr r10,r21
		ctx.r10.u64 = var_r21;
		// mr r11,r31
		ctx.r11.u64 = var_r31;
	loc_82246A4C:
		// lfs f9,0(r9)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// lfs f8,20(r27)
		temp.u32 = PPC_LOAD_U32(var_r27 + 20);
		ctx.f8.f64 = double(temp.f32);
		// add r9,r8,r9
		ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
		// fsubs f7,f9,f8
		ctx.f7.f64 = double(float(ctx.f9.f64 - ctx.f8.f64));
		// cmplwi cr6,r11,0
		// fmuls f6,f7,f31
		ctx.f6.f64 = double(float(ctx.f7.f64 * var_f31));
		// stfs f6,0(r10)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bne cr6,0x82246a4c
		if (ctx.r11.u32 != 0) goto loc_82246A4C;
	}
loc_82246A74:
	// li r24,0
	var_r24 = 0;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f30;
	// addi r6,r31,-1
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + -1;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r21
	ctx.r4.u64 = var_r21;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r24);
	// mr r31,r24
	var_r31 = (uint32_t)(var_r24);
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, var_r24);
	// stw r24,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, var_r24);
	// bl 0x82246cb0
	atSingleton_6CB0(ctx, base);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82246c5c
	if (ctx.r7.u32 != 0) {
		// lwz r28,88(r1)
		var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
		// addi r26,r27,8
		var_r26 = (uint32_t)(var_r27 + 8);
		// cmplwi cr6,r28,0
		// beq cr6,0x82246b50
		if (var_r28 != 0) {
			// lis r6,8191
			ctx.r6.s64 = 536805376;
			// ori r5,r6,65535
			ctx.r5.u64 = ctx.r6.u64 | 65535;
			// cmplw cr6,r28,r5
			// bgt cr6,0x82246ae4
			if (var_r28 <= ctx.r5.u32) {
				// rlwinm r11,r28,3,0,28
				ctx.r11.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 3) & 0xFFFFFFF8;
				// li r4,-5
				// addi r31,r11,4
				var_r31 = (uint32_t)(ctx.r11.s64 + 4);  // lbl_82080004 @ 0x82080004
				// cmplw cr6,r11,r4
				// ble cr6,0x82246ae8
				if (ctx.r11.u32 <= ctx.r4.u32) goto loc_82246AE8;
			}
		loc_82246AE4:
			// mr r31,r25
			var_r31 = (uint32_t)(var_r25);
		loc_82246AE8:
			// bl 0x820c0038
			xe_main_thread_init_0038(ctx, base);
			// lwzx r3,r23,r22
			ctx.r3.u64 = PPC_LOAD_U32(var_r23 + var_r22);
			// li r5,16
			ctx.r5.s64 = 16;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwz r10,4(r11)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// cmplwi cr6,r3,0
			// beq cr6,0x82246b50
			if (ctx.r3.u32 == 0) goto loc_82246B50;
			// addi r9,r3,4
			ctx.r9.s64 = ctx.r3.s64 + 4;
			// stw r28,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, var_r28);
			// addi r10,r28,-1
			ctx.r10.s64 = (int64_t)(int32_t)var_r28 + -1;
			// mr r11,r9
			ctx.r11.u64 = ctx.r9.u64;
			// cmpwi cr6,r10,0
			// blt cr6,0x82246b48
		while (!ctx.cr6.lt) {
			loc_82246B28:
				// addi r10,r10,-1
				ctx.r10.s64 = ctx.r10.s64 + -1;
				// sth r24,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, (uint16_t)var_r24);
				// stb r24,2(r11)
				PPC_STORE_U8(ctx.r11.u32 + 2, (uint8_t)var_r24);
				// stb r24,3(r11)
				PPC_STORE_U8(ctx.r11.u32 + 3, (uint8_t)var_r24);
				// cmpwi cr6,r10,0
				ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
				// stw r24,4(r11)
				PPC_STORE_U32(ctx.r11.u32 + 4, var_r24);
				// addi r11,r11,8
				ctx.r11.s64 = ctx.r11.s64 + 8;
				// bge cr6,0x82246b28
		}
		loc_82246B48:
			// mr r11,r9
			ctx.r11.u64 = ctx.r9.u64;
			// b 0x82246b54
		} else {
		loc_82246B50:
			// mr r11,r24
			ctx.r11.u64 = var_r24;
		}
	loc_82246B54:
		// lwz r25,84(r1)
		var_r25 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 84));
		// lwz r27,80(r1)
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
		// stw r11,0(r26)
		PPC_STORE_U32(var_r26 + 0, ctx.r11.u32);
		// sth r28,6(r26)
		PPC_STORE_U16(var_r26 + 6, (uint16_t)var_r28);
	loc_82246B64:
		// mr r30,r27
		var_r30 = (uint32_t)(var_r27);
		// cmplwi cr6,r27,0
		// beq cr6,0x82246c4c
		if (var_r27 == 0) goto loc_82246C4C;
		// addi r11,r27,4
		ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 4;
		// cmplw cr6,r25,r30
		// lwz r27,0(r11)
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 0));
		// stw r24,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, var_r24);
		// bne cr6,0x82246b88
		if (var_r25 == var_r30) {
			// mr r25,r24
			var_r25 = (uint32_t)(var_r24);
		}
	loc_82246B88:
		// lhz r11,4(r26)
		ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// lwz r9,0(r26)
		ctx.r9.u64 = PPC_LOAD_U32(var_r26 + 0);
		// addi r8,r11,1
		ctx.r8.s64 = ctx.r11.s64 + 1;
		// rotlwi r10,r11,3
		ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 3);
		// add r3,r10,r9
		ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
		// sth r8,4(r26)
		PPC_STORE_U16(var_r26 + 4, ctx.r8.u16);
		// lwz r4,0(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 0);
		// bl 0x822476b0
		cmOperatorCtor_76B0_w(ctx, base);
		// lwz r31,0(r30)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// cmplwi cr6,r31,0
		// beq cr6,0x82246c1c
		if (var_r31 != 0) {
			// lwz r29,4(r31)
			var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 4));
			// cmplwi cr6,r29,0
			// beq cr6,0x82246bf0
			if (var_r29 != 0) {
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r6,r3,24
				ctx.r6.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r6,0
				// bne cr6,0x82246bf0
				if (ctx.r6.u32 != 0) goto loc_82246BF0;
				// lwzx r3,r23,r22
				ctx.r3.u64 = PPC_LOAD_U32(var_r23 + var_r22);
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// lwz r11,8(r5)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_82246BF0:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r10,r3,24
			ctx.r10.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r10,0
			// bne cr6,0x82246c1c
			if (ctx.r10.u32 != 0) goto loc_82246C1C;
			// lwzx r3,r23,r22
			ctx.r3.u64 = PPC_LOAD_U32(var_r23 + var_r22);
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwz r8,8(r9)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		}
	loc_82246C1C:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r7,r3,24
		ctx.r7.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// bne cr6,0x82246b64
		if (ctx.r7.u32 != 0) goto loc_82246B64;
		// lwzx r3,r23,r22
		ctx.r3.u64 = PPC_LOAD_U32(var_r23 + var_r22);
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// lwz r5,8(r6)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// b 0x82246b64
		goto loc_82246B64;
	loc_82246C4C:
		// stw r28,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r28);
		// li r31,1
		var_r31 = 1;
		// stw r25,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, var_r25);
		// stw r27,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r27);
	}
loc_82246C5C:
	// cmplwi cr6,r21,0
	// beq cr6,0x82246c90
	if (var_r21 != 0) {
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r4,r3,24
		ctx.r4.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// bne cr6,0x82246c90
		if (ctx.r4.u32 != 0) {
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bl 0x82248028
			rage_8028(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			return;
		}
		// lwzx r3,r23,r22
		ctx.r3.u64 = PPC_LOAD_U32(var_r23 + var_r22);
		// mr r4,r21
		ctx.r4.u64 = var_r21;
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_82246C90:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82248028
	rage_8028(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__atSingleton_6CB0"))) PPC_WEAK_FUNC(atSingleton_6CB0);
PPC_FUNC_IMPL(__imp__atSingleton_6CB0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=288, savegprlr_20
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// fmr f30,f1
	var_f30 = ctx.f1.f64;
	// mr r25,r6
	var_r25 = ctx.r6.u32;
	// mr r23,r3
	var_r23 = ctx.r3.u32;
	// subf r11,r28,r25
	ctx.r11.s64 = (int64_t)(int32_t)var_r25 - (int64_t)(int32_t)var_r28;
	// mr r26,r4
	var_r26 = ctx.r4.u32;
	// addi r24,r11,1
	var_r24 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82080001
	// mr r20,r8
	var_r20 = ctx.r8.u32;
	// addi r11,r24,-1
	ctx.r11.s64 = (int64_t)(int32_t)var_r24 + -1;
	// cmpwi cr6,r11,8
	// mr r22,r11
	var_r22 = ctx.r11.u32;
	// blt cr6,0x82246cfc
	if (ctx.r11.s32 >= 8) {
		// li r22,8
		var_r22 = 8;
	}
loc_82246CFC:
	// li r21,0
	var_r21 = 0;
	// cmpwi cr6,r22,0
	// mr r27,r21
	var_r27 = (uint32_t)(var_r21);
	// blt cr6,0x82246dbc
	if ((int32_t)var_r22 < 0) goto loc_82246DBC;
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f29,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	var_f29 = double(temp.f32);
loc_82246D14:
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// mr r7,r27
	ctx.r7.u64 = var_r27;
	// mr r6,r25
	ctx.r6.u64 = var_r25;
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// mr r4,r26
	ctx.r4.u64 = var_r26;
	// mr r3,r23
	ctx.r3.u64 = var_r23;
	// bl 0x822470d8
	atSingleton_70D8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x82246db0
	if (ctx.r11.u32 != 0) {
		// fmr f31,f29
		ctx.fpscr.disableFlushMode();
		var_f31 = var_f29;
		// cmpw cr6,r28,r25
		// bgt cr6,0x82246da8
		if ((int32_t)var_r28 <= (int32_t)var_r25) {
			// rlwinm r11,r28,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC;
			// mr r30,r21
			var_r30 = (uint32_t)(var_r21);
			// add r29,r11,r26
			var_r29 = (uint32_t)(ctx.r11.u64 + var_r26);
			// mr r31,r24
			var_r31 = (uint32_t)(var_r24);
		loc_82246D58:
			// extsw r10,r30
			ctx.r10.s64 = (int32_t)var_r30;
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// mr r5,r27
			ctx.r5.u64 = var_r27;
			// addi r4,r1,112
			ctx.r4.s64 = ctx.r1.s64 + 112;
			// std r10,88(r1)
			PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
			// lfd f0,88(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
			// fcfid f13,f0
			ctx.f13.f64 = double(ctx.f0.s64);
			// frsp f1,f13
			ctx.f1.f64 = double(float(ctx.f13.f64));
			// bl 0x82244be8
			crAnimChannelRawFloat_4BE8_p46(ctx, base);
			// lfs f12,0(r29)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r29 + 0)/* atSingleton::vtable@+0x0 */;
			ctx.f12.f64 = double(temp.f32);
			// addi r31,r31,-1
			var_r31 = (uint32_t)(var_r31 + -1);
			// lfs f11,80(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f11.f64 = double(temp.f32);
			// addi r30,r30,1
			var_r30 = (uint32_t)(var_r30 + 1);
			// fsubs f10,f11,f12
			ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f12.f64));
			// addi r29,r29,4
			var_r29 = (uint32_t)(var_r29 + 4);
			// cmplwi cr6,r31,0
			// fabs f0,f10
			ctx.f0.u64 = ctx.f10.u64 & ~0x8000000000000000;
			// fsubs f9,f31,f0
			ctx.f9.f64 = double(float(var_f31 - ctx.f0.f64));
			// fsel f31,f9,f31,f0
			var_f31 = ctx.f9.f64 >= 0.0 ? var_f31 : ctx.f0.f64;
			// bne cr6,0x82246d58
			if (var_r31 != 0) goto loc_82246D58;
		}
	loc_82246DA8:
		// fcmpu cr6,f31,f30
		ctx.fpscr.disableFlushMode();
		// blt cr6,0x82246eb0
		if (var_f31 < var_f30) goto loc_82246EB0;
	}
loc_82246DB0:
	// addi r27,r27,1
	var_r27 = (uint32_t)(var_r27 + 1);
	// cmpw cr6,r27,r22
	// ble cr6,0x82246d14
	if ((int32_t)var_r27 <= (int32_t)var_r22) goto loc_82246D14;
loc_82246DBC:
	// cmpwi cr6,r24,2
	// ble cr6,0x822470bc
	if ((int32_t)var_r24 <= 2) {
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		return;
	}
	// add r11,r28,r25
	ctx.r11.u64 = var_r28 + var_r25;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f30;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stw r21,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, var_r21);
	// addi r3,r11,1
	ctx.r3.s64 = ctx.r11.s64 + 1;
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, var_r21);
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// stw r21,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, var_r21);
	// srawi r11,r3,1
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r3.s32 >> 1;
	// mr r4,r26
	ctx.r4.u64 = var_r26;
	// addze r31,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	var_r31 = (uint32_t)(temp.s64);
	// mr r3,r23
	ctx.r3.u64 = var_r23;
	// mr r6,r31
	ctx.r6.u64 = var_r31;
	// bl 0x82246cb0
	atSingleton_6CB0(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	// beq cr6,0x82246e34
	if (ctx.r10.u32 != 0) {
		// addi r8,r1,96
		ctx.r8.s64 = ctx.r1.s64 + 96;
		// fmr f1,f30
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f30;
		// mr r6,r25
		ctx.r6.u64 = var_r25;
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// mr r4,r26
		ctx.r4.u64 = var_r26;
		// mr r3,r23
		ctx.r3.u64 = var_r23;
		// bl 0x82246cb0
		atSingleton_6CB0(ctx, base);
		// clrlwi r9,r3,24
		ctx.r9.u64 = ctx.r3.u32 & 0xFF;
		// li r11,1
		ctx.r11.s64 = 1;
		// cmplwi cr6,r9,0
		// bne cr6,0x82246e38
		if (ctx.r9.u32 != 0) goto loc_82246E38;
	}
loc_82246E34:
	// mr r11,r21
	ctx.r11.u64 = var_r21;
loc_82246E38:
	// clrlwi r28,r11,24
	var_r28 = (uint32_t)(ctx.r11.u32 & 0xFF);
	// cmplwi cr6,r28,0
	// beq cr6,0x82247000
	if (var_r28 == 0) goto loc_82247000;
loc_82246E44:
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r10,0
	// beq cr6,0x822470a4
	if (ctx.r10.u32 == 0) {
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		return;
	}
	// addi r11,r10,4
	ctx.r11.s64 = ctx.r10.s64 + 4;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r7,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r7.u32);
	// stw r21,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r21);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r6,r10
	// bne cr6,0x82246e70
	if (ctx.r6.u32 == ctx.r10.u32) {
		// stw r21,100(r1)
		PPC_STORE_U32(ctx.r1.u32 + 100, var_r21);
	}
loc_82246E70:
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r11,4(r20)
	ctx.r11.u64 = PPC_LOAD_U32(var_r20 + 4);
	// addi r4,r5,-1
	ctx.r4.s64 = ctx.r5.s64 + -1;
	// stw r10,4(r20)
	PPC_STORE_U32(var_r20 + 4, ctx.r10.u32);
	// cmplwi cr6,r11,0
	// stw r4,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r4.u32);
	// beq cr6,0x82246e90
	if (ctx.r11.u32 != 0) {
		// stw r10,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	}
loc_82246E90:
	// lwz r3,0(r20)
	ctx.r3.u64 = PPC_LOAD_U32(var_r20 + 0);
	// cmplwi cr6,r3,0
	// bne cr6,0x82246ea0
	if (ctx.r3.u32 == 0) {
		// stw r10,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r10.u32);
	}
loc_82246EA0:
	// lwz r11,8(r20)
	ctx.r11.u64 = PPC_LOAD_U32(var_r20 + 8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,8(r20)
	PPC_STORE_U32(var_r20 + 8, ctx.r11.u32);
	// b 0x82246e44
	goto loc_82246E44;
loc_82246EB0:
	// cmpw cr6,r27,r22
	// bgt cr6,0x82246dbc
	if ((int32_t)var_r27 > (int32_t)var_r22) goto loc_82246DBC;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r26,0(r13)
	var_r26 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
	// li r28,4
	var_r28 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,8
	ctx.r4.s64 = 8;
	// lwzx r3,r28,r26
	ctx.r3.u64 = PPC_LOAD_U32(var_r28 + var_r26);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x82246f00
	if (ctx.r3.u32 != 0) {
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// sth r21,0(r3)
		PPC_STORE_U16(ctx.r3.u32 + 0, (uint16_t)var_r21);
		// stb r21,2(r3)
		PPC_STORE_U8(ctx.r3.u32 + 2, (uint8_t)var_r21);
		// stb r21,3(r3)
		PPC_STORE_U8(ctx.r3.u32 + 3, (uint8_t)var_r21);
		// stw r21,4(r3)
		PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ var_r21);
		// b 0x82246f04
	} else {
	loc_82246F00:
		// mr r31,r21
		var_r31 = (uint32_t)(var_r21);
	}
loc_82246F04:
	// lis r5,16383
	ctx.r5.s64 = 1073676288;
	// sth r25,0(r31)
	PPC_STORE_U16(var_r31 + 0, (uint16_t)var_r25);
	// addi r29,r27,1
	var_r29 = (uint32_t)(var_r27 + 1);
	// stb r27,2(r31)
	PPC_STORE_U8(var_r31 + 2, (uint8_t)var_r27);
	// ori r4,r5,65535
	ctx.r4.u64 = ctx.r5.u64 | 65535;
	// rlwinm r30,r29,2,0,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC);
	// cmplw cr6,r29,r4
	// ble cr6,0x82246f28
	if (var_r29 > ctx.r4.u32) {
		// li r30,-1
		var_r30 = (uint32_t)(-1);
	}
loc_82246F28:
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r28,r26
	ctx.r3.u64 = PPC_LOAD_U32(var_r28 + var_r26);
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmpwi cr6,r27,0
	// stw r3,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r3.u32);
	// blt cr6,0x82246f7c
	if ((int32_t)var_r27 >= 0) {
		// mr r11,r21
		ctx.r11.u64 = var_r21;
		// mr r10,r29
		ctx.r10.u64 = var_r29;
	loc_82246F5C:
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// lwz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4)/* atSingleton::flags@+0x4 */;
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// cmplwi cr6,r10,0
		// lfsx f8,r11,r8
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
		ctx.f8.f64 = double(temp.f32);
		// stfsx f8,r11,r9
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, temp.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// bne cr6,0x82246f5c
		if (ctx.r10.u32 != 0) goto loc_82246F5C;
	}
loc_82246F7C:
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwzx r3,r28,r26
	ctx.r3.u64 = PPC_LOAD_U32(var_r28 + var_r26);
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,8
	ctx.r4.s64 = 8;
	// lwz r6,4(r7)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	// beq cr6,0x82246fb0
	if (ctx.r11.u32 != 0) {
		// stw r21,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, var_r21);
		// b 0x82246fb4
	} else {
	loc_82246FB0:
		// mr r11,r21
		ctx.r11.u64 = var_r21;
	}
loc_82246FB4:
	// stw r31,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r31);
	// lwz r10,4(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 4);
	// stw r11,4(r20)
	PPC_STORE_U32(var_r20 + 4, ctx.r11.u32);
	// cmplwi cr6,r10,0
	// beq cr6,0x82246fcc
	if (ctx.r10.u32 != 0) {
		// stw r11,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r11.u32);
	}
loc_82246FCC:
	// lwz r5,0(r20)
	ctx.r5.u64 = PPC_LOAD_U32(var_r20 + 0);
	// cmplwi cr6,r5,0
	// bne cr6,0x82246fdc
	if (ctx.r5.u32 == 0) {
		// stw r11,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r11.u32);
	}
loc_82246FDC:
	// lwz r11,8(r20)
	ctx.r11.u64 = PPC_LOAD_U32(var_r20 + 8);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r4,r11,1
	ctx.r4.s64 = ctx.r11.s64 + 1;
	// stw r4,8(r20)
	PPC_STORE_U32(var_r20 + 8, ctx.r4.u32);
	return;
loc_82247000:
	// lwz r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// mr r29,r11
	var_r29 = ctx.r11.u32;
	// cmplwi cr6,r11,0
	// beq cr6,0x8224709c
while (var_r29 != 0) {
	loc_82247010:
		// lwz r30,0(r29)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0)/* atSingleton::vtable@+0x0 */);
		// cmplwi cr6,r30,0
		// beq cr6,0x82247090
		if (!(var_r30 == 0)) {
			// lwz r31,4(r30)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 4)/* atSingleton::flags@+0x4 */);
			// cmplwi cr6,r31,0
			// beq cr6,0x8224705c
			if (var_r31 != 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r10,r3,24
			ctx.r10.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r10,0
			// bne cr6,0x8224705c
			if (ctx.r10.u32 != 0) goto loc_8224705C;
			// lwz r9,0(r13)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
			// li r8,4
			ctx.r8.s64 = 4;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwzx r3,r8,r9
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
			// lwz r6,8(r7)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
			loc_8224705C:
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r5,r3,24
			ctx.r5.u64 = ctx.r3.u32 & 0xFF;
		} else {
			if (!(ctx.r5.u32 != 0)) {
				// lwz r3,0(r13)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r11,4
				ctx.r11.s64 = 4;
				// mr r4,r30
				ctx.r4.u64 = var_r30;
				// lwzx r3,r11,r3
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
				// lwz r9,8(r10)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		}
	loc_82247090:
		// lwz r29,4(r29)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r29 + 4)/* atSingleton::flags@+0x4 */);
		// cmplwi cr6,r29,0
		// bne cr6,0x82247010
}
loc_8224709C:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82248028
	rage_8028(ctx, base);
loc_822470A4:
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	return;
}

__attribute__((alias("__imp__atSingleton_70D8"))) PPC_WEAK_FUNC(atSingleton_70D8);
PPC_FUNC_IMPL(__imp__atSingleton_70D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r23 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=256, savegprlr_23
	// mr r25,r7
	var_r25 = ctx.r7.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// mr r24,r8
	var_r24 = ctx.r8.u32;
	// cmplwi cr6,r25,1
	// blt cr6,0x8224752c
	if (var_r25 >= 1) {
		// beq cr6,0x822474e0
		if (!(ctx.cr6.eq)) {
			// subf r11,r31,r6
			ctx.r11.s64 = ctx.r6.s64 - (int64_t)(int32_t)var_r31;
			// lis r10,-32248
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// addi r3,r1,144
			ctx.r3.s64 = ctx.r1.s64 + 144;
			// extsh r29,r11
			var_r29 = (uint32_t)(ctx.r11.s16);
			// lfd f31,-25856(r10)
			ctx.f31.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25856);
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// fmr f1,f31
			ctx.f1.f64 = var_f31;
			// bl 0x8241def8
			xe_DEF8(ctx, base);
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// addi r3,r1,128
			ctx.r3.s64 = ctx.r1.s64 + 128;
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// bl 0x8241def8
			xe_DEF8(ctx, base);
			// mr r9,r29
			ctx.r9.u64 = var_r29;
			// li r27,0
			var_r27 = 0;
			// cmpwi cr6,r9,0
			// ble cr6,0x8224718c
			if (ctx.r9.s32 > 0) {
				// mr r11,r27
				ctx.r11.u64 = var_r27;
			loc_8224714C:
				// extsw r7,r11
				ctx.r7.s64 = ctx.r11.s32;
				// lwz r4,152(r1)
				ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
				// rlwinm r10,r11,3,0,28
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
				// add r6,r11,r31
				ctx.r6.u64 = ctx.r11.u64 + var_r31;
				// addi r8,r11,1
				ctx.r8.s64 = ctx.r11.s64 + 1;
				// rlwinm r5,r6,2,0,29
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
				// std r7,80(r1)
				PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
				// lfd f0,80(r1)
				ctx.fpscr.disableFlushMode();
				ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
				// fcfid f13,f0
				ctx.f13.f64 = double(ctx.f0.s64);
				// stfdx f13,r10,r4
				PPC_STORE_U64(ctx.r10.u32 + ctx.r4.u32, ctx.f13.u64);
				// lwz r3,136(r1)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
				// extsh r11,r8
				ctx.r11.s64 = ctx.r8.s16;
				// lfsx f12,r5,r30
				temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + var_r30);
				ctx.f12.f64 = double(temp.f32);
				// cmpw cr6,r11,r9
				// stfdx f12,r10,r3
				PPC_STORE_U64(ctx.r10.u32 + ctx.r3.u32, ctx.f12.u64);
				// blt cr6,0x8224714c
				if (ctx.r11.s32 < ctx.r9.s32) goto loc_8224714C;
			}
		loc_8224718C:
			// addi r10,r9,-1
			ctx.r10.s64 = ctx.r9.s64 + -1;
			// lwz r29,152(r1)
			var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 152));
			// extsh r31,r25
			var_r31 = (uint32_t)((int16_t)var_r25);
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// extsh r26,r10
			var_r26 = (uint32_t)(ctx.r10.s16);
			// sth r27,84(r1)
			PPC_STORE_U16(ctx.r1.u32 + 84, (uint16_t)var_r27);
			// lis r10,-32248
			// sth r27,86(r1)
			PPC_STORE_U16(ctx.r1.u32 + 86, (uint16_t)var_r27);
			// mr r30,r26
			var_r30 = (uint32_t)(var_r26);
			// stw r27,88(r1)
			PPC_STORE_U32(ctx.r1.u32 + 88, var_r27);
			// addi r28,r10,-28868
			var_r28 = (uint32_t)(ctx.r10.s64 + -28868);  // lbl_82078F3C @ 0x82078f3c
			// addi r9,r31,1
			ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 1;
			// addi r8,r30,1
			ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 1;
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// extsh r5,r9
			ctx.r5.s64 = ctx.r9.s16;
			// extsh r4,r8
			ctx.r4.s64 = ctx.r8.s16;
			// stw r28,80(r1)
			PPC_STORE_U32(ctx.r1.u32 + 80, var_r28);
			// bl 0x8241e2b0
			cmOperatorCtor_E2B0_w(ctx, base);
			// lfd f11,0(r29)
			ctx.fpscr.disableFlushMode();
			ctx.f11.u64 = PPC_LOAD_U64(var_r29 + 0);
			// lwz r23,88(r1)
			var_r23 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
			// cmpwi cr6,r30,0
			// blt cr6,0x82247244
			if ((int32_t)var_r30 >= 0) {
				// lis r11,-32248
				// mr r9,r27
				ctx.r9.u64 = var_r27;
				// lfd f12,-25848(r11)
				ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
			loc_822471F0:
				// rlwinm r7,r9,2,0,29
				ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
				// fmr f0,f12
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = ctx.f12.f64;
				// rlwinm r6,r9,3,0,28
				ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
				// cmpwi cr6,r31,1
				// lwzx r10,r7,r23
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r23);
				// lfdx f10,r6,r29
				ctx.f10.u64 = PPC_LOAD_U64(ctx.r6.u32 + var_r29);
				// fsub f13,f10,f11
				ctx.f13.f64 = ctx.f10.f64 - ctx.f11.f64;
				// stfd f12,0(r10)
				PPC_STORE_U64(ctx.r10.u32 + 0, ctx.f12.u64);
				// blt cr6,0x82247234
				if ((int32_t)var_r31 >= 1) {
					// li r11,1
					ctx.r11.s64 = 1;
				loc_82247218:
					// rlwinm r4,r11,3,0,28
					ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
					// fmul f0,f0,f13
					ctx.fpscr.disableFlushMode();
					ctx.f0.f64 = ctx.f0.f64 * ctx.f13.f64;
					// addi r5,r11,1
					ctx.r5.s64 = ctx.r11.s64 + 1;
					// extsh r11,r5
					ctx.r11.s64 = ctx.r5.s16;
					// stfdx f0,r4,r10
					PPC_STORE_U64(ctx.r4.u32 + ctx.r10.u32, ctx.f0.u64);
					// cmpw cr6,r11,r31
					// ble cr6,0x82247218
					if (ctx.r11.s32 <= (int32_t)var_r31) goto loc_82247218;
				}
			loc_82247234:
				// addi r3,r9,1
				ctx.r3.s64 = ctx.r9.s64 + 1;
				// extsh r9,r3
				ctx.r9.s64 = ctx.r3.s16;
				// cmpw cr6,r9,r30
				// ble cr6,0x822471f0
				if (ctx.r9.s32 <= (int32_t)var_r30) goto loc_822471F0;
			}
		loc_82247244:
			// addi r4,r1,96
			ctx.r4.s64 = ctx.r1.s64 + 96;
			// stw r28,96(r1)
			PPC_STORE_U32(ctx.r1.u32 + 96, var_r28);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// sth r27,100(r1)
			PPC_STORE_U16(ctx.r1.u32 + 100, (uint16_t)var_r27);
			// sth r27,102(r1)
			PPC_STORE_U16(ctx.r1.u32 + 102, (uint16_t)var_r27);
			// stw r27,104(r1)
			PPC_STORE_U32(ctx.r1.u32 + 104, var_r27);
			// bl 0x8241e458
			atSingleton_E458_2hr(ctx, base);
			// addi r11,r25,1
			ctx.r11.s64 = (int64_t)(int32_t)var_r25 + 1;
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// addi r3,r1,112
			ctx.r3.s64 = ctx.r1.s64 + 112;
			// extsh r4,r11
			ctx.r4.s64 = ctx.r11.s16;
			// bl 0x8241def8
			xe_DEF8(ctx, base);
			// lhz r10,86(r1)
			ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
			// extsh r30,r10
			var_r30 = (uint32_t)(ctx.r10.s16);
			// cmpwi cr6,r30,0
			// ble cr6,0x822472bc
			if ((int32_t)var_r30 > 0) {
				// mr r31,r27
				var_r31 = (uint32_t)(var_r27);
			loc_82247288:
				// mr r7,r31
				ctx.r7.u64 = var_r31;
				// addi r6,r1,80
				ctx.r6.s64 = ctx.r1.s64 + 80;
				// mr r5,r26
				ctx.r5.u64 = var_r26;
				// li r4,0
				ctx.r4.s64 = 0;
				// addi r3,r1,128
				ctx.r3.s64 = ctx.r1.s64 + 128;
				// bl 0x8241dfc8
				atSingleton_DFC8_2h(ctx, base);
				// lwz r7,120(r1)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
				// rlwinm r8,r31,3,0,28
				ctx.r8.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 3) & 0xFFFFFFF8;
				// addi r9,r31,1
				ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 1;
				// extsh r31,r9
				var_r31 = (uint32_t)(ctx.r9.s16);
				// stfdx f1,r8,r7
				ctx.fpscr.disableFlushMode();
				PPC_STORE_U64(ctx.r8.u32 + ctx.r7.u32, ctx.f1.u64);
				// cmpw cr6,r31,r30
				// blt cr6,0x82247288
				if ((int32_t)var_r31 < (int32_t)var_r30) goto loc_82247288;
			}
		loc_822472BC:
			// addi r4,r1,112
			ctx.r4.s64 = ctx.r1.s64 + 112;
			// addi r3,r1,96
			ctx.r3.s64 = ctx.r1.s64 + 96;
			// bl 0x8241ebf0
			atSingleton_EBF0_h(ctx, base);
			// lwz r3,120(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
			// cmpwi cr6,r25,0
			// blt cr6,0x82247304
			if ((int32_t)var_r25 >= 0) {
				// mr r11,r27
				ctx.r11.u64 = var_r27;
			loc_822472D8:
				// subf r5,r11,r25
				ctx.r5.s64 = (int64_t)(int32_t)var_r25 - ctx.r11.s64;
				// rlwinm r4,r11,2,0,29
				ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
				// extsh r9,r5
				ctx.r9.s64 = ctx.r5.s16;
				// addi r6,r11,1
				ctx.r6.s64 = ctx.r11.s64 + 1;
				// rlwinm r8,r9,3,0,28
				ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
				// extsh r11,r6
				ctx.r11.s64 = ctx.r6.s16;
				// cmpw cr6,r11,r25
				// lfdx f9,r8,r3
				ctx.fpscr.disableFlushMode();
				ctx.f9.u64 = PPC_LOAD_U64(ctx.r8.u32 + ctx.r3.u32);
				// frsp f8,f9
				ctx.f8.f64 = double(float(ctx.f9.f64));
				// stfsx f8,r4,r24
				temp.f32 = float(ctx.f8.f64);
				PPC_STORE_U32(ctx.r4.u32 + var_r24, temp.u32);
				// ble cr6,0x822472d8
				if (ctx.r11.s32 <= (int32_t)var_r25) goto loc_822472D8;
			}
		loc_82247304:
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// cmplwi cr6,r3,0
			// addi r29,r11,-28844
			var_r29 = (uint32_t)(ctx.r11.s64 + -28844);  // lbl_82078F54 @ 0x82078f54
			// stw r29,112(r1)
			PPC_STORE_U32(ctx.r1.u32 + 112, var_r29);
			// beq cr6,0x8224734c
			if (ctx.r3.u32 != 0) {
				// mr r31,r3
				var_r31 = ctx.r3.u32;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r7,r3,24
				ctx.r7.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r7,0
				// bne cr6,0x8224734c
				if (ctx.r7.u32 != 0) goto loc_8224734C;
				// lwz r6,0(r13)
				ctx.r6.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r5,4
				ctx.r5.s64 = 4;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// lwzx r3,r5,r6
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r6.u32);
				// lwz r10,8(r11)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_8224734C:
			// lwz r30,104(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 104));
			// cmplwi cr6,r30,0
			// beq cr6,0x822473cc
			if (var_r30 != 0) {
				// lwz r31,0(r30)
				var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0)/* atSingleton::vtable@+0x0 */);
				// cmplwi cr6,r31,0
				// beq cr6,0x82247398
				if (var_r31 != 0) {
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// bl 0x820f90d0
					atSingleton_Find_90D0(ctx, base);
					// clrlwi r9,r3,24
					ctx.r9.u64 = ctx.r3.u32 & 0xFF;
					// cmplwi cr6,r9,0
					// bne cr6,0x82247398
					if (ctx.r9.u32 != 0) goto loc_82247398;
					// lwz r8,0(r13)
					ctx.r8.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
					// li r7,4
					ctx.r7.s64 = 4;
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// lwzx r3,r7,r8
					ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
					// lwz r5,8(r6)
					// bctrl
					VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
				}
			loc_82247398:
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r4,r3,24
				ctx.r4.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r4,0
				// bne cr6,0x822473cc
				if (ctx.r4.u32 != 0) goto loc_822473CC;
				// lwz r3,0(r13)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r11,4
				ctx.r11.s64 = 4;
				// mr r4,r30
				ctx.r4.u64 = var_r30;
				// lwzx r3,r11,r3
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
				// lwz r9,8(r10)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_822473CC:
			// cmplwi cr6,r23,0
			// beq cr6,0x82247448
			if (var_r23 != 0) {
				// lwz r31,0(r23)
				var_r31 = (uint32_t)(PPC_LOAD_U32(var_r23 + 0));
				// cmplwi cr6,r31,0
				// beq cr6,0x82247414
				if (var_r31 != 0) {
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// bl 0x820f90d0
					atSingleton_Find_90D0(ctx, base);
					// clrlwi r8,r3,24
					ctx.r8.u64 = ctx.r3.u32 & 0xFF;
					// cmplwi cr6,r8,0
					// bne cr6,0x82247414
					if (ctx.r8.u32 != 0) goto loc_82247414;
					// lwz r7,0(r13)
					ctx.r7.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
					// li r6,4
					ctx.r6.s64 = 4;
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// lwzx r3,r6,r7
					ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
					// lwz r11,8(r5)
					// bctrl
					VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
				}
			loc_82247414:
				// mr r3,r23
				ctx.r3.u64 = var_r23;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r10,r3,24
				ctx.r10.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r10,0
				// bne cr6,0x82247448
				if (ctx.r10.u32 != 0) goto loc_82247448;
				// lwz r9,0(r13)
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r8,4
				ctx.r8.s64 = 4;
				// mr r4,r23
				ctx.r4.u64 = var_r23;
				// lwzx r3,r8,r9
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
				// lwz r6,8(r7)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_82247448:
			// lwz r3,136(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
			// stw r29,128(r1)
			PPC_STORE_U32(ctx.r1.u32 + 128, var_r29);
			// cmplwi cr6,r3,0
			// beq cr6,0x8224748c
			if (ctx.r3.u32 != 0) {
				// mr r31,r3
				var_r31 = ctx.r3.u32;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r5,r3,24
				ctx.r5.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r5,0
				// bne cr6,0x8224748c
				if (ctx.r5.u32 != 0) goto loc_8224748C;
				// lwz r3,0(r13)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r11,4
				ctx.r11.s64 = 4;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// lwzx r3,r11,r3
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
				// lwz r9,8(r10)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_8224748C:
			// lwz r3,152(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
			// stw r29,144(r1)
			PPC_STORE_U32(ctx.r1.u32 + 144, var_r29);
			// cmplwi cr6,r3,0
			// beq cr6,0x82247550
			if (ctx.r3.u32 == 0) {
				// li r3,1
				ctx.r3.s64 = 1;
				return;
			}
			// mr r31,r3
			var_r31 = ctx.r3.u32;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r8,r3,24
			ctx.r8.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r8,0
			// bne cr6,0x82247550
			if (ctx.r8.u32 != 0) {
				// li r3,1
				ctx.r3.s64 = 1;
				return;
			}
			// lwz r7,0(r13)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
			// li r6,4
			ctx.r6.s64 = 4;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwzx r3,r6,r7
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
			// lwz r11,8(r5)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			// li r3,1
			ctx.r3.s64 = 1;
			return;
		}
	loc_822474E0:
		// subf r10,r31,r6
		ctx.r10.s64 = ctx.r6.s64 - (int64_t)(int32_t)var_r31;
		// rlwinm r11,r31,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// rlwinm r8,r6,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// li r3,1
		ctx.r3.s64 = 1;
		// lfsx f6,r11,r30
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + var_r30);
		ctx.f6.f64 = double(temp.f32);
		// std r9,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
		// lfd f4,80(r1)
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f3,f4
		ctx.f3.f64 = double(ctx.f4.s64);
		// lfsx f7,r8,r30
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r30);
		ctx.f7.f64 = double(temp.f32);
		// fsubs f5,f7,f6
		ctx.f5.f64 = double(float(ctx.f7.f64 - ctx.f6.f64));
		// frsp f2,f3
		ctx.f2.f64 = double(float(ctx.f3.f64));
		// fdivs f1,f5,f2
		ctx.f1.f64 = double(float(ctx.f5.f64 / ctx.f2.f64));
		// stfs f1,0(r24)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r24 + 0, temp.u32);
		// lfsx f0,r11,r30
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + var_r30);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,4(r24)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r24 + 4, temp.u32);
		return;
	}
loc_8224752C:
	// rlwinm r7,r31,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r11,-32253
	// lfsx f13,r7,r30
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + var_r30);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r6,r30
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + var_r30);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f0,-12020(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12020);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f10,0(r24)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r24 + 0, temp.u32);
loc_82247550:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_13"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_13);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_13) {
	PPC_FUNC_PROLOGUE();
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// li r3,12
	ctx.r3.s64 = 12;
	// lhz r11,12(r10)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 12);
	// cmpwi cr6,r11,0
	// blelr cr6
	if (ctx.r11.s32 <= 0) return;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
loc_8224757C:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r9,r9,3
	ctx.r9.s64 = ctx.r9.s64 + 3;
	// cmplwi cr6,r11,0
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r9,r3
	ctx.r3.u64 = ctx.r9.u64 + ctx.r3.u64;
	// bne cr6,0x8224757c
	if (ctx.r11.u32 != 0) goto loc_8224757C;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelCurveFloat_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelCurveFloat_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelCurveFloat_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r4,r28,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lhz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x822475d0
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822475D0:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// beq cr6,0x822475ec
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x822475f0
	} else {
	loc_822475EC:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_822475F0:
	// lhz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U16(var_r29 + 0);
	// addi r4,r28,20
	ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 20;
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x8224760c
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224760C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4);
	// beq cr6,0x82247628
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x8224762c
	} else {
	loc_82247628:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_8224762C:
	// lhz r11,12(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 12);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U16(var_r29 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82247658
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82247658:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82247670
	if (ctx.r7.u32 != 0) {
		// addi r3,r28,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 8;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x82247da8
		xe_7DA8(ctx, base);
	}
loc_82247670:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r4,0
	// beq cr6,0x822476a8
	if (ctx.r4.u32 != 0) {
		// li r31,0
		var_r31 = 0;
	loc_82247684:
		// lwz r11,8(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 8);
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// add r3,r11,r31
		ctx.r3.u64 = ctx.r11.u64 + var_r31;
		// bl 0x82247768
		grc_7768(ctx, base);
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,8
		var_r31 = (uint32_t)(var_r31 + 8);
		// cmpw cr6,r30,r11
		// blt cr6,0x82247684
		if ((int32_t)var_r30 < ctx.r11.s32) goto loc_82247684;
	}
loc_822476A8:
	return;
}

__attribute__((alias("__imp__cmOperatorCtor_76B0_w"))) PPC_WEAK_FUNC(cmOperatorCtor_76B0_w);
PPC_FUNC_IMPL(__imp__cmOperatorCtor_76B0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=112, savegprlr_29
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r10,16383
	ctx.r10.s64 = 1073676288;
	// ori r9,r10,65535
	ctx.r9.u64 = ctx.r10.u64 | 65535;
	// lhz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 0);
	// sth r11,0(r31)
	PPC_STORE_U16(var_r31 + 0, ctx.r11.u16);
	// lbz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U8(var_r30 + 2);
	// clrlwi r11,r8,24
	ctx.r11.u64 = ctx.r8.u32 & 0xFF;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stb r8,2(r31)
	PPC_STORE_U8(var_r31 + 2, ctx.r8.u8);
	// lbz r7,3(r30)
	ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 3);
	// cmplw cr6,r11,r9
	// rlwinm r29,r11,2,0,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
	// stb r7,3(r31)
	PPC_STORE_U8(var_r31 + 3, ctx.r7.u8);
	// ble cr6,0x822476fc
	if (ctx.r11.u32 > ctx.r9.u32) {
		// li r29,-1
		var_r29 = (uint32_t)(-1);
	}
loc_822476FC:
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r6,0(r13)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r3,4
	ctx.r3.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// lwzx r3,r3,r6
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r6.u32);
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// lbz r9,2(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 2);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r3,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r3.u32);
	// cmplwi cr6,r9,0
	// blt cr6,0x82247760
	if (ctx.r9.u32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_8224773C:
		// lwz r8,4(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// lwz r7,4(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lfsx f0,r8,r11
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
		ctx.f0.f64 = double(temp.f32);
		// stfsx f0,r7,r11
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r7.u32 + ctx.r11.u32, temp.u32);
		// lbz r6,2(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 2);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// cmpw cr6,r10,r6
		// ble cr6,0x8224773c
		if (ctx.r10.s32 <= ctx.r6.s32) goto loc_8224773C;
	}
loc_82247760:
	return;
}

__attribute__((alias("__imp__grc_7768"))) PPC_WEAK_FUNC(grc_7768);
PPC_FUNC_IMPL(__imp__grc_7768) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 0);
	// addi r27,r28,2
	var_r27 = (uint32_t)(var_r28 + 2);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x822477a4
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822477A4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// beq cr6,0x822477c4
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3828
		rage_obj_bind_3828(ctx, base);
		// b 0x822477c8
	} else {
	loc_822477C4:
		// bl 0x822e39b0
		util_39B0(ctx, base);
	}
loc_822477C8:
	// lhz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U16(var_r30 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x822477e0
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822477E0:
	// clrlwi r4,r11,24
	ctx.r4.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r4,0
	// beq cr6,0x82247814
	if (ctx.r4.u32 != 0) {
		// lbz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 0);
		// lis r3,16383
		ctx.r3.s64 = 1073676288;
		// ori r10,r3,65535
		ctx.r10.u64 = ctx.r3.u64 | 65535;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplw cr6,r11,r10
		// rlwinm r3,r11,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// ble cr6,0x8224780c
		if (ctx.r11.u32 > ctx.r10.u32) {
			// li r3,-1
		}
	loc_8224780C:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// stw r3,4(r28)
		PPC_STORE_U32(var_r28 + 4, ctx.r3.u32);
	}
loc_82247814:
	// lbz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 0);
	// li r29,0
	var_r29 = 0;
	// cmplwi cr6,r9,0
	// blt cr6,0x8224787c
	if (ctx.r9.u32 >= 0) {
		// li r31,0
		var_r31 = 0;
	loc_82247828:
		// lhz r8,0(r30)
		ctx.r8.u64 = PPC_LOAD_U16(var_r30 + 0);
		// lwz r11,4(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 4);
		// clrlwi r7,r8,31
		ctx.r7.u64 = ctx.r8.u32 & 0x1;
		// add r4,r11,r31
		ctx.r4.u64 = ctx.r11.u64 + var_r31;
		// cmplwi cr6,r7,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x82247848
		if (ctx.r7.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82247848:
		// clrlwi r5,r11,24
		ctx.r5.u64 = ctx.r11.u32 & 0xFF;
		// lwz r3,4(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4);
		// cmplwi cr6,r5,0
		// li r5,1
		ctx.r5.s64 = 1;
		// beq cr6,0x82247864
		if (ctx.r5.u32 != 0) {
			// bl 0x822e3cd8
			grc_3CD8(ctx, base);
			// b 0x82247868
		} else {
		loc_82247864:
			// bl 0x822e3dc0
			util_3DC0(ctx, base);
		}
	loc_82247868:
		// lbz r4,0(r27)
		ctx.r4.u64 = PPC_LOAD_U8(var_r27 + 0);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r4
		// ble cr6,0x82247828
		if ((int32_t)var_r29 <= ctx.r4.s32) goto loc_82247828;
	}
loc_8224787C:
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_1"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32251
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r11,-1620
	ctx.r11.s64 = ctx.r11.s64 + -1620;
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,76(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_5"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_5);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// bl 0x82431910
	phBoundOTGrid_1910_g(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f13.u32);
	// lwz r31,80(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
	// lis r11,-32248
	// extsw r10,r31
	ctx.r10.s64 = (int32_t)var_r31;
	// lfs f0,-25528(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25528);
	ctx.f0.f64 = double(temp.f32);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fsubs f31,f31,f10
	var_f31 = double(float(var_f31 - ctx.f10.f64));
	// fcmpu cr6,f31,f0
	// ble cr6,0x82247964
	if (var_f31 > ctx.f0.f64) {
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
	loc_82247944:
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82244cb8
		crAnimChannelQuantizeFloat_4CB8_h(ctx, base);
		// stfs f1,0(r29)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r29 + 0, temp.u32);
		return;
	}
loc_82247964:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f0,-11928(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -11928);  /* glob:lbl_8202D168 @ 0x8202d168 */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	// ble cr6,0x82247944
	if (var_f31 <= ctx.f0.f64) goto loc_82247944;
	// addi r4,r31,1
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 1;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82244cb8
	crAnimChannelQuantizeFloat_4CB8_h(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	var_f30 = ctx.f1.f64;
	// bl 0x82244cb8
	crAnimChannelQuantizeFloat_4CB8_h(ctx, base);
	// fsubs f9,f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(var_f30 - ctx.f1.f64));
	// fmadds f8,f9,f31,f1
	ctx.f8.f64 = double(float(ctx.f9.f64 * var_f31 + ctx.f1.f64));
	// stfs f8,0(r29)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r29 + 0, temp.u32);
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_8"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_8);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_27
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// cmpwi cr6,r31,3
	// bge cr6,0x822479ec
	if ((int32_t)var_r31 < 3) {
	loc_822479D8:
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_822479EC:
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25524(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25524);  /* glob:lbl_82079C4C @ 0x82079c4c */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// blt cr6,0x822479d8
	if (ctx.f1.f64 < ctx.f0.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
	// lis r11,-32248
	// addi r29,r6,1
	var_r29 = (uint32_t)(ctx.r6.s64 + 1);  // addr:0x825f0001
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// cmpwi cr6,r31,0
	// lfs f12,-25792(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25792);  /* glob:lbl_82079B40 @ 0x82079b40 */
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25896(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);  /* glob:lbl_82079AD8 @ 0x82079ad8 */
	ctx.f13.f64 = double(temp.f32);
	// ble cr6,0x82247a48
	if ((int32_t)var_r31 > 0) {
		// rlwinm r9,r29,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC;
		// mr r11,r31
		ctx.r11.u64 = var_r31;
	loc_82247A24:
		// lfs f0,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// fsubs f11,f12,f0
		ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
		// add r10,r9,r10
		ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
		// fsubs f10,f13,f0
		ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// cmplwi cr6,r11,0
		// fsel f12,f11,f12,f0
		ctx.f12.f64 = ctx.f11.f64 >= 0.0 ? ctx.f12.f64 : ctx.f0.f64;
		// fsel f13,f10,f0,f13
		ctx.f13.f64 = ctx.f10.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
		// bne cr6,0x82247a24
		if (ctx.r11.u32 != 0) goto loc_82247A24;
	}
loc_82247A48:
	// fsubs f0,f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// lis r11,-32253
	// stfs f13,20(r27)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r27 + 20, temp.u32);
	// addi r28,r27,8
	var_r28 = (uint32_t)(var_r27 + 8);
	// stfs f0,16(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r27 + 16, temp.u32);
	// cmplwi cr6,r31,0
	// lfs f13,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f31,f13,f0
	var_f31 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// beq cr6,0x82247a8c
	if (var_r31 != 0) {
		// lis r11,32767
		ctx.r11.s64 = 2147418112;
		// rlwinm r3,r31,1,0,30
		ctx.r3.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0xFFFFFFFE;
		// ori r10,r11,65535
		ctx.r10.u64 = ctx.r11.u64 | 65535;
		// cmplw cr6,r31,r10
		// ble cr6,0x82247a84
		if (var_r31 > ctx.r10.u32) {
			// li r3,-1
		}
	loc_82247A84:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// b 0x82247a90
	} else {
	loc_82247A8C:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82247A90:
	// cmpwi cr6,r31,0
	// stw r3,0(r28)
	PPC_STORE_U32(var_r28 + 0, ctx.r3.u32);
	// sth r31,6(r28)
	PPC_STORE_U16(var_r28 + 6, (uint16_t)var_r31);
	// ble cr6,0x82247aec
	if ((int32_t)var_r31 > 0) {
		// lis r11,-32248
		// rlwinm r29,r29,2,0,29
		var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC);
		// lfs f30,-24016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24016);
		var_f30 = double(temp.f32);
	loc_82247AAC:
		// li r4,16
		ctx.r4.s64 = 16;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x82378040
		pongNetMessageHolder_8040_w(ctx, base);
		// lfs f9,0(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 0);
		ctx.f9.f64 = double(temp.f32);
		// lfs f8,20(r27)
		temp.u32 = PPC_LOAD_U32(var_r27 + 20);
		ctx.f8.f64 = double(temp.f32);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// fsubs f7,f9,f8
		ctx.f7.f64 = double(float(ctx.f9.f64 - ctx.f8.f64));
		// add r30,r29,r30
		var_r30 = (uint32_t)(var_r29 + var_r30);
		// cmplwi cr6,r31,0
		// fmuls f6,f7,f31
		ctx.f6.f64 = double(float(ctx.f7.f64 * var_f31));
		// fmuls f5,f6,f30
		ctx.f5.f64 = double(float(ctx.f6.f64 * var_f30));
		// fctidz f4,f5
		ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f5.f64));
		// stfd f4,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f4.u64);
		// lhz r8,86(r1)
		ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
		// sth r8,0(r3)
		PPC_STORE_U16(ctx.r3.u32 + 0, ctx.r8.u16);
		// bne cr6,0x82247aac
		if (var_r31 != 0) goto loc_82247AAC;
	}
loc_82247AEC:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_13"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_13);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_13) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimChannelQuantizeFloat_vfn_14"))) PPC_WEAK_FUNC(crAnimChannelQuantizeFloat_vfn_14);
PPC_FUNC_IMPL(__imp__crAnimChannelQuantizeFloat_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r4,r28,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lhz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82247b40
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82247B40:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// beq cr6,0x82247b5c
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82247b60
	} else {
	loc_82247B5C:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82247B60:
	// lhz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U16(var_r29 + 0);
	// addi r4,r28,20
	ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 20;
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x82247b7c
	if (ctx.r6.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82247B7C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4);
	// beq cr6,0x82247b98
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x82247b9c
	} else {
	loc_82247B98:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_82247B9C:
	// lhz r11,12(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 12);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U16(var_r29 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82247bc8
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82247BC8:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82247be0
	if (ctx.r7.u32 != 0) {
		// addi r3,r28,8
		ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 8;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x820fff00
		xe_FF00(ctx, base);
	}
loc_82247BE0:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r4,0
	// beq cr6,0x82247c18
	if (ctx.r4.u32 != 0) {
		// li r31,0
		var_r31 = 0;
	loc_82247BF4:
		// lwz r11,8(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 8);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// add r4,r11,r31
		ctx.r4.u64 = ctx.r11.u64 + var_r31;
		// bl 0x82244d38
		grc_4D38(ctx, base);
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,2
		var_r31 = (uint32_t)(var_r31 + 2);
		// cmpw cr6,r30,r11
		// blt cr6,0x82247bf4
		if ((int32_t)var_r30 < ctx.r11.s32) goto loc_82247BF4;
	}
loc_82247C18:
	return;
}

__attribute__((alias("__imp__xe_7C20"))) PPC_WEAK_FUNC(xe_7C20);
PPC_FUNC_IMPL(__imp__xe_7C20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lhz r11,6(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 6);
	// cmplwi cr6,r11,0
	// bne cr6,0x82247c7c
	if (ctx.r11.u32 == 0) {
		// cmplwi cr6,r31,0
		// sth r31,6(r30)
		PPC_STORE_U16(var_r30 + 6, (uint16_t)var_r31);
		// beq cr6,0x82247c74
		if (var_r31 != 0) {
			// lis r9,4095
			ctx.r9.s64 = 268369920;
			// rlwinm r3,r31,4,0,27
			ctx.r3.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
			// ori r8,r9,65535
			ctx.r8.u64 = ctx.r9.u64 | 65535;
			// cmplw cr6,r31,r8
			// ble cr6,0x82247c6c
			if (var_r31 > ctx.r8.u32) {
				// li r3,-1
			}
		loc_82247C6C:
			// bl 0x820dec88
			xe_EC88(ctx, base);
			// b 0x82247c78
		} else {
		loc_82247C74:
			// li r3,0
			ctx.r3.s64 = 0;
		}
	loc_82247C78:
		// stw r3,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
	}
loc_82247C7C:
	// sth r31,4(r30)
	PPC_STORE_U16(var_r30 + 4, (uint16_t)var_r31);
	// blr
	return;
}

__attribute__((alias("__imp__xe_7C98"))) PPC_WEAK_FUNC(xe_7C98);
PPC_FUNC_IMPL(__imp__xe_7C98) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lhz r11,6(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 6);
	// cmplwi cr6,r11,0
	// bne cr6,0x82247ce0
	if (ctx.r11.u32 == 0) {
		// cmplwi cr6,r31,0
		// sth r31,6(r30)
		PPC_STORE_U16(var_r30 + 6, (uint16_t)var_r31);
		// beq cr6,0x82247cd8
		if (var_r31 != 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x820dec88
			xe_EC88(ctx, base);
			// b 0x82247cdc
		} else {
		loc_82247CD8:
			// li r3,0
			ctx.r3.s64 = 0;
		}
	loc_82247CDC:
		// stw r3,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r3.u32);
	}
loc_82247CE0:
	// sth r31,4(r30)
	PPC_STORE_U16(var_r30 + 4, (uint16_t)var_r31);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_7D00_h"))) PPC_WEAK_FUNC(atSingleton_7D00_h);
PPC_FUNC_IMPL(__imp__atSingleton_7D00_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// lhz r11,6(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 6);
	// cmplwi cr6,r11,0
	// beq cr6,0x82247d9c
	if (ctx.r11.u32 != 0) {
		// lwz r10,0(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* atSingleton::vtable@+0x0 */;
		// cmplwi cr6,r10,0
		// beq cr6,0x82247d9c
		if (ctx.r10.u32 == 0) {
			return;
		}
		// addi r28,r10,-4
		var_r28 = (uint32_t)(ctx.r10.s64 + -4);  // addr:0x8207fffc
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// addi r30,r11,-1
		var_r30 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8207ffff
		// rlwinm r9,r11,3,0,28
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// cmpwi cr6,r30,0
		// add r11,r9,r10
		ctx.r11.u64 = ctx.r9.u64 + ctx.r10.u64;
		// blt cr6,0x82247d94
		if ((int32_t)var_r30 >= 0) {
			// addi r29,r11,4
			var_r29 = (uint32_t)(ctx.r11.s64 + 4);  // lbl_82080004 @ 0x82080004
		loc_82247D44:
			// addi r29,r29,-8
			var_r29 = (uint32_t)(var_r29 + -8);
			// lwz r31,0(r29)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0)/* atSingleton::vtable@+0x0 */);
			// cmplwi cr6,r31,0
			// beq cr6,0x82247d88
			if (var_r31 != 0) {
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r10,r3,24
				ctx.r10.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r10,0
				// bne cr6,0x82247d88
				if (ctx.r10.u32 != 0) goto loc_82247D88;
				// lwz r9,0(r13)
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r8,4
				ctx.r8.s64 = 4;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// lwzx r3,r8,r9
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
				// lwz r6,8(r7)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_82247D88:
			// addi r30,r30,-1
			var_r30 = (uint32_t)(var_r30 + -1);
			// cmpwi cr6,r30,0
			// bge cr6,0x82247d44
			if ((int32_t)var_r30 >= 0) goto loc_82247D44;
		}
	loc_82247D94:
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_82247D9C:
	return;
}

__attribute__((alias("__imp__xe_7DA8"))) PPC_WEAK_FUNC(xe_7DA8);
PPC_FUNC_IMPL(__imp__xe_7DA8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lhz r11,6(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 6);
	// cmplwi cr6,r11,0
	// bne cr6,0x82247e5c
	if (ctx.r11.u32 == 0) {
		// cmplwi cr6,r31,0
		// sth r31,6(r30)
		PPC_STORE_U16(var_r30 + 6, (uint16_t)var_r31);
		// beq cr6,0x82247e54
		if (var_r31 != 0) {
			// lis r9,8191
			ctx.r9.s64 = 536805376;
			// ori r8,r9,65535
			ctx.r8.u64 = ctx.r9.u64 | 65535;
			// cmplw cr6,r31,r8
			// bgt cr6,0x82247e00
			if (var_r31 <= ctx.r8.u32) {
				// rlwinm r11,r31,3,0,28
				ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 3) & 0xFFFFFFF8;
				// li r7,-5
				// addi r3,r11,4
				ctx.r3.s64 = ctx.r11.s64 + 4;
				// cmplw cr6,r11,r7
				// ble cr6,0x82247e04
				if (ctx.r11.u32 <= ctx.r7.u32) goto loc_82247E04;
			}
		loc_82247E00:
			// li r3,-1
		loc_82247E04:
			// bl 0x820dec88
			xe_EC88(ctx, base);
			// cmplwi cr6,r3,0
			// beq cr6,0x82247e54
			if (ctx.r3.u32 == 0) goto loc_82247E54;
			// addi r8,r3,4
			ctx.r8.s64 = ctx.r3.s64 + 4;
			// stw r31,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, var_r31);
			// addi r10,r31,-1
			ctx.r10.s64 = (int64_t)(int32_t)var_r31 + -1;
			// mr r11,r8
			ctx.r11.u64 = ctx.r8.u64;
			// cmpwi cr6,r10,0
			// blt cr6,0x82247e4c
			if (ctx.r10.s32 >= 0) {
				// li r9,0
				ctx.r9.s64 = 0;
			loc_82247E2C:
				// addi r10,r10,-1
				ctx.r10.s64 = ctx.r10.s64 + -1;
				// sth r9,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r9.u16);
				// stb r9,2(r11)
				PPC_STORE_U8(ctx.r11.u32 + 2, ctx.r9.u8);
				// stb r9,3(r11)
				PPC_STORE_U8(ctx.r11.u32 + 3, ctx.r9.u8);
				// cmpwi cr6,r10,0
				// stw r9,4(r11)
				PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
				// addi r11,r11,8
				ctx.r11.s64 = ctx.r11.s64 + 8;
				// bge cr6,0x82247e2c
				if (ctx.r10.s32 >= 0) goto loc_82247E2C;
			}
		loc_82247E4C:
			// mr r11,r8
			ctx.r11.u64 = ctx.r8.u64;
			// b 0x82247e58
		} else {
		loc_82247E54:
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82247E58:
		// stw r11,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	}
loc_82247E5C:
	// sth r31,4(r30)
	PPC_STORE_U16(var_r30 + 4, (uint16_t)var_r31);
	// blr
	return;
}

__attribute__((alias("__imp__xe_7E78"))) PPC_WEAK_FUNC(xe_7E78);
PPC_FUNC_IMPL(__imp__xe_7E78) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lhz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// cmplwi cr6,r10,0
	// sth r11,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r11.u16);
	// sth r11,6(r31)
	PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
	// beq cr6,0x82247ed0
	if (ctx.r10.u32 != 0) {
		// lis r11,4095
		ctx.r11.s64 = 268369920;
		// rlwinm r3,r10,4,0,27
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
		// ori r9,r11,65535
		ctx.r9.u64 = ctx.r11.u64 | 65535;
		// cmplw cr6,r10,r9
		// ble cr6,0x82247ec4
		if (ctx.r10.u32 > ctx.r9.u32) {
			// li r3,-1
		}
	loc_82247EC4:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// stw r3,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r3.u32);
		// b 0x82247ed8
	} else {
	loc_82247ED0:
		// li r8,0
		ctx.r8.s64 = 0;
		// stw r8,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r8.u32);
	}
loc_82247ED8:
	// lhz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 4);
	// cmplwi cr6,r7,0
	// beq cr6,0x82247f10
	if (ctx.r7.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_82247EE8:
		// lwz r5,0(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 0);
		// rlwinm r10,r11,4,0,27
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
		// lwz r4,0(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r6,r11,1
		ctx.r6.s64 = ctx.r11.s64 + 1;
		// clrlwi r11,r6,16
		ctx.r11.u64 = ctx.r6.u32 & 0xFFFF;
		// lvx128 v0,r5,r10
		ea = (ctx.r5.u32 + ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx128 v0,r10,r4
		ea = (ctx.r10.u32 + ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lhz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 4);
		// cmplw cr6,r11,r3
		// blt cr6,0x82247ee8
		if (ctx.r11.u32 < ctx.r3.u32) goto loc_82247EE8;
	}
loc_82247F10:
	// blr
	return;
}

__attribute__((alias("__imp__pongNetMessageHolder_7F28_w"))) PPC_WEAK_FUNC(pongNetMessageHolder_7F28_w);
PPC_FUNC_IMPL(__imp__pongNetMessageHolder_7F28_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lhz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 4);
	// mr r31,r11
	var_r31 = ctx.r11.u32;
	// cmplwi cr6,r31,0
	// sth r11,4(r30)
	PPC_STORE_U16(var_r30 + 4, ctx.r11.u16);
	// sth r11,6(r30)
	PPC_STORE_U16(var_r30 + 6, ctx.r11.u16);
	// beq cr6,0x82247fdc
	if (var_r31 != 0) {
		// lis r11,8191
		ctx.r11.s64 = 536805376;
		// ori r10,r11,65535
		ctx.r10.u64 = ctx.r11.u64 | 65535;
		// cmplw cr6,r31,r10
		// bgt cr6,0x82247f78
		if (var_r31 <= ctx.r10.u32) {
			// rlwinm r11,r31,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 3) & 0xFFFFFFF8;
			// li r9,-5
			// addi r3,r11,4
			ctx.r3.s64 = ctx.r11.s64 + 4;
			// cmplw cr6,r11,r9
			// ble cr6,0x82247f7c
			if (ctx.r11.u32 <= ctx.r9.u32) goto loc_82247F7C;
		}
	loc_82247F78:
		// li r3,-1
	loc_82247F7C:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x82247fcc
		if (ctx.r3.u32 != 0) {
			// addi r10,r31,-1
			ctx.r10.s64 = (int64_t)(int32_t)var_r31 + -1;
			// stw r31,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0,/* pongNetMessageHolder::vtable@+0x0 */ var_r31);
			// addi r9,r3,4
			ctx.r9.s64 = ctx.r3.s64 + 4;
			// li r31,0
			var_r31 = 0;
			// mr r11,r9
			ctx.r11.u64 = ctx.r9.u64;
			// cmpwi cr6,r10,0
			// blt cr6,0x82247fd4
			if (ctx.r10.s32 < 0) goto loc_82247FD4;
		loc_82247FA4:
			// addi r10,r10,-1
			ctx.r10.s64 = ctx.r10.s64 + -1;
			// sth r31,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, (uint16_t)var_r31);
			// stb r31,2(r11)
			PPC_STORE_U8(ctx.r11.u32 + 2, (uint8_t)var_r31);
			// stb r31,3(r11)
			PPC_STORE_U8(ctx.r11.u32 + 3, (uint8_t)var_r31);
			// cmpwi cr6,r10,0
			// stw r31,4(r11)
			PPC_STORE_U32(ctx.r11.u32 + 4, var_r31);
			// addi r11,r11,8
			ctx.r11.s64 = ctx.r11.s64 + 8;
			// bge cr6,0x82247fa4
			if (ctx.r10.s32 >= 0) goto loc_82247FA4;
			// stw r9,0(r30)
			PPC_STORE_U32(var_r30 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r9.u32);
			// b 0x82247fe4
			goto loc_82247FE4;
		}
	loc_82247FCC:
		// li r31,0
		var_r31 = 0;
		// mr r9,r31
		ctx.r9.u64 = var_r31;
	loc_82247FD4:
		// stw r9,0(r30)
		PPC_STORE_U32(var_r30 + 0,/* pongNetMessageHolder::vtable@+0x0 */ ctx.r9.u32);
		// b 0x82247fe4
	} else {
	loc_82247FDC:
		// li r31,0
		var_r31 = 0;
		// stw r31,0(r30)
		PPC_STORE_U32(var_r30 + 0,/* pongNetMessageHolder::vtable@+0x0 */ var_r31);
	}
loc_82247FE4:
	// lhz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U16(var_r30 + 4);
	// cmplwi cr6,r8,0
	// beq cr6,0x8224801c
while (var_r31 < ctx.r6.u32) {
	loc_82247FF0:
		// lwz r9,0(r29)
		ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 0)/* pongNetMessageHolder::vtable@+0x0 */;
		// rlwinm r11,r31,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 3) & 0xFFFFFFF8;
		// lwz r10,0(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 0)/* pongNetMessageHolder::vtable@+0x0 */;
		// add r4,r9,r11
		ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// bl 0x822476b0
		cmOperatorCtor_76B0_w(ctx, base);
		// addi r7,r31,1
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 1;
		// lhz r6,4(r30)
		ctx.r6.u64 = PPC_LOAD_U16(var_r30 + 4);
		// clrlwi r31,r7,16
		var_r31 = (uint32_t)(ctx.r7.u32 & 0xFFFF);
		// cmplw cr6,r31,r6
		// blt cr6,0x82247ff0
}
loc_8224801C:
	return;
}

__attribute__((alias("__imp__rage_8028"))) PPC_WEAK_FUNC(rage_8028);
PPC_FUNC_IMPL(__imp__rage_8028) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
	// cmplwi cr6,r11,0
	// beq cr6,0x82248094
	if (ctx.r11.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_82248050:
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
		// cmplwi cr6,r3,0
		// beq cr6,0x82248084
		if (ctx.r3.u32 != 0) {
			// lwz r10,4(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* rage_GameObject::flags@+0x4 */;
			// stw r10,0(r31)
			PPC_STORE_U32(var_r31 + 0,/* rage_GameObject::vtable@+0x0 */ ctx.r10.u32);
			// stw r30,4(r3)
			PPC_STORE_U32(ctx.r3.u32 + 4,/* rage_GameObject::flags@+0x4 */ var_r30);
			// lwz r9,4(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4)/* rage_GameObject::flags@+0x4 */;
			// cmplw cr6,r9,r3
			// bne cr6,0x82248078
			if (ctx.r9.u32 == ctx.r3.u32) {
				// stw r30,4(r31)
				PPC_STORE_U32(var_r31 + 4,/* rage_GameObject::flags@+0x4 */ var_r30);
			}
		loc_82248078:
			// lwz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
			// addi r8,r11,-1
			ctx.r8.s64 = ctx.r11.s64 + -1;
			// stw r8,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
		}
	loc_82248084:
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// lwz r7,0(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
		// cmplwi cr6,r7,0
		// bne cr6,0x82248050
		if (ctx.r7.u32 != 0) goto loc_82248050;
	}
loc_82248094:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofFloat_80B0_2h"))) PPC_WEAK_FUNC(crAnimDofFloat_80B0_2h);
PPC_FUNC_IMPL(__imp__crAnimDofFloat_80B0_2h) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	// beq cr6,0x822480e4
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r4)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r9,76(r4)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r8,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r10,2
		ctx.r7.s64 = ctx.r10.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r6,r4
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r5.u32);
	}
loc_822480E4:
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// rotlwi r10,r11,0
	ctx.r10.u64 = ctx.r11.u32;
	// lwz r8,76(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
	// subf r7,r9,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r11,r7,r8
	ctx.r11.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
	// addi r6,r11,2
	ctx.r6.s64 = ctx.r11.s64 + 2;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r5,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lis r10,-32251
	// addi r10,r10,-748
	ctx.r10.s64 = ctx.r10.s64 + -748;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r10,0
	// beq cr6,0x82248164
	if (ctx.r10.u32 != 0) {
		// lwz r9,4(r4)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r8,76(r4)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
		// subf r7,r9,r10
		ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
		// twllei r8,0
		if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
		// divwu r9,r7,r8
		ctx.r9.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
		// addi r6,r9,2
		ctx.r6.s64 = ctx.r9.s64 + 2;
		// rlwinm r5,r6,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r9,r5,r4
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
		// add r10,r9,r10
		ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
		// stw r10,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	}
loc_82248164:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplwi cr6,r10,0
	// beq cr6,0x82248198
	if (ctx.r10.u32 != 0) {
		// lwz r9,4(r4)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r8,76(r4)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
		// subf r7,r9,r10
		ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
		// twllei r8,0
		if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
		// divwu r9,r7,r8
		ctx.r9.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
		// addi r6,r9,2
		ctx.r6.s64 = ctx.r9.s64 + 2;
		// rlwinm r5,r6,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r9,r5,r4
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
		// add r4,r9,r10
		ctx.r4.u64 = ctx.r9.u64 + ctx.r10.u64;
		// stw r4,16(r11)
		PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r4.u32);
	}
loc_82248198:
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_81A8"))) PPC_WEAK_FUNC(LocomotionStateAnim_81A8);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_81A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// cmplwi cr6,r11,1
	// beq cr6,0x822481e0
	if (ctx.r11.u32 != 1) {
		// cmplwi cr6,r11,0
		// bne cr6,0x82248230
		if (ctx.r11.u32 != 0) goto loc_82248230;
	}
loc_822481E0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// bl 0x821d04f0
	LocomotionStateAnim_04F0_g(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x82248230
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lis r11,-32253
		// lhz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// rotlwi r8,r9,2
		ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
		// lwz r7,16(r10)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
		// lfs f13,-12016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		ctx.f13.f64 = double(temp.f32);
		// li r11,1
		ctx.r11.s64 = 1;
		// lfsx f0,r8,r7
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r30)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r30 + 0, temp.u32);
		// fcmpu cr6,f0,f13
		// bgt cr6,0x82248228
		if (ctx.f0.f64 <= ctx.f13.f64) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82248228:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// b 0x82248234
	} else {
	loc_82248230:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82248234:
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_8250_fw"))) PPC_WEAK_FUNC(LocomotionStateAnim_8250_fw);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_8250_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// cmplwi cr6,r11,1
	// beq cr6,0x82248288
	if (ctx.r11.u32 != 1) {
		// cmplwi cr6,r11,0
		// bne cr6,0x822482e8
		if (!(ctx.r11.u32 != 0)) {
			}
			loc_82248288:
			// addi r5,r1,80
			ctx.r5.s64 = ctx.r1.s64 + 80;
			// lwz r3,0(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
			// bl 0x821d04f0
			LocomotionStateAnim_04F0_g(ctx, base);
			// clrlwi r11,r3,24
			ctx.r11.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r11,0
			// beq cr6,0x822482e8
			if (ctx.r11.u32 != 0) {
			// lhz r9,80(r1)
			ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
			// lwz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
			// rotlwi r8,r9,2
			ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
			// lwzx r11,r8,r10
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
		} else {
			if (!(ctx.r11.s32 < 0)) {
				// rlwinm r6,r11,2,0,29
				ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
				// lwz r7,12(r31)
				ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 12);
				// lis r11,-32253
				// lfsx f0,r6,r7
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
				ctx.f0.f64 = double(temp.f32);
				// lfs f13,-12016(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
				ctx.f13.f64 = double(temp.f32);
				// li r11,1
				ctx.r11.s64 = 1;
				// stfs f0,0(r30)
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(var_r30 + 0, temp.u32);
				// fcmpu cr6,f0,f13
				// bgt cr6,0x822482e0
				if (ctx.f0.f64 <= ctx.f13.f64) {
				// li r11,0
				ctx.r11.s64 = 0;
				}
				loc_822482E0:
				// clrlwi r3,r11,24
				ctx.r3.u64 = ctx.r11.u32 & 0xFF;
				// b 0x822482ec
				} else {
			}
		}
	loc_822482E8:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_822482EC:
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_8308_fw"))) PPC_WEAK_FUNC(LocomotionStateAnim_8308_fw);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_8308_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// clrlwi r10,r4,24
	ctx.r10.u64 = ctx.r4.u32 & 0xFF;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// cmplwi cr6,r10,1
	// beq cr6,0x8224833c
	if (ctx.r10.u32 != 1) {
		// cmplwi cr6,r10,0
		// bne cr6,0x8224841c
		if (ctx.r10.u32 != 0) goto loc_8224841C;
	}
loc_8224833C:
	// mr r6,r11
	ctx.r6.u64 = ctx.r11.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// cmplwi cr6,r3,0
	// sth r6,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r6.u16);
	// beq cr6,0x82248360
	if (ctx.r3.u32 != 0) {
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// mr r4,r11
		ctx.r4.u64 = ctx.r11.u64;
		// bl 0x821d04f0
		LocomotionStateAnim_04F0_g(ctx, base);
		// lhz r6,80(r1)
		ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	}
loc_82248360:
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 12);
	// cmpwi cr6,r11,0
	// blt cr6,0x82248380
	if (ctx.r11.s32 >= 0) {
		// clrlwi r10,r6,16
		ctx.r10.u64 = ctx.r6.u32 & 0xFFFF;
		// cmpw cr6,r10,r11
		// ble cr6,0x82248380
		if (ctx.r10.s32 <= ctx.r11.s32) goto loc_82248380;
	loc_82248378:
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x82248420
		// blr
		return;
	}
loc_82248380:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
	// cmpwi cr6,r11,0
	// blt cr6,0x82248398
	if (ctx.r11.s32 < 0) goto loc_82248398;
	// clrlwi r9,r6,16
	ctx.r9.u64 = ctx.r6.u32 & 0xFFFF;
	// cmpw cr6,r9,r11
	// blt cr6,0x82248378
	if (ctx.r9.s32 < ctx.r11.s32) {
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x82248420
		// blr
		return;
	}
loc_82248398:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
	// cmplwi cr6,r11,0
	// beq cr6,0x82248400
	if (ctx.r11.u32 == 0) goto loc_82248400;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 16);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r9,0
	// ble cr6,0x822483f4
	if (ctx.r9.s32 > 0) {
		// clrlwi r8,r6,16
		ctx.r8.u64 = ctx.r6.u32 & 0xFFFF;
	loc_822483BC:
		// lhz r5,0(r11)
		ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
		// cmplw cr6,r8,r5
		// ble cr6,0x822483dc
		if (ctx.r8.u32 > ctx.r5.u32) {
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// addi r11,r11,2
			ctx.r11.s64 = ctx.r11.s64 + 2;
			// cmpw cr6,r10,r9
			// blt cr6,0x822483bc
			if (ctx.r10.s32 < ctx.r9.s32) goto loc_822483BC;
			// b 0x822483f4
		} else {
		loc_822483DC:
			// lhz r4,0(r11)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
			// li r11,1
			ctx.r11.s64 = 1;
			// cmplw cr6,r8,r4
			// beq cr6,0x822483f0
			if (ctx.r8.u32 != ctx.r4.u32) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_822483F0:
			// clrlwi r7,r11,24
			ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		}
	}
loc_822483F4:
	// clrlwi r3,r7,24
	ctx.r3.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r3,0
	// beq cr6,0x82248378
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x82248420
		// blr
		return;
	}
loc_82248400:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224841c
	if (ctx.r11.u32 != 0) {
		// lwz r11,16(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
		// rlwinm r10,r6,2,14,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x3FFFC;
		// lfsx f0,r10,r11
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r30)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r30 + 0, temp.u32);
	}
loc_8224841C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82248420:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimation_vfn_0"))) PPC_WEAK_FUNC(crAnimation_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimation_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-848
	ctx.r11.s64 = ctx.r11.s64 + -848;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82248720
	atSingleton_8720_h(ctx, base);
	// lhz r11,42(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 42);
	// cmplwi cr6,r11,0
	// beq cr6,0x82248478
	if (ctx.r11.u32 != 0) {
		// lwz r3,36(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 36);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_82248478:
	// lis r11,-32254
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,30404
	ctx.r11.s64 = ctx.r11.s64 + 30404;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x8224849c
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8224849C:
	// blr
	return;
}

__attribute__((alias("__imp__util_84B8"))) PPC_WEAK_FUNC(util_84B8);
PPC_FUNC_IMPL(__imp__util_84B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,-32251
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r11,r11,-848
	ctx.r11.s64 = ctx.r11.s64 + -848;
	// addi r31,r27,36
	var_r31 = (uint32_t)(var_r27 + 36);
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// stw r11,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
	// cmplwi cr6,r11,0
	// beq cr6,0x82248510
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 4);
		// lwz r9,76(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r8,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r10,2
		ctx.r7.s64 = ctx.r10.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r6,r28
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r28);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r5.u32);
	}
loc_82248510:
	// lhz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U16(var_r31 + 4);
	// li r29,0
	var_r29 = 0;
	// cmplwi cr6,r4,0
	// beq cr6,0x82248544
	if (ctx.r4.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_82248524:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// add r3,r11,r30
		ctx.r3.u64 = ctx.r11.u64 + var_r30;
		// bl 0x8224a6d0
		ke_A6D0(ctx, base);
		// lhz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 4);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r29,r3
		// blt cr6,0x82248524
		if ((int32_t)var_r29 < ctx.r3.s32) goto loc_82248524;
	}
loc_82248544:
	// lwz r11,32(r27)
	ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 32);
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// cmplwi cr6,r11,0
	// beq cr6,0x8224857c
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 4);
		// lwz r9,76(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r8,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r10,2
		ctx.r7.s64 = ctx.r10.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r6,r28
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r28);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,32(r27)
		PPC_STORE_U32(var_r27 + 32, ctx.r5.u32);
	}
loc_8224857C:
	return;
}

__attribute__((alias("__imp__atSingleton_8588_g_8588_1"))) PPC_WEAK_FUNC(atSingleton_8588_g_8588_1);
PPC_FUNC_IMPL(__imp__atSingleton_8588_g_8588_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=192, savegprlr_19
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r11,1
	ctx.r11.s64 = 1;
	// mr r25,r4
	var_r25 = ctx.r4.u32;
	// addi r9,r31,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 16;
	// addi r10,r25,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r25 + 16;
	// li r24,0
	var_r24 = 0;
	// sth r11,8(r31)
	PPC_STORE_U16(var_r31 + 8, ctx.r11.u16);
	// lwz r8,4(r25)
	ctx.r8.u64 = PPC_LOAD_U32(var_r25 + 4);
	// stw r8,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r8.u32);
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,12(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r25 + 12);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,12(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 12, temp.u32);
	// lhz r7,10(r25)
	ctx.r7.u64 = PPC_LOAD_U16(var_r25 + 10);
	// sth r7,10(r31)
	PPC_STORE_U16(var_r31 + 10, ctx.r7.u16);
	// lwz r3,32(r25)
	ctx.r3.u64 = PPC_LOAD_U32(var_r25 + 32);
	// cmplwi cr6,r3,0
	// beq cr6,0x822485e8
	if (ctx.r3.u32 != 0) {
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// stw r3,32(r31)
		PPC_STORE_U32(var_r31 + 32, ctx.r3.u32);
		// b 0x822485ec
	} else {
	loc_822485E8:
		// stw r24,32(r31)
		PPC_STORE_U32(var_r31 + 32, var_r24);
	}
loc_822485EC:
	// addi r21,r31,36
	var_r21 = (uint32_t)(var_r31 + 36);
	// lhz r4,40(r25)
	ctx.r4.u64 = PPC_LOAD_U16(var_r25 + 40);
	// mr r3,r21
	ctx.r3.u64 = var_r21;
	// bl 0x8224a510
	xe_A510(ctx, base);
	// lhz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U16(var_r25 + 40);
	// mr r22,r24
	var_r22 = (uint32_t)(var_r24);
	// cmpwi cr6,r11,0
	// ble cr6,0x82248714
	if (ctx.r11.s32 > 0) {
		// lwz r19,0(r13)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
		// li r20,4
		var_r20 = 4;
		// mr r23,r24
		var_r23 = (uint32_t)(var_r24);
	loc_82248618:
		// bl 0x820c0038
		xe_main_thread_init_0038(ctx, base);
		// lwzx r3,r20,r19
		ctx.r3.u64 = PPC_LOAD_U32(var_r20 + var_r19);
		// li r5,16
		ctx.r5.s64 = 16;
		// li r4,12
		ctx.r4.s64 = 12;
		// lwz r11,4(r6)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// mr r26,r3
		var_r26 = ctx.r3.u32;
		// cmplwi cr6,r26,0
		// beq cr6,0x822486ec
		if (var_r26 != 0) {
			// lwz r10,36(r25)
			ctx.r10.u64 = PPC_LOAD_U32(var_r25 + 36);
			// addi r28,r26,4
			var_r28 = (uint32_t)(var_r26 + 4);
			// lwzx r31,r10,r23
			var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r23));
			// lbz r9,0(r31)
			ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 0);
			// stb r9,0(r26)
			PPC_STORE_U8(var_r26 + 0, ctx.r9.u8);
			// lbz r8,1(r31)
			ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 1);
			// stb r8,1(r26)
			PPC_STORE_U8(var_r26 + 1, ctx.r8.u8);
			// lhz r7,2(r31)
			ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 2);
			// sth r7,2(r26)
			PPC_STORE_U16(var_r26 + 2, ctx.r7.u16);
			// stw r24,0(r28)
			PPC_STORE_U32(var_r28 + 0, var_r24);
			// sth r24,4(r28)
			PPC_STORE_U16(var_r28 + 4, (uint16_t)var_r24);
			// sth r24,6(r28)
			PPC_STORE_U16(var_r28 + 6, (uint16_t)var_r24);
			// lhz r4,8(r31)
			ctx.r4.u64 = PPC_LOAD_U16(var_r31 + 8);
			// cmplwi cr6,r4,0
			// beq cr6,0x8224868c
			if (ctx.r4.u32 != 0) {
				// mr r3,r28
				ctx.r3.u64 = var_r28;
				// bl 0x8224dde8
				atSingleton_DDE8_g(ctx, base);
				// b 0x82248694
			} else {
			loc_8224868C:
				// stw r24,0(r28)
				PPC_STORE_U32(var_r28 + 0, var_r24);
				// sth r24,6(r28)
				PPC_STORE_U16(var_r28 + 6, (uint16_t)var_r24);
			}
		loc_82248694:
			// lhz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 8);
			// mr r30,r24
			var_r30 = (uint32_t)(var_r24);
			// cmpwi cr6,r11,0
			// ble cr6,0x822486f0
			if (ctx.r11.s32 <= 0) goto loc_822486F0;
			// mr r29,r24
			var_r29 = (uint32_t)(var_r24);
		loc_822486A8:
			// lwz r6,4(r31)
			ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 4)/* atSingleton::flags@+0x4 */;
			// lwzx r3,r29,r6
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + ctx.r6.u32);
			// lwz r4,8(r5)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			// mr r27,r3
			var_r27 = ctx.r3.u32;
			// li r4,16
			ctx.r4.s64 = 16;
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// bl 0x8224de98
			atSingleton_DE98_g(ctx, base);
			// stw r27,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r27);
			// lhz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 8);
			// addi r30,r30,1
			var_r30 = (uint32_t)(var_r30 + 1);
			// addi r29,r29,4
			var_r29 = (uint32_t)(var_r29 + 4);
			// cmpw cr6,r30,r11
			// blt cr6,0x822486a8
			if ((int32_t)var_r30 < ctx.r11.s32) goto loc_822486A8;
			// b 0x822486f0
		} else {
		loc_822486EC:
			// mr r26,r24
			var_r26 = (uint32_t)(var_r24);
		}
	loc_822486F0:
		// li r4,16
		ctx.r4.s64 = 16;
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		// bl 0x8224a5a8
		pongNetMessageHolder_A5A8_w(ctx, base);
		// stw r26,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r26);
		// lhz r11,40(r25)
		ctx.r11.u64 = PPC_LOAD_U16(var_r25 + 40);
		// addi r22,r22,1
		var_r22 = (uint32_t)(var_r22 + 1);
		// addi r23,r23,4
		var_r23 = (uint32_t)(var_r23 + 4);
		// cmpw cr6,r22,r11
		// blt cr6,0x82248618
		if ((int32_t)var_r22 < ctx.r11.s32) goto loc_82248618;
	}
loc_82248714:
	return;
}

__attribute__((alias("__imp__atSingleton_8720_h"))) PPC_WEAK_FUNC(atSingleton_8720_h);
PPC_FUNC_IMPL(__imp__atSingleton_8720_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r27,0
	var_r27 = 0;
	// lhz r28,40(r30)
	var_r28 = (uint32_t)(PPC_LOAD_U16(var_r30 + 40));
	// cmpwi cr6,r28,0
	// ble cr6,0x82248790
	if ((int32_t)var_r28 > 0) {
		// mr r31,r27
		var_r31 = (uint32_t)(var_r27);
	loc_82248744:
		// lwz r11,36(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 36);
		// lwzx r29,r31,r11
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + ctx.r11.u32));
		// cmplwi cr6,r29,0
		// beq cr6,0x82248778
		if (var_r29 != 0) {
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x8224d5b8
			xe_D5B8(ctx, base);
			// lhz r10,10(r29)
			ctx.r10.u64 = PPC_LOAD_U16(var_r29 + 10);
			// cmplwi cr6,r10,0
			// beq cr6,0x82248770
			if (ctx.r10.u32 != 0) {
				// lwz r3,4(r29)
				ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4)/* atSingleton::flags@+0x4 */;
				// bl 0x820c00c0
				rage_free_00C0(ctx, base);
			}
		loc_82248770:
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		}
	loc_82248778:
		// lwz r9,36(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 36);
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// cmplwi cr6,r28,0
		// stwx r27,r31,r9
		PPC_STORE_U32(var_r31 + ctx.r9.u32, var_r27);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// bne cr6,0x82248744
		if (var_r28 != 0) goto loc_82248744;
	}
loc_82248790:
	// lwz r31,36(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 36));
	// cmplwi cr6,r31,0
	// beq cr6,0x822487d0
	if (var_r31 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r8,r3,24
		ctx.r8.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r8,0
		// bne cr6,0x822487d0
		if (ctx.r8.u32 != 0) goto loc_822487D0;
		// lwz r7,0(r13)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r6,4
		ctx.r6.s64 = 4;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwzx r3,r6,r7
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
		// lwz r11,8(r5)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_822487D0:
	// stw r27,36(r30)
	PPC_STORE_U32(var_r30 + 36, var_r27);
	// sth r27,40(r30)
	PPC_STORE_U16(var_r30 + 40, (uint16_t)var_r27);
	// sth r27,42(r30)
	PPC_STORE_U16(var_r30 + 42, (uint16_t)var_r27);
	// lwz r31,32(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 32));
	// cmplwi cr6,r31,0
	// beq cr6,0x8224881c
	if (var_r31 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r10,r3,24
		ctx.r10.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// bne cr6,0x8224881c
		if (ctx.r10.u32 != 0) {
			// stw r27,32(r30)
			PPC_STORE_U32(var_r30 + 32, var_r27);
			return;
		}
		// lwz r9,0(r13)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r8,4
		ctx.r8.s64 = 4;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwzx r3,r8,r9
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
		// lwz r6,8(r7)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_8224881C:
	// stw r27,32(r30)
	PPC_STORE_U32(var_r30 + 32, var_r27);
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_8828_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_8828_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_8828_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lhz r11,10(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 10);
	// lfs f0,12(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lhz r5,6(r30)
	ctx.r5.u64 = PPC_LOAD_U16(var_r30 + 6);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// lbz r4,5(r30)
	ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 5);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fdivs f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 / ctx.f0.f64));
	// fmuls f1,f10,f1
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// bl 0x8224a390
	LocomotionStateAnim_A390_v12(ctx, base);
	// cmplwi cr6,r3,0
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// beq cr6,0x822488b0
	if (ctx.r3.u32 != 0) {
		// lwz r7,80(r1)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
		// lwz r8,36(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 36);
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwz r4,16(r9)
  // [ph4a] slot load collapsed
		// lwzx r5,r6,r8
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r8.u32);
		// bctrl
		VCALL(var_r30, 4, ctx, base);  // pattern-B slot 4 (byte +16)
		// li r3,1
		ctx.r3.s64 = 1;
		// b 0x822488c4
	} else {
		// lwz r10,52(r11)
		// bctrl
		VCALL(ctx.r3.u32, 13, ctx, base);  // vtable slot 13 (byte +52)
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_822488C4:
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_88E0_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_88E0_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_88E0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=176, savegprlr_23
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// clrlwi r11,r6,24
	ctx.r11.u64 = ctx.r6.u32 & 0xFF;
	// mr r25,r5
	var_r25 = ctx.r5.u32;
	// cmplwi cr6,r11,0
	// mr r23,r7
	var_r23 = ctx.r7.u32;
	// lhz r11,10(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 10);
	// beq cr6,0x82248960
	if (ctx.r11.u32 != 0) {
		// addi r10,r11,-1
		ctx.r10.s64 = ctx.r11.s64 + -1;
		// lfs f0,12(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + 12);
		ctx.f0.f64 = double(temp.f32);
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// extsw r8,r10
		ctx.r8.s64 = ctx.r10.s32;
		// lis r11,-32253
		// std r8,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
		// lfd f13,80(r1)
		ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f12,f13
		ctx.f12.f64 = double(ctx.f13.s64);
		// frsp f11,f12
		ctx.f11.f64 = double(float(ctx.f12.f64));
		// fdivs f10,f11,f0
		ctx.f10.f64 = double(float(ctx.f11.f64 / ctx.f0.f64));
		// lfs f0,-12020(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12020);
		ctx.f0.f64 = double(temp.f32);
		// fmadds f9,f10,f1,f0
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f0.f64));
		// fctiwz f8,f9
		ctx.f8.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
		// stfiwx f8,0,r9
		PPC_STORE_U32(ctx.r9.u32, ctx.f8.u32);
		// lwz r7,80(r1)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// extsw r6,r7
		ctx.r6.s64 = ctx.r7.s32;
		// std r6,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
		// lfd f7,80(r1)
		ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f6,f7
		ctx.f6.f64 = double(ctx.f7.s64);
		// frsp f31,f6
		var_f31 = double(float(ctx.f6.f64));
		// b 0x82248984
	} else {
	loc_82248960:
		// addi r5,r11,-1
		ctx.r5.s64 = ctx.r11.s64 + -1;
		// lfs f5,12(r28)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r28 + 12);
		ctx.f5.f64 = double(temp.f32);
		// extsw r4,r5
		ctx.r4.s64 = ctx.r5.s32;
		// std r4,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
		// lfd f4,80(r1)
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f3,f4
		ctx.f3.f64 = double(ctx.f4.s64);
		// frsp f2,f3
		ctx.f2.f64 = double(float(ctx.f3.f64));
		// fdivs f0,f2,f5
		ctx.f0.f64 = double(float(ctx.f2.f64 / ctx.f5.f64));
		// fmuls f31,f0,f1
		var_f31 = double(float(ctx.f0.f64 * ctx.f1.f64));
	}
loc_82248984:
	// lhz r26,12(r25)
	var_r26 = (uint32_t)(PPC_LOAD_U16(var_r25 + 12));
	// li r31,0
	var_r31 = 0;
	// lhz r29,40(r28)
	var_r29 = (uint32_t)(PPC_LOAD_U16(var_r28 + 40));
	// li r24,0
	var_r24 = 0;
	// li r30,0
	var_r30 = 0;
	// cmpwi cr6,r26,0
	// ble cr6,0x82248ad4
	if ((int32_t)var_r26 > 0) {
		// li r27,0
		var_r27 = 0;
	loc_822489A4:
		// lwz r3,8(r25)
		ctx.r3.u64 = PPC_LOAD_U32(var_r25 + 8);
		// cmpw cr6,r31,r29
		// lwzx r3,r3,r27
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r27);
		// bge cr6,0x82248a9c
		if ((int32_t)var_r31 >= (int32_t)var_r29) goto loc_82248A9C;
		// lbz r9,5(r3)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
		// rlwinm r11,r31,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
		// lhz r8,6(r3)
		ctx.r8.u64 = PPC_LOAD_U16(ctx.r3.u32 + 6);
		// rotlwi r7,r9,16
		ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 16);
		// lwz r10,36(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 36);
		// or r7,r7,r8
		ctx.r7.u64 = ctx.r7.u64 | ctx.r8.u64;
		// add r8,r11,r10
		ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	loc_822489D0:
		// lwz r5,0(r8)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// lbz r6,0(r5)
		ctx.r6.u64 = PPC_LOAD_U8(ctx.r5.u32 + 0);
		// lhz r4,2(r5)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r5.u32 + 2);
		// rotlwi r11,r6,16
		ctx.r11.u64 = __builtin_rotateleft32(ctx.r6.u32, 16);
		// or r10,r11,r4
		ctx.r10.u64 = ctx.r11.u64 | ctx.r4.u64;
		// li r11,1
		ctx.r11.s64 = 1;
		// cmplw cr6,r10,r7
		// bgt cr6,0x822489f4
		if (ctx.r10.u32 <= ctx.r7.u32) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_822489F4:
		// clrlwi r9,r11,24
		ctx.r9.u64 = ctx.r11.u32 & 0xFF;
		// cmplw cr6,r10,r7
		// li r11,1
		ctx.r11.s64 = 1;
		// beq cr6,0x82248a08
		if (ctx.r10.u32 != ctx.r7.u32) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82248A08:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r6,0
		// bne cr6,0x82248a60
		if (ctx.r6.u32 != 0) goto loc_82248A60;
		// clrlwi r5,r9,24
		ctx.r5.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// bne cr6,0x82248a80
		if (ctx.r5.u32 != 0) goto loc_82248A80;
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// cmpw cr6,r31,r29
		// blt cr6,0x822489d0
		if ((int32_t)var_r31 < (int32_t)var_r29) goto loc_822489D0;
		// clrlwi r7,r23,24
		ctx.r7.u64 = var_r23 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x82248a4c
		if (ctx.r7.u32 == 0) goto loc_82248A4C;
		// lwz r6,0(r3)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r5,52(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 52);
		// mtctr r5
		ctx.ctr.u64 = ctx.r5.u64;
	loc_82248A48:
		// bctrl
		ctx.lr = 0x82248A4C;
		PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	loc_82248A4C:
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r27,r27,4
		var_r27 = (uint32_t)(var_r27 + 4);
		// cmpw cr6,r30,r26
		// blt cr6,0x822489a4
		if ((int32_t)var_r30 < (int32_t)var_r26) goto loc_822489A4;
		// b 0x82248ad4
		goto loc_82248AD4;
	loc_82248A60:
		// lwz r4,0(r3)
  // [ph4a] vtable load collapsed
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// lwz r11,16(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 4, ctx, base);  // pattern-B slot 4 (byte +16)
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r24,r24,1
		var_r24 = (uint32_t)(var_r24 + 1);
		// b 0x82248a4c
		goto loc_82248A4C;
	loc_82248A80:
		// clrlwi r10,r23,24
		ctx.r10.u64 = var_r23 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x82248a4c
		if (ctx.r10.u32 == 0) goto loc_82248A4C;
		// lwz r9,0(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r8,52(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 52);
		// mtctr r8
		ctx.ctr.u64 = ctx.r8.u64;
		// b 0x82248a48
		goto loc_82248A48;
	loc_82248A9C:
		// clrlwi r4,r23,24
		ctx.r4.u64 = var_r23 & 0xFF;
		// cmplwi cr6,r4,0
		// beq cr6,0x82248ad4
		if (ctx.r4.u32 == 0) goto loc_82248AD4;
		// rlwinm r31,r30,2,0,29
		var_r31 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC);
	loc_82248AAC:
		// lwz r3,8(r25)
		ctx.r3.u64 = PPC_LOAD_U32(var_r25 + 8);
		// lwzx r3,r3,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r31);
		// lwz r10,52(r11)
		// bctrl
		VCALL(ctx.r3.u32, 13, ctx, base);  // vtable slot 13 (byte +52)
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r30,r26
		// blt cr6,0x82248aac
		if ((int32_t)var_r30 < (int32_t)var_r26) goto loc_82248AAC;
	}
loc_82248AD4:
	// cmpw cr6,r24,r26
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x82248ae4
	if ((int32_t)var_r24 != (int32_t)var_r26) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82248AE4:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_8AF8_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_8AF8_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_8AF8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_29
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// li r7,1
	ctx.r7.s64 = 1;
	// fmr f30,f2
	var_f30 = ctx.f2.f64;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x822488e0
	LocomotionStateAnim_88E0_g(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rlwinm r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x82248b44
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82248B44:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82248c44
	if (ctx.r8.u32 != 0) {
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,6
		ctx.r4.s64 = 6;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8224bef8
		LocomotionStateAnim_BEF8_g(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,5
		ctx.r4.s64 = 5;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8224bef8
		LocomotionStateAnim_BEF8_g(ctx, base);
		// mr r7,r3
		ctx.r7.u64 = ctx.r3.u64;
		// cmplwi cr6,r30,0
		// beq cr6,0x82248c40
		if (var_r30 != 0) {
			// cmplwi cr6,r7,0
			// beq cr6,0x82248c40
			if (ctx.r7.u32 == 0) goto loc_82248C40;
			// lis r11,-32248
			// lfs f12,12(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 12);
			ctx.f12.f64 = double(temp.f32);
			// lis r9,-32253
			// mr r6,r30
			ctx.r6.u64 = var_r30;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lfd f13,-25856(r11)
			ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25856);
			// lhz r11,10(r31)
			ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 10);
			// lfs f0,-12024(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -12024);
			ctx.f0.f64 = double(temp.f32);
			// mr r10,r11
			ctx.r10.u64 = ctx.r11.u64;
			// fdivs f0,f0,f12
			ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
			// extsw r9,r11
			ctx.r9.s64 = ctx.r11.s32;
			// addi r5,r10,-1
			ctx.r5.s64 = ctx.r10.s64 + -1;
			// addi r10,r11,-1
			ctx.r10.s64 = ctx.r11.s64 + -1;
			// extsw r4,r5
			ctx.r4.s64 = ctx.r5.s32;
			// extsw r8,r10
			ctx.r8.s64 = ctx.r10.s32;
			// std r9,88(r1)
			PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
			// std r4,80(r1)
			PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
			// std r8,96(r1)
			PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
			// lfd f9,88(r1)
			ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
			// fcfid f7,f9
			ctx.f7.f64 = double(ctx.f9.s64);
			// lfd f11,80(r1)
			ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
			// lfd f8,96(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
			// fcfid f10,f11
			ctx.f10.f64 = double(ctx.f11.s64);
			// fcfid f6,f8
			ctx.f6.f64 = double(ctx.f8.s64);
			// frsp f11,f10
			ctx.f11.f64 = double(float(ctx.f10.f64));
			// frsp f9,f6
			ctx.f9.f64 = double(float(ctx.f6.f64));
			// frsp f10,f7
			ctx.f10.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f0,f9
			ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
			// fmuls f4,f0,f9
			ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
			// fmuls f0,f5,f31
			ctx.f0.f64 = double(float(ctx.f5.f64 * var_f31));
			// fmuls f9,f4,f30
			ctx.f9.f64 = double(float(ctx.f4.f64 * var_f30));
			// fsel f8,f0,f0,f13
			ctx.f8.f64 = ctx.f0.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
			// fsel f0,f0,f0,f13
			ctx.f0.f64 = ctx.f0.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
			// fneg f3,f9
			ctx.f3.u64 = ctx.f9.u64 ^ 0x8000000000000000;
			// fsub f1,f8,f10
			ctx.f1.f64 = ctx.f8.f64 - ctx.f10.f64;
			// fsel f13,f3,f13,f9
			ctx.f13.f64 = ctx.f3.f64 >= 0.0 ? ctx.f13.f64 : ctx.f9.f64;
			// fsel f0,f1,f10,f0
			ctx.f0.f64 = ctx.f1.f64 >= 0.0 ? ctx.f10.f64 : ctx.f0.f64;
			// fmuls f2,f12,f13
			ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
			// fmuls f0,f12,f0
			ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
			// fdivs f2,f2,f11
			ctx.f2.f64 = double(float(ctx.f2.f64 / ctx.f11.f64));
			// fdivs f1,f0,f11
			ctx.f1.f64 = double(float(ctx.f0.f64 / ctx.f11.f64));
			// bl 0x82248db0
			LocomotionStateAnim_8DB0_g(ctx, base);
			return;
		}
	loc_82248C40:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82248C44:
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_8C58_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_8C58_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_8C58_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 84);
	// li r4,6
	ctx.r4.s64 = 6;
	// lis r10,-32254
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// rlwinm r9,r11,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// fmr f30,f2
	var_f30 = ctx.f2.f64;
	// lis r11,-32254
	// ori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 | 1;
	// lbz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 116);
	// addi r11,r11,31732
	ctx.r11.s64 = ctx.r11.s64 + 31732;
	// stb r4,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, ctx.r4.u8);
	// addi r10,r10,31644
	ctx.r10.s64 = ctx.r10.s64 + 31644;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r30,r6
	var_r30 = ctx.r6.u32;
	// stb r8,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, ctx.r8.u8);
	// rlwinm r8,r9,0,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stb r4,117(r1)
	PPC_STORE_U8(ctx.r1.u32 + 117, ctx.r4.u8);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// stb r8,116(r1)
	PPC_STORE_U8(ctx.r1.u32 + 116, ctx.r8.u8);
	// sth r11,86(r1)
	PPC_STORE_U16(ctx.r1.u32 + 86, ctx.r11.u16);
	// sth r11,118(r1)
	PPC_STORE_U16(ctx.r1.u32 + 118, ctx.r11.u16);
	// bl 0x82248828
	LocomotionStateAnim_8828_g(ctx, base);
	// clrlwi r3,r3,24
	ctx.r3.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r3,0
	// beq cr6,0x82248d8c
	if (ctx.r3.u32 != 0) {
		// li r7,1
		ctx.r7.s64 = 1;
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// li r6,0
		ctx.r6.s64 = 0;
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82248828
		LocomotionStateAnim_8828_g(ctx, base);
		// clrlwi r11,r3,24
		ctx.r11.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x82248d8c
		if (!(ctx.r11.u32 == 0)) {
			// lis r11,-32248
			// lfs f13,12(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 12);
			ctx.f13.f64 = double(temp.f32);
			// fneg f10,f30
			ctx.f10.u64 = ctx.f30.u64 ^ 0x8000000000000000;
			// addi r7,r1,112
			ctx.r7.s64 = ctx.r1.s64 + 112;
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lfd f0,-25856(r11)
			ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25856);
			// fsel f12,f31,f31,f0
			ctx.f12.f64 = var_f31 >= 0.0 ? var_f31 : ctx.f0.f64;
			// fsel f11,f31,f31,f0
			ctx.f11.f64 = var_f31 >= 0.0 ? var_f31 : ctx.f0.f64;
			// fsel f2,f10,f0,f30
			ctx.f2.f64 = ctx.f10.f64 >= 0.0 ? ctx.f0.f64 : var_f30;
			// fsub f9,f12,f13
			ctx.f9.f64 = ctx.f12.f64 - ctx.f13.f64;
			// fsel f1,f9,f13,f11
			ctx.f1.f64 = ctx.f9.f64 >= 0.0 ? ctx.f13.f64 : ctx.f11.f64;
			// bl 0x82248db0
			LocomotionStateAnim_8DB0_g(ctx, base);
			// clrlwi r10,r3,24
			ctx.r10.u64 = ctx.r3.u32 & 0xFF;
		} else {
			if (!(ctx.r10.u32 == 0)) {
				// addi r8,r1,128
				ctx.r8.s64 = ctx.r1.s64 + 128;
				// lfs f8,96(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
				ctx.f8.f64 = double(temp.f32);
				// addi r9,r30,16
				ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
				// lfs f7,100(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
				ctx.f7.f64 = double(temp.f32);
				// lfs f6,104(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
				ctx.f6.f64 = double(temp.f32);
				// li r3,1
				ctx.r3.s64 = 1;
				// lfs f5,108(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
				ctx.f5.f64 = double(temp.f32);
				// stfs f8,0(r30)
				temp.f32 = float(ctx.f8.f64);
				PPC_STORE_U32(var_r30 + 0, temp.u32);
				// lvx128 v0,r0,r8
				ea = (ctx.r8.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stfs f7,4(r30)
				temp.f32 = float(ctx.f7.f64);
				PPC_STORE_U32(var_r30 + 4, temp.u32);
				// stfs f6,8(r30)
				temp.f32 = float(ctx.f6.f64);
				PPC_STORE_U32(var_r30 + 8, temp.u32);
				// stfs f5,12(r30)
				temp.f32 = float(ctx.f5.f64);
				PPC_STORE_U32(var_r30 + 12, temp.u32);
				// stvx v0,r0,r9
				ea = (ctx.r9.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// b 0x82248d90
				} else {
			}
		}
	loc_82248D8C:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82248D90:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_8DB0_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_8DB0_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_8DB0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=336, savegprlr_29
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// lis r11,-32253
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// fsubs f0,f31,f2
	ctx.f0.f64 = double(float(var_f31 - ctx.f2.f64));
	// fsel f30,f0,f2,f31
	var_f30 = ctx.f0.f64 >= 0.0 ? ctx.f2.f64 : var_f31;
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f30,f0
	// ble cr6,0x82248f5c
	if (var_f30 > ctx.f0.f64) {
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,60(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// addi r3,r1,160
		ctx.r3.s64 = ctx.r1.s64 + 160;
		// bl 0x8223ad30
		LocomotionStateAnim_AD30_g(ctx, base);
		// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r8,56(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// lbz r7,116(r1)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + 116);
		// lis r10,-32254
		// addi r8,r1,208
		ctx.r8.s64 = ctx.r1.s64 + 208;
		// fsubs f31,f31,f30
		ctx.fpscr.disableFlushMode();
		var_f31 = double(float(var_f31 - var_f30));
		// rlwinm r4,r7,0,0,27
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFF0;
		// addi r10,r10,31732
		ctx.r10.s64 = ctx.r10.s64 + 31732;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// ori r11,r4,1
		ctx.r11.u64 = ctx.r4.u64 | 1;
		// li r4,6
		ctx.r4.s64 = 6;
		// lis r9,-32254
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// li r7,1
		ctx.r7.s64 = 1;
		// stw r10,112(r1)
		PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
		// addi r9,r9,31644
		ctx.r9.s64 = ctx.r9.s64 + 31644;
		// lbz r10,84(r1)
		ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 84);
		// li r6,0
		ctx.r6.s64 = 0;
		// stb r11,116(r1)
		PPC_STORE_U8(ctx.r1.u32 + 116, ctx.r11.u8);
		// li r11,0
		ctx.r11.s64 = 0;
		// stb r4,117(r1)
		PPC_STORE_U8(ctx.r1.u32 + 117, ctx.r4.u8);
		// rlwinm r8,r10,0,0,27
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
		// li r4,5
		ctx.r4.s64 = 5;
		// fmr f1,f31
		ctx.f1.f64 = var_f31;
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// stw r9,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// sth r11,118(r1)
		PPC_STORE_U16(ctx.r1.u32 + 118, ctx.r11.u16);
		// stb r8,84(r1)
		PPC_STORE_U8(ctx.r1.u32 + 84, ctx.r8.u8);
		// stb r4,85(r1)
		PPC_STORE_U8(ctx.r1.u32 + 85, ctx.r4.u8);
		// sth r11,86(r1)
		PPC_STORE_U16(ctx.r1.u32 + 86, ctx.r11.u16);
		// bl 0x82248828
		LocomotionStateAnim_8828_g(ctx, base);
		// clrlwi r3,r3,24
		ctx.r3.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x82248f4c
		if (ctx.r3.u32 != 0) {
			// li r7,1
			ctx.r7.s64 = 1;
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// li r6,0
			ctx.r6.s64 = 0;
			// addi r5,r1,80
			ctx.r5.s64 = ctx.r1.s64 + 80;
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x82248828
			LocomotionStateAnim_8828_g(ctx, base);
			// clrlwi r11,r3,24
			ctx.r11.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r11,0
			// beq cr6,0x82248f4c
			if (ctx.r11.u32 == 0) goto loc_82248F4C;
			// addi r4,r1,128
			ctx.r4.s64 = ctx.r1.s64 + 128;
			// addi r3,r1,224
			ctx.r3.s64 = ctx.r1.s64 + 224;
			// bl 0x8223ad30
			LocomotionStateAnim_AD30_g(ctx, base);
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// addi r9,r1,272
			ctx.r9.s64 = ctx.r1.s64 + 272;
			// addi r3,r1,224
			ctx.r3.s64 = ctx.r1.s64 + 224;
			// lvx128 v13,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x820d7b30
			LocomotionStateAnim_7B30_g(ctx, base);
			// addi r4,r1,224
			ctx.r4.s64 = ctx.r1.s64 + 224;
			// addi r3,r1,160
			ctx.r3.s64 = ctx.r1.s64 + 160;
			// bl 0x820c3ad8
			LocomotionStateAnim_3AD8_g(ctx, base);
			// addi r4,r1,160
			ctx.r4.s64 = ctx.r1.s64 + 160;
			// addi r3,r1,144
			ctx.r3.s64 = ctx.r1.s64 + 144;
			// bl 0x8223b218
			LocomotionStateAnim_B218_g(ctx, base);
			// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
			// addi r4,r1,144
			ctx.r4.s64 = ctx.r1.s64 + 144;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r7,72(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 18, ctx, base);  // pattern-B slot 18 (byte +72)
			// lwz r6,0(r30)
  // [ph4a] vtable load collapsed
			// addi r4,r1,208
			ctx.r4.s64 = ctx.r1.s64 + 208;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r5,68(r6)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
			// li r3,1
			ctx.r3.s64 = 1;
			return;
		}
	loc_82248F4C:
		// lis r11,-32254
		// addi r11,r11,31556
		ctx.r11.s64 = ctx.r11.s64 + 31556;
		// stw r11,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
		// stw r11,112(r1)
		PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r11.u32);
	}
loc_82248F5C:
	// lwz r4,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r11,52(r4)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 13, ctx, base);  // pattern-B slot 13 (byte +52)
	// lwz r10,0(r30)
  // [ph4a] vtable load collapsed
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r9,52(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r30, 13, ctx, base);  // pattern-B slot 13 (byte +52)
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__crAnimation_ctor_8F98"))) PPC_WEAK_FUNC(crAnimation_ctor_8F98);
PPC_FUNC_IMPL(__imp__crAnimation_ctor_8F98) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,48
	ctx.r4.s64 = 48;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// li r31,0
	var_r31 = 0;
	// beq cr6,0x82249030
	if (ctx.r3.u32 != 0) {
		// lis r11,-32253
		// stw r31,4(r3)
		PPC_STORE_U32(ctx.r3.u32 + 4, var_r31);
		// li r7,1
		ctx.r7.s64 = 1;
		// sth r31,10(r3)
		PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r31);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// lfs f0,-12016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		ctx.f0.f64 = double(temp.f32);
		// lis r11,-32251
		// stfs f0,12(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
		// sth r7,8(r3)
		PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r7.u16);
		// addi r11,r11,-848
		ctx.r11.s64 = ctx.r11.s64 + -848;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// stfs f0,16(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
		// stfs f0,20(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
		// stfs f0,24(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
		// stw r31,32(r3)
		PPC_STORE_U32(ctx.r3.u32 + 32, var_r31);
		// stw r31,36(r3)
		PPC_STORE_U32(ctx.r3.u32 + 36, var_r31);
		// sth r31,40(r3)
		PPC_STORE_U16(ctx.r3.u32 + 40, (uint16_t)var_r31);
		// sth r31,42(r3)
		PPC_STORE_U16(ctx.r3.u32 + 42, (uint16_t)var_r31);
		// stw r31,44(r3)
		PPC_STORE_U32(ctx.r3.u32 + 44, var_r31);
		// b 0x82249034
	} else {
	loc_82249030:
		// mr r30,r31
		var_r30 = (uint32_t)(var_r31);
	}
loc_82249034:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82249708
	rage_9708(ctx, base);
	// clrlwi r6,r3,24
	ctx.r6.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// bne cr6,0x82249090
	if (ctx.r6.u32 == 0) {
		// cmplwi cr6,r30,0
		// beq cr6,0x82249074
		if (var_r30 != 0) {
			// lwz r5,0(r30)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r11,0(r5)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 0, ctx, base);  // pattern-B slot 0 (byte +0)
		}
	loc_82249074:
		// lis r11,-32251
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// addi r3,r11,-1556
		ctx.r3.s64 = ctx.r11.s64 + -1556;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		return;
	}
loc_82249090:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__grc_90A0"))) PPC_WEAK_FUNC(grc_90A0);
PPC_FUNC_IMPL(__imp__grc_90A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// addi r4,r31,4
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 4;
	// lhz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x822490d4
	if (ctx.r10.u32 == 0) {
		// mr r11,r28
		ctx.r11.u64 = var_r28;
	}
loc_822490D4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// beq cr6,0x822490f0
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x822490f4
	} else {
	loc_822490F0:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_822490F4:
	// addi r4,r31,10
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 10;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U16(var_r29 + 0);
	// addi r4,r31,12
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 12;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r6,0
	// bne cr6,0x8224911c
	if (ctx.r6.u32 == 0) {
		// mr r11,r28
		ctx.r11.u64 = var_r28;
	}
loc_8224911C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 4);
	// beq cr6,0x82249138
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// b 0x8224913c
	} else {
	loc_82249138:
		// bl 0x822e3dc0
		util_3DC0(ctx, base);
	}
loc_8224913C:
	// addi r4,r31,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x82244db8
	grc_4DB8(ctx, base);
	// lhz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 40);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U16(var_r29 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82249174
	if (ctx.r9.u32 == 0) {
		// mr r11,r28
		ctx.r11.u64 = var_r28;
	}
loc_82249174:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x8224918c
	if (ctx.r7.u32 != 0) {
		// addi r3,r31,36
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 36;
		// lhz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// bl 0x8224a510
		xe_A510(ctx, base);
	}
loc_8224918C:
	// lhz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
	// cmplwi cr6,r4,0
	// beq cr6,0x8224923c
	if (ctx.r4.u32 != 0) {
		// addi r31,r31,36
		var_r31 = (uint32_t)(var_r31 + 36);
		// mr r30,r28
		var_r30 = (uint32_t)(var_r28);
	loc_822491A0:
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x82244d18
		atSingleton_4D18_w(ctx, base);
		// clrlwi r3,r3,24
		ctx.r3.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x82249214
		if (ctx.r3.u32 != 0) {
			// bl 0x820c0038
			xe_main_thread_init_0038(ctx, base);
			// lwz r11,0(r13)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
			// li r10,4
			ctx.r10.s64 = 4;
			// li r5,16
			ctx.r5.s64 = 16;
			// li r4,12
			ctx.r4.s64 = 12;
			// lwzx r3,r10,r11
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
			// lwz r8,4(r9)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// cmplwi cr6,r3,0
			// beq cr6,0x822491f4
			if (ctx.r3.u32 != 0) {
				// stw r28,4(r3)
				PPC_STORE_U32(ctx.r3.u32 + 4, var_r28);
				// sth r28,8(r3)
				PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r28);
				// sth r28,10(r3)
				PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r28);
				// b 0x822491f8
			} else {
			loc_822491F4:
				// mr r3,r28
				ctx.r3.u64 = var_r28;
			}
		loc_822491F8:
			// lhz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
			// lwz r9,0(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
			// addi r7,r11,1
			ctx.r7.s64 = ctx.r11.s64 + 1;
			// rotlwi r10,r11,2
			ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
			// sth r7,4(r31)
			PPC_STORE_U16(var_r31 + 4, ctx.r7.u16);
			// stwx r3,r10,r9
			PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r3.u32);
			// b 0x82249220
		} else {
		loc_82249214:
			// lwz r5,0(r31)
			ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 0);
			// rlwinm r4,r30,2,0,29
			ctx.r4.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
			// lwzx r3,r4,r5
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r5.u32);
		}
	loc_82249220:
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// bl 0x8224d670
		atSingleton_D670(ctx, base);
		// addi r3,r30,1
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 1;
		// lhz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r30,r3,16
		var_r30 = (uint32_t)(ctx.r3.u32 & 0xFFFF);
		// cmplw cr6,r30,r10
		// blt cr6,0x822491a0
		if (var_r30 < ctx.r10.u32) goto loc_822491A0;
	}
loc_8224923C:
	return;
}

__attribute__((alias("__imp__xe_9248"))) PPC_WEAK_FUNC(xe_9248);
PPC_FUNC_IMPL(__imp__xe_9248) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=368, savegprlr_22
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r24,r3
	var_r24 = ctx.r3.u32;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r23,0
	var_r23 = 0;
	// rlwinm r11,r9,0,0,0
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80000000;
	// li r10,32
	ctx.r10.s64 = 32;
	// cmplwi cr6,r11,0
	// stw r23,4(r24)
	PPC_STORE_U32(var_r24 + 4, var_r23);
	// bne cr6,0x8224928c
	if (ctx.r11.u32 == 0) {
		// mr r10,r23
		ctx.r10.u64 = var_r23;
	}
loc_8224928C:
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// stw r10,4(r24)
	PPC_STORE_U32(var_r24 + 4, ctx.r10.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r8,0
	// bne cr6,0x822492a4
	if (ctx.r8.u32 == 0) {
		// mr r11,r23
		ctx.r11.u64 = var_r23;
	}
loc_822492A4:
	// or r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 | ctx.r10.u64;
	// rlwinm r7,r9,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
	// li r10,16
	ctx.r10.s64 = 16;
	// cmplwi cr6,r7,0
	// stw r11,4(r24)
	PPC_STORE_U32(var_r24 + 4, ctx.r11.u32);
	// bne cr6,0x822492c0
	if (ctx.r7.u32 == 0) {
		// mr r10,r23
		ctx.r10.u64 = var_r23;
	}
loc_822492C0:
	// or r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 | ctx.r11.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// stw r6,4(r24)
	PPC_STORE_U32(var_r24 + 4, ctx.r6.u32);
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// clrlwi r11,r5,16
	ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// sth r11,10(r24)
	PPC_STORE_U16(var_r24 + 10, ctx.r11.u16);
	// lis r11,-32248
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfs f0,-25672(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25672);
	ctx.f0.f64 = double(temp.f32);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,12(r24)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r24 + 12, temp.u32);
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r24,16
	ctx.r4.s64 = (int64_t)(int32_t)var_r24 + 16;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r24,20
	ctx.r4.s64 = (int64_t)(int32_t)var_r24 + 20;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r24,24
	ctx.r4.s64 = (int64_t)(int32_t)var_r24 + 24;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r31,r11,16
	var_r31 = (uint32_t)(ctx.r11.u32 & 0xFFFF);
	// cmplwi cr6,r11,0
	// sth r31,94(r1)
	PPC_STORE_U16(ctx.r1.u32 + 94, (uint16_t)var_r31);
	// beq cr6,0x8224938c
	if (ctx.r11.u32 != 0) {
		// lis r8,16383
		ctx.r8.s64 = 1073676288;
		// rlwinm r3,r11,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// ori r7,r8,65535
		ctx.r7.u64 = ctx.r8.u64 | 65535;
		// cmplw cr6,r11,r7
		// ble cr6,0x8224937c
		if (ctx.r11.u32 > ctx.r7.u32) {
			// li r3,-1
		}
	loc_8224937C:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// mr r22,r3
		var_r22 = ctx.r3.u32;
		// b 0x82249390
	} else {
	loc_8224938C:
		// mr r22,r23
		var_r22 = (uint32_t)(var_r23);
	}
loc_82249390:
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r9,6
	ctx.r9.s64 = 6;
	// stw r22,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, var_r22);
	// rlwinm r5,r6,0,24,24
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x80;
	// sth r31,92(r1)
	PPC_STORE_U16(ctx.r1.u32 + 92, (uint16_t)var_r31);
	// cmplwi cr6,r5,0
	// bne cr6,0x822493b0
	if (ctx.r5.u32 == 0) {
		// mr r9,r23
		ctx.r9.u64 = var_r23;
	}
loc_822493B0:
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// mr r30,r23
	var_r30 = (uint32_t)(var_r23);
	// cmpwi cr6,r11,0
	// add r28,r9,r10
	var_r28 = (uint32_t)(ctx.r9.u64 + ctx.r10.u64);
	// ble cr6,0x82249450
	if (ctx.r11.s32 > 0) {
		// lis r11,-32251
		// lwz r25,0(r13)
		var_r25 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
		// li r26,4
		var_r26 = 4;
		// mr r31,r22
		var_r31 = (uint32_t)(var_r22);
		// addi r27,r11,-836
		var_r27 = (uint32_t)(ctx.r11.s64 + -836);  // lbl_8204FCBC @ 0x8204fcbc
	loc_822493D8:
		// bl 0x820c0038
		xe_main_thread_init_0038(ctx, base);
		// lwzx r3,r26,r25
		ctx.r3.u64 = PPC_LOAD_U32(var_r26 + var_r25);
		// li r5,16
		ctx.r5.s64 = 16;
		// li r4,16
		ctx.r4.s64 = 16;
		// lwz r10,4(r11)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// cmplwi cr6,r3,0
		// beq cr6,0x8224941c
		if (ctx.r3.u32 != 0) {
			// stw r23,4(r3)
			PPC_STORE_U32(ctx.r3.u32 + 4, var_r23);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r27,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, var_r27);
			// stw r23,8(r3)
			PPC_STORE_U32(ctx.r3.u32 + 8, var_r23);
			// sth r23,12(r3)
			PPC_STORE_U16(ctx.r3.u32 + 12, (uint16_t)var_r23);
			// sth r23,14(r3)
			PPC_STORE_U16(ctx.r3.u32 + 14, (uint16_t)var_r23);
			// b 0x82249420
		} else {
		loc_8224941C:
			// mr r11,r23
			ctx.r11.u64 = var_r23;
		}
	loc_82249420:
		// li r7,0
		ctx.r7.s64 = 0;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// mr r5,r28
		ctx.r5.u64 = var_r28;
		// lwz r6,84(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// rotlwi r3,r11,0
		ctx.r3.u64 = ctx.r11.u32;
		// bl 0x8224b4f8
		LocomotionStateAnim_B4F8_g(ctx, base);
		// lwz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r30,r9
		// blt cr6,0x822493d8
		if ((int32_t)var_r30 < ctx.r9.s32) goto loc_822493D8;
	}
loc_82249450:
	// lwz r8,4(r24)
	ctx.r8.u64 = PPC_LOAD_U32(var_r24 + 4);
	// rlwinm r7,r8,0,27,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r7,0
	// beq cr6,0x82249648
	if (ctx.r7.u32 != 0) {
		// lwz r30,0(r22)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r22 + 0));
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,6
		ctx.r4.s64 = 6;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8224bef8
		LocomotionStateAnim_BEF8_g(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,5
		ctx.r4.s64 = 5;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8224bef8
		LocomotionStateAnim_BEF8_g(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmplwi cr6,r31,0
		// beq cr6,0x82249648
		if (var_r31 == 0) goto loc_82249648;
		// cmplwi cr6,r30,0
		// beq cr6,0x82249648
		if (var_r30 == 0) goto loc_82249648;
		// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r5,60(r6)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// bl 0x8223ad30
		LocomotionStateAnim_AD30_g(ctx, base);
		// lwz r4,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r11,56(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// addi r9,r1,160
		ctx.r9.s64 = ctx.r1.s64 + 160;
		// lwz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lvx128 v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// li r28,1
		var_r28 = 1;
		// cmpwi cr6,r10,1
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// ble cr6,0x82249648
		if (ctx.r10.s32 <= 1) goto loc_82249648;
		// addi r29,r22,4
		var_r29 = (uint32_t)(var_r22 + 4);
	loc_822494F0:
		// cmplwi cr6,r31,0
		// beq cr6,0x82249648
		if (var_r31 == 0) goto loc_82249648;
		// cmplwi cr6,r30,0
		// beq cr6,0x82249648
		if (var_r30 == 0) goto loc_82249648;
		// lwz r30,0(r29)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0));
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,6
		ctx.r4.s64 = 6;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8224bef8
		LocomotionStateAnim_BEF8_g(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,5
		ctx.r4.s64 = 5;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8224bef8
		LocomotionStateAnim_BEF8_g(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmplwi cr6,r31,0
		// beq cr6,0x82249634
		if (var_r31 != 0) {
			// cmplwi cr6,r30,0
			// beq cr6,0x82249634
			if (var_r30 == 0) goto loc_82249634;
			// lwz r7,56(r8)
			// bctrl
			VCALL(ctx.r3.u32, 14, ctx, base);  // vtable slot 14 (byte +56)
			// addi r6,r1,160
			ctx.r6.s64 = ctx.r1.s64 + 160;
			// addi r5,r1,144
			ctx.r5.s64 = ctx.r1.s64 + 144;
			// addi r4,r1,112
			ctx.r4.s64 = ctx.r1.s64 + 112;
			// addi r10,r1,128
			ctx.r10.s64 = ctx.r1.s64 + 128;
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// lvx128 v13,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,160
			ctx.r9.s64 = ctx.r1.s64 + 160;
			// lvx128 v0,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lvx128 v10,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v9,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmrghw v12,v10,v0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
			// vmrghw v11,v9,v13
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
			// vmrglw v10,v10,v0
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
			// lvx128 v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmrglw v9,v9,v13
			simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
			// vmrghw v8,v12,v11
			simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
			// vmrglw v12,v12,v11
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
			// vmrghw v10,v10,v9
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
			// vmsum3fp128 v11,v0,v8
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
			// vmsum3fp128 v12,v0,v12
			simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
			// vmsum3fp128 v10,v0,v10
			simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
			// vmrghw v0,v11,v10
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
			// vmrghw v12,v12,v0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
			// vmrghw v0,v0,v12
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
			// vaddfp v13,v13,v0
			simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v13,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r7,60(r8)
			// bctrl
			VCALL(ctx.r3.u32, 15, ctx, base);  // vtable slot 15 (byte +60)
			// mr r4,r3
			ctx.r4.u64 = ctx.r3.u64;
			// addi r3,r1,208
			ctx.r3.s64 = ctx.r1.s64 + 208;
			// bl 0x8223ad30
			LocomotionStateAnim_AD30_g(ctx, base);
			// addi r4,r1,192
			ctx.r4.s64 = ctx.r1.s64 + 192;
			// addi r3,r1,208
			ctx.r3.s64 = ctx.r1.s64 + 208;
			// bl 0x82137b08
			phBoundCapsule_7B08_g(ctx, base);
			// addi r3,r1,208
			ctx.r3.s64 = ctx.r1.s64 + 208;
			// lfs f1,196(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
			ctx.f1.f64 = double(temp.f32);
			// bl 0x820c3f98
			phBoundCapsule_3F98_g(ctx, base);
			// addi r4,r1,208
			ctx.r4.s64 = ctx.r1.s64 + 208;
			// addi r3,r1,112
			ctx.r3.s64 = ctx.r1.s64 + 112;
			// bl 0x820c3bc8
			phJoint_3BC8_g(ctx, base);
			// addi r4,r1,112
			ctx.r4.s64 = ctx.r1.s64 + 112;
			// addi r3,r1,176
			ctx.r3.s64 = ctx.r1.s64 + 176;
			// bl 0x8223b218
			LocomotionStateAnim_B218_g(ctx, base);
			// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
			// addi r4,r1,176
			ctx.r4.s64 = ctx.r1.s64 + 176;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r5,72(r6)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 18, ctx, base);  // pattern-B slot 18 (byte +72)
			// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
			// addi r4,r1,160
			ctx.r4.s64 = ctx.r1.s64 + 160;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r10,68(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		}
	loc_82249634:
		// lwz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpw cr6,r28,r9
		// blt cr6,0x822494f0
		if ((int32_t)var_r28 < ctx.r9.s32) goto loc_822494F0;
	}
loc_82249648:
	// lis r11,-32253
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r24
	ctx.r3.u64 = var_r24;
	// lfs f1,-12292(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12292);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82249a18
	xe_9A18(ctx, base);
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r23
	var_r31 = (uint32_t)(var_r23);
	// cmpwi cr6,r8,0
	// ble cr6,0x822496a8
	if (ctx.r8.s32 > 0) {
		// mr r30,r22
		var_r30 = (uint32_t)(var_r22);
	loc_82249674:
		// lwz r3,0(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
		// cmplwi cr6,r3,0
		// beq cr6,0x82249694
		if (ctx.r3.u32 != 0) {
			// lwz r7,0(r3)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r6,0(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 0, ctx, base);  // pattern-B slot 0 (byte +0)
		}
	loc_82249694:
		// lwz r5,80(r1)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r31,r5
		// blt cr6,0x82249674
		if ((int32_t)var_r31 < ctx.r5.s32) goto loc_82249674;
	}
loc_822496A8:
	// cmplwi cr6,r22,0
	// beq cr6,0x822496e4
	if (var_r22 != 0) {
		// mr r3,r22
		ctx.r3.u64 = var_r22;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r4,r3,24
		ctx.r4.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// bne cr6,0x822496e4
		if (ctx.r4.u32 != 0) goto loc_822496E4;
		// lwz r3,0(r13)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r11,4
		ctx.r11.s64 = 4;
		// mr r4,r22
		ctx.r4.u64 = var_r22;
		// lwzx r3,r11,r3
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
		// lwz r9,8(r10)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_822496E4:
	// lhz r8,10(r24)
	ctx.r8.u64 = PPC_LOAD_U16(var_r24 + 10);
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// cmplwi cr6,r8,1
	// bne cr6,0x82249700
	if (ctx.r8.u32 == 1) {
		// lwz r7,4(r24)
		ctx.r7.u64 = PPC_LOAD_U32(var_r24 + 4);
		// ori r6,r7,1
		ctx.r6.u64 = ctx.r7.u64 | 1;
		// stw r6,4(r24)
		PPC_STORE_U32(var_r24 + 4, ctx.r6.u32);
	}
loc_82249700:
	return;
}

__attribute__((alias("__imp__rage_9708"))) PPC_WEAK_FUNC(rage_9708);
PPC_FUNC_IMPL(__imp__rage_9708) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=176, savegprlr_23
	// lis r11,-32254
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// addi r5,r11,30308
	ctx.r5.s64 = ctx.r11.s64 + 30308;
	// lis r11,-32163
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// addi r3,r11,128
	ctx.r3.s64 = ctx.r11.s64 + 128;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// bl 0x822e3040
	rage_obj_factory_create_3040(ctx, base);
	// mr r23,r3
	var_r23 = ctx.r3.u32;
	// cmplwi cr6,r23,0
	// bne cr6,0x8224974c
	if (var_r23 == 0) {
		return;
	}
loc_8224974C:
	// li r28,0
	var_r28 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r23
	ctx.r3.u64 = var_r23;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, var_r28);
	// bl 0x822e3cd8
	grc_3CD8(ctx, base);
	// lis r11,12617
	ctx.r11.s64 = 826867712;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r24,r28
	var_r24 = (uint32_t)(var_r28);
	// ori r11,r11,20033
	ctx.r11.u64 = ctx.r11.u64 | 20033;
	// cmpw cr6,r5,r11
	// bgt cr6,0x82249954
	if (ctx.r5.s32 <= ctx.r11.s32) {
		// beq cr6,0x822497b8
		if (!(ctx.cr6.eq)) {
			// lis r10,105
			ctx.r10.s64 = 6881280;
			// ori r9,r10,28257
			ctx.r9.u64 = ctx.r10.u64 | 28257;
			// cmpw cr6,r5,r9
			// beq cr6,0x822497ac
			if (ctx.r5.s32 != ctx.r9.s32) {
				// lis r8,112
				ctx.r8.s64 = 7340032;
				// ori r7,r8,28003
				ctx.r7.u64 = ctx.r8.u64 | 28003;
				// cmpw cr6,r5,r7
				// bne cr6,0x82249974
				if (ctx.r5.s32 != ctx.r7.s32) goto loc_82249974;
				// lis r11,-32251
				ctx.r11.s64 = -2113601536;
				// addi r3,r11,-1536
				ctx.r3.s64 = ctx.r11.s64 + -1536;
				// b 0x822499f8
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// bl 0x8240e6d0
				nop_8240E6D0(ctx, base);
				// mr r3,r23
				ctx.r3.u64 = var_r23;
				// bl 0x822e3b38
				rage_obj_finalize_3B38(ctx, base);
				// mr r3,r24
				ctx.r3.u64 = var_r24;
				return;
			}
		loc_822497AC:
			// lis r11,-32251
			ctx.r11.s64 = -2113601536;
			// addi r3,r11,-1408
			ctx.r3.s64 = ctx.r11.s64 + -1408;
			// b 0x822499f8
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// bl 0x8240e6d0
			nop_8240E6D0(ctx, base);
			// mr r3,r23
			ctx.r3.u64 = var_r23;
			// bl 0x822e3b38
			rage_obj_finalize_3B38(ctx, base);
			// mr r3,r24
			ctx.r3.u64 = var_r24;
			return;
		}
	loc_822497B8:
		// lis r11,-32251
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// addi r3,r11,-1064
		ctx.r3.s64 = ctx.r11.s64 + -1064;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
		// lwz r31,0(r13)
		var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
		// li r27,4
		var_r27 = 4;
		// li r26,8
		var_r26 = 8;
		// lwzx r11,r31,r27
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + var_r27);
		// lwzx r10,r31,r26
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + var_r26);
		// cmplw cr6,r11,r10
		// bne cr6,0x822497f8
		if (ctx.r11.u32 == ctx.r10.u32) {
			// li r11,12
			ctx.r11.s64 = 12;
			// lwzx r10,r31,r11
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
			// addi r6,r10,1
			ctx.r6.s64 = ctx.r10.s64 + 1;
			// stwx r6,r31,r11
			PPC_STORE_U32(var_r31 + ctx.r11.u32, ctx.r6.u32);
			// b 0x82249804
		} else {
		loc_822497F8:
			// li r5,16
			ctx.r5.s64 = 16;
			// stwx r10,r31,r27
			PPC_STORE_U32(var_r31 + var_r27, ctx.r10.u32);
			// stwx r11,r31,r5
			PPC_STORE_U32(var_r31 + ctx.r5.u32, ctx.r11.u32);
		}
	loc_82249804:
		// li r3,48
		ctx.r3.s64 = 48;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x82249864
		if (ctx.r3.u32 != 0) {
			// lis r11,-32253
			// stw r28,4(r3)
			PPC_STORE_U32(ctx.r3.u32 + 4,/* rage_GameObject::flags@+0x4 */ var_r28);
			// li r4,1
			ctx.r4.s64 = 1;
			// sth r28,10(r3)
			PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r28);
			// mr r30,r3
			var_r30 = ctx.r3.u32;
			// lfs f0,-12016(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
			ctx.f0.f64 = double(temp.f32);
			// lis r11,-32251
			// stfs f0,12(r3)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
			// sth r4,8(r3)
			PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r4.u16);
			// addi r11,r11,-848
			ctx.r11.s64 = ctx.r11.s64 + -848;
			// stw r11,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0,/* rage_GameObject::vtable@+0x0 */ ctx.r11.u32);
			// stfs f0,16(r3)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
			// stfs f0,20(r3)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
			// stfs f0,24(r3)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
			// stw r28,32(r3)
			PPC_STORE_U32(ctx.r3.u32 + 32, var_r28);
			// stw r28,36(r3)
			PPC_STORE_U32(ctx.r3.u32 + 36, var_r28);
			// sth r28,40(r3)
			PPC_STORE_U16(ctx.r3.u32 + 40, (uint16_t)var_r28);
			// sth r28,42(r3)
			PPC_STORE_U16(ctx.r3.u32 + 42, (uint16_t)var_r28);
			// stw r28,44(r3)
			PPC_STORE_U32(ctx.r3.u32 + 44, var_r28);
			// b 0x82249868
		} else {
		loc_82249864:
			// mr r30,r28
			var_r30 = (uint32_t)(var_r28);
		}
	loc_82249868:
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// li r5,0
		ctx.r5.s64 = 0;
		// mr r4,r23
		ctx.r4.u64 = var_r23;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stw r11,32(r30)
		PPC_STORE_U32(var_r30 + 32, ctx.r11.u32);
		// bl 0x82249248
		xe_9248(ctx, base);
		// li r29,12
		var_r29 = 12;
		// mr r24,r3
		var_r24 = ctx.r3.u32;
		// lwzx r11,r31,r29
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + var_r29);
		// cmpwi cr6,r11,0
		// beq cr6,0x822498a8
		if (ctx.r11.s32 != 0) {
			// addi r3,r11,-1
			ctx.r3.s64 = ctx.r11.s64 + -1;
			// stwx r3,r31,r29
			PPC_STORE_U32(var_r31 + var_r29, ctx.r3.u32);
			// b 0x822498b8
		} else {
		loc_822498A8:
			// li r11,16
			ctx.r11.s64 = 16;
			// lwzx r10,r31,r11
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
			// stwx r28,r31,r11
			PPC_STORE_U32(var_r31 + ctx.r11.u32, var_r28);
			// stwx r10,r31,r27
			PPC_STORE_U32(var_r31 + var_r27, ctx.r10.u32);
		}
	loc_822498B8:
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// bl 0x82248588
		atSingleton_8588_g_8588_1(ctx, base);
		// lwzx r11,r31,r27
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + var_r27);
		// lwzx r10,r31,r26
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + var_r26);
		// cmplw cr6,r11,r10
		// bne cr6,0x822498e4
		if (ctx.r11.u32 == ctx.r10.u32) {
			// lwzx r11,r31,r29
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + var_r29);
			// addi r9,r11,1
			ctx.r9.s64 = ctx.r11.s64 + 1;
			// stwx r9,r31,r29
			PPC_STORE_U32(var_r31 + var_r29, ctx.r9.u32);
			// b 0x822498f0
		} else {
		loc_822498E4:
			// li r8,16
			ctx.r8.s64 = 16;
			// stwx r10,r31,r27
			PPC_STORE_U32(var_r31 + var_r27, ctx.r10.u32);
			// stwx r11,r31,r8
			PPC_STORE_U32(var_r31 + ctx.r8.u32, ctx.r11.u32);
		}
	loc_822498F0:
		// lwz r7,0(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r6,0(r7)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
		// lwzx r11,r31,r29
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + var_r29);
		// cmpwi cr6,r11,0
		// beq cr6,0x82249930
		if (ctx.r11.s32 != 0) {
			// addi r5,r11,-1
			ctx.r5.s64 = ctx.r11.s64 + -1;
			// mr r3,r23
			ctx.r3.u64 = var_r23;
			// stwx r5,r31,r29
			PPC_STORE_U32(var_r31 + var_r29, ctx.r5.u32);
			// bl 0x822e3b38
			rage_obj_finalize_3B38(ctx, base);
			// mr r3,r24
			ctx.r3.u64 = var_r24;
			return;
		}
	loc_82249930:
		// li r11,16
		ctx.r11.s64 = 16;
		// mr r3,r23
		ctx.r3.u64 = var_r23;
		// lwzx r4,r31,r11
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
		// stwx r28,r31,r11
		PPC_STORE_U32(var_r31 + ctx.r11.u32, var_r28);
		// stwx r4,r31,r27
		PPC_STORE_U32(var_r31 + var_r27, ctx.r4.u32);
		// bl 0x822e3b38
		rage_obj_finalize_3B38(ctx, base);
		// mr r3,r24
		ctx.r3.u64 = var_r24;
		return;
	}
loc_82249954:
	// lis r3,13641
	ctx.r3.s64 = 893976576;
	// ori r11,r3,20033
	ctx.r11.u64 = ctx.r3.u64 | 20033;
	// cmpw cr6,r5,r11
	// beq cr6,0x822499f0
	if (ctx.r5.s32 != ctx.r11.s32) {
		// lis r10,14409
		ctx.r10.s64 = 944308224;
		// ori r9,r10,20033
		ctx.r9.u64 = ctx.r10.u64 | 20033;
		// cmpw cr6,r5,r9
		// beq cr6,0x82249998
		if (ctx.r5.s32 != ctx.r9.s32) {
		loc_82249974:
			// lis r11,-32251
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// addi r3,r11,-1152
			ctx.r3.s64 = ctx.r11.s64 + -1152;
			// bl 0x8240e6d0
			nop_8240E6D0(ctx, base);
			// mr r3,r23
			ctx.r3.u64 = var_r23;
			// bl 0x822e3b38
			rage_obj_finalize_3B38(ctx, base);
			// mr r3,r24
			ctx.r3.u64 = var_r24;
			return;
		}
	loc_82249998:
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x820c29e0
		atSingleton_29E0_g(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// li r5,1
		ctx.r5.s64 = 1;
		// stb r28,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r28);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r23
		ctx.r3.u64 = var_r23;
		// stw r11,32(r25)
		PPC_STORE_U32(var_r25 + 32, ctx.r11.u32);
		// bl 0x822e3828
		rage_obj_bind_3828(ctx, base);
		// lbz r7,80(r1)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
		// li r24,1
		var_r24 = 1;
		// addi r4,r1,88
		ctx.r4.s64 = ctx.r1.s64 + 88;
		// stw r23,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r23);
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// sth r7,90(r1)
		PPC_STORE_U16(ctx.r1.u32 + 90, ctx.r7.u16);
		// sth r24,88(r1)
		PPC_STORE_U16(ctx.r1.u32 + 88, (uint16_t)var_r24);
		// bl 0x822490a0
		grc_90A0(ctx, base);
		// mr r3,r23
		ctx.r3.u64 = var_r23;
		// bl 0x822e3b38
		rage_obj_finalize_3B38(ctx, base);
		// mr r3,r24
		ctx.r3.u64 = var_r24;
		return;
	}
loc_822499F0:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r3,r11,-1280
	ctx.r3.s64 = ctx.r11.s64 + -1280;
loc_822499F8:
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
	// mr r3,r23
	ctx.r3.u64 = var_r23;
	// bl 0x822e3b38
	rage_obj_finalize_3B38(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = var_r24;
	return;
}

__attribute__((alias("__imp__xe_9A18"))) PPC_WEAK_FUNC(xe_9A18);
PPC_FUNC_IMPL(__imp__xe_9A18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=240, savegprlr_17
	// mr r26,r4
	var_r26 = ctx.r4.u32;
	// mr r21,r3
	var_r21 = ctx.r3.u32;
	// li r18,1
	var_r18 = 1;
	// addi r3,r21,36
	ctx.r3.s64 = (int64_t)(int32_t)var_r21 + 36;
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(var_r26 + 0);
	// lhz r11,40(r21)
	ctx.r11.u64 = PPC_LOAD_U16(var_r21 + 40);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lhz r31,12(r9)
	var_r31 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 12));
	// add r4,r11,r31
	ctx.r4.u64 = ctx.r11.u64 + var_r31;
	// bl 0x8224a510
	xe_A510(ctx, base);
	// cmpwi cr6,r31,0
	// ble cr6,0x82249ee8
	if ((int32_t)var_r31 > 0) {
		// lis r11,-32253
		// li r23,0
		var_r23 = 0;
		// mr r17,r31
		var_r17 = (uint32_t)(var_r31);
		// mr r20,r23
		var_r20 = (uint32_t)(var_r23);
		// li r19,2
		var_r19 = 2;
		// lfs f31,-12292(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12292);
		var_f31 = double(temp.f32);
	loc_82249A70:
		// lwz r11,0(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 0);
		// lwz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r7,8(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// lwzx r31,r7,r20
		var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + var_r20));
		// lbz r6,4(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 4);
		// clrlwi r11,r6,28
		ctx.r11.u64 = ctx.r6.u32 & 0xF;
		// cmplwi cr6,r11,1
		// blt cr6,0x82249d48
		if (ctx.r11.u32 >= 1) {
			// beq cr6,0x82249c04
			if (!(ctx.cr6.eq)) {
				// cmplwi cr6,r11,3
				// bge cr6,0x82249ed4
				if (ctx.r11.u32 >= 3) goto loc_82249ED4;
				// bl 0x820c0038
				xe_main_thread_init_0038(ctx, base);
				// lwz r25,0(r13)
				var_r25 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
				// li r27,4
				var_r27 = 4;
				// li r5,16
				ctx.r5.s64 = 16;
				// li r4,12
				ctx.r4.s64 = 12;
				// lwzx r3,r27,r25
				ctx.r3.u64 = PPC_LOAD_U32(var_r27 + var_r25);
				// lwz r10,4(r11)
				// bctrl
				VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
				// cmplwi cr6,r3,0
				// beq cr6,0x82249ae0
				if (ctx.r3.u32 != 0) {
					// mr r22,r3
					var_r22 = ctx.r3.u32;
					// stw r23,4(r3)
					PPC_STORE_U32(ctx.r3.u32 + 4, var_r23);
					// sth r23,8(r3)
					PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r23);
					// sth r23,10(r3)
					PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r23);
					// b 0x82249ae4
				} else {
				loc_82249AE0:
					// mr r22,r23
					var_r22 = (uint32_t)(var_r23);
				}
			loc_82249AE4:
				// addi r3,r1,80
				ctx.r3.s64 = ctx.r1.s64 + 80;
				// lhz r4,10(r21)
				ctx.r4.u64 = PPC_LOAD_U16(var_r21 + 10);
				// stw r23,80(r1)
				PPC_STORE_U32(ctx.r1.u32 + 80, var_r23);
				// sth r23,84(r1)
				PPC_STORE_U16(ctx.r1.u32 + 84, (uint16_t)var_r23);
				// sth r23,86(r1)
				PPC_STORE_U16(ctx.r1.u32 + 86, (uint16_t)var_r23);
				// bl 0x8240a250
				xe_A250(ctx, base);
				// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r8,64(r9)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 16, ctx, base);  // pattern-B slot 16 (byte +64)
				// lwz r28,80(r1)
				var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
				// li r29,1
				var_r29 = 1;
				// stfs f1,0(r28)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f1.f64);
				PPC_STORE_U32(var_r28 + 0, temp.u32);
				// lhz r11,4(r26)
				ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
				// cmpwi cr6,r11,1
				// ble cr6,0x82249b84
				if (ctx.r11.s32 > 1) {
					// li r30,4
					var_r30 = 4;
				loc_82249B2C:
					// lwz r7,0(r26)
					ctx.r7.u64 = PPC_LOAD_U32(var_r26 + 0);
					// lhz r5,6(r31)
					ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
					// lbz r4,5(r31)
					ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 5);
					// lwzx r3,r7,r30
					ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r30);
					// bl 0x8224bef8
					LocomotionStateAnim_BEF8_g(ctx, base);
					// cmplwi cr6,r3,0
					// beq cr6,0x82249b58
					if (ctx.r3.u32 != 0) {
						// lwz r6,0(r3)
						ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
						// lwz r5,64(r6)
						ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 64);
						// mtctr r5
						ctx.ctr.u64 = ctx.r5.u64;
						// b 0x82249b68
					} else {
					loc_82249B58:
						// lwz r4,0(r31)
						ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
						// mr r3,r31
						ctx.r3.u64 = var_r31;
						// lwz r11,64(r4)
						ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 64);
						// mtctr r11
						ctx.ctr.u64 = ctx.r11.u64;
					}
				loc_82249B68:
					// bctrl
					ctx.lr = 0x82249B6C;
					PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
					// stfsx f1,r28,r30
					ctx.fpscr.disableFlushMode();
					temp.f32 = float(ctx.f1.f64);
					PPC_STORE_U32(var_r28 + var_r30, temp.u32);
					// lhz r11,4(r26)
					ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
					// addi r29,r29,1
					var_r29 = (uint32_t)(var_r29 + 1);
					// addi r30,r30,4
					var_r30 = (uint32_t)(var_r30 + 4);
					// cmpw cr6,r29,r11
					// blt cr6,0x82249b2c
					if ((int32_t)var_r29 < ctx.r11.s32) goto loc_82249B2C;
				}
			loc_82249B84:
				// mr r3,r22
				ctx.r3.u64 = var_r22;
				// lhz r30,6(r31)
				var_r30 = (uint32_t)(PPC_LOAD_U16(var_r31 + 6));
				// lbz r31,5(r31)
				var_r31 = (uint32_t)(PPC_LOAD_U8(var_r31 + 5));
				// bl 0x8224d5b8
				xe_D5B8(ctx, base);
				// li r6,0
				ctx.r6.s64 = 0;
				// mr r4,r28
				ctx.r4.u64 = var_r28;
				// lhz r5,84(r1)
				ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
				// mr r3,r22
				ctx.r3.u64 = var_r22;
				// fmr f1,f31
				ctx.fpscr.disableFlushMode();
				ctx.f1.f64 = var_f31;
				// stb r31,0(r22)
				PPC_STORE_U8(var_r22 + 0, (uint8_t)var_r31);
				// sth r30,2(r22)
				PPC_STORE_U16(var_r22 + 2, (uint16_t)var_r30);
				// stb r19,1(r22)
				PPC_STORE_U8(var_r22 + 1, (uint8_t)var_r19);
				// bl 0x8224d850
				xe_D850(ctx, base);
				// mr r31,r3
				var_r31 = ctx.r3.u32;
				// li r4,1
				ctx.r4.s64 = 1;
				// addi r3,r22,4
				ctx.r3.s64 = (int64_t)(int32_t)var_r22 + 4;
				// bl 0x8224de98
				atSingleton_DE98_g(ctx, base);
				// lhz r8,86(r1)
				ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
				// stw r31,0(r3)
				PPC_STORE_U32(ctx.r3.u32 + 0, var_r31);
				// cmplwi cr6,r8,0
				// beq cr6,0x82249e90
				if (ctx.r8.u32 == 0) goto loc_82249E90;
				// mr r3,r28
				ctx.r3.u64 = var_r28;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r7,r3,24
				ctx.r7.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r7,0
				// bne cr6,0x82249e90
				if (ctx.r7.u32 != 0) goto loc_82249E90;
				// lwzx r3,r27,r25
				ctx.r3.u64 = PPC_LOAD_U32(var_r27 + var_r25);
				// mr r4,r28
				ctx.r4.u64 = var_r28;
				// lwz r6,0(r3)
				ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
				// lwz r5,8(r6)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
				// mtctr r5
				ctx.ctr.u64 = ctx.r5.u64;
				// b 0x82249e8c
				goto loc_82249E8C;
			}
		loc_82249C04:
			// bl 0x820c0038
			xe_main_thread_init_0038(ctx, base);
			// lwz r24,0(r13)
			var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
			// li r25,4
			var_r25 = 4;
			// li r5,16
			ctx.r5.s64 = 16;
			// li r4,12
			ctx.r4.s64 = 12;
			// lwzx r3,r25,r24
			ctx.r3.u64 = PPC_LOAD_U32(var_r25 + var_r24);
			// lwz r10,4(r11)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// cmplwi cr6,r3,0
			// beq cr6,0x82249c48
			if (ctx.r3.u32 != 0) {
				// mr r22,r3
				var_r22 = ctx.r3.u32;
				// stw r23,4(r3)
				PPC_STORE_U32(ctx.r3.u32 + 4, var_r23);
				// sth r23,8(r3)
				PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r23);
				// sth r23,10(r3)
				PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r23);
				// b 0x82249c4c
			} else {
			loc_82249C48:
				// mr r22,r23
				var_r22 = (uint32_t)(var_r23);
			}
		loc_82249C4C:
			// addi r3,r1,88
			ctx.r3.s64 = ctx.r1.s64 + 88;
			// lhz r4,10(r21)
			ctx.r4.u64 = PPC_LOAD_U16(var_r21 + 10);
			// stw r23,88(r1)
			PPC_STORE_U32(ctx.r1.u32 + 88, var_r23);
			// sth r23,92(r1)
			PPC_STORE_U16(ctx.r1.u32 + 92, (uint16_t)var_r23);
			// sth r23,94(r1)
			PPC_STORE_U16(ctx.r1.u32 + 94, (uint16_t)var_r23);
			// bl 0x82247c20
			xe_7C20(ctx, base);
			// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r8,60(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
			// ld r7,0(r3)
			ctx.r7.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
			// lwz r27,88(r1)
			var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
			// li r29,1
			var_r29 = 1;
			// std r7,0(r27)
			PPC_STORE_U64(var_r27 + 0, ctx.r7.u64);
			// ld r6,8(r3)
			ctx.r6.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
			// std r6,8(r27)
			PPC_STORE_U64(var_r27 + 8, ctx.r6.u64);
			// lhz r11,4(r26)
			ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
			// cmpwi cr6,r11,1
			// ble cr6,0x82249d10
			if (ctx.r11.s32 > 1) {
				// li r30,4
				var_r30 = 4;
				// addi r28,r27,16
				var_r28 = (uint32_t)(var_r27 + 16);
			loc_82249CA4:
				// lwz r3,0(r26)
				ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 0);
				// lhz r5,6(r31)
				ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
				// lbz r4,5(r31)
				ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 5);
				// lwzx r3,r3,r30
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r30);
				// bl 0x8224bef8
				LocomotionStateAnim_BEF8_g(ctx, base);
				// cmplwi cr6,r3,0
				// beq cr6,0x82249cd0
				if (ctx.r3.u32 != 0) {
					// lwz r11,0(r3)
					ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
					// lwz r10,60(r11)
					ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 60);
					// mtctr r10
					ctx.ctr.u64 = ctx.r10.u64;
					// b 0x82249ce0
				} else {
				loc_82249CD0:
					// lwz r9,0(r31)
					ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// lwz r8,60(r9)
					ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 60);
					// mtctr r8
					ctx.ctr.u64 = ctx.r8.u64;
				}
			loc_82249CE0:
				// bctrl
				ctx.lr = 0x82249CE4;
				PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
				// ld r7,0(r3)
				ctx.r7.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
				// mr r11,r28
				ctx.r11.u64 = var_r28;
				// addi r29,r29,1
				var_r29 = (uint32_t)(var_r29 + 1);
				// addi r30,r30,4
				var_r30 = (uint32_t)(var_r30 + 4);
				// addi r28,r28,16
				var_r28 = (uint32_t)(var_r28 + 16);
				// std r7,0(r11)
				PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r7.u64);
				// ld r6,8(r3)
				ctx.r6.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
				// std r6,8(r11)
				PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r6.u64);
				// lhz r11,4(r26)
				ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
				// cmpw cr6,r29,r11
				// blt cr6,0x82249ca4
				if ((int32_t)var_r29 < ctx.r11.s32) goto loc_82249CA4;
			}
		loc_82249D10:
			// addi r6,r1,88
			ctx.r6.s64 = ctx.r1.s64 + 88;
			// lhz r5,6(r31)
			ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// lbz r4,5(r31)
			ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 5);
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// bl 0x8224db50
			xe_DB50(ctx, base);
			// lhz r4,94(r1)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
			// cmplwi cr6,r4,0
			// beq cr6,0x82249e90
			if (ctx.r4.u32 == 0) goto loc_82249E90;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r3,r3,24
			ctx.r3.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r3,0
			ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
			// b 0x82249e74
		} else {
		loc_82249D48:
			// bl 0x820c0038
			xe_main_thread_init_0038(ctx, base);
			// lwz r24,0(r13)
			var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
			// li r25,4
			var_r25 = 4;
			// li r5,16
			ctx.r5.s64 = 16;
			// li r4,12
			ctx.r4.s64 = 12;
			// lwzx r3,r25,r24
			ctx.r3.u64 = PPC_LOAD_U32(var_r25 + var_r24);
			// lwz r8,4(r9)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// cmplwi cr6,r3,0
			// beq cr6,0x82249d8c
			if (ctx.r3.u32 != 0) {
				// mr r22,r3
				var_r22 = ctx.r3.u32;
				// stw r23,4(r3)
				PPC_STORE_U32(ctx.r3.u32 + 4, var_r23);
				// sth r23,8(r3)
				PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r23);
				// sth r23,10(r3)
				PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r23);
				// b 0x82249d90
			} else {
			loc_82249D8C:
				// mr r22,r23
				var_r22 = (uint32_t)(var_r23);
			}
		loc_82249D90:
			// addi r3,r1,96
			ctx.r3.s64 = ctx.r1.s64 + 96;
			// lhz r4,10(r21)
			ctx.r4.u64 = PPC_LOAD_U16(var_r21 + 10);
			// stw r23,96(r1)
			PPC_STORE_U32(ctx.r1.u32 + 96, var_r23);
			// sth r23,100(r1)
			PPC_STORE_U16(ctx.r1.u32 + 100, (uint16_t)var_r23);
			// sth r23,102(r1)
			PPC_STORE_U16(ctx.r1.u32 + 102, (uint16_t)var_r23);
			// bl 0x82247c20
			xe_7C20(ctx, base);
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,56(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 14, ctx, base);  // pattern-B slot 14 (byte +56)
			// lwz r27,96(r1)
			var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 96));
			// lvx128 v0,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// li r29,1
			var_r29 = 1;
			// stvx v0,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lhz r11,4(r26)
			ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
			// cmpwi cr6,r11,1
			// ble cr6,0x82249e40
			if (ctx.r11.s32 > 1) {
				// li r30,4
				var_r30 = 4;
				// addi r28,r27,16
				var_r28 = (uint32_t)(var_r27 + 16);
			loc_82249DE0:
				// lwz r3,0(r26)
				ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 0);
				// lhz r5,6(r31)
				ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
				// lbz r4,5(r31)
				ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 5);
				// lwzx r3,r3,r30
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r30);
				// bl 0x8224bef8
				LocomotionStateAnim_BEF8_g(ctx, base);
				// cmplwi cr6,r3,0
				// beq cr6,0x82249e0c
				if (ctx.r3.u32 != 0) {
					// lwz r11,0(r3)
					ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
					// lwz r10,56(r11)
					ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 56);
					// mtctr r10
					ctx.ctr.u64 = ctx.r10.u64;
					// b 0x82249e1c
				} else {
				loc_82249E0C:
					// lwz r9,0(r31)
					ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// lwz r8,56(r9)
					ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 56);
					// mtctr r8
					ctx.ctr.u64 = ctx.r8.u64;
				}
			loc_82249E1C:
				// bctrl
				ctx.lr = 0x82249E20;
				PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
				// lvx128 v13,r0,r3
				ea = (ctx.r3.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx v13,r0,r28
				ea = (var_r28) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lhz r11,4(r26)
				ctx.r11.u64 = PPC_LOAD_U16(var_r26 + 4);
				// addi r29,r29,1
				var_r29 = (uint32_t)(var_r29 + 1);
				// addi r30,r30,4
				var_r30 = (uint32_t)(var_r30 + 4);
				// addi r28,r28,16
				var_r28 = (uint32_t)(var_r28 + 16);
				// cmpw cr6,r29,r11
				// blt cr6,0x82249de0
				if ((int32_t)var_r29 < ctx.r11.s32) goto loc_82249DE0;
			}
		loc_82249E40:
			// addi r6,r1,96
			ctx.r6.s64 = ctx.r1.s64 + 96;
			// lhz r5,6(r31)
			ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// lbz r4,5(r31)
			ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 5);
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// bl 0x8224da80
			xe_DA80(ctx, base);
			// lhz r6,102(r1)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
			// cmplwi cr6,r6,0
			// beq cr6,0x82249e90
			if (ctx.r6.u32 == 0) goto loc_82249E90;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r5,r3,24
			ctx.r5.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r5,0
			ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
		}
	loc_82249E74:
		// bne cr6,0x82249e90
		if (ctx.cr6.eq) {
			// lwzx r3,r25,r24
			ctx.r3.u64 = PPC_LOAD_U32(var_r25 + var_r24);
			// mr r4,r27
			ctx.r4.u64 = var_r27;
			// lwz r11,0(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// lwz r10,8(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			// mtctr r10
			ctx.ctr.u64 = ctx.r10.u64;
		loc_82249E8C:
			// bctrl
			ctx.lr = 0x82249E90;
			PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
		}
	loc_82249E90:
		// cmplwi cr6,r22,0
		// beq cr6,0x82249ed8
		if (var_r22 != 0) {
			// mr r4,r22
			ctx.r4.u64 = var_r22;
			// mr r3,r21
			ctx.r3.u64 = var_r21;
			// bl 0x8224a438
			xe_A438(ctx, base);
			// clrlwi r9,r3,24
			ctx.r9.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r9,0
			// bne cr6,0x82249ed8
			if (ctx.r9.u32 != 0) goto loc_82249ED8;
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// bl 0x8224d5b8
			xe_D5B8(ctx, base);
			// lhz r8,10(r22)
			ctx.r8.u64 = PPC_LOAD_U16(var_r22 + 10);
			// cmplwi cr6,r8,0
			// beq cr6,0x82249ecc
			if (ctx.r8.u32 != 0) {
				// lwz r3,4(r22)
				ctx.r3.u64 = PPC_LOAD_U32(var_r22 + 4);
				// bl 0x820c00c0
				rage_free_00C0(ctx, base);
			}
		loc_82249ECC:
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		loc_82249ED4:
			// mr r18,r23
			var_r18 = (uint32_t)(var_r23);
		}
	loc_82249ED8:
		// addi r17,r17,-1
		var_r17 = (uint32_t)(var_r17 + -1);
		// addi r20,r20,4
		var_r20 = (uint32_t)(var_r20 + 4);
		// cmplwi cr6,r17,0
		// bne cr6,0x82249a70
		if (var_r17 != 0) goto loc_82249A70;
	}
loc_82249EE8:
	// mr r3,r18
	ctx.r3.u64 = var_r18;
	return;
}

__attribute__((alias("__imp__xe_9EF8"))) PPC_WEAK_FUNC(xe_9EF8);
PPC_FUNC_IMPL(__imp__xe_9EF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r23 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=240, savegprlr_17
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r20,r3
	var_r20 = ctx.r3.u32;
	// li r18,1
	var_r18 = 1;
	// addi r3,r20,36
	ctx.r3.s64 = (int64_t)(int32_t)var_r20 + 36;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lhz r11,40(r20)
	ctx.r11.u64 = PPC_LOAD_U16(var_r20 + 40);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lhz r31,12(r9)
	var_r31 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 12));
	// add r4,r11,r31
	ctx.r4.u64 = ctx.r11.u64 + var_r31;
	// bl 0x8224a510
	xe_A510(ctx, base);
	// cmpwi cr6,r31,0
	// ble cr6,0x8224a380
	if ((int32_t)var_r31 > 0) {
		// lis r11,-32253
		// li r25,0
		var_r25 = 0;
		// mr r17,r31
		var_r17 = (uint32_t)(var_r31);
		// mr r21,r25
		var_r21 = (uint32_t)(var_r25);
		// li r19,2
		var_r19 = 2;
		// lfs f31,-12188(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12188);
		var_f31 = double(temp.f32);
	loc_82249F50:
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// lwz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r7,8(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// lwzx r26,r7,r21
		var_r26 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + var_r21));
		// lbz r6,4(r26)
		ctx.r6.u64 = PPC_LOAD_U8(var_r26 + 4);
		// clrlwi r11,r6,28
		ctx.r11.u64 = ctx.r6.u32 & 0xF;
		// cmplwi cr6,r11,1
		// blt cr6,0x8224a200
		if (ctx.r11.u32 >= 1) {
			// beq cr6,0x8224a0c4
			if (!(ctx.cr6.eq)) {
				// cmplwi cr6,r11,3
				// bge cr6,0x8224a36c
				if (ctx.r11.u32 >= 3) goto loc_8224A36C;
				// bl 0x820c0038
				xe_main_thread_init_0038(ctx, base);
				// lwz r24,0(r13)
				var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
				// li r27,4
				var_r27 = 4;
				// li r5,16
				ctx.r5.s64 = 16;
				// li r4,12
				ctx.r4.s64 = 12;
				// lwzx r3,r27,r24
				ctx.r3.u64 = PPC_LOAD_U32(var_r27 + var_r24);
				// lwz r10,4(r11)
				// bctrl
				VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
				// cmplwi cr6,r3,0
				// beq cr6,0x82249fc0
				if (ctx.r3.u32 != 0) {
					// mr r22,r3
					var_r22 = ctx.r3.u32;
					// stw r25,4(r3)
					PPC_STORE_U32(ctx.r3.u32 + 4, var_r25);
					// sth r25,8(r3)
					PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r25);
					// sth r25,10(r3)
					PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r25);
					// b 0x82249fc4
				} else {
				loc_82249FC0:
					// mr r22,r25
					var_r22 = (uint32_t)(var_r25);
				}
			loc_82249FC4:
				// addi r3,r1,80
				ctx.r3.s64 = ctx.r1.s64 + 80;
				// lhz r4,10(r20)
				ctx.r4.u64 = PPC_LOAD_U16(var_r20 + 10);
				// stw r25,80(r1)
				PPC_STORE_U32(ctx.r1.u32 + 80, var_r25);
				// sth r25,84(r1)
				PPC_STORE_U16(ctx.r1.u32 + 84, (uint16_t)var_r25);
				// sth r25,86(r1)
				PPC_STORE_U16(ctx.r1.u32 + 86, (uint16_t)var_r25);
				// bl 0x8240a250
				xe_A250(ctx, base);
				// lwz r9,0(r26)
  // [ph4a] vtable load collapsed
				// mr r3,r26
				ctx.r3.u64 = var_r26;
				// lwz r8,64(r9)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r26, 16, ctx, base);  // pattern-B slot 16 (byte +64)
				// lwz r29,80(r1)
				var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
				// li r30,1
				var_r30 = 1;
				// stfs f1,0(r29)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f1.f64);
				PPC_STORE_U32(var_r29 + 0, temp.u32);
				// lhz r11,4(r28)
				ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
				// cmpwi cr6,r11,1
				// ble cr6,0x8224a044
				if (ctx.r11.s32 > 1) {
					// li r31,4
					var_r31 = 4;
				loc_8224A00C:
					// lwz r11,0(r28)
					ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
					// lwzx r7,r11,r31
					ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
					// lwz r6,8(r7)
					ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
					// lwzx r3,r6,r21
					ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r21);
					// lwz r4,64(r5)
					// bctrl
					VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
					// stfsx f1,r29,r31
					ctx.fpscr.disableFlushMode();
					temp.f32 = float(ctx.f1.f64);
					PPC_STORE_U32(var_r29 + var_r31, temp.u32);
					// lhz r11,4(r28)
					ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
					// addi r30,r30,1
					var_r30 = (uint32_t)(var_r30 + 1);
					// addi r31,r31,4
					var_r31 = (uint32_t)(var_r31 + 4);
					// cmpw cr6,r30,r11
					// blt cr6,0x8224a00c
					if ((int32_t)var_r30 < ctx.r11.s32) goto loc_8224A00C;
				}
			loc_8224A044:
				// mr r3,r22
				ctx.r3.u64 = var_r22;
				// lhz r31,6(r26)
				var_r31 = (uint32_t)(PPC_LOAD_U16(var_r26 + 6));
				// lbz r30,5(r26)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r26 + 5));
				// bl 0x8224d5b8
				xe_D5B8(ctx, base);
				// li r6,0
				ctx.r6.s64 = 0;
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// lhz r5,84(r1)
				ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 84);
				// mr r3,r22
				ctx.r3.u64 = var_r22;
				// fmr f1,f31
				ctx.fpscr.disableFlushMode();
				ctx.f1.f64 = var_f31;
				// stb r30,0(r22)
				PPC_STORE_U8(var_r22 + 0, (uint8_t)var_r30);
				// sth r31,2(r22)
				PPC_STORE_U16(var_r22 + 2, (uint16_t)var_r31);
				// stb r19,1(r22)
				PPC_STORE_U8(var_r22 + 1, (uint8_t)var_r19);
				// bl 0x8224d850
				xe_D850(ctx, base);
				// mr r31,r3
				var_r31 = ctx.r3.u32;
				// li r4,1
				ctx.r4.s64 = 1;
				// addi r3,r22,4
				ctx.r3.s64 = (int64_t)(int32_t)var_r22 + 4;
				// bl 0x8224de98
				atSingleton_DE98_g(ctx, base);
				// lhz r10,86(r1)
				ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
				// stw r31,0(r3)
				PPC_STORE_U32(ctx.r3.u32 + 0, var_r31);
				// cmplwi cr6,r10,0
				// beq cr6,0x8224a328
				if (ctx.r10.u32 == 0) goto loc_8224A328;
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r9,r3,24
				ctx.r9.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r9,0
				// bne cr6,0x8224a328
				if (ctx.r9.u32 != 0) goto loc_8224A328;
				// lwzx r3,r27,r24
				ctx.r3.u64 = PPC_LOAD_U32(var_r27 + var_r24);
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// lwz r8,0(r3)
				ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
				// lwz r7,8(r8)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
				// mtctr r7
				ctx.ctr.u64 = ctx.r7.u64;
				// b 0x8224a324
				goto loc_8224A324;
			}
		loc_8224A0C4:
			// bl 0x820c0038
			xe_main_thread_init_0038(ctx, base);
			// lwz r23,0(r13)
			var_r23 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
			// li r24,4
			var_r24 = 4;
			// li r5,16
			ctx.r5.s64 = 16;
			// li r4,12
			ctx.r4.s64 = 12;
			// lwzx r3,r24,r23
			ctx.r3.u64 = PPC_LOAD_U32(var_r24 + var_r23);
			// lwz r11,4(r6)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// cmplwi cr6,r3,0
			// beq cr6,0x8224a108
			if (ctx.r3.u32 != 0) {
				// mr r22,r3
				var_r22 = ctx.r3.u32;
				// stw r25,4(r3)
				PPC_STORE_U32(ctx.r3.u32 + 4, var_r25);
				// sth r25,8(r3)
				PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r25);
				// sth r25,10(r3)
				PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r25);
				// b 0x8224a10c
			} else {
			loc_8224A108:
				// mr r22,r25
				var_r22 = (uint32_t)(var_r25);
			}
		loc_8224A10C:
			// addi r3,r1,88
			ctx.r3.s64 = ctx.r1.s64 + 88;
			// lhz r4,10(r20)
			ctx.r4.u64 = PPC_LOAD_U16(var_r20 + 10);
			// stw r25,88(r1)
			PPC_STORE_U32(ctx.r1.u32 + 88, var_r25);
			// sth r25,92(r1)
			PPC_STORE_U16(ctx.r1.u32 + 92, (uint16_t)var_r25);
			// sth r25,94(r1)
			PPC_STORE_U16(ctx.r1.u32 + 94, (uint16_t)var_r25);
			// bl 0x82247c20
			xe_7C20(ctx, base);
			// lwz r10,0(r26)
  // [ph4a] vtable load collapsed
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// lwz r9,60(r10)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r26, 15, ctx, base);  // pattern-B slot 15 (byte +60)
			// ld r8,0(r3)
			ctx.r8.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
			// lwz r27,88(r1)
			var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
			// li r31,1
			var_r31 = 1;
			// std r8,0(r27)
			PPC_STORE_U64(var_r27 + 0, ctx.r8.u64);
			// ld r7,8(r3)
			ctx.r7.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
			// std r7,8(r27)
			PPC_STORE_U64(var_r27 + 8, ctx.r7.u64);
			// lhz r11,4(r28)
			ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
			// cmpwi cr6,r11,1
			// ble cr6,0x8224a1b0
			if (ctx.r11.s32 > 1) {
				// li r30,4
				var_r30 = 4;
				// addi r29,r27,16
				var_r29 = (uint32_t)(var_r27 + 16);
			loc_8224A164:
				// lwz r11,0(r28)
				ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
				// lwzx r6,r11,r30
				ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r30);
				// lwz r5,8(r6)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
				// lwzx r3,r5,r21
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r21);
				// lwz r11,60(r4)
				// bctrl
				VCALL(ctx.r3.u32, 15, ctx, base);  // vtable slot 15 (byte +60)
				// ld r10,0(r3)
				ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
				// mr r11,r29
				ctx.r11.u64 = var_r29;
				// addi r31,r31,1
				var_r31 = (uint32_t)(var_r31 + 1);
				// addi r30,r30,4
				var_r30 = (uint32_t)(var_r30 + 4);
				// addi r29,r29,16
				var_r29 = (uint32_t)(var_r29 + 16);
				// std r10,0(r11)
				PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
				// ld r9,8(r3)
				ctx.r9.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
				// std r9,8(r11)
				PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r9.u64);
				// lhz r11,4(r28)
				ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
				// cmpw cr6,r31,r11
				// blt cr6,0x8224a164
				if ((int32_t)var_r31 < ctx.r11.s32) goto loc_8224A164;
			}
		loc_8224A1B0:
			// addi r6,r1,88
			ctx.r6.s64 = ctx.r1.s64 + 88;
			// lhz r5,6(r26)
			ctx.r5.u64 = PPC_LOAD_U16(var_r26 + 6);
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// lbz r4,5(r26)
			ctx.r4.u64 = PPC_LOAD_U8(var_r26 + 5);
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// bl 0x8224db50
			xe_DB50(ctx, base);
			// lhz r7,94(r1)
			ctx.r7.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
			// cmplwi cr6,r7,0
			// beq cr6,0x8224a328
			if (ctx.r7.u32 == 0) goto loc_8224A328;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r6,r3,24
			ctx.r6.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r6,0
			// bne cr6,0x8224a328
			if (ctx.r6.u32 != 0) goto loc_8224A328;
			// lwzx r3,r24,r23
			ctx.r3.u64 = PPC_LOAD_U32(var_r24 + var_r23);
			// mr r4,r27
			ctx.r4.u64 = var_r27;
			// lwz r5,0(r3)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// lwz r11,8(r5)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 8);
			// mtctr r11
			ctx.ctr.u64 = ctx.r11.u64;
			// b 0x8224a324
		} else {
		loc_8224A200:
			// bl 0x820c0038
			xe_main_thread_init_0038(ctx, base);
			// lwz r23,0(r13)
			var_r23 = (uint32_t)(PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */);
			// li r24,4
			var_r24 = 4;
			// li r5,16
			ctx.r5.s64 = 16;
			// li r4,12
			ctx.r4.s64 = 12;
			// lwzx r3,r24,r23
			ctx.r3.u64 = PPC_LOAD_U32(var_r24 + var_r23);
			// lwz r9,4(r10)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// cmplwi cr6,r3,0
			// beq cr6,0x8224a244
			if (ctx.r3.u32 != 0) {
				// mr r22,r3
				var_r22 = ctx.r3.u32;
				// stw r25,4(r3)
				PPC_STORE_U32(ctx.r3.u32 + 4, var_r25);
				// sth r25,8(r3)
				PPC_STORE_U16(ctx.r3.u32 + 8, (uint16_t)var_r25);
				// sth r25,10(r3)
				PPC_STORE_U16(ctx.r3.u32 + 10, (uint16_t)var_r25);
				// b 0x8224a248
			} else {
			loc_8224A244:
				// mr r22,r25
				var_r22 = (uint32_t)(var_r25);
			}
		loc_8224A248:
			// addi r3,r1,96
			ctx.r3.s64 = ctx.r1.s64 + 96;
			// lhz r4,10(r20)
			ctx.r4.u64 = PPC_LOAD_U16(var_r20 + 10);
			// stw r25,96(r1)
			PPC_STORE_U32(ctx.r1.u32 + 96, var_r25);
			// sth r25,100(r1)
			PPC_STORE_U16(ctx.r1.u32 + 100, (uint16_t)var_r25);
			// sth r25,102(r1)
			PPC_STORE_U16(ctx.r1.u32 + 102, (uint16_t)var_r25);
			// bl 0x82247c20
			xe_7C20(ctx, base);
			// lwz r8,0(r26)
  // [ph4a] vtable load collapsed
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// lwz r7,56(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r26, 14, ctx, base);  // pattern-B slot 14 (byte +56)
			// lwz r27,96(r1)
			var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 96));
			// lvx128 v0,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// li r31,1
			var_r31 = 1;
			// stvx v0,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lhz r11,4(r28)
			ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
			// cmpwi cr6,r11,1
			// ble cr6,0x8224a2d8
			if (ctx.r11.s32 > 1) {
				// li r30,4
				var_r30 = 4;
				// addi r29,r27,16
				var_r29 = (uint32_t)(var_r27 + 16);
			loc_8224A298:
				// lwz r11,0(r28)
				ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
				// lwzx r6,r11,r30
				ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r30);
				// lwz r5,8(r6)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
				// lwzx r3,r5,r21
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r21);
				// lwz r11,56(r4)
				// bctrl
				VCALL(ctx.r3.u32, 14, ctx, base);  // vtable slot 14 (byte +56)
				// lvx128 v13,r0,r3
				ea = (ctx.r3.u32) & ~0xF;
				simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// stvx v13,r0,r29
				ea = (var_r29) & ~0xF;
				simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
				// lhz r11,4(r28)
				ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
				// addi r31,r31,1
				var_r31 = (uint32_t)(var_r31 + 1);
				// addi r30,r30,4
				var_r30 = (uint32_t)(var_r30 + 4);
				// addi r29,r29,16
				var_r29 = (uint32_t)(var_r29 + 16);
				// cmpw cr6,r31,r11
				// blt cr6,0x8224a298
				if ((int32_t)var_r31 < ctx.r11.s32) goto loc_8224A298;
			}
		loc_8224A2D8:
			// addi r6,r1,96
			ctx.r6.s64 = ctx.r1.s64 + 96;
			// lhz r5,6(r26)
			ctx.r5.u64 = PPC_LOAD_U16(var_r26 + 6);
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// lbz r4,5(r26)
			ctx.r4.u64 = PPC_LOAD_U8(var_r26 + 5);
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// bl 0x8224da80
			xe_DA80(ctx, base);
			// lhz r9,102(r1)
			ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
			// cmplwi cr6,r9,0
			// beq cr6,0x8224a328
			if (ctx.r9.u32 == 0) goto loc_8224A328;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r8,r3,24
			ctx.r8.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r8,0
			// bne cr6,0x8224a328
			if (ctx.r8.u32 != 0) goto loc_8224A328;
			// lwzx r3,r24,r23
			ctx.r3.u64 = PPC_LOAD_U32(var_r24 + var_r23);
			// mr r4,r27
			ctx.r4.u64 = var_r27;
			// lwz r7,0(r3)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// lwz r6,8(r7)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
			// mtctr r6
			ctx.ctr.u64 = ctx.r6.u64;
		}
	loc_8224A324:
		// bctrl
		ctx.lr = 0x8224A328;
		PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	loc_8224A328:
		// cmplwi cr6,r22,0
		// beq cr6,0x8224a370
		if (var_r22 != 0) {
			// mr r4,r22
			ctx.r4.u64 = var_r22;
			// mr r3,r20
			ctx.r3.u64 = var_r20;
			// bl 0x8224a438
			xe_A438(ctx, base);
			// clrlwi r5,r3,24
			ctx.r5.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r5,0
			// bne cr6,0x8224a370
			if (ctx.r5.u32 != 0) goto loc_8224A370;
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// bl 0x8224d5b8
			xe_D5B8(ctx, base);
			// lhz r4,10(r22)
			ctx.r4.u64 = PPC_LOAD_U16(var_r22 + 10);
			// cmplwi cr6,r4,0
			// beq cr6,0x8224a364
			if (ctx.r4.u32 != 0) {
				// lwz r3,4(r22)
				ctx.r3.u64 = PPC_LOAD_U32(var_r22 + 4);
				// bl 0x820c00c0
				rage_free_00C0(ctx, base);
			}
		loc_8224A364:
			// mr r3,r22
			ctx.r3.u64 = var_r22;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		loc_8224A36C:
			// mr r18,r25
			var_r18 = (uint32_t)(var_r25);
		}
	loc_8224A370:
		// addi r17,r17,-1
		var_r17 = (uint32_t)(var_r17 + -1);
		// addi r21,r21,4
		var_r21 = (uint32_t)(var_r21 + 4);
		// cmplwi cr6,r17,0
		// bne cr6,0x82249f50
		if (var_r17 != 0) goto loc_82249F50;
	}
loc_8224A380:
	// mr r3,r18
	ctx.r3.u64 = var_r18;
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_A390_v12"))) PPC_WEAK_FUNC(LocomotionStateAnim_A390_v12);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_A390_v12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lhz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 40);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r8,r11,-1
	ctx.r8.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r8,0
	// blt cr6,0x8224a428
	if (ctx.r8.s32 >= 0) {
		// rlwimi r5,r4,16,8,15
		ctx.r5.u64 = (__builtin_rotateleft32(ctx.r4.u32, 16) & 0xFF0000) | (ctx.r5.u64 & 0xFFFFFFFFFF00FFFF);
		// lwz r31,36(r3)
		var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 36));
		// clrlwi r5,r5,8
		ctx.r5.u64 = ctx.r5.u32 & 0xFFFFFF;
	loc_8224A3B4:
		// add r11,r8,r7
		ctx.r11.u64 = ctx.r8.u64 + ctx.r7.u64;
		// srawi r10,r11,1
		ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
		ctx.r10.s64 = ctx.r11.s32 >> 1;
		// rlwinm r9,r10,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// stw r10,0(r6)
		PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
		// lwzx r3,r9,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r31);
		// lbz r4,0(r3)
		ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
		// lhz r11,2(r3)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 2);
		// rotlwi r9,r4,16
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 16);
		// or r11,r9,r11
		ctx.r11.u64 = ctx.r9.u64 | ctx.r11.u64;
		// li r9,1
		ctx.r9.s64 = 1;
		// cmplw cr6,r11,r5
		// bgt cr6,0x8224a3e8
		if (ctx.r11.u32 <= ctx.r5.u32) {
			// li r9,0
			ctx.r9.s64 = 0;
		}
	loc_8224A3E8:
		// cmplw cr6,r11,r5
		// clrlwi r9,r9,24
		ctx.r9.u64 = ctx.r9.u32 & 0xFF;
		// li r11,1
		ctx.r11.s64 = 1;
		// beq cr6,0x8224a3fc
		if (ctx.r11.u32 != ctx.r5.u32) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224A3FC:
		// clrlwi r11,r11,24
		ctx.r11.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// bne cr6,0x8224a430
		if (ctx.r11.u32 != 0) {
			// ld r31,-8(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
			// blr
			return;
		}
		// clrlwi r9,r9,24
		ctx.r9.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x8224a41c
		if (ctx.r9.u32 != 0) {
			// addi r8,r10,-1
			ctx.r8.s64 = ctx.r10.s64 + -1;
			// b 0x8224a420
		} else {
		loc_8224A41C:
			// addi r7,r10,1
			ctx.r7.s64 = ctx.r10.s64 + 1;
		}
	loc_8224A420:
		// cmpw cr6,r7,r8
		// ble cr6,0x8224a3b4
		if (ctx.r7.s32 <= ctx.r8.s32) goto loc_8224A3B4;
	}
loc_8224A428:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
loc_8224A430:
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__xe_A438"))) PPC_WEAK_FUNC(xe_A438);
PPC_FUNC_IMPL(__imp__xe_A438) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lhz r5,2(r29)
	ctx.r5.u64 = PPC_LOAD_U16(var_r29 + 2);
	// lbz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U8(var_r29 + 0);
	// bl 0x8224a390
	LocomotionStateAnim_A390_v12(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224a470
	if (ctx.r3.u32 != 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_8224A470:
	// addi r31,r30,36
	var_r31 = (uint32_t)(var_r30 + 36);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224a5a8
	pongNetMessageHolder_A5A8_w(ctx, base);
	// lhz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lhz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 40);
	// addis r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 65536;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// clrlwi r10,r7,16
	ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
	// rlwinm r6,r10,2,14,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
	// sth r10,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r10.u16);
	// lwzx r5,r6,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// stw r5,-4(r8)
	PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r5.u32);
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpw cr6,r11,r8
	// ble cr6,0x8224a4e8
	if (ctx.r11.s32 > ctx.r8.s32) {
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r11,r8,r11
		ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	loc_8224A4C8:
		// lwz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// add r9,r9,r10
		ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
		// addi r10,r10,-4
		ctx.r10.s64 = ctx.r10.s64 + -4;
		// cmplwi cr6,r11,0
		// lwz r4,-4(r9)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
		// stw r4,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
		// bne cr6,0x8224a4c8
		if (ctx.r11.u32 != 0) goto loc_8224A4C8;
	}
loc_8224A4E8:
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// sth r8,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r8.u16);
	// stwx r29,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, var_r29);
	return;
}

__attribute__((alias("__imp__xe_A510"))) PPC_WEAK_FUNC(xe_A510);
PPC_FUNC_IMPL(__imp__xe_A510) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// cmplwi cr6,r28,0
	// beq cr6,0x8224a590
	if (var_r28 != 0) {
		// lis r11,16383
		ctx.r11.s64 = 1073676288;
		// rlwinm r3,r28,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC;
		// ori r10,r11,65535
		ctx.r10.u64 = ctx.r11.u64 | 65535;
		// cmplw cr6,r28,r10
		// ble cr6,0x8224a544
		if (var_r28 > ctx.r10.u32) {
			// li r3,-1
		}
	loc_8224A544:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmplwi cr6,r29,0
		// beq cr6,0x8224a590
		if (var_r29 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
			// sth r28,6(r27)
			PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r28);
			// stw r11,0(r27)
			PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
			return;
		}
		// addi r31,r28,-1
		var_r31 = (uint32_t)(var_r28 + -1);
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// cmpwi cr6,r31,0
		// blt cr6,0x8224a57c
	while ((int32_t)var_r31 >= 0) {
		loc_8224A564:
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8224a6d0
			ke_A6D0(ctx, base);
			// addi r31,r31,-1
			var_r31 = (uint32_t)(var_r31 + -1);
			// addi r30,r30,4
			var_r30 = (uint32_t)(var_r30 + 4);
			// cmpwi cr6,r31,0
			// bge cr6,0x8224a564
	}
	loc_8224A57C:
		// mr r11,r29
		ctx.r11.u64 = var_r29;
		// sth r28,6(r27)
		PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r28);
		// stw r11,0(r27)
		PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
		return;
	}
loc_8224A590:
	// li r11,0
	ctx.r11.s64 = 0;
	// sth r28,6(r27)
	PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r28);
	// stw r11,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__pongNetMessageHolder_A5A8_w"))) PPC_WEAK_FUNC(pongNetMessageHolder_A5A8_w);
PPC_FUNC_IMPL(__imp__pongNetMessageHolder_A5A8_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lhz r11,6(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 6);
	// lhz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 4);
	// cmplw cr6,r10,r11
	// bne cr6,0x8224a6ac
	if (ctx.r10.u32 == ctx.r11.u32) {
		// clrlwi r10,r4,16
		ctx.r10.u64 = ctx.r4.u32 & 0xFFFF;
		// lis r9,16383
		ctx.r9.s64 = 1073676288;
		// add r7,r10,r11
		ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
		// ori r8,r9,65535
		ctx.r8.u64 = ctx.r9.u64 | 65535;
		// clrlwi r11,r7,16
		ctx.r11.u64 = ctx.r7.u32 & 0xFFFF;
		// mr r30,r11
		var_r30 = ctx.r11.u32;
		// cmplw cr6,r30,r8
		// sth r11,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
		// rlwinm r3,r30,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
		// ble cr6,0x8224a5f4
		if (var_r30 > ctx.r8.u32) {
			// li r3,-1
		}
	loc_8224A5F4:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// cmplwi cr6,r28,0
		// beq cr6,0x8224a630
		if (var_r28 != 0) {
			// addi r30,r30,-1
			var_r30 = (uint32_t)(var_r30 + -1);
			// mr r29,r28
			var_r29 = (uint32_t)(var_r28);
			// cmpwi cr6,r30,0
			// blt cr6,0x8224a634
			if ((int32_t)var_r30 < 0) goto loc_8224A634;
		loc_8224A614:
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x8224a6d0
			ke_A6D0(ctx, base);
			// addi r30,r30,-1
			var_r30 = (uint32_t)(var_r30 + -1);
			// addi r29,r29,4
			var_r29 = (uint32_t)(var_r29 + 4);
			// cmpwi cr6,r30,0
			// bge cr6,0x8224a614
			if ((int32_t)var_r30 >= 0) goto loc_8224A614;
			// b 0x8224a634
		} else {
		loc_8224A630:
			// li r28,0
			var_r28 = 0;
		}
	loc_8224A634:
		// lhz r6,4(r31)
		ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 4);
		// cmplwi cr6,r6,0
		// beq cr6,0x8224a668
		if (ctx.r6.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		loc_8224A644:
			// lwz r4,0(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0)/* pongNetMessageHolder::vtable@+0x0 */;
			// rlwinm r10,r11,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r5,r11,1
			ctx.r5.s64 = ctx.r11.s64 + 1;
			// clrlwi r11,r5,16
			ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
			// lwzx r3,r10,r4
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
			// stwx r3,r10,r28
			PPC_STORE_U32(ctx.r10.u32 + var_r28, ctx.r3.u32);
			// lhz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 4);
			// cmplw cr6,r11,r10
			// blt cr6,0x8224a644
			if (ctx.r11.u32 < ctx.r10.u32) goto loc_8224A644;
		}
	loc_8224A668:
		// lwz r30,0(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 0)/* pongNetMessageHolder::vtable@+0x0 */);
		// cmplwi cr6,r30,0
		// beq cr6,0x8224a6a8
		if (var_r30 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r9,r3,24
			ctx.r9.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r9,0
			// bne cr6,0x8224a6a8
			if (ctx.r9.u32 != 0) goto loc_8224A6A8;
			// lwz r8,0(r13)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
			// li r7,4
			ctx.r7.s64 = 4;
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// lwzx r3,r7,r8
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
			// lwz r5,8(r6)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		}
	loc_8224A6A8:
		// stw r28,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* pongNetMessageHolder::vtable@+0x0 */ var_r28);
	}
loc_8224A6AC:
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* pongNetMessageHolder::vtable@+0x0 */;
	// addi r4,r11,1
	ctx.r4.s64 = ctx.r11.s64 + 1;
	// rotlwi r10,r11,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sth r4,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r4.u16);
	return;
}

__attribute__((alias("__imp__ke_A6D0"))) PPC_WEAK_FUNC(ke_A6D0);
PPC_FUNC_IMPL(__imp__ke_A6D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// lis r30,-32161
	var_r30 = (uint32_t)(-2107703296);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,23208(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
	// cmplwi cr6,r3,0
	// bne cr6,0x8224a6f8
	if (ctx.r3.u32 == 0) {
		// bl 0x8258623c
		__imp__KeTlsAlloc(ctx, base);
		// stw r3,23208(r30)
		PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
	}
loc_8224A6F8:
	// bl 0x8258621c
	__imp__KeTlsGetValue(ctx, base);
	// li r29,0
	var_r29 = 0;
	// cmplwi cr6,r3,0
	// beq cr6,0x8224a874
	if (ctx.r3.u32 != 0) {
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmplwi cr6,r11,0
		// beq cr6,0x8224a874
		if (ctx.r11.u32 == 0) goto loc_8224A874;
		// lwz r3,23208(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
		// cmplwi cr6,r3,0
		// bne cr6,0x8224a728
		if (ctx.r3.u32 == 0) {
			// bl 0x8258623c
			__imp__KeTlsAlloc(ctx, base);
			// stw r3,23208(r30)
			PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
		}
	loc_8224A728:
		// bl 0x8258621c
		__imp__KeTlsGetValue(ctx, base);
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// cmplw cr6,r31,r11
		// blt cr6,0x8224a74c
		if (var_r31 >= ctx.r11.u32) {
			// lwz r10,72(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
			// add r10,r10,r11
			ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
			// li r11,1
			ctx.r11.s64 = 1;
			// cmplw cr6,r31,r10
			// ble cr6,0x8224a750
			if (var_r31 <= ctx.r10.u32) goto loc_8224A750;
		}
	loc_8224A74C:
		// mr r11,r29
		ctx.r11.u64 = var_r29;
	loc_8224A750:
		// clrlwi r8,r11,24
		ctx.r8.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r8,0
		// beq cr6,0x8224a874
		if (ctx.r8.u32 == 0) goto loc_8224A874;
		// lwz r3,23208(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
		// cmplwi cr6,r3,0
		// bne cr6,0x8224a770
		if (ctx.r3.u32 == 0) {
			// bl 0x8258623c
			__imp__KeTlsAlloc(ctx, base);
			// stw r3,23208(r30)
			PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
		}
	loc_8224A770:
		// bl 0x8258621c
		__imp__KeTlsGetValue(ctx, base);
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmplwi cr6,r11,0
		// beq cr6,0x8224a7a8
		if (ctx.r11.u32 != 0) {
			// lwz r7,4(r3)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
			// lwz r6,76(r3)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
			// subf r5,r7,r11
			ctx.r5.s64 = ctx.r11.s64 - ctx.r7.s64;
			// twllei r6,0
			if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
			// divwu r10,r5,r6
			ctx.r10.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
			// addi r4,r10,2
			ctx.r4.s64 = ctx.r10.s64 + 2;
			// rlwinm r10,r4,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r10,r10,r3
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
			// add r9,r10,r11
			ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
			// stw r9,0(r31)
			PPC_STORE_U32(var_r31 + 0, ctx.r9.u32);
		}
	loc_8224A7A8:
		// lwz r3,23208(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
		// cmplwi cr6,r3,0
		// bne cr6,0x8224a7bc
		if (ctx.r3.u32 == 0) {
			// bl 0x8258623c
			__imp__KeTlsAlloc(ctx, base);
			// stw r3,23208(r30)
			PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
		}
	loc_8224A7BC:
		// bl 0x8258621c
		__imp__KeTlsGetValue(ctx, base);
		// lwz r30,0(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 0));
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// cmplwi cr6,r30,0
		// beq cr6,0x8224a878
		if (var_r30 == 0) {
			return;
		}
		// lwz r11,4(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
		// cmplwi cr6,r11,0
		// beq cr6,0x8224a804
		if (ctx.r11.u32 != 0) {
			// lwz r8,4(r4)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
			// lwz r7,76(r4)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
			// subf r6,r8,r11
			ctx.r6.s64 = ctx.r11.s64 - ctx.r8.s64;
			// twllei r7,0
			if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
			// divwu r10,r6,r7
			ctx.r10.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
			// addi r5,r10,2
			ctx.r5.s64 = ctx.r10.s64 + 2;
			// rlwinm r3,r5,2,0,29
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r10,r3,r4
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r4.u32);
			// add r11,r10,r11
			ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
			// stw r11,4(r30)
			PPC_STORE_U32(var_r30 + 4, ctx.r11.u32);
		}
	loc_8224A804:
		// lhz r11,8(r30)
		ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 8);
		// cmpwi cr6,r11,0
		// ble cr6,0x8224a878
		if (ctx.r11.s32 <= 0) {
			return;
		}
		// mr r31,r29
		var_r31 = (uint32_t)(var_r29);
	loc_8224A814:
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwzx r11,r10,r31
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
		// cmplwi cr6,r11,0
		// beq cr6,0x8224a84c
		if (ctx.r11.u32 != 0) {
			// lwz r9,4(r4)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
			// lwz r8,76(r4)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 76);
			// subf r7,r9,r11
			ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
			// twllei r8,0
			if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
			// divwu r9,r7,r8
			ctx.r9.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
			// addi r6,r9,2
			ctx.r6.s64 = ctx.r9.s64 + 2;
			// rlwinm r5,r6,2,0,29
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r9,r5,r4
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r4.u32);
			// add r3,r9,r11
			ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
			// stwx r3,r10,r31
			PPC_STORE_U32(ctx.r10.u32 + var_r31, ctx.r3.u32);
		}
	loc_8224A84C:
		// lwz r11,4(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwzx r3,r11,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
		// bl 0x82245278
		crAnimChannelRawFloat_5278_p46(ctx, base);
		// lhz r11,8(r30)
		ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 8);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r29,r11
		// blt cr6,0x8224a814
		if ((int32_t)var_r29 < ctx.r11.s32) goto loc_8224A814;
		return;
	}
loc_8224A874:
	// stw r29,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r29);
loc_8224A878:
	return;
}

__attribute__((alias("__imp__xe_A880"))) PPC_WEAK_FUNC(xe_A880);
PPC_FUNC_IMPL(__imp__xe_A880) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,1
	// blt cr6,0x8224a91c
	if (ctx.r11.u32 >= 1) {
		// beq cr6,0x8224a8e0
		if (!(ctx.cr6.eq)) {
			// cmplwi cr6,r11,3
			// bge cr6,0x8224a954
			if (ctx.r11.u32 >= 3) {
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
			// li r3,12
			ctx.r3.s64 = 12;
			// bl 0x820dec88
			xe_EC88(ctx, base);
			// cmplwi cr6,r3,0
			// beq cr6,0x8224a954
			if (ctx.r3.u32 == 0) {
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
			// lbz r9,4(r3)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
			// li r10,1
			ctx.r10.s64 = 1;
			// lis r11,-32253
			// rlwimi r9,r10,1,28,23
			ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 1) & 0xFFFFFFFFFFFFFF0F) | (ctx.r9.u64 & 0xF0);
			// addi r11,r11,26756
			ctx.r11.s64 = ctx.r11.s64 + 26756;
			// stb r9,4(r3)
			PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r9.u8);
			// stw r11,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
			// blr
			return;
		}
	loc_8224A8E0:
		// li r3,32
		ctx.r3.s64 = 32;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x8224a954
		if (ctx.r3.u32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// lbz r7,4(r3)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// li r8,1
		ctx.r8.s64 = 1;
		// lis r11,-32254
		// rlwimi r7,r8,0,28,23
		ctx.r7.u64 = (ctx.r8.u32 & 0xFFFFFFFFFFFFFF0F) | (ctx.r7.u64 & 0xF0);
		// addi r11,r11,31732
		ctx.r11.s64 = ctx.r11.s64 + 31732;
		// stb r7,4(r3)
		PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r7.u8);
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// blr
		return;
	}
loc_8224A91C:
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224a954
	if (ctx.r3.u32 != 0) {
		// lbz r6,4(r3)
		ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// lis r11,-32254
		// addi r11,r11,31644
		ctx.r11.s64 = ctx.r11.s64 + 31644;
		// rlwinm r5,r6,0,0,27
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFF0;
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// stb r5,4(r3)
		PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r5.u8);
		// blr
		return;
	}
loc_8224A954:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofFloat_ctor_A968"))) PPC_WEAK_FUNC(crAnimDofFloat_ctor_A968);
PPC_FUNC_IMPL(__imp__crAnimDofFloat_ctor_A968) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r11,1
	// blt cr6,0x8224a9d0
	if (ctx.r11.u32 >= 1) {
		// beq cr6,0x8224a9a8
		if (!(ctx.cr6.eq)) {
			// cmplwi cr6,r11,3
			// bgelr cr6
			if (ctx.r11.u32 >= 3) return;
			// cmplwi cr6,r3,0
			// beqlr cr6
			if (ctx.r3.u32 == 0) return;
			// lbz r9,4(r3)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
			// li r10,1
			ctx.r10.s64 = 1;
			// lis r11,-32253
			// rlwimi r9,r10,1,28,23
			ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 1) & 0xFFFFFFFFFFFFFF0F) | (ctx.r9.u64 & 0xF0);
			// addi r11,r11,26756
			ctx.r11.s64 = ctx.r11.s64 + 26756;
			// stb r9,4(r3)
			PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r9.u8);
			// stw r11,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
			// blr
			return;
		}
	loc_8224A9A8:
		// cmplwi cr6,r3,0
		// beqlr cr6
		if (ctx.r3.u32 == 0) return;
		// lbz r7,4(r3)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// li r8,1
		ctx.r8.s64 = 1;
		// lis r11,-32254
		// rlwimi r7,r8,0,28,23
		ctx.r7.u64 = (ctx.r8.u32 & 0xFFFFFFFFFFFFFF0F) | (ctx.r7.u64 & 0xF0);
		// addi r11,r11,31732
		ctx.r11.s64 = ctx.r11.s64 + 31732;
		// stb r7,4(r3)
		PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r7.u8);
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// blr
		return;
	}
loc_8224A9D0:
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lbz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// lis r11,-32254
	// rlwinm r5,r6,0,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFF0;
	// addi r11,r11,31644
	ctx.r11.s64 = ctx.r11.s64 + 31644;
	// stb r5,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r5.u8);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_A9F8"))) PPC_WEAK_FUNC(game_A9F8);
PPC_FUNC_IMPL(__imp__game_A9F8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8224aa10
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224AA10:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8224aa58
	if (ctx.r8.u32 != 0) {
		// lhz r7,12(r3)
		ctx.r7.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
		// clrlwi r11,r4,16
		ctx.r11.u64 = ctx.r4.u32 & 0xFFFF;
		// cmplw cr6,r11,r7
		// bge cr6,0x8224aa50
		if (ctx.r11.u32 < ctx.r7.u32) {
			// rlwinm r9,r11,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// lwz r10,0(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// li r3,1
			ctx.r3.s64 = 1;
			// add r6,r11,r9
			ctx.r6.u64 = ctx.r11.u64 + ctx.r9.u64;
			// rlwinm r11,r6,6,0,25
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 6) & 0xFFFFFFC0;
			// add r4,r11,r10
			ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lhz r11,22(r4)
			ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 22);
			// sth r11,0(r5)
			PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r11.u16);
			// blr
			return;
		}
	loc_8224AA50:
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8224AA58:
	// sth r4,0(r5)
	PPC_STORE_U16(ctx.r5.u32 + 0, ctx.r4.u16);
	// li r3,1
	ctx.r3.s64 = 1;
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_AA68_g"))) PPC_WEAK_FUNC(atSingleton_AA68_g);
PPC_FUNC_IMPL(__imp__atSingleton_AA68_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r10,r11,-836
	ctx.r10.s64 = ctx.r11.s64 + -836;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r10,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
	// sth r11,12(r31)
	PPC_STORE_U16(var_r31 + 12, ctx.r11.u16);
	// sth r11,14(r31)
	PPC_STORE_U16(var_r31 + 14, ctx.r11.u16);
	// bl 0x8224aca0
	atSingleton_ACA0_g(ctx, base);
	// lis r10,-32253
	// lis r11,-32200
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r9,r11,-31664
	ctx.r9.s64 = ctx.r11.s64 + -31664;
	// li r6,0
	ctx.r6.s64 = 0;
	// lfs f1,-12016(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f1.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224b800
	LocomotionStateAnim_B800_g_B800_1(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofFloat_AB00_fw"))) PPC_WEAK_FUNC(crAnimDofFloat_AB00_fw);
PPC_FUNC_IMPL(__imp__crAnimDofFloat_AB00_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// lis r11,-32251
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r11,r11,-836
	ctx.r11.s64 = ctx.r11.s64 + -836;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 8);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224ab54
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 4);
		// lwz r9,76(r29)
		ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 76);
		// subf r8,r10,r11
		ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
		// twllei r9,0
		if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
		// divwu r10,r8,r9
		ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
		// addi r7,r10,2
		ctx.r7.s64 = ctx.r10.s64 + 2;
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r6,r29
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r29);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,8(r30)
		PPC_STORE_U32(var_r30 + 8, ctx.r5.u32);
	}
loc_8224AB54:
	// lhz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 12);
	// li r28,0
	var_r28 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x8224abc8
	if (ctx.r11.s32 > 0) {
		// li r31,0
		var_r31 = 0;
	loc_8224AB68:
		// lwz r10,8(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 8);
		// lwzx r11,r31,r10
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + ctx.r10.u32);
		// cmplwi cr6,r11,0
		// beq cr6,0x8224aba0
		if (ctx.r11.u32 != 0) {
			// lwz r4,4(r29)
			ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 4);
			// lwz r3,76(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 76);
			// subf r9,r4,r11
			ctx.r9.s64 = ctx.r11.s64 - ctx.r4.s64;
			// twllei r3,0
			if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
			// divwu r9,r9,r3
			ctx.r9.u32 = ctx.r3.u32 ? ctx.r9.u32 / ctx.r3.u32 : 0;
			// addi r8,r9,2
			ctx.r8.s64 = ctx.r9.s64 + 2;
			// rlwinm r7,r8,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r9,r7,r29
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r29);
			// add r6,r9,r11
			ctx.r6.u64 = ctx.r9.u64 + ctx.r11.u64;
			// stwx r6,r31,r10
			PPC_STORE_U32(var_r31 + ctx.r10.u32, ctx.r6.u32);
		}
	loc_8224ABA0:
		// lwz r5,8(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 8);
		// lwzx r3,r31,r5
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + ctx.r5.u32);
		// lbz r4,4(r3)
		ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// clrlwi r4,r4,28
		ctx.r4.u64 = ctx.r4.u32 & 0xF;
		// bl 0x8224a968
		crAnimDofFloat_ctor_A968(ctx, base);
		// lhz r11,12(r30)
		ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 12);
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r28,r11
		// blt cr6,0x8224ab68
		if ((int32_t)var_r28 < ctx.r11.s32) goto loc_8224AB68;
	}
loc_8224ABC8:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__crAnimFrame_ABD8_h"))) PPC_WEAK_FUNC(crAnimFrame_ABD8_h);
PPC_FUNC_IMPL(__imp__crAnimFrame_ABD8_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-836
	ctx.r11.s64 = ctx.r11.s64 + -836;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x8224ac28
	atSingleton_AC28_g(ctx, base);
	// lhz r11,14(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 14);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224ac10
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224AC10:
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_AC28_g"))) PPC_WEAK_FUNC(atSingleton_AC28_g);
PPC_FUNC_IMPL(__imp__atSingleton_AC28_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// lhz r29,12(r31)
	var_r29 = (uint32_t)(PPC_LOAD_U16(var_r31 + 12));
	// cmpwi cr6,r29,0
	// ble cr6,0x8224ac80
	if ((int32_t)var_r29 > 0) {
		// mr r30,r28
		var_r30 = (uint32_t)(var_r28);
	loc_8224AC4C:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r9,8(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// lwzx r4,r10,r30
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r30);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// addi r29,r29,-1
		var_r29 = (uint32_t)(var_r29 + -1);
		// cmplwi cr6,r29,0
		// stwx r28,r8,r30
		PPC_STORE_U32(ctx.r8.u32 + var_r30, var_r28);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// bne cr6,0x8224ac4c
		if (var_r29 != 0) goto loc_8224AC4C;
	}
loc_8224AC80:
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// stw r28,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r28);
	// sth r28,12(r31)
	PPC_STORE_U16(var_r31 + 12, (uint16_t)var_r28);
	// sth r28,14(r31)
	PPC_STORE_U16(var_r31 + 14, (uint16_t)var_r28);
	return;
}

__attribute__((alias("__imp__atSingleton_ACA0_g"))) PPC_WEAK_FUNC(atSingleton_ACA0_g);
PPC_FUNC_IMPL(__imp__atSingleton_ACA0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=160, savegprlr_24
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r24,r4
	var_r24 = ctx.r4.u32;
	// lhz r11,12(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 12);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224acc4
	if (ctx.r11.u32 != 0) {
		// bl 0x8224ac28
		atSingleton_AC28_g(ctx, base);
	}
loc_8224ACC4:
	// lhz r30,12(r24)
	var_r30 = (uint32_t)(PPC_LOAD_U16(var_r24 + 12));
	// addi r25,r28,8
	var_r25 = (uint32_t)(var_r28 + 8);
	// lhz r11,12(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 12);
	// li r29,0
	var_r29 = 0;
	// add r4,r11,r30
	ctx.r4.u64 = ctx.r11.u64 + var_r30;
	// cmplwi cr6,r4,0
	// beq cr6,0x8224acec
	if (ctx.r4.u32 != 0) {
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// bl 0x8224dde8
		atSingleton_DDE8_g(ctx, base);
		// b 0x8224acf4
	} else {
	loc_8224ACEC:
		// stw r29,0(r25)
		PPC_STORE_U32(var_r25 + 0, var_r29);
		// sth r29,6(r25)
		PPC_STORE_U16(var_r25 + 6, (uint16_t)var_r29);
	}
loc_8224ACF4:
	// cmpwi cr6,r30,0
	// ble cr6,0x8224ad54
while (var_r30 != 0) {
	loc_8224ACFC:
		// lwz r10,8(r24)
		ctx.r10.u64 = PPC_LOAD_U32(var_r24 + 8);
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r9,0(r28)
  // [ph4a] vtable load collapsed
		// lwzx r11,r29,r10
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + ctx.r10.u32);
		// lwz r8,4(r9)
  // [ph4a] slot load collapsed
		// lbz r7,4(r11)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
		// lhz r27,6(r11)
		var_r27 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + 6));
		// lbz r26,5(r11)
		var_r26 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 5));
		// clrlwi r4,r7,28
		ctx.r4.u64 = ctx.r7.u32 & 0xF;
		// bctrl
		VCALL(var_r28, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// stb r26,5(r31)
		PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r26);
		// sth r27,6(r31)
		PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r27);
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// addi r30,r30,-1
		var_r30 = (uint32_t)(var_r30 + -1);
		// stw r31,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmplwi cr6,r30,0
		// bne cr6,0x8224acfc
}
loc_8224AD54:
	return;
}

__attribute__((alias("__imp__atSingleton_AD60_g"))) PPC_WEAK_FUNC(atSingleton_AD60_g);
PPC_FUNC_IMPL(__imp__atSingleton_AD60_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r24 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=192, savegprlr_21
	// mr r24,r4
	var_r24 = ctx.r4.u32;
	// li r21,0
	var_r21 = 0;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r11,r21
	ctx.r11.u64 = var_r21;
	// lhz r22,12(r24)
	var_r22 = (uint32_t)(PPC_LOAD_U16(var_r24 + 12));
	// cmpwi cr6,r22,0
	// ble cr6,0x8224add8
	if ((int32_t)var_r22 > 0) {
		// lwz r10,0(r24)
		ctx.r10.u64 = PPC_LOAD_U32(var_r24 + 0);
		// mr r9,r22
		ctx.r9.u64 = var_r22;
	loc_8224AD90:
		// cmplwi cr6,r10,0
		// beq cr6,0x8224adc8
		if (ctx.r10.u32 != 0) {
			// lbz r8,27(r10)
			ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 27);
			// cmplwi cr6,r8,0
			// beq cr6,0x8224adb0
			if (ctx.r8.u32 != 0) {
				// clrlwi r11,r11,16
				ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
				// addi r7,r11,1
				ctx.r7.s64 = ctx.r11.s64 + 1;
				// clrlwi r11,r7,16
				ctx.r11.u64 = ctx.r7.u32 & 0xFFFF;
			}
		loc_8224ADB0:
			// lbz r6,26(r10)
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 26);
			// cmplwi cr6,r6,0
			// beq cr6,0x8224adc8
			if (ctx.r6.u32 == 0) goto loc_8224ADC8;
			// clrlwi r11,r11,16
			ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
			// addi r5,r11,1
			ctx.r5.s64 = ctx.r11.s64 + 1;
			// clrlwi r11,r5,16
			ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
		}
	loc_8224ADC8:
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// addi r10,r10,192
		ctx.r10.s64 = ctx.r10.s64 + 192;
		// cmplwi cr6,r9,0
		// bne cr6,0x8224ad90
		if (ctx.r9.u32 != 0) goto loc_8224AD90;
	}
loc_8224ADD8:
	// clrlwi r11,r11,16
	ctx.r11.u64 = ctx.r11.u32 & 0xFFFF;
	// lhz r3,12(r27)
	ctx.r3.u64 = PPC_LOAD_U16(var_r27 + 12);
	// addi r4,r11,2
	ctx.r4.s64 = ctx.r11.s64 + 2;
	// cmplwi cr6,r3,0
	// clrlwi r31,r4,16
	var_r31 = (uint32_t)(ctx.r4.u32 & 0xFFFF);
	// beq cr6,0x8224ae20
	if (ctx.r3.u32 != 0) {
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8224ac28
		atSingleton_AC28_g(ctx, base);
		// clrlwi r4,r31,16
		ctx.r4.u64 = var_r31 & 0xFFFF;
		// addi r25,r27,8
		var_r25 = (uint32_t)(var_r27 + 8);
		// cmplwi cr6,r4,0
		// beq cr6,0x8224ae14
		if (ctx.r4.u32 != 0) {
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// bl 0x8224dde8
			atSingleton_DDE8_g(ctx, base);
			// b 0x8224ae60
			goto loc_8224AE60;
		}
	loc_8224AE14:
		// stw r21,0(r25)
		PPC_STORE_U32(var_r25 + 0, var_r21);
		// sth r21,6(r25)
		PPC_STORE_U16(var_r25 + 6, (uint16_t)var_r21);
		// b 0x8224ae60
	} else {
	loc_8224AE20:
		// addi r25,r27,8
		var_r25 = (uint32_t)(var_r27 + 8);
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// lhz r9,4(r25)
		ctx.r9.u64 = PPC_LOAD_U16(var_r25 + 4);
		// lhz r10,12(r27)
		ctx.r10.u64 = PPC_LOAD_U16(var_r27 + 12);
		// addis r7,r9,1
		ctx.r7.s64 = ctx.r9.s64 + 65536;
		// lwz r11,0(r25)
		ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 0);
		// rotlwi r10,r10,2
		ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// add r8,r10,r11
		ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
		// clrlwi r10,r7,16
		ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
		// rlwinm r6,r10,2,14,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
		// sth r10,4(r25)
		PPC_STORE_U16(var_r25 + 4, ctx.r10.u16);
		// lwzx r5,r6,r11
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
		// stw r5,-4(r8)
		PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r5.u32);
	}
loc_8224AE60:
	// lwz r4,20(r24)
	ctx.r4.u64 = PPC_LOAD_U32(var_r24 + 20);
	// li r23,1
	var_r23 = 1;
	// clrlwi r3,r4,31
	ctx.r3.u64 = ctx.r4.u32 & 0x1;
	// mr r11,r23
	ctx.r11.u64 = var_r23;
	// cmplwi cr6,r3,0
	// bne cr6,0x8224ae7c
	if (ctx.r3.u32 == 0) {
		// mr r11,r21
		ctx.r11.u64 = var_r21;
	}
loc_8224AE7C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// mr r26,r23
	var_r26 = (uint32_t)(var_r23);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224af94
	if (ctx.r11.u32 != 0) {
		// lwz r10,0(r27)
  // [ph4a] vtable load collapsed
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r9,4(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// stb r21,5(r31)
		PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r21);
		// sth r21,6(r31)
		PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r21);
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// mr r30,r21
		var_r30 = (uint32_t)(var_r21);
		// stw r31,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
		// cmpwi cr6,r22,0
		// ble cr6,0x8224b134
		if ((int32_t)var_r22 <= 0) goto loc_8224B134;
		// lhz r29,80(r1)
		var_r29 = (uint32_t)(PPC_LOAD_U16(ctx.r1.u32 + 80));
	loc_8224AED0:
		// lwz r7,20(r24)
		ctx.r7.u64 = PPC_LOAD_U32(var_r24 + 20);
		// clrlwi r8,r30,16
		ctx.r8.u64 = var_r30 & 0xFFFF;
		// mr r11,r23
		ctx.r11.u64 = var_r23;
		// rlwinm r6,r7,0,30,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r6,0
		// bne cr6,0x8224aeec
		if (ctx.r6.u32 == 0) {
			// mr r11,r21
			ctx.r11.u64 = var_r21;
		}
	loc_8224AEEC:
		// clrlwi r4,r11,24
		ctx.r4.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// beq cr6,0x8224af24
		if (ctx.r4.u32 != 0) {
			// lhz r3,12(r24)
			ctx.r3.u64 = PPC_LOAD_U16(var_r24 + 12);
			// clrlwi r11,r8,16
			ctx.r11.u64 = ctx.r8.u32 & 0xFFFF;
			// cmplw cr6,r11,r3
			// bge cr6,0x8224af28
			if (ctx.r11.u32 >= ctx.r3.u32) goto loc_8224AF28;
			// rlwinm r9,r11,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// lwz r10,0(r24)
			ctx.r10.u64 = PPC_LOAD_U32(var_r24 + 0);
			// add r11,r11,r9
			ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
			// rlwinm r11,r11,6,0,25
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0xFFFFFFC0;
			// add r10,r11,r10
			ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lhz r29,22(r10)
			var_r29 = (uint32_t)(PPC_LOAD_U16(ctx.r10.u32 + 22));
			// b 0x8224af28
		} else {
		loc_8224AF24:
			// mr r29,r8
			var_r29 = ctx.r8.u32;
		}
	loc_8224AF28:
		// clrlwi r9,r26,24
		ctx.r9.u64 = var_r26 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x8224af48
		if (ctx.r9.u32 != 0) {
			// clrlwi r8,r8,16
			ctx.r8.u64 = ctx.r8.u32 & 0xFFFF;
			// clrlwi r7,r29,16
			ctx.r7.u64 = var_r29 & 0xFFFF;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// cmplw cr6,r8,r7
			// beq cr6,0x8224af4c
			if (ctx.r8.u32 == ctx.r7.u32) goto loc_8224AF4C;
		}
	loc_8224AF48:
		// mr r11,r21
		ctx.r11.u64 = var_r21;
	loc_8224AF4C:
		// lwz r6,0(r27)
  // [ph4a] vtable load collapsed
		// clrlwi r26,r11,24
		var_r26 = (uint32_t)(ctx.r11.u32 & 0xFF);
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r5,4(r6)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// stb r23,5(r31)
		PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r23);
		// sth r29,6(r31)
		PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r29);
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// stw r31,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
		// cmpw cr6,r30,r22
		// blt cr6,0x8224aed0
		if ((int32_t)var_r30 < (int32_t)var_r22) goto loc_8224AED0;
		// b 0x8224b134
	} else {
	loc_8224AF94:
		// mr r29,r21
		var_r29 = (uint32_t)(var_r21);
		// cmpwi cr6,r22,0
		// ble cr6,0x8224b044
		if ((int32_t)var_r22 > 0) {
			// mr r28,r21
			var_r28 = (uint32_t)(var_r21);
		loc_8224AFA4:
			// lwz r11,0(r24)
			ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 0);
			// add r11,r11,r28
			ctx.r11.u64 = ctx.r11.u64 + var_r28;
			// cmplwi cr6,r11,0
			// beq cr6,0x8224b034
			if (ctx.r11.u32 != 0) {
				// lbz r4,26(r11)
				ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + 26);
				// cmplwi cr6,r4,0
				// beq cr6,0x8224b034
				if (ctx.r4.u32 == 0) goto loc_8224B034;
				// clrlwi r31,r29,16
				var_r31 = (uint32_t)(var_r29 & 0xFFFF);
				// addi r5,r1,80
				ctx.r5.s64 = ctx.r1.s64 + 80;
				// mr r3,r24
				ctx.r3.u64 = var_r24;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// bl 0x8224a9f8
				game_A9F8(ctx, base);
				// clrlwi r3,r26,24
				ctx.r3.u64 = var_r26 & 0xFF;
				// lhz r30,80(r1)
				var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r1.u32 + 80));
				// cmplwi cr6,r3,0
				// beq cr6,0x8224aff8
				if (ctx.r3.u32 != 0) {
					// clrlwi r11,r31,16
					ctx.r11.u64 = var_r31 & 0xFFFF;
					// clrlwi r10,r30,16
					ctx.r10.u64 = var_r30 & 0xFFFF;
					// cmplw cr6,r11,r10
					// mr r11,r23
					ctx.r11.u64 = var_r23;
					// beq cr6,0x8224affc
					if (ctx.r11.u32 == ctx.r10.u32) goto loc_8224AFFC;
				}
			loc_8224AFF8:
				// mr r11,r21
				ctx.r11.u64 = var_r21;
			loc_8224AFFC:
				// lwz r9,0(r27)
  // [ph4a] vtable load collapsed
				// clrlwi r26,r11,24
				var_r26 = (uint32_t)(ctx.r11.u32 & 0xFF);
				// li r4,0
				ctx.r4.s64 = 0;
				// mr r3,r27
				ctx.r3.u64 = var_r27;
				// lwz r8,4(r9)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
				// mr r31,r3
				var_r31 = ctx.r3.u32;
				// li r4,1
				ctx.r4.s64 = 1;
				// mr r3,r25
				ctx.r3.u64 = var_r25;
				// stb r21,5(r31)
				PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r21);
				// sth r30,6(r31)
				PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r30);
				// bl 0x8224de98
				atSingleton_DE98_g(ctx, base);
				// stw r31,0(r3)
				PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
			}
		loc_8224B034:
			// addi r29,r29,1
			var_r29 = (uint32_t)(var_r29 + 1);
			// addi r28,r28,192
			var_r28 = (uint32_t)(var_r28 + 192);
			// cmpw cr6,r29,r22
			// blt cr6,0x8224afa4
			if ((int32_t)var_r29 < (int32_t)var_r22) goto loc_8224AFA4;
		}
	loc_8224B044:
		// mr r29,r21
		var_r29 = (uint32_t)(var_r21);
		// cmpwi cr6,r22,0
		// ble cr6,0x8224b134
		if ((int32_t)var_r22 <= 0) goto loc_8224B134;
		// lhz r30,80(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r1.u32 + 80));
		// mr r28,r21
		var_r28 = (uint32_t)(var_r21);
	loc_8224B058:
		// lwz r9,0(r24)
		ctx.r9.u64 = PPC_LOAD_U32(var_r24 + 0);
		// add r11,r9,r28
		ctx.r11.u64 = ctx.r9.u64 + var_r28;
		// cmplwi cr6,r11,0
		// beq cr6,0x8224b124
		if (ctx.r11.u32 != 0) {
			// lbz r7,27(r11)
			ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 27);
			// cmplwi cr6,r7,0
			// beq cr6,0x8224b124
			if (ctx.r7.u32 == 0) goto loc_8224B124;
			// lwz r6,20(r24)
			ctx.r6.u64 = PPC_LOAD_U32(var_r24 + 20);
			// clrlwi r8,r29,16
			ctx.r8.u64 = var_r29 & 0xFFFF;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// rlwinm r5,r6,0,30,30
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x2;
			// cmplwi cr6,r5,0
			// bne cr6,0x8224b090
			if (ctx.r5.u32 == 0) {
				// mr r11,r21
				ctx.r11.u64 = var_r21;
			}
		loc_8224B090:
			// clrlwi r3,r11,24
			ctx.r3.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r3,0
			// beq cr6,0x8224b0c4
			if (ctx.r3.u32 != 0) {
				// lhz r10,12(r24)
				ctx.r10.u64 = PPC_LOAD_U16(var_r24 + 12);
				// clrlwi r11,r8,16
				ctx.r11.u64 = ctx.r8.u32 & 0xFFFF;
				// cmplw cr6,r11,r10
				// bge cr6,0x8224b0c8
				if (ctx.r11.u32 >= ctx.r10.u32) goto loc_8224B0C8;
				// rlwinm r10,r11,1,0,30
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
				// add r7,r11,r10
				ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
				// rlwinm r11,r7,6,0,25
				ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
				// add r6,r11,r9
				ctx.r6.u64 = ctx.r11.u64 + ctx.r9.u64;
				// lhz r30,22(r6)
				var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r6.u32 + 22));
				// b 0x8224b0c8
			} else {
			loc_8224B0C4:
				// mr r30,r8
				var_r30 = ctx.r8.u32;
			}
		loc_8224B0C8:
			// clrlwi r5,r26,24
			ctx.r5.u64 = var_r26 & 0xFF;
			// cmplwi cr6,r5,0
			// beq cr6,0x8224b0e8
			if (ctx.r5.u32 != 0) {
				// clrlwi r4,r8,16
				ctx.r4.u64 = ctx.r8.u32 & 0xFFFF;
				// clrlwi r3,r30,16
				ctx.r3.u64 = var_r30 & 0xFFFF;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
				// cmplw cr6,r4,r3
				// beq cr6,0x8224b0ec
				if (ctx.r4.u32 == ctx.r3.u32) goto loc_8224B0EC;
			}
		loc_8224B0E8:
			// mr r11,r21
			ctx.r11.u64 = var_r21;
		loc_8224B0EC:
			// lwz r10,0(r27)
  // [ph4a] vtable load collapsed
			// clrlwi r26,r11,24
			var_r26 = (uint32_t)(ctx.r11.u32 & 0xFF);
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// lwz r9,4(r10)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
			// mr r31,r3
			var_r31 = ctx.r3.u32;
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// stb r23,5(r31)
			PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r23);
			// sth r30,6(r31)
			PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r30);
			// bl 0x8224de98
			atSingleton_DE98_g(ctx, base);
			// stw r31,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
		}
	loc_8224B124:
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r28,r28,192
		var_r28 = (uint32_t)(var_r28 + 192);
		// cmpw cr6,r29,r22
		// blt cr6,0x8224b058
		if ((int32_t)var_r29 < (int32_t)var_r22) goto loc_8224B058;
	}
loc_8224B134:
	// lwz r8,0(r27)
  // [ph4a] vtable load collapsed
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// lwz r7,4(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r6,5
	ctx.r6.s64 = 5;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// sth r21,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r21);
	// stb r6,5(r31)
	PPC_STORE_U8(var_r31 + 5, ctx.r6.u8);
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// stw r31,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r31);
	// lwz r11,4(r5)
	// bctrl
	atSingleton_vfn_1(ctx, base);  // vtable slot 1 (byte +4)  // atSingleton::vfn_1
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r10,6
	ctx.r10.s64 = 6;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// sth r21,6(r31)
	PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r21);
	// stb r10,5(r31)
	PPC_STORE_U8(var_r31 + 5, ctx.r10.u8);
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// clrlwi r9,r26,24
	ctx.r9.u64 = var_r26 & 0xFF;
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
	// cmplwi cr6,r9,0
	// bne cr6,0x8224b1c0
	if (ctx.r9.u32 == 0) {
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8224bf98
		atSingleton_BF98_g(ctx, base);
	}
loc_8224B1C0:
	return;
}

__attribute__((alias("__imp__atSingleton_B1C8_g"))) PPC_WEAK_FUNC(atSingleton_B1C8_g);
PPC_FUNC_IMPL(__imp__atSingleton_B1C8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r26,0
	var_r26 = 0;
	// lhz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 12);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224b248
	if (ctx.r11.u32 != 0) {
		// bl 0x8224ac28
		atSingleton_AC28_g(ctx, base);
		// li r3,8
		ctx.r3.s64 = 8;
		// addi r31,r30,8
		var_r31 = (uint32_t)(var_r30 + 8);
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// mr r27,r3
		var_r27 = ctx.r3.u32;
		// cmplwi cr6,r27,0
		// beq cr6,0x8224b234
		if (var_r27 != 0) {
			// mr r28,r27
			var_r28 = (uint32_t)(var_r27);
			// li r29,1
			var_r29 = 1;
		loc_8224B20C:
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// bl 0x8224dfc0
			ke_DFC0(ctx, base);
			// addi r29,r29,-1
			var_r29 = (uint32_t)(var_r29 + -1);
			// addi r28,r28,4
			var_r28 = (uint32_t)(var_r28 + 4);
			// cmpwi cr6,r29,0
			// bge cr6,0x8224b20c
			if ((int32_t)var_r29 >= 0) goto loc_8224B20C;
			// li r10,2
			ctx.r10.s64 = 2;
			// stw r27,0(r31)
			PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ var_r27);
			// sth r10,6(r31)
			PPC_STORE_U16(var_r31 + 6, ctx.r10.u16);
			// b 0x8224b288
			goto loc_8224B288;
		}
	loc_8224B234:
		// mr r27,r26
		var_r27 = (uint32_t)(var_r26);
		// li r10,2
		ctx.r10.s64 = 2;
		// stw r27,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ var_r27);
		// sth r10,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r10.u16);
		// b 0x8224b288
	} else {
	loc_8224B248:
		// addi r31,r30,8
		var_r31 = (uint32_t)(var_r30 + 8);
		// li r4,2
		ctx.r4.s64 = 2;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// lhz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U16(var_r31 + 4);
		// lhz r10,12(r30)
		ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 12);
		// addis r7,r9,1
		ctx.r7.s64 = ctx.r9.s64 + 65536;
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
		// rotlwi r10,r10,2
		ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// add r8,r10,r11
		ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
		// clrlwi r10,r7,16
		ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
		// rlwinm r6,r10,2,14,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
		// sth r10,4(r31)
		PPC_STORE_U16(var_r31 + 4, ctx.r10.u16);
		// lwzx r5,r6,r11
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
		// stw r5,-4(r8)
		PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r5.u32);
	}
loc_8224B288:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0)/* atSingleton::vtable@+0x0 */;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r9,5
	ctx.r9.s64 = 5;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// sth r26,6(r29)
	PPC_STORE_U16(var_r29 + 6, (uint16_t)var_r26);
	// stb r9,5(r29)
	PPC_STORE_U8(var_r29 + 5, ctx.r9.u8);
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r29,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r29);
	// lwz r7,4(r8)
	// bctrl
	atSingleton_vfn_1(ctx, base);  // vtable slot 1 (byte +4)  // atSingleton::vfn_1
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r6,6
	ctx.r6.s64 = 6;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// sth r26,6(r30)
	PPC_STORE_U16(var_r30 + 6, (uint16_t)var_r26);
	// stb r6,5(r30)
	PPC_STORE_U8(var_r30 + 5, ctx.r6.u8);
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r30);
	return;
}

__attribute__((alias("__imp__atSingleton_B308_g"))) PPC_WEAK_FUNC(atSingleton_B308_g);
PPC_FUNC_IMPL(__imp__atSingleton_B308_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	// FRAME: size=160, savegprlr_23
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// lhz r11,12(r27)
	ctx.r11.u64 = PPC_LOAD_U16(var_r27 + 12);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224b348
	if (ctx.r11.u32 != 0) {
		// bl 0x8224ac28
		atSingleton_AC28_g(ctx, base);
		// addi r26,r27,8
		var_r26 = (uint32_t)(var_r27 + 8);
		// li r4,2
		ctx.r4.s64 = 2;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x8224dde8
		atSingleton_DDE8_g(ctx, base);
		// b 0x8224b388
	} else {
	loc_8224B348:
		// addi r26,r27,8
		var_r26 = (uint32_t)(var_r27 + 8);
		// li r4,2
		ctx.r4.s64 = 2;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// lhz r9,4(r26)
		ctx.r9.u64 = PPC_LOAD_U16(var_r26 + 4);
		// lhz r10,12(r27)
		ctx.r10.u64 = PPC_LOAD_U16(var_r27 + 12);
		// addis r7,r9,1
		ctx.r7.s64 = ctx.r9.s64 + 65536;
		// lwz r11,0(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 0);
		// rotlwi r10,r10,2
		ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// add r8,r10,r11
		ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
		// clrlwi r10,r7,16
		ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
		// rlwinm r6,r10,2,14,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
		// sth r10,4(r26)
		PPC_STORE_U16(var_r26 + 4, ctx.r10.u16);
		// lwzx r5,r6,r11
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
		// stw r5,-4(r8)
		PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r5.u32);
	}
loc_8224B388:
	// subf r23,r31,r30
	var_r23 = var_r30 - var_r31;
	// li r29,2
	var_r29 = 2;
loc_8224B390:
	// lwz r11,0(r27)
  // [ph4a] vtable load collapsed
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// lbzx r4,r23,r31
	ctx.r4.u64 = PPC_LOAD_U8(var_r23 + var_r31);
	// lhz r25,0(r28)
	var_r25 = (uint32_t)(PPC_LOAD_U16(var_r28 + 0));
	// lbz r24,0(r31)
	var_r24 = (uint32_t)(PPC_LOAD_U8(var_r31 + 0));
	// lwz r10,4(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	// stb r24,5(r30)
	PPC_STORE_U8(var_r30 + 5, (uint8_t)var_r24);
	// sth r25,6(r30)
	PPC_STORE_U16(var_r30 + 6, (uint16_t)var_r25);
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// addi r29,r29,-1
	var_r29 = (uint32_t)(var_r29 + -1);
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r30);
	// addi r28,r28,2
	var_r28 = (uint32_t)(var_r28 + 2);
	// addi r31,r31,1
	var_r31 = (uint32_t)(var_r31 + 1);
	// cmplwi cr6,r29,0
	// bne cr6,0x8224b390
	if (var_r29 != 0) goto loc_8224B390;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x8224bf98
	atSingleton_BF98_g(ctx, base);
	return;
}

__attribute__((alias("__imp__atSingleton_B3F8_g"))) PPC_WEAK_FUNC(atSingleton_B3F8_g);
PPC_FUNC_IMPL(__imp__atSingleton_B3F8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=160, savegprlr_24
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r24,r4
	var_r24 = ctx.r4.u32;
	// li r29,0
	var_r29 = 0;
	// lhz r11,12(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 12);
	// lhz r30,40(r24)
	var_r30 = (uint32_t)(PPC_LOAD_U16(var_r24 + 40));
	// cmplwi cr6,r11,0
	// beq cr6,0x8224b44c
	if (ctx.r11.u32 != 0) {
		// bl 0x8224ac28
		atSingleton_AC28_g(ctx, base);
		// addi r27,r28,8
		var_r27 = (uint32_t)(var_r28 + 8);
		// cmplwi cr6,r30,0
		// beq cr6,0x8224b440
		if (var_r30 != 0) {
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x8224dde8
			atSingleton_DDE8_g(ctx, base);
			// b 0x8224b48c
			goto loc_8224B48C;
		}
	loc_8224B440:
		// stw r29,0(r27)
		PPC_STORE_U32(var_r27 + 0, var_r29);
		// sth r29,6(r27)
		PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r29);
		// b 0x8224b48c
	} else {
	loc_8224B44C:
		// addi r27,r28,8
		var_r27 = (uint32_t)(var_r28 + 8);
		// clrlwi r4,r30,16
		ctx.r4.u64 = var_r30 & 0xFFFF;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// lhz r9,4(r27)
		ctx.r9.u64 = PPC_LOAD_U16(var_r27 + 4);
		// lhz r10,12(r28)
		ctx.r10.u64 = PPC_LOAD_U16(var_r28 + 12);
		// addis r7,r9,1
		ctx.r7.s64 = ctx.r9.s64 + 65536;
		// lwz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
		// rotlwi r10,r10,2
		ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// add r8,r10,r11
		ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
		// clrlwi r10,r7,16
		ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
		// rlwinm r6,r10,2,14,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
		// sth r10,4(r27)
		PPC_STORE_U16(var_r27 + 4, ctx.r10.u16);
		// lwzx r5,r6,r11
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
		// stw r5,-4(r8)
		PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r5.u32);
	}
loc_8224B48C:
	// cmpwi cr6,r30,0
	// ble cr6,0x8224b4ec
while (var_r30 != 0) {
	loc_8224B494:
		// lwz r4,36(r24)
		ctx.r4.u64 = PPC_LOAD_U32(var_r24 + 36);
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r10,0(r28)
  // [ph4a] vtable load collapsed
		// lwzx r11,r29,r4
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + ctx.r4.u32);
		// lwz r9,4(r10)
  // [ph4a] slot load collapsed
		// lbz r8,1(r11)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
		// lhz r26,2(r11)
		var_r26 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + 2));
		// lbz r25,0(r11)
		var_r25 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 0));
		// clrlwi r4,r8,28
		ctx.r4.u64 = ctx.r8.u32 & 0xF;
		// bctrl
		VCALL(var_r28, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// stb r25,5(r31)
		PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r25);
		// sth r26,6(r31)
		PPC_STORE_U16(var_r31 + 6, (uint16_t)var_r26);
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// addi r30,r30,-1
		var_r30 = (uint32_t)(var_r30 + -1);
		// stw r31,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmplwi cr6,r30,0
		// bne cr6,0x8224b494
}
loc_8224B4EC:
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_B4F8_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_B4F8_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_B4F8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=304, savegprlr_23
	// mr r23,r6
	var_r23 = ctx.r6.u32;
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// rlwinm r24,r23,0,24,24
	var_r24 = (uint32_t)(__builtin_rotateleft64(var_r23 | (var_r23 << 32), 0) & 0x80);
	// mr r26,r4
	var_r26 = ctx.r4.u32;
	// cmpwi cr6,r24,0
	// li r9,1
	ctx.r9.s64 = 1;
	// bne cr6,0x8224b524
	if ((int32_t)var_r24 == 0) {
		// li r9,0
		ctx.r9.s64 = 0;
	}
loc_8224B524:
	// lis r11,21845
	ctx.r11.s64 = 1431633920;
	// ori r10,r11,21846
	ctx.r10.u64 = ctx.r11.u64 | 21846;
	// mulhw r11,r5,r10
	ctx.r11.s64 = (int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32)) >> 32;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0x1;
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf r11,r9,r8
	ctx.r11.s64 = ctx.r8.s64 - ctx.r9.s64;
	// addi r27,r11,-1
	var_r27 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8204ffff
	// cmpwi cr6,r27,0
	// ble cr6,0x8224b6cc
	if ((int32_t)var_r27 > 0) {
		// addi r25,r27,-1
		var_r25 = (uint32_t)(var_r27 + -1);
		// li r29,0
		var_r29 = 0;
	loc_8224B550:
		// cmpw cr6,r29,r25
		// bne cr6,0x8224b564
		if ((int32_t)var_r29 == (int32_t)var_r25) {
			// cmpwi cr6,r24,0
			// li r11,1
			ctx.r11.s64 = 1;
			// bne cr6,0x8224b568
			if ((int32_t)var_r24 != 0) goto loc_8224B568;
		}
	loc_8224B564:
		// li r11,0
		ctx.r11.s64 = 0;
	loc_8224B568:
		// li r5,3
		ctx.r5.s64 = 3;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// clrlwi r31,r11,24
		var_r31 = (uint32_t)(ctx.r11.u32 & 0xFF);
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
		// cmplwi cr6,r29,0
		// beq cr6,0x8224b590
		if (var_r29 != 0) {
			// clrlwi r30,r31,24
			var_r30 = (uint32_t)(var_r31 & 0xFF);
			// cmplwi cr6,r30,0
			// beq cr6,0x8224b624
			if (var_r30 == 0) goto loc_8224B624;
		}
	loc_8224B590:
		// lwz r7,0(r28)
  // [ph4a] vtable load collapsed
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r6,4(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r28, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// clrlwi r30,r31,24
		var_r30 = (uint32_t)(var_r31 & 0xFF);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmplwi cr6,r30,0
		// li r11,5
		ctx.r11.s64 = 5;
		// bne cr6,0x8224b5c0
		if (var_r30 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224B5C0:
		// stb r11,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r11.u8);
		// cmplwi cr6,r30,0
		// li r11,0
		ctx.r11.s64 = 0;
		// bne cr6,0x8224b5d4
		if (var_r30 == 0) {
			// mr r11,r29
			ctx.r11.u64 = var_r29;
		}
	loc_8224B5D4:
		// sth r11,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
		// lfs f0,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f13.f64 = double(temp.f32);
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// lfs f12,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f12.f64 = double(temp.f32);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stfs f0,96(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// stfs f13,100(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
		// stfs f12,104(r1)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
		// lwz r9,68(r10)
		// bctrl
		VCALL(ctx.r3.u32, 17, ctx, base);  // LocomotionStateAnim::vfn_17 (unnamed)  // vtable slot 17 (byte +68)
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x8224bd98
		atSingleton_BD98_fw(ctx, base);
		// li r5,3
		ctx.r5.s64 = 3;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
	loc_8224B624:
		// lwz r8,0(r28)
  // [ph4a] vtable load collapsed
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r7,4(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r28, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmplwi cr6,r30,0
		// li r11,6
		ctx.r11.s64 = 6;
		// bne cr6,0x8224b650
		if (var_r30 == 0) {
			// li r11,1
			ctx.r11.s64 = 1;
		}
	loc_8224B650:
		// stb r11,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r11.u8);
		// cmplwi cr6,r30,0
		// li r11,0
		ctx.r11.s64 = 0;
		// bne cr6,0x8224b664
		if (var_r30 == 0) {
			// mr r11,r29
			ctx.r11.u64 = var_r29;
		}
	loc_8224B664:
		// sth r11,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
		// addi r4,r1,112
		ctx.r4.s64 = ctx.r1.s64 + 112;
		// addi r3,r1,160
		ctx.r3.s64 = ctx.r1.s64 + 160;
		// lfs f11,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f9.f64 = double(temp.f32);
		// stfs f11,112(r1)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// stfs f10,116(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
		// stfs f9,120(r1)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
		// bl 0x82118278
		LocomotionStateAnim_8278_g(ctx, base);
		// addi r4,r1,160
		ctx.r4.s64 = ctx.r1.s64 + 160;
		// addi r3,r1,128
		ctx.r3.s64 = ctx.r1.s64 + 128;
		// bl 0x8223b218
		LocomotionStateAnim_B218_g(ctx, base);
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// addi r4,r1,128
		ctx.r4.s64 = ctx.r1.s64 + 128;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,72(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 18, ctx, base);  // pattern-B slot 18 (byte +72)
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x8224bd98
		atSingleton_BD98_fw(ctx, base);
		// addi r9,r29,1
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 1;
		// clrlwi r29,r9,16
		var_r29 = (uint32_t)(ctx.r9.u32 & 0xFFFF);
		// cmpw cr6,r29,r27
		// blt cr6,0x8224b550
		if ((int32_t)var_r29 < (int32_t)var_r27) goto loc_8224B550;
	}
loc_8224B6CC:
	// rlwinm r8,r23,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(var_r23 | (var_r23 << 32), 0) & 0x80000000;
	// cmpwi cr6,r8,0
	// beq cr6,0x8224b6e8
	if (ctx.r8.s32 != 0) {
		// li r5,1
		ctx.r5.s64 = 1;
		// addi r4,r1,144
		ctx.r4.s64 = ctx.r1.s64 + 144;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x822e3cd8
		grc_3CD8(ctx, base);
	}
loc_8224B6E8:
	return;
}

__attribute__((alias("__imp__lvlLevelPiece_B6F0_2hr"))) PPC_WEAK_FUNC(lvlLevelPiece_B6F0_2hr);
PPC_FUNC_IMPL(__imp__lvlLevelPiece_B6F0_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=176, savegprlr_24
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r27,r5
	var_r27 = ctx.r5.u32;
	// mr r25,r7
	var_r25 = ctx.r7.u32;
	// mr r24,r8
	var_r24 = ctx.r8.u32;
	// lhz r31,12(r29)
	var_r31 = (uint32_t)(PPC_LOAD_U16(var_r29 + 12));
	// cmplwi cr6,r28,0
	// beq cr6,0x8224b7b0
	if (var_r28 != 0) {
		// cmpwi cr6,r31,0
		// ble cr6,0x8224b7ec
		if ((int32_t)var_r31 <= 0) {
			return;
		}
		// lis r11,-32253
		// li r30,0
		var_r30 = 0;
		// addi r11,r11,-12016
		ctx.r11.s64 = ctx.r11.s64 + -12016;
		// mr r26,r31
		var_r26 = (uint32_t)(var_r31);
		// lfs f31,-8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		var_f31 = double(temp.f32);
		// lfs f30,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		var_f30 = double(temp.f32);
	loc_8224B744:
		// lwz r11,8(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 8);
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// mr r6,r27
		ctx.r6.u64 = var_r27;
		// li r3,0
		ctx.r3.s64 = 0;
		// lwzx r31,r11,r30
		var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + var_r30));
		// stfs f31,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// lhz r5,6(r31)
		ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 6);
		// lbz r4,5(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 5);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(var_r28);
		// clrlwi r10,r3,24
		ctx.r10.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x8224b790
		if (ctx.r10.u32 != 0) {
			// lfs f0,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f0.f64 = double(temp.f32);
			// mr r5,r24
			ctx.r5.u64 = var_r24;
			// fmuls f1,f0,f30
			ctx.f1.f64 = double(float(ctx.f0.f64 * var_f30));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bctrl
			PPC_CALL_INDIRECT_FUNC(var_r25);
		}
	loc_8224B790:
		// addi r26,r26,-1
		var_r26 = (uint32_t)(var_r26 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r26,0
		// bne cr6,0x8224b744
		if (var_r26 != 0) goto loc_8224B744;
		return;
	}
loc_8224B7B0:
	// cmpwi cr6,r31,0
	// ble cr6,0x8224b7ec
	if ((int32_t)var_r31 > 0) {
		// lis r11,-32253
		// li r30,0
		var_r30 = 0;
		// lfs f31,-12016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		var_f31 = double(temp.f32);
	loc_8224B7C4:
		// lwz r9,8(r29)
		ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 8);
		// mr r5,r24
		ctx.r5.u64 = var_r24;
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// lwzx r3,r9,r30
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r30);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(var_r25);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r31,0
		// bne cr6,0x8224b7c4
		if (var_r31 != 0) goto loc_8224B7C4;
	}
loc_8224B7EC:
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_B800_g_B800_1"))) PPC_WEAK_FUNC(LocomotionStateAnim_B800_g_B800_1);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_B800_g_B800_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r21 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=208, savegprlr_20
	// mr r21,r3
	var_r21 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r22,r4
	var_r22 = ctx.r4.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// mr r26,r5
	var_r26 = ctx.r5.u32;
	// mr r25,r7
	var_r25 = ctx.r7.u32;
	// lhz r11,12(r21)
	ctx.r11.u64 = PPC_LOAD_U16(var_r21 + 12);
	// mr r20,r9
	var_r20 = ctx.r9.u32;
	// lhz r23,12(r22)
	var_r23 = (uint32_t)(PPC_LOAD_U16(var_r22 + 12));
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r28,0
	// beq cr6,0x8224b954
	if (var_r28 != 0) {
		// cmpwi cr6,r11,0
		// ble cr6,0x8224ba18
		if (ctx.r11.s32 <= 0) {
			return;
		}
		// mr r24,r11
		var_r24 = ctx.r11.u32;
		// lis r11,-32253
		// li r27,0
		var_r27 = 0;
		// lfs f30,-12024(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
		var_f30 = double(temp.f32);
	loc_8224B85C:
		// lwz r11,8(r21)
		ctx.r11.u64 = PPC_LOAD_U32(var_r21 + 8);
		// cmpw cr6,r30,r23
		// lwzx r29,r27,r11
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r27 + ctx.r11.u32));
		// bge cr6,0x8224b934
		if ((int32_t)var_r30 < (int32_t)var_r23) {
			// lwz r10,8(r22)
			ctx.r10.u64 = PPC_LOAD_U32(var_r22 + 8);
			// rlwinm r11,r30,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
			// lhz r5,6(r29)
			ctx.r5.u64 = PPC_LOAD_U16(var_r29 + 6);
			// add r9,r11,r10
			ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lbz r4,5(r29)
			ctx.r4.u64 = PPC_LOAD_U8(var_r29 + 5);
			// mr r10,r5
			ctx.r10.u64 = ctx.r5.u64;
			// rlwimi r10,r4,16,8,15
			ctx.r10.u64 = (__builtin_rotateleft32(ctx.r4.u32, 16) & 0xFF0000) | (ctx.r10.u64 & 0xFFFFFFFFFF00FFFF);
			// clrlwi r7,r10,8
			ctx.r7.u64 = ctx.r10.u32 & 0xFFFFFF;
		loc_8224B88C:
			// lwz r31,0(r9)
			var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + 0));
			// li r11,1
			ctx.r11.s64 = 1;
			// lbz r8,5(r31)
			ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 5);
			// lhz r6,6(r31)
			ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 6);
			// rotlwi r3,r8,16
			ctx.r3.u64 = __builtin_rotateleft32(ctx.r8.u32, 16);
			// or r10,r3,r6
			ctx.r10.u64 = ctx.r3.u64 | ctx.r6.u64;
			// cmplw cr6,r10,r7
			// bgt cr6,0x8224b8b0
			if (ctx.r10.u32 <= ctx.r7.u32) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8224B8B0:
			// clrlwi r8,r11,24
			ctx.r8.u64 = ctx.r11.u32 & 0xFF;
			// cmplw cr6,r10,r7
			// li r11,1
			ctx.r11.s64 = 1;
			// beq cr6,0x8224b8c4
			if (ctx.r10.u32 != ctx.r7.u32) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8224B8C4:
			// clrlwi r10,r11,24
			ctx.r10.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r10,0
			// bne cr6,0x8224b8f0
			if (ctx.r10.u32 == 0) {
				// clrlwi r8,r8,24
				ctx.r8.u64 = ctx.r8.u32 & 0xFF;
				// cmplwi cr6,r8,0
				// bne cr6,0x8224b934
				if (ctx.r8.u32 != 0) goto loc_8224B934;
				// addi r30,r30,1
				var_r30 = (uint32_t)(var_r30 + 1);
				// addi r9,r9,4
				ctx.r9.s64 = ctx.r9.s64 + 4;
				// cmpw cr6,r30,r23
				// blt cr6,0x8224b88c
				if ((int32_t)var_r30 < (int32_t)var_r23) goto loc_8224B88C;
				// b 0x8224b934
			} else {
			loc_8224B8F0:
				// stfs f30,80(r1)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(var_f30);
				PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
				// addi r7,r1,80
				ctx.r7.s64 = ctx.r1.s64 + 80;
				// mr r6,r25
				ctx.r6.u64 = var_r25;
				// mr r3,r26
				ctx.r3.u64 = var_r26;
				// bctrl
				PPC_CALL_INDIRECT_FUNC(var_r28);
				// clrlwi r7,r3,24
				ctx.r7.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r7,0
				// beq cr6,0x8224b930
				if (ctx.r7.u32 != 0) {
					// lfs f0,80(r1)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
					ctx.f0.f64 = double(temp.f32);
					// li r6,0
					ctx.r6.s64 = 0;
					// fmuls f1,f0,f31
					ctx.f1.f64 = double(float(ctx.f0.f64 * var_f31));
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// mr r3,r29
					ctx.r3.u64 = var_r29;
					// bctrl
					PPC_CALL_INDIRECT_FUNC(var_r20);
				}
			loc_8224B930:
				// addi r30,r30,1
				var_r30 = (uint32_t)(var_r30 + 1);
			}
		}
	loc_8224B934:
		// addi r24,r24,-1
		var_r24 = (uint32_t)(var_r24 + -1);
		// addi r27,r27,4
		var_r27 = (uint32_t)(var_r27 + 4);
		// cmplwi cr6,r24,0
		// bne cr6,0x8224b85c
		if (var_r24 != 0) goto loc_8224B85C;
		return;
	}
loc_8224B954:
	// cmpwi cr6,r11,0
	// ble cr6,0x8224ba18
	if (ctx.r11.s32 > 0) {
		// li r31,0
		var_r31 = 0;
		// mr r29,r11
		var_r29 = ctx.r11.u32;
	loc_8224B964:
		// lwz r6,8(r21)
		ctx.r6.u64 = PPC_LOAD_U32(var_r21 + 8);
		// cmpw cr6,r30,r23
		// lwzx r3,r31,r6
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + ctx.r6.u32);
		// bge cr6,0x8224ba08
		if ((int32_t)var_r30 < (int32_t)var_r23) {
			// lbz r5,5(r3)
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
			// rlwinm r11,r30,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
			// lhz r4,6(r3)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r3.u32 + 6);
			// lwz r10,8(r22)
			ctx.r10.u64 = PPC_LOAD_U32(var_r22 + 8);
			// rotlwi r9,r5,16
			ctx.r9.u64 = __builtin_rotateleft32(ctx.r5.u32, 16);
			// or r7,r9,r4
			ctx.r7.u64 = ctx.r9.u64 | ctx.r4.u64;
			// add r8,r11,r10
			ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
		loc_8224B990:
			// lwz r4,0(r8)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
			// lbz r6,5(r4)
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
			// lhz r5,6(r4)
			ctx.r5.u64 = PPC_LOAD_U16(ctx.r4.u32 + 6);
			// rotlwi r11,r6,16
			ctx.r11.u64 = __builtin_rotateleft32(ctx.r6.u32, 16);
			// or r10,r11,r5
			ctx.r10.u64 = ctx.r11.u64 | ctx.r5.u64;
			// li r11,1
			ctx.r11.s64 = 1;
			// cmplw cr6,r10,r7
			// bgt cr6,0x8224b9b4
			if (ctx.r10.u32 <= ctx.r7.u32) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8224B9B4:
			// clrlwi r9,r11,24
			ctx.r9.u64 = ctx.r11.u32 & 0xFF;
			// cmplw cr6,r10,r7
			// li r11,1
			ctx.r11.s64 = 1;
			// beq cr6,0x8224b9c8
			if (ctx.r10.u32 != ctx.r7.u32) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8224B9C8:
			// clrlwi r6,r11,24
			ctx.r6.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r6,0
			// bne cr6,0x8224b9f4
			if (ctx.r6.u32 == 0) {
				// clrlwi r5,r9,24
				ctx.r5.u64 = ctx.r9.u32 & 0xFF;
				// cmplwi cr6,r5,0
				// bne cr6,0x8224ba08
				if (ctx.r5.u32 != 0) goto loc_8224BA08;
				// addi r30,r30,1
				var_r30 = (uint32_t)(var_r30 + 1);
				// addi r8,r8,4
				ctx.r8.s64 = ctx.r8.s64 + 4;
				// cmpw cr6,r30,r23
				// blt cr6,0x8224b990
				if ((int32_t)var_r30 < (int32_t)var_r23) goto loc_8224B990;
				// b 0x8224ba08
			} else {
			loc_8224B9F4:
				// li r6,0
				ctx.r6.s64 = 0;
				// fmr f1,f31
				ctx.fpscr.disableFlushMode();
				ctx.f1.f64 = var_f31;
				// bctrl
				PPC_CALL_INDIRECT_FUNC(var_r20);
				// addi r30,r30,1
				var_r30 = (uint32_t)(var_r30 + 1);
			}
		}
	loc_8224BA08:
		// addi r29,r29,-1
		var_r29 = (uint32_t)(var_r29 + -1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmplwi cr6,r29,0
		// bne cr6,0x8224b964
		if (var_r29 != 0) goto loc_8224B964;
	}
loc_8224BA18:
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_BA28_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_BA28_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_BA28_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_25
	// lis r10,-32253
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// addi r7,r10,-11928
	ctx.r7.s64 = ctx.r10.s64 + -11928;
	// lis r11,-32219
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r26,r4
	var_r26 = ctx.r4.u32;
	// lhz r28,4(r29)
	var_r28 = (uint32_t)(PPC_LOAD_U16(var_r29 + 4));
	// li r10,0
	ctx.r10.s64 = 0;
	// lfs f31,-88(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -88);
	var_f31 = double(temp.f32);
	// addi r25,r11,-17512
	var_r25 = (uint32_t)(ctx.r11.s64 + -17512);  // LocomotionStateAnim_BB98 @ 0x8224bb98
	// fmr f0,f31
	ctx.f0.f64 = var_f31;
	// cmpwi cr6,r28,4
	// blt cr6,0x8224babc
	if ((int32_t)var_r28 >= 4) {
		// addi r10,r28,-4
		ctx.r10.s64 = (int64_t)(int32_t)var_r28 + -4;
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// rlwinm r10,r10,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// addi r9,r10,1
		ctx.r9.s64 = ctx.r10.s64 + 1;
		// rlwinm r10,r9,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_8224BA8C:
		// lfs f13,-8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		ctx.f13.f64 = double(temp.f32);
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// fadds f0,f13,f0
		ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
		// lfs f12,-4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		ctx.f12.f64 = double(temp.f32);
		// lfs f11,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// cmplwi cr6,r9,0
		// lfs f10,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f10.f64 = double(temp.f32);
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// fadds f0,f12,f0
		ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
		// fadds f0,f11,f0
		ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
		// fadds f0,f10,f0
		ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
		// bne cr6,0x8224ba8c
		if (ctx.r9.u32 != 0) goto loc_8224BA8C;
	}
loc_8224BABC:
	// cmpw cr6,r10,r28
	// bge cr6,0x8224baec
	if (ctx.r10.s32 < (int32_t)var_r28) {
		// lwz r8,0(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 0);
		// rlwinm r9,r10,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r11,r10,r28
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 - ctx.r10.s64;
		// add r10,r9,r8
		ctx.r10.u64 = ctx.r9.u64 + ctx.r8.u64;
	loc_8224BAD4:
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// lfs f9,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// fadds f0,f0,f9
		ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f9.f64));
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmplwi cr6,r11,0
		// bne cr6,0x8224bad4
		if (ctx.r11.u32 != 0) goto loc_8224BAD4;
	}
loc_8224BAEC:
	// lfs f13,-96(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -96);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32248
	// fsubs f8,f0,f13
	ctx.f8.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfd f12,-25848(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// lis r11,-32248
	// fsel f0,f8,f12,f0
	ctx.f0.f64 = ctx.f8.f64 >= 0.0 ? ctx.f12.f64 : ctx.f0.f64;
	// lfs f12,-25528(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25528);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f12
	// bgt cr6,0x8224bb14
	if (ctx.f0.f64 <= ctx.f12.f64) {
		// fsubs f31,f13,f0
		var_f31 = double(float(ctx.f13.f64 - ctx.f0.f64));
	}
loc_8224BB14:
	// li r30,0
	var_r30 = 0;
	// cmpwi cr6,r28,0
	// ble cr6,0x8224bb88
	if ((int32_t)var_r28 > 0) {
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lfs f30,0(r7)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		var_f30 = double(temp.f32);
		// li r31,0
		var_r31 = 0;
	loc_8224BB2C:
		// lfsx f7,r31,r11
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
		ctx.f7.f64 = double(temp.f32);
		// fcmpu cr6,f7,f30
		// blt cr6,0x8224bb78
		if (ctx.f7.f64 >= var_f30) {
			// bso cr6,0x8224bb78
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8224BB38, "bso");
			// fmr f0,f7
			ctx.f0.f64 = ctx.f7.f64;
			// lwz r8,0(r26)
			ctx.r8.u64 = PPC_LOAD_U32(var_r26 + 0);
			// li r10,0
			ctx.r10.s64 = 0;
			// mr r9,r25
			ctx.r9.u64 = var_r25;
			// li r7,0
			ctx.r7.s64 = 0;
			// li r6,0
			ctx.r6.s64 = 0;
			// mr r5,r30
			ctx.r5.u64 = var_r30;
			// lwzx r4,r31,r8
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + ctx.r8.u32);
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// fadds f6,f0,f31
			ctx.f6.f64 = double(float(ctx.f0.f64 + var_f31));
			// fdivs f1,f0,f6
			ctx.f1.f64 = double(float(ctx.f0.f64 / ctx.f6.f64));
			// bl 0x8224b800
			LocomotionStateAnim_B800_g_B800_1(ctx, base);
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// lfsx f5,r31,r11
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
			ctx.f5.f64 = double(temp.f32);
			// fadds f31,f5,f31
			var_f31 = double(float(ctx.f5.f64 + var_f31));
		}
	loc_8224BB78:
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r30,r28
		// blt cr6,0x8224bb2c
		if ((int32_t)var_r30 < (int32_t)var_r28) goto loc_8224BB2C;
	}
loc_8224BB88:
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_BB98"))) PPC_WEAK_FUNC(LocomotionStateAnim_BB98);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_BB98) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_BBA8_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_BBA8_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_BBA8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=160, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// lbz r11,5(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 5);
	// lwz r31,4(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 4));
	// cmplwi cr6,r11,1
	// blt cr6,0x8224bc60
	if (ctx.r11.u32 >= 1) {
		// bne cr6,0x8224bd0c
		if (!ctx.cr6.eq) {
			return;
		}
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lhz r4,6(r29)
		ctx.r4.u64 = PPC_LOAD_U16(var_r29 + 6);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x821d04f0
		LocomotionStateAnim_04F0_g(ctx, base);
		// clrlwi r11,r3,24
		ctx.r11.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x8224bd0c
		if (ctx.r11.u32 == 0) {
			return;
		}
		// lwz r9,20(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r8,r9,31
		ctx.r8.u64 = ctx.r9.u32 & 0x1;
		// lwz r10,12(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 12);
		// rotlwi r9,r11,2
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
		// cmplwi cr6,r8,0
		// add r7,r11,r9
		ctx.r7.u64 = ctx.r11.u64 + ctx.r9.u64;
		// rlwinm r11,r7,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224bc1c
		if (ctx.r8.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224BC1C:
		// clrlwi r5,r11,24
		ctx.r5.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// bne cr6,0x8224bc38
		if (ctx.r5.u32 == 0) {
			// lwz r3,72(r4)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 72);
			// clrlwi r11,r3,28
			ctx.r11.u64 = ctx.r3.u32 & 0xF;
			// cmplwi cr6,r11,0
			// beq cr6,0x8224bcf8
			if (ctx.r11.u32 == 0) goto loc_8224BCF8;
		}
	loc_8224BC38:
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// bl 0x8223b218
		LocomotionStateAnim_B218_g(ctx, base);
		// lwz r8,0(r29)
  // [ph4a] vtable load collapsed
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r7,72(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 18, ctx, base);  // pattern-B slot 18 (byte +72)
		return;
	}
loc_8224BC60:
	// lhz r28,6(r29)
	var_r28 = (uint32_t)(PPC_LOAD_U16(var_r29 + 6));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// bl 0x821d04f0
	LocomotionStateAnim_04F0_g(ctx, base);
	// clrlwi r6,r3,24
	ctx.r6.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8224bd0c
	if (ctx.r6.u32 != 0) {
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// lwz r4,20(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 20);
		// rotlwi r9,r11,2
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
		// lwz r10,12(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 12);
		// clrlwi r3,r4,31
		ctx.r3.u64 = ctx.r4.u32 & 0x1;
		// add r11,r11,r9
		ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
		// cmplwi cr6,r3,0
		// rlwinm r11,r11,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
		// li r10,1
		ctx.r10.s64 = 1;
		// bne cr6,0x8224bcb0
		if (ctx.r3.u32 == 0) {
			// li r10,0
			ctx.r10.s64 = 0;
		}
	loc_8224BCB0:
		// clrlwi r9,r10,24
		ctx.r9.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x8224bcc8
		if (ctx.r9.u32 != 0) {
			// clrlwi r8,r28,16
			ctx.r8.u64 = var_r28 & 0xFFFF;
			// cmplwi cr6,r8,0
			// beq cr6,0x8224bcd8
			if (ctx.r8.u32 == 0) {
				// lwz r5,0(r29)
  // [ph4a] vtable load collapsed
				// addi r4,r11,48
				ctx.r4.s64 = ctx.r11.s64 + 48;
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// lwz r11,68(r5)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r29, 17, ctx, base);  // pattern-B slot 17 (byte +68)
				return;
			}
		}
	loc_8224BCC8:
		// lwz r7,72(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
		// rlwinm r6,r7,0,22,24
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x380;
		// cmplwi cr6,r6,0
		// beq cr6,0x8224bcf8
		if (ctx.r6.u32 != 0) {
		loc_8224BCD8:
			// lwz r5,0(r29)
  // [ph4a] vtable load collapsed
			// addi r4,r11,48
			ctx.r4.s64 = ctx.r11.s64 + 48;
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// lwz r11,68(r5)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r29, 17, ctx, base);  // pattern-B slot 17 (byte +68)
			return;
		}
	loc_8224BCF8:
		// lwz r10,0(r29)
  // [ph4a] vtable load collapsed
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r9,52(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 13, ctx, base);  // pattern-B slot 13 (byte +52)
	}
loc_8224BD0C:
	return;
}

__attribute__((alias("__imp__atSingleton_BD18_fw"))) PPC_WEAK_FUNC(atSingleton_BD18_fw);
PPC_FUNC_IMPL(__imp__atSingleton_BD18_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r10,4(r11)
	// bctrl
	atSingleton_vfn_1(ctx, base);  // vtable slot 1 (byte +4)  // atSingleton::vfn_1
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// stb r29,5(r30)
	PPC_STORE_U8(var_r30 + 5, (uint8_t)var_r29);
	// sth r9,6(r30)
	PPC_STORE_U16(var_r30 + 6, ctx.r9.u16);
	// bl 0x8224bd98
	atSingleton_BD98_fw(ctx, base);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// bne cr6,0x8224bd8c
	if (ctx.r8.u32 == 0) {
		// lwz r7,0(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r6,8(r7)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_8224BD8C:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__atSingleton_BD98_fw"))) PPC_WEAK_FUNC(atSingleton_BD98_fw);
PPC_FUNC_IMPL(__imp__atSingleton_BD98_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// li r7,0
	ctx.r7.s64 = 0;
	// lhz r11,12(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 12);
	// lhz r10,6(r28)
	ctx.r10.u64 = PPC_LOAD_U16(var_r28 + 6);
	// addi r8,r11,-1
	ctx.r8.s64 = ctx.r11.s64 + -1;
	// lbz r9,5(r28)
	ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 5);
	// cmpwi cr6,r8,0
	// blt cr6,0x8224be44
	if (ctx.r8.s32 >= 0) {
		// rlwimi r10,r9,16,8,15
		ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 16) & 0xFF0000) | (ctx.r10.u64 & 0xFFFFFFFFFF00FFFF);
		// lwz r5,8(r29)
		ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 8);
		// clrlwi r6,r10,8
		ctx.r6.u64 = ctx.r10.u32 & 0xFFFFFF;
	loc_8224BDD4:
		// add r11,r8,r7
		ctx.r11.u64 = ctx.r8.u64 + ctx.r7.u64;
		// srawi r30,r11,1
		ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
		var_r30 = (uint32_t)(ctx.r11.s32 >> 1);
		// rlwinm r10,r30,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
		// lwzx r10,r10,r5
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r5.u32);
		// lbz r9,5(r10)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
		// lhz r4,6(r10)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
		// rotlwi r3,r9,16
		ctx.r3.u64 = __builtin_rotateleft32(ctx.r9.u32, 16);
		// li r9,1
		ctx.r9.s64 = 1;
		// or r11,r3,r4
		ctx.r11.u64 = ctx.r3.u64 | ctx.r4.u64;
		// cmplw cr6,r11,r6
		// bgt cr6,0x8224be04
		if (ctx.r11.u32 <= ctx.r6.u32) {
			// li r9,0
			ctx.r9.s64 = 0;
		}
	loc_8224BE04:
		// cmplw cr6,r11,r6
		// clrlwi r9,r9,24
		ctx.r9.u64 = ctx.r9.u32 & 0xFF;
		// li r11,1
		ctx.r11.s64 = 1;
		// beq cr6,0x8224be18
		if (ctx.r11.u32 != ctx.r6.u32) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224BE18:
		// clrlwi r4,r11,24
		ctx.r4.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// bne cr6,0x8224bee0
		if (ctx.r4.u32 != 0) goto loc_8224BEE0;
		// clrlwi r3,r9,24
		ctx.r3.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x8224be38
		if (ctx.r3.u32 != 0) {
			// addi r8,r30,-1
			ctx.r8.s64 = (int64_t)(int32_t)var_r30 + -1;
			// b 0x8224be3c
		} else {
		loc_8224BE38:
			// addi r7,r30,1
			ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 1;
		}
	loc_8224BE3C:
		// cmpw cr6,r7,r8
		// ble cr6,0x8224bdd4
		if (ctx.r7.s32 <= ctx.r8.s32) goto loc_8224BDD4;
	}
loc_8224BE44:
	// mr r30,r7
	var_r30 = ctx.r7.u32;
loc_8224BE48:
	// addi r31,r29,8
	var_r31 = (uint32_t)(var_r29 + 8);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// lhz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lhz r10,12(r29)
	ctx.r10.u64 = PPC_LOAD_U16(var_r29 + 12);
	// addis r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 65536;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// clrlwi r10,r7,16
	ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
	// rlwinm r6,r10,2,14,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
	// sth r10,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r10.u16);
	// lwzx r5,r6,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// stw r5,-4(r8)
	PPC_STORE_U32(ctx.r8.u32 + -4, ctx.r5.u32);
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// cmpw cr6,r11,r30
	// ble cr6,0x8224bebc
	if (ctx.r11.s32 > (int32_t)var_r30) {
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r11,r30,r11
		ctx.r11.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r30;
	loc_8224BE9C:
		// lwz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// add r9,r10,r9
		ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
		// addi r10,r10,-4
		ctx.r10.s64 = ctx.r10.s64 + -4;
		// cmplwi cr6,r11,0
		// lwz r4,-4(r9)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + -4);
		// stw r4,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
		// bne cr6,0x8224be9c
		if (ctx.r11.u32 != 0) goto loc_8224BE9C;
	}
loc_8224BEBC:
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// sth r8,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r8.u16);
	// stwx r28,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, var_r28);
	return;
loc_8224BEE0:
	// cmplwi cr6,r10,0
	// beq cr6,0x8224be48
	if (ctx.r10.u32 == 0) goto loc_8224BE48;
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_BEF8_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_BEF8_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_BEF8_g) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 12);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r8,r11,-1
	ctx.r8.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r8,0
	// blt cr6,0x8224bf8c
	if (ctx.r8.s32 >= 0) {
		// mr r11,r5
		ctx.r11.u64 = ctx.r5.u64;
		// lwz r5,8(r3)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// rlwimi r11,r4,16,8,15
		ctx.r11.u64 = (__builtin_rotateleft32(ctx.r4.u32, 16) & 0xFF0000) | (ctx.r11.u64 & 0xFFFFFFFFFF00FFFF);
		// clrlwi r7,r11,8
		ctx.r7.u64 = ctx.r11.u32 & 0xFFFFFF;
	loc_8224BF1C:
		// add r10,r8,r6
		ctx.r10.u64 = ctx.r8.u64 + ctx.r6.u64;
		// srawi r9,r10,1
		ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
		ctx.r9.s64 = ctx.r10.s32 >> 1;
		// rlwinm r4,r9,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r3,r4,r5
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r5.u32);
		// lbz r11,5(r3)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
		// lhz r10,6(r3)
		ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 6);
		// rotlwi r4,r11,16
		ctx.r4.u64 = __builtin_rotateleft32(ctx.r11.u32, 16);
		// or r11,r4,r10
		ctx.r11.u64 = ctx.r4.u64 | ctx.r10.u64;
		// li r10,1
		ctx.r10.s64 = 1;
		// cmplw cr6,r11,r7
		// bgt cr6,0x8224bf4c
		if (ctx.r11.u32 <= ctx.r7.u32) {
			// li r10,0
			ctx.r10.s64 = 0;
		}
	loc_8224BF4C:
		// cmplw cr6,r11,r7
		// clrlwi r10,r10,24
		ctx.r10.u64 = ctx.r10.u32 & 0xFF;
		// li r11,1
		ctx.r11.s64 = 1;
		// beq cr6,0x8224bf60
		if (ctx.r11.u32 != ctx.r7.u32) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224BF60:
		// clrlwi r4,r11,24
		ctx.r4.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// bnelr cr6
		if (ctx.r4.u32 != 0) return;
		// clrlwi r3,r10,24
		ctx.r3.u64 = ctx.r10.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x8224bf80
		if (ctx.r3.u32 != 0) {
			// addi r8,r9,-1
			ctx.r8.s64 = ctx.r9.s64 + -1;
			// b 0x8224bf84
		} else {
		loc_8224BF80:
			// addi r6,r9,1
			ctx.r6.s64 = ctx.r9.s64 + 1;
		}
	loc_8224BF84:
		// cmpw cr6,r6,r8
		// ble cr6,0x8224bf1c
		if (ctx.r6.s32 <= ctx.r8.s32) goto loc_8224BF1C;
	}
loc_8224BF8C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_BF98_g"))) PPC_WEAK_FUNC(atSingleton_BF98_g);
PPC_FUNC_IMPL(__imp__atSingleton_BF98_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	// FRAME: size=144, savegprlr_25
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// lhz r29,12(r25)
	var_r29 = (uint32_t)(PPC_LOAD_U16(var_r25 + 12));
	// cmpwi cr6,r29,1
	// ble cr6,0x8224c0ec
	if ((int32_t)var_r29 > 1) {
		// li r26,0
		var_r26 = 0;
		// cmpwi cr6,r29,0
		// ble cr6,0x8224c0ec
		if ((int32_t)var_r29 <= 0) {
			return;
		}
		// lis r11,0
		ctx.r11.s64 = 0;
		// ori r28,r11,65535
		var_r28 = (uint32_t)(ctx.r11.u64 | 65535);
	loc_8224BFC8:
		// addi r11,r29,-2
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + -2;
		// li r8,1
		ctx.r8.s64 = 1;
		// cmpw cr6,r11,r26
		// blt cr6,0x8224c0d4
		if (ctx.r11.s32 >= (int32_t)var_r26) {
			// subf r10,r26,r11
			ctx.r10.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r26;
			// addi r31,r25,8
			var_r31 = (uint32_t)(var_r25 + 8);
			// rlwinm r30,r11,2,0,29
			var_r30 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
			// addi r27,r10,1
			var_r27 = (uint32_t)(ctx.r10.s64 + 1);  // addr:0x82030001
		loc_8224BFE8:
			// lwz r11,0(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
			// add r11,r30,r11
			ctx.r11.u64 = var_r30 + ctx.r11.u64;
			// lwz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r11,4(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// lbz r9,5(r10)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5);
			// lbz r7,5(r11)
			ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
			// lhz r6,6(r10)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
			// rotlwi r5,r9,16
			ctx.r5.u64 = __builtin_rotateleft32(ctx.r9.u32, 16);
			// lhz r4,6(r11)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 6);
			// rotlwi r3,r7,16
			ctx.r3.u64 = __builtin_rotateleft32(ctx.r7.u32, 16);
			// or r11,r5,r6
			ctx.r11.u64 = ctx.r5.u64 | ctx.r6.u64;
			// or r10,r3,r4
			ctx.r10.u64 = ctx.r3.u64 | ctx.r4.u64;
			// li r9,1
			ctx.r9.s64 = 1;
			// cmplw cr6,r11,r10
			// bgt cr6,0x8224c028
			if (ctx.r11.u32 <= ctx.r10.u32) {
				// li r9,0
				ctx.r9.s64 = 0;
			}
		loc_8224C028:
			// cmplw cr6,r11,r10
			// clrlwi r9,r9,24
			ctx.r9.u64 = ctx.r9.u32 & 0xFF;
			// li r11,1
			ctx.r11.s64 = 1;
			// beq cr6,0x8224c03c
			if (ctx.r11.u32 != ctx.r10.u32) {
				// li r11,0
				ctx.r11.s64 = 0;
			}
		loc_8224C03C:
			// clrlwi r10,r11,24
			ctx.r10.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r10,0
			// beq cr6,0x8224c09c
			if (ctx.r10.u32 != 0) {
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
				// add r9,r30,r11
				ctx.r9.u64 = var_r30 + ctx.r11.u64;
				// lwz r3,4(r9)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
				// cmplwi cr6,r3,0
				// beq cr6,0x8224c070
				if (ctx.r3.u32 != 0) {
					// lwz r8,0(r3)
					ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* atSingleton::vtable@+0x0 */;
					// li r4,1
					ctx.r4.s64 = 1;
					// lwz r7,0(r8)
					ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
					// bctrl
					PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
				}
			loc_8224C070:
				// lhz r6,4(r31)
				ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 4);
				// addi r29,r29,-1
				var_r29 = (uint32_t)(var_r29 + -1);
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
				// add r4,r6,r28
				ctx.r4.u64 = ctx.r6.u64 + var_r28;
				// add r5,r30,r11
				ctx.r5.u64 = var_r30 + ctx.r11.u64;
				// clrlwi r10,r4,16
				ctx.r10.u64 = ctx.r4.u32 & 0xFFFF;
				// rlwinm r3,r10,2,14,29
				ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3FFFC;
				// sth r10,4(r31)
				PPC_STORE_U16(var_r31 + 4, ctx.r10.u16);
				// lwzx r11,r3,r11
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r11.u32);
				// stw r11,4(r5)
				PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
				// b 0x8224c0c0
			} else {
			loc_8224C09C:
				// clrlwi r10,r9,24
				ctx.r10.u64 = ctx.r9.u32 & 0xFF;
				// cmplwi cr6,r10,0
				// beq cr6,0x8224c0c4
				if (ctx.r10.u32 == 0) goto loc_8224C0C4;
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
				// add r11,r30,r11
				ctx.r11.u64 = var_r30 + ctx.r11.u64;
				// lwz r9,4(r11)
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				// lwz r10,0(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
				// stw r9,0(r11)
				PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
				// stw r10,4(r11)
				PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
			}
		loc_8224C0C0:
			// li r8,0
			ctx.r8.s64 = 0;
		loc_8224C0C4:
			// addi r27,r27,-1
			var_r27 = (uint32_t)(var_r27 + -1);
			// addi r30,r30,-4
			var_r30 = (uint32_t)(var_r30 + -4);
			// cmplwi cr6,r27,0
			// bne cr6,0x8224bfe8
			if (var_r27 != 0) goto loc_8224BFE8;
		}
	loc_8224C0D4:
		// clrlwi r8,r8,24
		ctx.r8.u64 = ctx.r8.u32 & 0xFF;
		// cmplwi cr6,r8,0
		// bne cr6,0x8224c0ec
		if (ctx.r8.u32 != 0) {
			return;
		}
		// addi r26,r26,1
		var_r26 = (uint32_t)(var_r26 + 1);
		// cmpw cr6,r26,r29
		// blt cr6,0x8224bfc8
		if ((int32_t)var_r26 < (int32_t)var_r29) goto loc_8224BFC8;
	}
loc_8224C0EC:
	return;
}

__attribute__((alias("__imp__crAnimFrame_vfn_1"))) PPC_WEAK_FUNC(crAnimFrame_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimFrame_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// b 0x8224a880
	xe_A880(ctx, base);
	return;
}

__attribute__((alias("__imp__crAnimFrame_vfn_2"))) PPC_WEAK_FUNC(crAnimFrame_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimFrame_vfn_2) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_C128_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_C128_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_C128_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t ea{};
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// lbz r11,5(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 5);
	// lwz r31,4(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 4));
	// cmplwi cr6,r11,1
	// blt cr6,0x8224c1e4
	if (ctx.r11.u32 >= 1) {
		// bne cr6,0x8224c280
		if (!ctx.cr6.eq) {
			return;
		}
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lhz r4,6(r29)
		ctx.r4.u64 = PPC_LOAD_U16(var_r29 + 6);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x821d04f0
		LocomotionStateAnim_04F0_g(ctx, base);
		// clrlwi r11,r3,24
		ctx.r11.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x8224c280
		if (ctx.r11.u32 == 0) {
			return;
		}
		// lwz r9,20(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// clrlwi r8,r9,31
		ctx.r8.u64 = ctx.r9.u32 & 0x1;
		// lwz r10,12(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 12);
		// rotlwi r9,r11,2
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
		// cmplwi cr6,r8,0
		// add r7,r11,r9
		ctx.r7.u64 = ctx.r11.u64 + ctx.r9.u64;
		// rlwinm r11,r7,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
		// add r31,r11,r10
		var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224c19c
		if (ctx.r8.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224C19C:
		// clrlwi r5,r11,24
		ctx.r5.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// bne cr6,0x8224c1bc
		if (ctx.r5.u32 == 0) {
			// lwz r4,72(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 72);
			// li r3,7
			ctx.r3.s64 = 7;
			// rlwimi r4,r3,1,0,30
			ctx.r4.u64 = (__builtin_rotateleft32(ctx.r3.u32, 1) & 0xFFFFFFFE) | (ctx.r4.u64 & 0xFFFFFFFF00000001);
			// cmplwi cr6,r4,0
			// beq cr6,0x8224c280
			if (ctx.r4.u32 == 0) {
				return;
			}
		}
	loc_8224C1BC:
		// lwz r11,0(r29)
  // [ph4a] vtable load collapsed
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r10,60(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 15, ctx, base);  // pattern-B slot 15 (byte +60)
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8223ad30
		LocomotionStateAnim_AD30_g(ctx, base);
		return;
	}
loc_8224C1E4:
	// lhz r28,6(r29)
	var_r28 = (uint32_t)(PPC_LOAD_U16(var_r29 + 6));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// bl 0x821d04f0
	LocomotionStateAnim_04F0_g(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x8224c280
	if (ctx.r9.u32 != 0) {
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// lwz r7,20(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 20);
		// rotlwi r9,r11,2
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
		// lwz r10,12(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 12);
		// clrlwi r6,r7,31
		ctx.r6.u64 = ctx.r7.u32 & 0x1;
		// add r5,r11,r9
		ctx.r5.u64 = ctx.r11.u64 + ctx.r9.u64;
		// cmplwi cr6,r6,0
		// rlwinm r11,r5,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
		// add r31,r11,r10
		var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224c234
		if (ctx.r6.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224C234:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x8224c24c
		if (ctx.r3.u32 != 0) {
			// clrlwi r11,r28,16
			ctx.r11.u64 = var_r28 & 0xFFFF;
			// cmplwi cr6,r11,0
			// beq cr6,0x8224c260
			if (ctx.r11.u32 == 0) goto loc_8224C260;
		}
	loc_8224C24C:
		// lwz r10,72(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 72);
		// li r9,3
		ctx.r9.s64 = 3;
		// rlwimi r10,r9,8,25,23
		ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 8) & 0xFFFFFFFFFFFFFF7F) | (ctx.r10.u64 & 0x80);
		// cmplwi cr6,r10,0
		// beq cr6,0x8224c280
		if (ctx.r10.u32 == 0) {
			return;
		}
	loc_8224C260:
		// lwz r8,0(r29)
  // [ph4a] vtable load collapsed
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r7,56(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// addi r6,r31,48
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
		// lvx128 v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8224C280:
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_C288_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_C288_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_C288_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// lbz r11,5(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 5);
	// cmplwi cr6,r11,1
	// blt cr6,0x8224c30c
	if (ctx.r11.u32 >= 1) {
		// bne cr6,0x8224c33c
		if (!ctx.cr6.eq) {
			// blr
			return;
		}
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lhz r4,6(r31)
		ctx.r4.u64 = PPC_LOAD_U16(var_r31 + 6);
		// lwz r3,4(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4);
		// bl 0x821d04f0
		LocomotionStateAnim_04F0_g(ctx, base);
		// clrlwi r11,r3,24
		ctx.r11.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x8224c33c
		if (ctx.r11.u32 == 0) {
			// blr
			return;
		}
		// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lhz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 80);
		// lwz r10,12(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 12);
		// lwz r8,60(r9)
  // [ph4a] slot load collapsed
		// rotlwi r9,r11,2
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
		// add r7,r11,r9
		ctx.r7.u64 = ctx.r11.u64 + ctx.r9.u64;
		// rlwinm r11,r7,4,0,27
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
		// add r31,r11,r10
		var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// bctrl
		VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8223ad30
		LocomotionStateAnim_AD30_g(ctx, base);
		// b 0x8224c33c
	} else {
	loc_8224C30C:
		// lhz r6,6(r31)
		ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 6);
		// cmplwi cr6,r6,0
		// bne cr6,0x8224c33c
		if (ctx.r6.u32 != 0) {
			// blr
			return;
		}
		// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r30,12(r30)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r30 + 12));
		// lwz r4,56(r5)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// addi r11,r30,48
		ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 48;
		// lvx128 v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8224C33C:
	// blr
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_C358_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_C358_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_C358_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, savegprlr_28
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// bl 0x8224bef8
	LocomotionStateAnim_BEF8_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x8224bef8
	LocomotionStateAnim_BEF8_g(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x8224c408
	if (var_r30 != 0) {
		// cmplwi cr6,r29,0
		// beq cr6,0x8224c408
		if (var_r29 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r10,56(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// addi r9,r31,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 16;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r7,60(r8)
		// bctrl
		LocomotionStateAnim_vfn_15(ctx, base);  // vtable slot 15 (byte +60)  // LocomotionStateAnim::vfn_15
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// li r3,1
		ctx.r3.s64 = 1;
		// lfs f0,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 0, temp.u32);
		// lfs f13,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,4(r31)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r31 + 4, temp.u32);
		// lfs f12,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f12.f64 = double(temp.f32);
		// stfs f12,8(r31)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r31 + 8, temp.u32);
		// lfs f11,12(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		ctx.f11.f64 = double(temp.f32);
		// stfs f11,12(r31)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r31 + 12, temp.u32);
		return;
	}
loc_8224C408:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_C418_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_C418_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_C418_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r28,r7
	var_r28 = ctx.r7.u32;
	// bl 0x8224bef8
	LocomotionStateAnim_BEF8_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224bef8
	LocomotionStateAnim_BEF8_g(ctx, base);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x8224c49c
	if (var_r30 != 0) {
		// cmplwi cr6,r31,0
		// beq cr6,0x8224c49c
		if (var_r31 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
		// addi r4,r28,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 16;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r10,68(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r8,72(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 18, ctx, base);  // pattern-B slot 18 (byte +72)
		// li r3,1
		ctx.r3.s64 = 1;
		return;
	}
loc_8224C49C:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_C4A8_g"))) PPC_WEAK_FUNC(LocomotionStateAnim_C4A8_g);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_C4A8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=128, savegprlr_28
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// bl 0x8224bef8
	LocomotionStateAnim_BEF8_g(ctx, base);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8224bef8
	LocomotionStateAnim_BEF8_g(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x8224c540
	if (var_r31 != 0) {
		// cmplwi cr6,r30,0
		// beq cr6,0x8224c540
		if (var_r30 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,56(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// addi r9,r29,48
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 48;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r7,60(r8)
		// bctrl
		LocomotionStateAnim_vfn_15(ctx, base);  // vtable slot 15 (byte +60)  // LocomotionStateAnim::vfn_15
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8223ad30
		LocomotionStateAnim_AD30_g(ctx, base);
		// li r3,1
		ctx.r3.s64 = 1;
		return;
	}
loc_8224C540:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__LocomotionStateAnim_C550"))) PPC_WEAK_FUNC(LocomotionStateAnim_C550);
PPC_FUNC_IMPL(__imp__LocomotionStateAnim_C550) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=208, savegprlr_22
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r22,r3
	var_r22 = ctx.r3.u32;
	// mr r23,r4
	var_r23 = ctx.r4.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// cmpwi cr6,r30,0
	// ble cr6,0x8224c744
	if ((int32_t)var_r30 > 0) {
		// lis r10,32767
		ctx.r10.s64 = 2147418112;
		// lwz r11,16(r23)
		ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 16);
		// ori r9,r10,65535
		ctx.r9.u64 = ctx.r10.u64 | 65535;
		// rlwinm r3,r11,1,0,30
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
		// cmplw cr6,r11,r9
		// ble cr6,0x8224c598
		if (ctx.r11.u32 > ctx.r9.u32) {
			// li r3,-1
		}
	loc_8224C598:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// mr r24,r3
		var_r24 = ctx.r3.u32;
		// li r31,0
		var_r31 = 0;
		// cmpwi cr6,r30,0
		// ble cr6,0x8224c62c
		if ((int32_t)var_r30 <= 0) goto loc_8224C62C;
		// clrlwi r25,r29,24
		var_r25 = (uint32_t)(var_r29 & 0xFF);
		// mr r28,r27
		var_r28 = (uint32_t)(var_r27);
		// mr r26,r30
		var_r26 = (uint32_t)(var_r30);
	loc_8224C5B8:
		// rlwinm r11,r31,1,0,30
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0xFFFFFFFE;
		// lwz r8,0(r28)
		ctx.r8.u64 = PPC_LOAD_U32(var_r28 + 0);
		// cmplwi cr6,r25,0
		// add r30,r11,r24
		var_r30 = (uint32_t)(ctx.r11.u64 + var_r24);
		// sth r8,0(r30)
		PPC_STORE_U16(var_r30 + 0, ctx.r8.u16);
		// bne cr6,0x8224c618
		if (var_r25 == 0) {
			// rlwinm r6,r31,2,0,29
			ctx.r6.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
			// lwz r5,4(r23)
			ctx.r5.u64 = PPC_LOAD_U32(var_r23 + 4);
			// lwzx r11,r6,r27
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r27);
			// lwz r10,0(r5)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
			// rlwinm r9,r11,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// add r4,r11,r9
			ctx.r4.u64 = ctx.r11.u64 + ctx.r9.u64;
			// rlwinm r11,r4,6,0,25
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 6) & 0xFFFFFFC0;
			// add r29,r11,r10
			var_r29 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
			// mr r3,r29
			ctx.r3.u64 = var_r29;
		loc_8224C5F4:
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// bl 0x820def38
			LocomotionState_EF38_p46(ctx, base);
			// cmplwi cr6,r3,0
			// beq cr6,0x8224c618
			if (ctx.r3.u32 == 0) goto loc_8224C618;
			// lhz r11,20(r3)
			ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 20);
			// addi r30,r30,2
			var_r30 = (uint32_t)(var_r30 + 2);
			// addi r31,r31,1
			var_r31 = (uint32_t)(var_r31 + 1);
			// sth r11,0(r30)
			PPC_STORE_U16(var_r30 + 0, ctx.r11.u16);
			// b 0x8224c5f4
			goto loc_8224C5F4;
		}
	loc_8224C618:
		// addi r26,r26,-1
		var_r26 = (uint32_t)(var_r26 + -1);
		// addi r28,r28,4
		var_r28 = (uint32_t)(var_r28 + 4);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// cmplwi cr6,r26,0
		// bne cr6,0x8224c5b8
		if (var_r26 != 0) goto loc_8224C5B8;
	loc_8224C62C:
		// li r8,1
		ctx.r8.s64 = 1;
		// cmpwi cr6,r31,1
		// ble cr6,0x8224c674
		if ((int32_t)var_r31 > 1) {
			// addi r11,r24,2
			ctx.r11.s64 = (int64_t)(int32_t)var_r24 + 2;
			// addi r9,r31,-1
			ctx.r9.s64 = (int64_t)(int32_t)var_r31 + -1;
		loc_8224C640:
			// lhz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
			// lhz r7,-2(r11)
			ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + -2);
			// cmplw cr6,r7,r10
			// ble cr6,0x8224c664
			if (ctx.r7.u32 > ctx.r10.u32) {
				// lhz r6,0(r11)
				ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
				// clrlwi r10,r7,16
				ctx.r10.u64 = ctx.r7.u32 & 0xFFFF;
				// li r8,0
				ctx.r8.s64 = 0;
				// sth r6,-2(r11)
				PPC_STORE_U16(ctx.r11.u32 + -2, ctx.r6.u16);
				// sth r10,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
			}
		loc_8224C664:
			// addi r9,r9,-1
			ctx.r9.s64 = ctx.r9.s64 + -1;
			// addi r11,r11,2
			ctx.r11.s64 = ctx.r11.s64 + 2;
			// cmplwi cr6,r9,0
			// bne cr6,0x8224c640
			if (ctx.r9.u32 != 0) goto loc_8224C640;
		}
	loc_8224C674:
		// clrlwi r5,r8,24
		ctx.r5.u64 = ctx.r8.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// beq cr6,0x8224c62c
		if (ctx.r5.u32 == 0) goto loc_8224C62C;
		// lwz r11,4(r23)
		ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 4);
		// li r10,-1
		// li r4,0
		ctx.r4.s64 = 0;
		// stw r31,96(r1)
		PPC_STORE_U32(ctx.r1.u32 + 96, var_r31);
		// stw r24,100(r1)
		PPC_STORE_U32(ctx.r1.u32 + 100, var_r24);
		// stw r11,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
		// stw r10,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
		// stw r10,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
		// lis r10,-32219
		// stw r4,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
		// lwz r3,20(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
		// addi r7,r10,-16088
		ctx.r7.s64 = ctx.r10.s64 + -16088;
		// clrlwi r11,r3,31
		ctx.r11.u64 = ctx.r3.u32 & 0x1;
		// cmplwi cr6,r11,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224c6c4
		if (ctx.r11.u32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224C6C4:
		// clrlwi r9,r11,24
		ctx.r9.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x8224c6d8
		if (ctx.r9.u32 != 0) {
			// lis r11,-32219
			ctx.r11.s64 = -2111504384;
			// addi r7,r11,-15736
			ctx.r7.s64 = ctx.r11.s64 + -15736;
		}
	loc_8224C6D8:
		// lis r10,-32253
		// lis r11,-32219
		// mr r8,r23
		ctx.r8.u64 = var_r23;
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// addi r4,r11,-31992
		ctx.r4.s64 = ctx.r11.s64 + -31992;
		// mr r3,r22
		ctx.r3.u64 = var_r22;
		// lfs f1,-12016(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
		ctx.f1.f64 = double(temp.f32);
		// bl 0x8224b6f0
		lvlLevelPiece_B6F0_2hr(ctx, base);
		// lwz r3,100(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		// cmplwi cr6,r3,0
		// beq cr6,0x8224c7bc
		if (ctx.r3.u32 == 0) {
			return;
		}
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r8,r3,24
		ctx.r8.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r8,0
		// bne cr6,0x8224c7bc
		if (ctx.r8.u32 != 0) {
			return;
		}
		// lwz r7,0(r13)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r6,4
		ctx.r6.s64 = 4;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwzx r3,r6,r7
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
		// lwz r11,8(r5)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		return;
	}
loc_8224C744:
	// lwz r10,4(r23)
	ctx.r10.u64 = PPC_LOAD_U32(var_r23 + 4);
	// lis r11,-32219
	ctx.r11.s64 = -2111504384;
	// addi r29,r11,-16088
	var_r29 = (uint32_t)(ctx.r11.s64 + -16088);  // LocomotionStateAnim_C128_g @ 0x8224c128
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// cmplwi cr6,r8,0
	// bne cr6,0x8224c768
	if (ctx.r8.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224C768:
	// clrlwi r6,r11,24
	ctx.r6.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8224c77c
	if (ctx.r6.u32 != 0) {
		// lis r11,-32219
		ctx.r11.s64 = -2111504384;
		// addi r29,r11,-15736
		var_r29 = (uint32_t)(ctx.r11.s64 + -15736);  // LocomotionStateAnim_C288_g @ 0x8224c288
	}
loc_8224C77C:
	// lhz r31,12(r22)
	var_r31 = (uint32_t)(PPC_LOAD_U16(var_r22 + 12));
	// cmpwi cr6,r31,0
	// ble cr6,0x8224c7bc
	if ((int32_t)var_r31 > 0) {
		// lis r11,-32253
		// li r30,0
		var_r30 = 0;
		// lfs f31,-12016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:0x8224d110 */
		var_f31 = double(temp.f32);
	loc_8224C794:
		// lwz r4,8(r22)
		ctx.r4.u64 = PPC_LOAD_U32(var_r22 + 8);
		// mr r5,r23
		ctx.r5.u64 = var_r23;
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// lwzx r3,r30,r4
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + ctx.r4.u32);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(var_r29);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r31,0
		// bne cr6,0x8224c794
		if (var_r31 != 0) goto loc_8224C794;
	}
loc_8224C7BC:
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_vfn_1"))) PPC_WEAK_FUNC(crAnimDofQuaternion_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32254
	// addi r11,r11,31732
	ctx.r11.s64 = ctx.r11.s64 + 31732;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofVector3_vfn_1"))) PPC_WEAK_FUNC(crAnimDofVector3_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimDofVector3_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32254
	// addi r11,r11,31644
	ctx.r11.s64 = ctx.r11.s64 + 31644;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofFloat_vfn_1"))) PPC_WEAK_FUNC(crAnimDofFloat_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimDofFloat_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lis r11,-32253
	// addi r11,r11,26756
	ctx.r11.s64 = ctx.r11.s64 + 26756;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_C810"))) PPC_WEAK_FUNC(game_C810);
PPC_FUNC_IMPL(__imp__game_C810) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x8224C818;
	__savegprlr_26(ctx, base);
	// lis r11,-32158
	// li r4,-1
	// addi r8,r11,-24384
	ctx.r8.s64 = ctx.r11.s64 + -24384;
	// lis r11,-32253
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// addi r11,r11,-12024
	ctx.r11.s64 = ctx.r11.s64 + -12024;
	// sth r4,144(r3)
	PPC_STORE_U16(ctx.r3.u32 + 144, ctx.r4.u16);
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// sth r4,146(r3)
	PPC_STORE_U16(ctx.r3.u32 + 146, ctx.r4.u16);
	// lis r29,-32191
	var_r29 = (uint32_t)(-2109669376);
	// sth r4,142(r3)
	PPC_STORE_U16(ctx.r3.u32 + 142, ctx.r4.u16);
	// lis r4,-32226
	// lis r30,-32226
	var_r30 = (uint32_t)(-2111963136);
	// lfs f0,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r4,r4,31680
	ctx.r4.s64 = ctx.r4.s64 + 31680;
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r11,0
	ctx.r11.s64 = 0;
	// stfs f0,80(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 80, temp.u32);
	// lis r31,-32191
	var_r31 = (uint32_t)(-2109669376);
	// stfs f0,84(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 84, temp.u32);
	// addi r29,r29,-6448
	var_r29 = (uint32_t)(var_r29 + -6448);
	// stfs f13,120(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 120, temp.u32);
	// addi r30,r30,31680
	var_r30 = (uint32_t)(var_r30 + 31680);
	// stw r4,-68(r1)
	PPC_STORE_U32(ctx.r1.u32 + -68, ctx.r4.u32);
	// stfs f0,124(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 124, temp.u32);
	// stfs f13,128(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 128, temp.u32);
	// sth r11,74(r3)
	PPC_STORE_U16(ctx.r3.u32 + 74, ctx.r11.u16);
	// stfs f0,132(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 132, temp.u32);
	// sth r11,140(r3)
	PPC_STORE_U16(ctx.r3.u32 + 140, ctx.r11.u16);
	// stfs f13,136(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 136, temp.u32);
	// addi r31,r31,-6448
	var_r31 = (uint32_t)(var_r31 + -6448);
	// stfs f0,152(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 152, temp.u32);
	// addi r8,r1,-96
	ctx.r8.s64 = ctx.r1.s64 + -96;
	// stfs f0,156(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 156, temp.u32);
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r7,r1,-80
	ctx.r7.s64 = ctx.r1.s64 + -80;
	// stw r30,-84(r1)
	PPC_STORE_U32(ctx.r1.u32 + -84, var_r30);
	// stw r11,-88(r1)
	PPC_STORE_U32(ctx.r1.u32 + -88, ctx.r11.u32);
	// addi r6,r3,32
	ctx.r6.s64 = ctx.r3.s64 + 32;
	// stw r31,-80(r1)
	PPC_STORE_U32(ctx.r1.u32 + -80, var_r31);
	// addi r5,r3,48
	ctx.r5.s64 = ctx.r3.s64 + 48;
	// stw r29,-96(r1)
	PPC_STORE_U32(ctx.r1.u32 + -96, var_r29);
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// lwz r4,4(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// stw r11,-72(r1)
	PPC_STORE_U32(ctx.r1.u32 + -72, ctx.r11.u32);
	// lwz r31,0(r8)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + 0));
	// lwz r30,0(r7)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 0));
	// lwz r29,4(r8)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + 4));
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
	// lwz r4,8(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lwz r28,4(r7)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 4));
	// lwz r27,8(r8)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + 8));
	// lwz r26,8(r7)
	var_r26 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 8));
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// stw r4,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r4.u32);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r7,12(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// stw r9,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r9.u32);
	// stw r31,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, var_r31);
	// stw r30,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, var_r30);
	// stw r29,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, var_r29);
	// stw r28,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, var_r28);
	// stw r27,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, var_r27);
	// stw r26,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, var_r26);
	// stw r8,12(r6)
	PPC_STORE_U32(ctx.r6.u32 + 12, ctx.r8.u32);
	// stw r7,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, ctx.r7.u32);
	// sth r11,72(r3)
	PPC_STORE_U16(ctx.r3.u32 + 72, ctx.r11.u16);
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r11.u32);
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r11.u32);
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__util_C930"))) PPC_WEAK_FUNC(util_C930);
PPC_FUNC_IMPL(__imp__util_C930) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	double var_f30 = 0.0;
	double var_f28 = 0.0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f894
	ctx.lr = 0x8224C938;
	__savegprlr_27(ctx, base);
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82436618
	__savefpr_28(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r27,0
	var_r27 = 0;
	// lhz r11,74(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 74);
	// lfs f0,80(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 80);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,84(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 84, temp.u32);
	// stb r27,160(r31)
	PPC_STORE_U8(var_r31 + 160, (uint8_t)var_r27);
	// andi. r11,r11,65531
	ctx.r11.u64 = ctx.r11.u64 & 65531;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	// sth r11,74(r31)
	PPC_STORE_U16(var_r31 + 74, ctx.r11.u16);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x8224c978
	if (ctx.r10.u32 == 0) {
		// mr r11,r27
		ctx.r11.u64 = var_r27;
	}
loc_8224C978:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8224cd28
	if (ctx.r8.u32 != 0) {
		// lhz r7,74(r31)
		ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 74);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r6,r7,0,30,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r6,0
		// bne cr6,0x8224c99c
		if (ctx.r6.u32 == 0) {
			// mr r11,r27
			ctx.r11.u64 = var_r27;
		}
	loc_8224C99C:
		// clrlwi r4,r11,24
		ctx.r4.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// bne cr6,0x8224cd28
		if (ctx.r4.u32 != 0) {
			// addi r1,r1,160
			ctx.r1.s64 = ctx.r1.s64 + 160;
			// addi r12,r1,-48
			ctx.r12.s64 = ctx.r1.s64 + -48;
			// bl 0x82436664
			__restfpr_28(ctx, base);
			// b 0x8242f8e4
			__restgprlr_27(ctx, base);
			return;
		}
		// lhz r3,74(r31)
		ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 74);
		// rlwinm r11,r3,0,21,21
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x400;
		// cmplwi cr6,r11,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224c9c0
		if (ctx.r11.u32 == 0) {
			// mr r11,r27
			ctx.r11.u64 = var_r27;
		}
	loc_8224C9C0:
		// clrlwi r9,r11,24
		ctx.r9.u64 = ctx.r11.u32 & 0xFF;
		// lis r11,-32253
		// lis r10,-32164
		// addi r30,r11,-12008
		var_r30 = (uint32_t)(ctx.r11.s64 + -12008);  // lbl_8202D118 @ 0x8202d118
		// cmplwi cr6,r9,0
		// lfs f30,22840(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22840);
		var_f30 = double(temp.f32);
		// lfs f28,-8(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + -8);
		var_f28 = double(temp.f32);
		// lfs f31,-16(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + -16);
		var_f31 = double(temp.f32);
		// beq cr6,0x8224ca50
		if (ctx.r9.u32 != 0) {
			// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r7,4(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
			// lfs f13,152(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 152);
			ctx.f13.f64 = double(temp.f32);
			// lfs f11,136(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 136);
			ctx.f11.f64 = double(temp.f32);
			// fsubs f0,f13,f11
			ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
			// fabs f12,f0
			ctx.f12.u64 = ctx.f0.u64 & ~0x8000000000000000;
			// fcmpu cr6,f12,f1
			// bgt cr6,0x8224ca24
			if (ctx.f12.f64 <= ctx.f1.f64) {
				// bso cr6,0x8224ca24
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x8224CA10, "bso");
				// lhz r6,74(r31)
				ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 74);
				// andi. r5,r6,64511
				ctx.r5.u64 = ctx.r6.u64 & 64511;
				ctx.cr0.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
				// sth r5,74(r31)
				PPC_STORE_U16(var_r31 + 74, ctx.r5.u16);
				// b 0x8224ca4c
			} else {
			loc_8224CA24:
				// fcmpu cr6,f0,f28
				ctx.fpscr.disableFlushMode();
				// ble cr6,0x8224ca34
				if (ctx.f0.f64 > var_f28) {
					// fmr f0,f31
					ctx.f0.f64 = var_f31;
					// b 0x8224ca48
				} else {
				loc_8224CA34:
					// fcmpu cr6,f0,f28
					ctx.fpscr.disableFlushMode();
					// bge cr6,0x8224ca44
					if (ctx.f0.f64 < var_f28) {
						// fmr f0,f30
						ctx.f0.f64 = var_f30;
						// b 0x8224ca48
					} else {
					loc_8224CA44:
						// fmr f0,f28
						ctx.fpscr.disableFlushMode();
						ctx.f0.f64 = var_f28;
					}
				}
			loc_8224CA48:
				// fmadds f13,f0,f1,f11
				ctx.fpscr.disableFlushMode();
				ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f11.f64));
			}
		loc_8224CA4C:
			// stfs f13,136(r31)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(var_r31 + 136, temp.u32);
		}
	loc_8224CA50:
		// lhz r11,72(r31)
		ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 72);
		// cmpwi cr6,r11,1
		// beq cr6,0x8224cae0
		if (ctx.r11.s32 != 1) {
			// cmpwi cr6,r11,3
			// bne cr6,0x8224cd14
			if (ctx.r11.s32 != 3) goto loc_8224CD14;
			// lwz r30,68(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 68));
			// stfs f28,80(r31)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(var_f28);
			PPC_STORE_U32(var_r31 + 80, temp.u32);
			// lbz r4,45(r30)
			ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 45);
			// cmplwi cr6,r4,0
			// bne cr6,0x8224cd14
			if (ctx.r4.u32 != 0) goto loc_8224CD14;
			// li r3,1
			ctx.r3.s64 = 1;
			// lwz r11,36(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 36);
			// mr r28,r27
			var_r28 = (uint32_t)(var_r27);
			// cmpwi cr6,r11,0
			// stb r3,45(r30)
			PPC_STORE_U8(var_r30 + 45, ctx.r3.u8);
			// ble cr6,0x8224cad8
			if (ctx.r11.s32 > 0) {
				// mr r29,r27
				var_r29 = (uint32_t)(var_r27);
			loc_8224CA94:
				// lwz r11,40(r30)
				ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 40);
				// add r3,r29,r11
				ctx.r3.u64 = var_r29 + ctx.r11.u64;
				// li r11,1
				ctx.r11.s64 = 1;
				// lhz r10,74(r3)
				ctx.r10.u64 = PPC_LOAD_U16(ctx.r3.u32 + 74);
				// clrlwi r9,r10,31
				ctx.r9.u64 = ctx.r10.u32 & 0x1;
				// cmplwi cr6,r9,0
				// bne cr6,0x8224cab4
				if (ctx.r9.u32 == 0) {
					// mr r11,r27
					ctx.r11.u64 = var_r27;
				}
			loc_8224CAB4:
				// clrlwi r7,r11,24
				ctx.r7.u64 = ctx.r11.u32 & 0xFF;
				// cmplwi cr6,r7,0
				// beq cr6,0x8224cac4
				if (ctx.r7.u32 != 0) {
					// bl 0x8224c930
					util_C930(ctx, base);
				}
			loc_8224CAC4:
				// lwz r6,36(r30)
				ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 36);
				// addi r28,r28,1
				var_r28 = (uint32_t)(var_r28 + 1);
				// addi r29,r29,368
				var_r29 = (uint32_t)(var_r29 + 368);
				// cmpw cr6,r28,r6
				// blt cr6,0x8224ca94
				if ((int32_t)var_r28 < ctx.r6.s32) goto loc_8224CA94;
			}
		loc_8224CAD8:
			// stb r27,45(r30)
			PPC_STORE_U8(var_r30 + 45, (uint8_t)var_r27);
			// b 0x8224cd14
			goto loc_8224CD14;
		}
	loc_8224CAE0:
		// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r4,8(r5)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 2, ctx, base);  // pattern-B slot 2 (byte +8)
		// lfs f12,80(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 80);
		ctx.f12.f64 = double(temp.f32);
		// fadds f1,f1,f12
		ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
		// stfs f1,80(r31)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r31 + 80, temp.u32);
		// fcmpu cr6,f1,f31
		// blt cr6,0x8224cc3c
		if (ctx.f1.f64 >= var_f31) {
			// bso cr6,0x8224cc3c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8224CB08, "bso");
			// lwz r3,64(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
			// cmplwi cr6,r3,0
			// beq cr6,0x8224cc3c
			if (ctx.r3.u32 == 0) goto loc_8224CC3C;
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfd f30,-25848(r11)
			ctx.f30.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);  /* glob:lbl_82079B08 @ 0x82079b08 */
			// fmr f2,f30
			ctx.f2.f64 = var_f30;
			// bl 0x82431d78
			util_1D78(ctx, base);
			// lhz r11,74(r31)
			ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 74);
			// frsp f11,f1
			ctx.fpscr.disableFlushMode();
			ctx.f11.f64 = double(float(ctx.f1.f64));
			// stfs f28,132(r31)
			temp.f32 = float(var_f28);
			PPC_STORE_U32(var_r31 + 132, temp.u32);
			// rlwinm r10,r11,0,18,18
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2000;
			// stfs f11,80(r31)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(var_r31 + 80, temp.u32);
			// li r11,1
			ctx.r11.s64 = 1;
			// cmplwi cr6,r10,0
			// bne cr6,0x8224cb4c
			if (ctx.r10.u32 == 0) {
				// mr r11,r27
				ctx.r11.u64 = var_r27;
			}
		loc_8224CB4C:
			// clrlwi r8,r11,24
			ctx.r8.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r8,0
			// beq cr6,0x8224cb68
			if (ctx.r8.u32 != 0) {
				// lhz r7,74(r31)
				ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 74);
				// andi. r6,r7,57343
				ctx.r6.u64 = ctx.r7.u64 & 57343;
				ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
				// sth r6,74(r31)
				PPC_STORE_U16(var_r31 + 74, ctx.r6.u16);
				// b 0x8224cb70
			} else {
			loc_8224CB68:
				// stfs f28,124(r31)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(var_f28);
				PPC_STORE_U32(var_r31 + 124, temp.u32);
				// stfs f31,128(r31)
				temp.f32 = float(var_f31);
				PPC_STORE_U32(var_r31 + 128, temp.u32);
			}
		loc_8224CB70:
			// lwz r11,64(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 64);
			// lis r10,-32248
			// lwz r3,68(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
			// lfs f29,120(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 120);
			var_f29 = double(temp.f32);
			// stfs f31,120(r31)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(var_r31 + 120, temp.u32);
			// lhz r5,74(r31)
			ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 74);
			// rlwinm r4,r5,0,19,19
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x1000;
			// lfs f0,-25668(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25668);
			ctx.f0.f64 = double(temp.f32);
			// lfs f10,12(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			ctx.f10.f64 = double(temp.f32);
			// cmplwi cr6,r4,0
			ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
			// lfs f8,12(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
			ctx.f8.f64 = double(temp.f32);
			// fmadds f9,f10,f0,f31
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + var_f31));
			// fmadds f7,f8,f0,f31
			ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + var_f31));
			// stw r11,68(r31)
			PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
			// stw r27,64(r31)
			PPC_STORE_U32(var_r31 + 64, var_r27);
			// li r11,1
			ctx.r11.s64 = 1;
			// fsubs f13,f9,f31
			ctx.f13.f64 = double(float(ctx.f9.f64 - var_f31));
			// fsubs f12,f7,f31
			ctx.f12.f64 = double(float(ctx.f7.f64 - var_f31));
			// fsubs f6,f31,f13
			ctx.f6.f64 = double(float(var_f31 - ctx.f13.f64));
			// fsubs f5,f31,f12
			ctx.f5.f64 = double(float(var_f31 - ctx.f12.f64));
			// fsel f13,f6,f30,f13
			ctx.f13.f64 = ctx.f6.f64 >= 0.0 ? var_f30 : ctx.f13.f64;
			// fsel f12,f5,f30,f12
			ctx.f12.f64 = ctx.f5.f64 >= 0.0 ? var_f30 : ctx.f12.f64;
			// fdivs f31,f0,f13
			var_f31 = double(float(ctx.f0.f64 / ctx.f13.f64));
			// fdivs f30,f0,f12
			var_f30 = double(float(ctx.f0.f64 / ctx.f12.f64));
			// bne cr6,0x8224cbd8
			if (ctx.cr6.eq) {
				// mr r11,r27
				ctx.r11.u64 = var_r27;
			}
		loc_8224CBD8:
			// clrlwi r10,r11,24
			ctx.r10.u64 = ctx.r11.u32 & 0xFF;
			// cmplwi cr6,r10,0
			// beq cr6,0x8224cbf4
			if (ctx.r10.u32 != 0) {
				// lhz r9,74(r31)
				ctx.r9.u64 = PPC_LOAD_U16(var_r31 + 74);
				// ori r8,r9,32
				ctx.r8.u64 = ctx.r9.u64 | 32;
				// sth r8,74(r31)
				PPC_STORE_U16(var_r31 + 74, ctx.r8.u16);
				// b 0x8224cc00
			} else {
			loc_8224CBF4:
				// lhz r7,74(r31)
				ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 74);
				// andi. r6,r7,65503
				ctx.r6.u64 = ctx.r7.u64 & 65503;
				ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
				// sth r6,74(r31)
				PPC_STORE_U16(var_r31 + 74, ctx.r6.u16);
			}
		loc_8224CC00:
			// lhz r5,74(r31)
			ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 74);
			// addi r3,r31,48
			ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 48;
			// andi. r4,r5,61439
			ctx.r4.u64 = ctx.r5.u64 & 61439;
			ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
			// sth r4,74(r31)
			PPC_STORE_U16(var_r31 + 74, ctx.r4.u16);
			// lwz r11,12(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
			// bctrl
			PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
			// lfs f4,120(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 120);
			ctx.f4.f64 = double(temp.f32);
			// fmuls f3,f30,f29
			ctx.f3.f64 = double(float(var_f30 * var_f29));
			// lfs f1,80(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 80);
			ctx.f1.f64 = double(temp.f32);
			// fmuls f2,f4,f31
			ctx.f2.f64 = double(float(ctx.f4.f64 * var_f31));
			// fdivs f0,f2,f3
			ctx.f0.f64 = double(float(ctx.f2.f64 / ctx.f3.f64));
			// fmuls f13,f0,f1
			ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
			// stfs f13,80(r31)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(var_r31 + 80, temp.u32);
			// b 0x8224cd14
			goto loc_8224CD14;
		}
	loc_8224CC3C:
		// fcmpu cr6,f1,f31
		ctx.fpscr.disableFlushMode();
		// blt cr6,0x8224cd38
		if (ctx.f1.f64 < var_f31) goto loc_8224CD38;
		// bso cr6,0x8224cd38
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8224CC44, "bso");
		// lhz r10,74(r31)
		ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 74);
		// ori r11,r10,4
		ctx.r11.u64 = ctx.r10.u64 | 4;
		// rlwinm r9,r11,0,26,26
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20;
		// cmplwi cr6,r9,0
		// sth r11,74(r31)
		PPC_STORE_U16(var_r31 + 74, ctx.r11.u16);
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224cc68
		if (ctx.r9.u32 == 0) {
			// mr r11,r27
			ctx.r11.u64 = var_r27;
		}
	loc_8224CC68:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8224cc94
		if (ctx.r7.u32 != 0) {
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfd f2,-25848(r11)
			ctx.fpscr.disableFlushMode();
			ctx.f2.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);  /* glob:lbl_82079B08 @ 0x82079b08 */
			// bl 0x82431d78
			util_1D78(ctx, base);
			// frsp f12,f1
			ctx.fpscr.disableFlushMode();
			ctx.f12.f64 = double(float(ctx.f1.f64));
			// stfs f28,124(r31)
			temp.f32 = float(var_f28);
			PPC_STORE_U32(var_r31 + 124, temp.u32);
			// stfs f28,132(r31)
			temp.f32 = float(var_f28);
			PPC_STORE_U32(var_r31 + 132, temp.u32);
			// stfs f12,80(r31)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(var_r31 + 80, temp.u32);
			// b 0x8224cd14
			goto loc_8224CD14;
		}
	loc_8224CC94:
		// lhz r6,74(r31)
		ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 74);
		// li r11,1
		ctx.r11.s64 = 1;
		// rlwinm r5,r6,0,25,25
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x40;
		// cmplwi cr6,r5,0
		// bne cr6,0x8224ccac
		if (ctx.r5.u32 == 0) {
			// mr r11,r27
			ctx.r11.u64 = var_r27;
		}
	loc_8224CCAC:
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x8224ccc8
		if (ctx.r3.u32 != 0) {
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfs f0,-25528(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25528);  /* glob:lbl_82079C48 @ 0x82079c48 */
			ctx.f0.f64 = double(temp.f32);
			// stfs f0,80(r31)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r31 + 80, temp.u32);
			// b 0x8224cd14
			goto loc_8224CD14;
		}
	loc_8224CCC8:
		// lhz r11,74(r31)
		ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 74);
		// rlwinm r10,r11,0,24,24
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80;
		// li r11,1
		ctx.r11.s64 = 1;
		// cmplwi cr6,r10,0
		// bne cr6,0x8224cce0
		if (ctx.r10.u32 == 0) {
			// mr r11,r27
			ctx.r11.u64 = var_r27;
		}
	loc_8224CCE0:
		// clrlwi r8,r11,24
		ctx.r8.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r8,0
		// beq cr6,0x8224cd08
		if (ctx.r8.u32 != 0) {
			// lis r11,-32248
			// lfs f11,120(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 120);
			ctx.f11.f64 = double(temp.f32);
			// fmuls f10,f11,f30
			ctx.f10.f64 = double(float(ctx.f11.f64 * var_f30));
			// stfs f10,120(r31)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(var_r31 + 120, temp.u32);
			// lfs f0,-25528(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25528);
			ctx.f0.f64 = double(temp.f32);
			// stfs f0,80(r31)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r31 + 80, temp.u32);
			// b 0x8224cd14
			goto loc_8224CD14;
		}
	loc_8224CD08:
		// stfs f31,80(r31)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r31 + 80, temp.u32);
	loc_8224CD0C:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8224cea8
		atSingleton_CEA8_p46(ctx, base);
	loc_8224CD14:
		// lfs f6,84(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 84);
		ctx.f6.f64 = double(temp.f32);
		// lfs f5,80(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 80);
		ctx.f5.f64 = double(temp.f32);
		// fcmpu cr6,f6,f5
		// ble cr6,0x8224cd28
		if (ctx.f6.f64 <= ctx.f5.f64) {
			// addi r1,r1,160
			ctx.r1.s64 = ctx.r1.s64 + 160;
			// addi r12,r1,-48
			ctx.r12.s64 = ctx.r1.s64 + -48;
			// bl 0x82436664
			__restfpr_28(ctx, base);
			// b 0x8242f8e4
			__restgprlr_27(ctx, base);
			return;
		}
		// stfs f28,84(r31)
		temp.f32 = float(var_f28);
		PPC_STORE_U32(var_r31 + 84, temp.u32);
	}
loc_8224CD28:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82436664
	__restfpr_28(ctx, base);
	// b 0x8242f8e4
	__restgprlr_27(ctx, base);
	return;
loc_8224CD38:
	// fcmpu cr6,f1,f28
	ctx.fpscr.disableFlushMode();
	// bge cr6,0x8224cd14
	if (ctx.f1.f64 >= var_f28) goto loc_8224CD14;
	// lhz r7,74(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 74);
	// ori r11,r7,4
	ctx.r11.u64 = ctx.r7.u64 | 4;
	// rlwinm r6,r11,0,26,26
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r6,0
	// sth r11,74(r31)
	PPC_STORE_U16(var_r31 + 74, ctx.r11.u16);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x8224cd60
	if (ctx.r6.u32 == 0) {
		// mr r11,r27
		ctx.r11.u64 = var_r27;
	}
loc_8224CD60:
	// clrlwi r4,r11,24
	ctx.r4.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r4,0
	// beq cr6,0x8224cd8c
	if (ctx.r4.u32 == 0) goto loc_8224CD8C;
	// lis r11,-32248
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fadds f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// lfd f2,-25848(r11)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// bl 0x82431d78
	util_1D78(ctx, base);
	// frsp f9,f1
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// stfs f9,80(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// b 0x8224cd14
	goto loc_8224CD14;
loc_8224CD8C:
	// lhz r3,74(r31)
	ctx.r3.u64 = PPC_LOAD_U16(var_r31 + 74);
	// rlwinm r11,r3,0,25,25
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x40;
	// cmplwi cr6,r11,0
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x8224cda4
	if (ctx.r11.u32 == 0) {
		// mr r11,r27
		ctx.r11.u64 = var_r27;
	}
loc_8224CDA4:
	// clrlwi r9,r11,24
	ctx.r9.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x8224cdb8
	if (ctx.r9.u32 == 0) goto loc_8224CDB8;
	// stfs f28,80(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f28);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// b 0x8224cd14
	goto loc_8224CD14;
loc_8224CDB8:
	// lhz r8,74(r31)
	ctx.r8.u64 = PPC_LOAD_U16(var_r31 + 74);
	// li r11,1
	ctx.r11.s64 = 1;
	// rlwinm r7,r8,0,24,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r7,0
	// bne cr6,0x8224cdd0
	if (ctx.r7.u32 == 0) {
		// mr r11,r27
		ctx.r11.u64 = var_r27;
	}
loc_8224CDD0:
	// clrlwi r5,r11,24
	ctx.r5.u64 = ctx.r11.u32 & 0xFF;
	// stfs f28,80(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f28);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// cmplwi cr6,r5,0
	// beq cr6,0x8224cd0c
	if (ctx.r5.u32 == 0) goto loc_8224CD0C;
	// lfs f8,120(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 120);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f8,f30
	ctx.f7.f64 = double(float(ctx.f8.f64 * var_f30));
	// stfs f7,120(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + 120, temp.u32);
	// b 0x8224cd14
	goto loc_8224CD14;
}

__attribute__((alias("__imp__util_CDF0"))) PPC_WEAK_FUNC(util_CDF0);
PPC_FUNC_IMPL(__imp__util_CDF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lhz r11,74(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 74);
	// rlwinm r10,r11,0,23,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x100;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8224ce20
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224CE20:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8224ce38
	if (ctx.r8.u32 != 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_8224CE38:
	// lhz r7,72(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 72);
	// cmplwi cr6,r7,0
	// beq cr6,0x8224ce4c
	if (ctx.r7.u32 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8224cea8
		atSingleton_CEA8_p46(ctx, base);
	}
loc_8224CE4C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224c810
	game_C810(ctx, base);
	// lhz r6,74(r31)
	ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 74);
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r30,68(r31)
	PPC_STORE_U32(var_r31 + 68, var_r30);
	// ori r11,r6,1
	ctx.r11.u64 = ctx.r6.u64 | 1;
	// clrlwi r5,r29,24
	ctx.r5.u64 = var_r29 & 0xFF;
	// cmplwi cr6,r5,0
	// sth r4,72(r31)
	PPC_STORE_U16(var_r31 + 72, ctx.r4.u16);
	// sth r11,74(r31)
	PPC_STORE_U16(var_r31 + 74, ctx.r11.u16);
	// beq cr6,0x8224ce90
	if (ctx.r5.u32 != 0) {
		// clrlwi r3,r11,16
		ctx.r3.u64 = ctx.r11.u32 & 0xFFFF;
		// ori r11,r3,32
		ctx.r11.u64 = ctx.r3.u64 | 32;
		// li r3,1
		ctx.r3.s64 = 1;
		// sth r11,74(r31)
		PPC_STORE_U16(var_r31 + 74, ctx.r11.u16);
		return;
	}
loc_8224CE90:
	// andi. r10,r11,65503
	ctx.r10.u64 = ctx.r11.u64 & 65503;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// sth r10,74(r31)
	PPC_STORE_U16(var_r31 + 74, ctx.r10.u16);
	return;
}

__attribute__((alias("__imp__atSingleton_CEA8_p46"))) PPC_WEAK_FUNC(atSingleton_CEA8_p46);
PPC_FUNC_IMPL(__imp__atSingleton_CEA8_p46) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=144, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lhz r11,72(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 72);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224cfd4
	if (ctx.r11.u32 != 0) {
		// lhz r10,74(r30)
		ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 74);
		// li r29,0
		var_r29 = 0;
		// rlwinm r11,r10,0,0,30
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
		// rlwinm r9,r11,0,20,20
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x800;
		// cmplwi cr6,r9,0
		// sth r11,74(r30)
		PPC_STORE_U16(var_r30 + 74, ctx.r11.u16);
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224cee8
		if (ctx.r9.u32 == 0) {
			// mr r11,r29
			ctx.r11.u64 = var_r29;
		}
	loc_8224CEE8:
		// clrlwi r7,r11,24
		ctx.r7.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8224cf38
		if (ctx.r7.u32 != 0) {
			// lhz r6,74(r30)
			ctx.r6.u64 = PPC_LOAD_U16(var_r30 + 74);
			// addi r31,r30,16
			var_r31 = (uint32_t)(var_r30 + 16);
			// li r4,0
			ctx.r4.s64 = 0;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// andi. r5,r6,63487
			ctx.r5.u64 = ctx.r6.u64 & 63487;
			ctx.cr0.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
			// sth r5,74(r30)
			PPC_STORE_U16(var_r30 + 74, ctx.r5.u16);
			// bl 0x8225fff8
			util_FFF8(ctx, base);
			// lis r11,-32158
			// addi r11,r11,-24384
			ctx.r11.s64 = ctx.r11.s64 + -24384;
			// lwz r4,0(r11)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// stw r4,0(r31)
			PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r4.u32);
			// lwz r3,4(r11)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// stw r3,4(r31)
			PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r3.u32);
			// lwz r10,8(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			// stw r10,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
			// lwz r9,12(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
			// stw r9,12(r31)
			PPC_STORE_U32(var_r31 + 12, ctx.r9.u32);
		}
	loc_8224CF38:
		// addi r31,r30,32
		var_r31 = (uint32_t)(var_r30 + 32);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r8,12(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 12);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
		// lis r6,-32226
		// lis r7,-32191
		// stw r29,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r29);
		// lis r5,-32191
		// stw r29,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, var_r29);
		// lis r8,-32226
		// stw r29,64(r30)
		PPC_STORE_U32(var_r30 + 64, var_r29);
		// addi r4,r6,31680
		ctx.r4.s64 = ctx.r6.s64 + 31680;
		// addi r3,r7,-6448
		ctx.r3.s64 = ctx.r7.s64 + -6448;
		// addi r5,r5,-6448
		ctx.r5.s64 = ctx.r5.s64 + -6448;
		// addi r8,r8,31680
		ctx.r8.s64 = ctx.r8.s64 + 31680;
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// stw r4,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r4.u32);
		// stw r3,96(r1)
		PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
		// addi r9,r30,48
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 48;
		// stw r5,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r5.u32);
		// stw r8,108(r1)
		PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
		// lwz r7,0(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r6,0(r10)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// lwz r5,4(r11)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r4,4(r10)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// lwz r3,8(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// lwz r8,8(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// lwz r11,12(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		// lwz r10,12(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
		// stw r7,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r7.u32);
		// stw r6,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
		// stw r5,4(r31)
		PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r5.u32);
		// stw r4,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r4.u32);
		// stw r3,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r3.u32);
		// stw r8,8(r9)
		PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r8.u32);
		// stw r11,12(r31)
		PPC_STORE_U32(var_r31 + 12, ctx.r11.u32);
		// stw r10,12(r9)
		PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r10.u32);
	}
loc_8224CFD4:
	return;
}

__attribute__((alias("__imp__pg_CFE0_wrh"))) PPC_WEAK_FUNC(pg_CFE0_wrh);
PPC_FUNC_IMPL(__imp__pg_CFE0_wrh) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lwz r3,68(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	// lfs f0,80(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8224d008
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224D008:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8224d034
	if (ctx.r8.u32 != 0) {
		// lis r11,-32248
		// lfs f13,12(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
		ctx.f13.f64 = double(temp.f32);
		// li r8,1
		ctx.r8.s64 = 1;
		// fmuls f1,f13,f0
		ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// li r7,0
		ctx.r7.s64 = 0;
		// mr r6,r5
		ctx.r6.u64 = ctx.r5.u64;
		// lfs f2,-25672(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25672);
		ctx.f2.f64 = double(temp.f32);
		// b 0x82248af8
		LocomotionStateAnim_8AF8_g(ctx, base);
		return;
	}
loc_8224D034:
	// lfs f12,12(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// fmuls f1,f12,f0
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// b 0x822488e0
	LocomotionStateAnim_88E0_g(ctx, base);
	return;
}

__attribute__((alias("__imp__crAnimPlayer_vfn_1"))) PPC_WEAK_FUNC(crAnimPlayer_vfn_1);
PPC_FUNC_IMPL(__imp__crAnimPlayer_vfn_1) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32163
	// lfs f13,156(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-20808
	ctx.r11.s64 = ctx.r11.s64 + -20808;
	// lfs f0,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// blr
	return;
}

__attribute__((alias("__imp__crAnimPlayer_vfn_2"))) PPC_WEAK_FUNC(crAnimPlayer_vfn_2);
PPC_FUNC_IMPL(__imp__crAnimPlayer_vfn_2) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32248
	// lwz r10,68(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 68);
	// lfs f10,120(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,-25668(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25668);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32248
	// lfs f9,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// lfd f11,-25848(r11)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);  /* glob:lbl_8202D108 @ 0x8202d108 */
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32163
	// fmadds f8,f9,f13,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f0.f64));
	// addi r11,r11,-20808
	ctx.r11.s64 = ctx.r11.s64 + -20808;
	// fsubs f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 - ctx.f0.f64));
	// fsubs f7,f0,f12
	ctx.f7.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// fsel f0,f7,f11,f12
	ctx.f0.f64 = ctx.f7.f64 >= 0.0 ? ctx.f11.f64 : ctx.f12.f64;
	// fdivs f6,f10,f0
	ctx.f6.f64 = double(float(ctx.f10.f64 / ctx.f0.f64));
	// lfs f0,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f1,f5,f13
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofFloat_D0B0_w"))) PPC_WEAK_FUNC(crAnimDofFloat_D0B0_w);
PPC_FUNC_IMPL(__imp__crAnimDofFloat_D0B0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=160, savegprlr_27
	// lis r11,-32158
	// lis r10,-32251
	// addi r11,r11,-24384
	ctx.r11.s64 = ctx.r11.s64 + -24384;
	// addi r8,r10,-768
	ctx.r8.s64 = ctx.r10.s64 + -768;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lis r5,-32226
	// stw r8,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r8.u32);
	// lis r7,-32226
	// lis r4,-32191
	// stw r11,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r11.u32);
	// lis r6,-32191
	// stw r11,20(r31)
	PPC_STORE_U32(var_r31 + 20, ctx.r11.u32);
	// addi r3,r5,31680
	ctx.r3.s64 = ctx.r5.s64 + 31680;
	// stw r11,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r11.u32);
	// addi r5,r7,31680
	ctx.r5.s64 = ctx.r7.s64 + 31680;
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(var_r31 + 32, ctx.r11.u32);
	// addi r4,r4,-6448
	ctx.r4.s64 = ctx.r4.s64 + -6448;
	// stw r11,40(r31)
	PPC_STORE_U32(var_r31 + 40, ctx.r11.u32);
	// addi r6,r6,-6448
	ctx.r6.s64 = ctx.r6.s64 + -6448;
	// stw r11,48(r31)
	PPC_STORE_U32(var_r31 + 48, ctx.r11.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r28,0(r10)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 0));
	// lhz r27,74(r31)
	var_r27 = (uint32_t)(PPC_LOAD_U16(var_r31 + 74));
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r11.u32);
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r11.u32);
	// stw r28,16(r31)
	PPC_STORE_U32(var_r31 + 16, var_r28);
	// lwz r28,4(r10)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 4));
	// stw r5,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// stw r6,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r28,20(r31)
	PPC_STORE_U32(var_r31 + 20, var_r28);
	// lwz r28,8(r10)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 8));
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r5,8(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// stw r28,24(r31)
	PPC_STORE_U32(var_r31 + 24, var_r28);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// andi. r27,r27,63487
	var_r27 = (uint32_t)(var_r27 & 63487);
	ctx.cr0.compare<int32_t>((int32_t)var_r27, 0, ctx.xer);
	// lwz r29,8(r8)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + 8));
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lhz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 72);
	// stw r10,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r10.u32);
	// stw r4,32(r31)
	PPC_STORE_U32(var_r31 + 32, ctx.r4.u32);
	// cmplwi cr6,r11,3
	// stw r3,48(r31)
	PPC_STORE_U32(var_r31 + 48, ctx.r3.u32);
	// stw r7,36(r31)
	PPC_STORE_U32(var_r31 + 36, ctx.r7.u32);
	// stw r6,52(r31)
	PPC_STORE_U32(var_r31 + 52, ctx.r6.u32);
	// stw r5,40(r31)
	PPC_STORE_U32(var_r31 + 40, ctx.r5.u32);
	// stw r29,56(r31)
	PPC_STORE_U32(var_r31 + 56, var_r29);
	// stw r9,44(r31)
	PPC_STORE_U32(var_r31 + 44, ctx.r9.u32);
	// stw r8,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r8.u32);
	// sth r27,74(r31)
	PPC_STORE_U16(var_r31 + 74, (uint16_t)var_r27);
	// bgt cr6,0x8224d2bc
	if (ctx.r11.u32 > 3) goto loc_8224D2BC;
	// lis r12,-32219
	// addi r12,r12,-11824
	ctx.r12.s64 = ctx.r12.s64 + -11824;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		goto loc_8224D2C8;
	case 1:
		goto loc_8224D1E0;
	case 2:
		goto loc_8224D24C;
	case 3:
		goto loc_8224D284;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_8224D1E0:
	// lwz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d214
	if (ctx.r11.u32 == 0) goto loc_8224D214;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r8,76(r30)
	ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r7,r9,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r10,r7,r8
	ctx.r10.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r5,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r30);
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r4,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r4.u32);
loc_8224D214:
	// lwz r11,64(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 64);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d2c8
	if (ctx.r11.u32 == 0) goto loc_8224D2C8;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r10,76(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r9,r3,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r3.s64;
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r10,r9,r10
	ctx.r10.u32 = ctx.r10.u32 ? ctx.r9.u32 / ctx.r10.u32 : 0;
	// addi r8,r10,2
	ctx.r8.s64 = ctx.r10.s64 + 2;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r7,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r30);
	// add r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r6,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r6.u32);
	// b 0x8224d2c8
	goto loc_8224D2C8;
loc_8224D24C:
	// lwz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d2c8
	if (ctx.r11.u32 == 0) goto loc_8224D2C8;
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r4,76(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r3,r5,r11
	ctx.r3.s64 = ctx.r11.s64 - ctx.r5.s64;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// divwu r10,r3,r4
	ctx.r10.u32 = ctx.r4.u32 ? ctx.r3.u32 / ctx.r4.u32 : 0;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r30);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r8,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r8.u32);
	// b 0x8224d2c8
	goto loc_8224D2C8;
loc_8224D284:
	// lwz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d2c8
	if (ctx.r11.u32 == 0) goto loc_8224D2C8;
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r6,76(r30)
	ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r5,r7,r11
	ctx.r5.s64 = ctx.r11.s64 - ctx.r7.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r10,r5,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
	// addi r4,r10,2
	ctx.r4.s64 = ctx.r10.s64 + 2;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r3,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r30);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r11,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
	// b 0x8224d2c8
	goto loc_8224D2C8;
loc_8224D2BC:
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// addi r3,r11,-820
	ctx.r3.s64 = ctx.r11.s64 + -820;
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
loc_8224D2C8:
	// lwz r11,76(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 76);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d2fc
	if (ctx.r11.u32 == 0) goto loc_8224D2FC;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r9,76(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// addi r7,r10,2
	ctx.r7.s64 = ctx.r10.s64 + 2;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r6,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r30);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,76(r31)
	PPC_STORE_U32(var_r31 + 76, ctx.r5.u32);
loc_8224D2FC:
	// lwz r11,116(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 116);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d330
	if (ctx.r11.u32 == 0) goto loc_8224D330;
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r3,76(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r10,r4,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r4.s64;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// divwu r10,r10,r3
	ctx.r10.u32 = ctx.r3.u32 ? ctx.r10.u32 / ctx.r3.u32 : 0;
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r8,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r30);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r7,116(r31)
	PPC_STORE_U32(var_r31 + 116, ctx.r7.u32);
loc_8224D330:
	// lwz r11,148(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 148);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d368
	if (ctx.r11.u32 == 0) {
		return;
	}
	// lwz r6,4(r30)
	ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r5,76(r30)
	ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 76);
	// subf r4,r6,r11
	ctx.r4.s64 = ctx.r11.s64 - ctx.r6.s64;
	// twllei r5,0
	if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
	// divwu r10,r4,r5
	ctx.r10.u32 = ctx.r5.u32 ? ctx.r4.u32 / ctx.r5.u32 : 0;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r9,r30
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r30);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r8,148(r31)
	PPC_STORE_U32(var_r31 + 148, ctx.r8.u32);
loc_8224D368:
	return;
}

__attribute__((alias("__imp__aud_D370"))) PPC_WEAK_FUNC(aud_D370);
PPC_FUNC_IMPL(__imp__aud_D370) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r4,0
	// sth r11,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r11.u16);
	// beq cr6,0x8224d3c0
	if (ctx.r4.u32 != 0) {
		// mr r11,r4
		ctx.r11.u64 = ctx.r4.u64;
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	loc_8224D39C:
		// lbz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplwi cr6,r9,0
		// bne cr6,0x8224d39c
		if (ctx.r9.u32 != 0) goto loc_8224D39C;
		// subf r11,r10,r11
		ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// addi r8,r11,-1
		ctx.r8.s64 = ctx.r11.s64 + -1;
		// rotlwi r5,r8,0
		ctx.r5.u64 = ctx.r8.u32;
		// bl 0x8223a8d0
		aud_A8D0(ctx, base);
	}
loc_8224D3C0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimWeightSet_vfn_0"))) PPC_WEAK_FUNC(crAnimWeightSet_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimWeightSet_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32251
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-748
	ctx.r11.s64 = ctx.r11.s64 + -748;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 16);
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
	// stw r11,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(var_r31 + 12, ctx.r11.u32);
	// bl 0x820c00c0
	rage_free_00C0(ctx, base);
	// lis r11,-32254
	// clrlwi r10,r30,31
	ctx.r10.u64 = var_r30 & 0x1;
	// addi r11,r11,30404
	ctx.r11.s64 = ctx.r11.s64 + 30404;
	// cmplwi cr6,r10,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x8224d440
	if (ctx.r10.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8224D440:
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_D458_fw"))) PPC_WEAK_FUNC(atSingleton_D458_fw);
PPC_FUNC_IMPL(__imp__atSingleton_D458_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// lis r11,-32158
	// li r26,0
	var_r26 = 0;
	// addi r28,r11,-25328
	var_r28 = (uint32_t)(ctx.r11.s64 + -25328);  // lbl_82619D10 @ 0x82619d10
	// lhz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
	// lwz r31,0(r28)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
	// cmplwi cr6,r11,0
	// beq cr6,0x8224d530
	if (ctx.r11.u32 != 0) {
		// li r27,0
		var_r27 = 0;
	loc_8224D484:
		// lwzx r29,r27,r31
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r27 + var_r31));
		// cmplwi cr6,r29,0
		// beq cr6,0x8224d51c
		if (var_r29 != 0) {
		loc_8224D490:
			// mr r30,r29
			var_r30 = (uint32_t)(var_r29);
			// lwz r29,12(r29)
			var_r29 = (uint32_t)(PPC_LOAD_U32(var_r29 + 12));
			// lwz r31,0(r30)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0)/* atSingleton::vtable@+0x0 */);
			// cmplwi cr6,r31,0
			// beq cr6,0x8224d4d8
			if (var_r31 != 0) {
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r9,r3,24
				ctx.r9.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r9,0
				// bne cr6,0x8224d4d8
				if (ctx.r9.u32 != 0) goto loc_8224D4D8;
				// lwz r8,0(r13)
				ctx.r8.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r7,4
				ctx.r7.s64 = 4;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// lwzx r3,r7,r8
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
				// lwz r5,8(r6)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_8224D4D8:
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r4,r3,24
			ctx.r4.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r4,0
			// bne cr6,0x8224d50c
			if (ctx.r4.u32 == 0) {
				// lwz r3,0(r13)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r11,4
				ctx.r11.s64 = 4;
				// mr r4,r30
				ctx.r4.u64 = var_r30;
				// lwzx r3,r11,r3
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
				// lwz r9,8(r10)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_8224D50C:
			// cmplwi cr6,r29,0
			// bne cr6,0x8224d490
			if (var_r29 != 0) goto loc_8224D490;
			// lhz r11,4(r28)
			ctx.r11.u64 = PPC_LOAD_U16(var_r28 + 4);
			// lwz r31,0(r28)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
		}
	loc_8224D51C:
		// addi r26,r26,1
		var_r26 = (uint32_t)(var_r26 + 1);
		// clrlwi r8,r11,16
		ctx.r8.u64 = ctx.r11.u32 & 0xFFFF;
		// addi r27,r27,4
		var_r27 = (uint32_t)(var_r27 + 4);
		// cmplw cr6,r26,r8
		// blt cr6,0x8224d484
		if (var_r26 < ctx.r8.u32) goto loc_8224D484;
	}
loc_8224D530:
	// cmplwi cr6,r31,0
	// beq cr6,0x8224d56c
	if (var_r31 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r7,r3,24
		ctx.r7.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// bne cr6,0x8224d56c
		if (ctx.r7.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
			// stw r11,0(r28)
			PPC_STORE_U32(var_r28 + 0, ctx.r11.u32);
			// sth r11,6(r28)
			PPC_STORE_U16(var_r28 + 6, ctx.r11.u16);
			// sth r11,4(r28)
			PPC_STORE_U16(var_r28 + 4, ctx.r11.u16);
			return;
		}
		// lwz r6,0(r13)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r5,4
		ctx.r5.s64 = 4;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwzx r3,r5,r6
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r6.u32);
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_8224D56C:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r28)
	PPC_STORE_U32(var_r28 + 0, ctx.r11.u32);
	// sth r11,6(r28)
	PPC_STORE_U16(var_r28 + 6, ctx.r11.u16);
	// sth r11,4(r28)
	PPC_STORE_U16(var_r28 + 4, ctx.r11.u16);
	return;
}

__attribute__((alias("__imp__atSingleton_D588_2hr"))) PPC_WEAK_FUNC(atSingleton_D588_2hr);
PPC_FUNC_IMPL(__imp__atSingleton_D588_2hr) {
	PPC_FUNC_PROLOGUE();
	// lhz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8224d5a0
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224D5A0:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cntlzw r7,r8
	ctx.r7.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r6,r7,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// clrlwi r3,r6,24
	ctx.r3.u64 = ctx.r6.u32 & 0xFF;
	// blr
	return;
}

__attribute__((alias("__imp__xe_D5B8"))) PPC_WEAK_FUNC(xe_D5B8);
PPC_FUNC_IMPL(__imp__xe_D5B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// lhz r30,8(r29)
	var_r30 = (uint32_t)(PPC_LOAD_U16(var_r29 + 8));
	// cmpwi cr6,r30,0
	// ble cr6,0x8224d618
	if ((int32_t)var_r30 > 0) {
		// mr r31,r28
		var_r31 = (uint32_t)(var_r28);
	loc_8224D5DC:
		// lwz r11,4(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
		// lwzx r3,r11,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
		// cmplwi cr6,r3,0
		// beq cr6,0x8224d600
		if (ctx.r3.u32 != 0) {
			// lwz r10,0(r3)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r9,0(r10)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 0, ctx, base);  // pattern-B slot 0 (byte +0)
		}
	loc_8224D600:
		// lwz r8,4(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 4);
		// addi r30,r30,-1
		var_r30 = (uint32_t)(var_r30 + -1);
		// cmplwi cr6,r30,0
		// stwx r28,r8,r31
		PPC_STORE_U32(ctx.r8.u32 + var_r31, var_r28);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// bne cr6,0x8224d5dc
		if (var_r30 != 0) goto loc_8224D5DC;
	}
loc_8224D618:
	// lwz r31,4(r29)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 4));
	// cmplwi cr6,r31,0
	// beq cr6,0x8224d658
	if (var_r31 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r7,r3,24
		ctx.r7.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// bne cr6,0x8224d658
		if (ctx.r7.u32 != 0) {
			// stw r28,4(r29)
			PPC_STORE_U32(var_r29 + 4, var_r28);
			// sth r28,8(r29)
			PPC_STORE_U16(var_r29 + 8, (uint16_t)var_r28);
			// sth r28,10(r29)
			PPC_STORE_U16(var_r29 + 10, (uint16_t)var_r28);
			return;
		}
		// lwz r6,0(r13)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r5,4
		ctx.r5.s64 = 4;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwzx r3,r5,r6
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r6.u32);
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_8224D658:
	// stw r28,4(r29)
	PPC_STORE_U32(var_r29 + 4, var_r28);
	// sth r28,8(r29)
	PPC_STORE_U16(var_r29 + 8, (uint16_t)var_r28);
	// sth r28,10(r29)
	PPC_STORE_U16(var_r29 + 10, (uint16_t)var_r28);
	return;
}

__attribute__((alias("__imp__atSingleton_D670"))) PPC_WEAK_FUNC(atSingleton_D670);
PPC_FUNC_IMPL(__imp__atSingleton_D670) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// li r26,0
	var_r26 = 0;
	// lhz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 0);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8224d6a0
	if (ctx.r10.u32 == 0) {
		// mr r11,r26
		ctx.r11.u64 = var_r26;
	}
loc_8224D6A0:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4)/* atSingleton::flags@+0x4 */;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r8,0
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// beq cr6,0x8224d6c0
	if (ctx.r8.u32 != 0) {
		// bl 0x822e3828
		rage_obj_bind_3828(ctx, base);
		// b 0x8224d6c4
	} else {
	loc_8224D6C0:
		// bl 0x822e39b0
		util_39B0(ctx, base);
	}
loc_8224D6C4:
	// lhz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U16(var_r30 + 0);
	// addi r4,r27,1
	ctx.r4.s64 = (int64_t)(int32_t)var_r27 + 1;
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r6,r7,31
	ctx.r6.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x8224d6e0
	if (ctx.r6.u32 == 0) {
		// mr r11,r26
		ctx.r11.u64 = var_r26;
	}
loc_8224D6E0:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// li r5,1
	ctx.r5.s64 = 1;
	// cmplwi cr6,r3,0
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4)/* atSingleton::flags@+0x4 */;
	// beq cr6,0x8224d6fc
	if (ctx.r3.u32 != 0) {
		// bl 0x822e3828
		rage_obj_bind_3828(ctx, base);
		// b 0x8224d700
	} else {
	loc_8224D6FC:
		// bl 0x822e39b0
		util_39B0(ctx, base);
	}
loc_8224D700:
	// addi r4,r27,2
	ctx.r4.s64 = (int64_t)(int32_t)var_r27 + 2;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r11,8(r27)
	ctx.r11.u64 = PPC_LOAD_U16(var_r27 + 8);
	// addi r4,r1,82
	ctx.r4.s64 = ctx.r1.s64 + 82;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r11.u16);
	// bl 0x82244d38
	grc_4D38(ctx, base);
	// lhz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 0);
	// li r11,1
	ctx.r11.s64 = 1;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x8224d738
	if (ctx.r9.u32 == 0) {
		// mr r11,r26
		ctx.r11.u64 = var_r26;
	}
loc_8224D738:
	// clrlwi r7,r11,24
	ctx.r7.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x8224d764
	if (ctx.r7.u32 != 0) {
		// lhz r4,82(r1)
		ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
		// addi r3,r27,4
		ctx.r3.s64 = (int64_t)(int32_t)var_r27 + 4;
		// cmplwi cr6,r4,0
		// beq cr6,0x8224d75c
		if (ctx.r4.u32 != 0) {
			// bl 0x8224dde8
			atSingleton_DDE8_g(ctx, base);
			// b 0x8224d764
		} else {
		loc_8224D75C:
			// stw r26,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r26);
			// sth r26,6(r3)
			PPC_STORE_U16(ctx.r3.u32 + 6, (uint16_t)var_r26);
		}
	}
loc_8224D764:
	// lhz r4,82(r1)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// mr r28,r26
	var_r28 = (uint32_t)(var_r26);
	// cmplwi cr6,r4,0
	// beq cr6,0x8224d844
	if (ctx.r4.u32 != 0) {
		// mr r29,r26
		var_r29 = (uint32_t)(var_r26);
	loc_8224D778:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// mr r31,r26
		var_r31 = (uint32_t)(var_r26);
		// bl 0x8224d588
		atSingleton_D588_2hr(ctx, base);
		// clrlwi r3,r3,24
		ctx.r3.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// beq cr6,0x8224d7a0
		if (ctx.r3.u32 != 0) {
			// lwz r11,4(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
			// lwzx r31,r11,r29
			var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + var_r29));
			// lbz r10,5(r31)
			ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 5);
			// stb r10,80(r1)
			PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r10.u8);
		}
	loc_8224D7A0:
		// lhz r9,0(r30)
		ctx.r9.u64 = PPC_LOAD_U16(var_r30 + 0);
		// li r11,1
		ctx.r11.s64 = 1;
		// clrlwi r8,r9,31
		ctx.r8.u64 = ctx.r9.u32 & 0x1;
		// cmplwi cr6,r8,0
		// bne cr6,0x8224d7b8
		if (ctx.r8.u32 == 0) {
			// mr r11,r26
			ctx.r11.u64 = var_r26;
		}
	loc_8224D7B8:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// lwz r3,4(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 4)/* atSingleton::flags@+0x4 */;
		// li r5,1
		ctx.r5.s64 = 1;
		// cmplwi cr6,r6,0
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// beq cr6,0x8224d7d8
		if (ctx.r6.u32 != 0) {
			// bl 0x822e3828
			rage_obj_bind_3828(ctx, base);
			// b 0x8224d7dc
		} else {
		loc_8224D7D8:
			// bl 0x822e39b0
			util_39B0(ctx, base);
		}
	loc_8224D7DC:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82244d18
		atSingleton_4D18_w(ctx, base);
		// clrlwi r5,r3,24
		ctx.r5.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r5,0
		// beq cr6,0x8224d818
		if (ctx.r5.u32 != 0) {
			// lbz r3,80(r1)
			ctx.r3.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
			// bl 0x82244fc8
			atSingleton_4FC8_wrh(ctx, base);
			// addi r11,r27,4
			ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 4;
			// mr r31,r3
			var_r31 = ctx.r3.u32;
			// lhz r10,4(r11)
			ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 4);
			// lwz r8,0(r11)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// addi r4,r10,1
			ctx.r4.s64 = ctx.r10.s64 + 1;
			// rotlwi r9,r10,2
			ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
			// sth r4,4(r11)
			PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r4.u16);
			// stwx r31,r9,r8
			PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, var_r31);
		}
	loc_8224D818:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,56(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 56);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
		// lhz r8,82(r1)
		ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpw cr6,r28,r8
		// blt cr6,0x8224d778
		if ((int32_t)var_r28 < ctx.r8.s32) goto loc_8224D778;
	}
loc_8224D844:
	return;
}

__attribute__((alias("__imp__xe_D850"))) PPC_WEAK_FUNC(xe_D850);
PPC_FUNC_IMPL(__imp__xe_D850) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	// FRAME: size=144, savegprlr_26
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r27,r5
	var_r27 = ctx.r5.u32;
	// mr r26,r6
	var_r26 = ctx.r6.u32;
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,12
	ctx.r4.s64 = 12;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// lwz r8,4(r9)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// li r29,0
	var_r29 = 0;
	// cmplwi cr6,r3,0
	// beq cr6,0x8224d8c0
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// li r7,4
		ctx.r7.s64 = 4;
		// addi r11,r11,-1836
		ctx.r11.s64 = ctx.r11.s64 + -1836;
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// stb r7,5(r3)
		PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r7.u8);
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// b 0x8224d8c4
	} else {
	loc_8224D8C0:
		// mr r31,r29
		var_r31 = (uint32_t)(var_r29);
	}
loc_8224D8C4:
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// mr r6,r26
	ctx.r6.u64 = var_r26;
	// mr r5,r27
	ctx.r5.u64 = var_r27;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,32(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 8, ctx, base);  // pattern-B slot 8 (byte +32)
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r9,0
	// bne cr6,0x8224da74
	if (ctx.r9.u32 == 0) {
		// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r7,0(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 0, ctx, base);  // pattern-B slot 0 (byte +0)
		// li r3,24
		ctx.r3.s64 = 24;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x8224d944
		if (ctx.r3.u32 != 0) {
			// lis r11,-32251
			// li r6,6
			ctx.r6.s64 = 6;
			// addi r11,r11,-1620
			ctx.r11.s64 = ctx.r11.s64 + -1620;
			// mr r30,r3
			var_r30 = ctx.r3.u32;
			// stb r6,5(r3)
			PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r6.u8);
			// stw r11,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
			// stw r29,8(r3)
			PPC_STORE_U32(ctx.r3.u32 + 8, var_r29);
			// sth r29,12(r3)
			PPC_STORE_U16(ctx.r3.u32 + 12, (uint16_t)var_r29);
			// sth r29,14(r3)
			PPC_STORE_U16(ctx.r3.u32 + 14, (uint16_t)var_r29);
			// b 0x8224d948
		} else {
		loc_8224D944:
			// mr r30,r29
			var_r30 = (uint32_t)(var_r29);
		}
	loc_8224D948:
		// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// mr r6,r26
		ctx.r6.u64 = var_r26;
		// mr r5,r27
		ctx.r5.u64 = var_r27;
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r10,32(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 8, ctx, base);  // pattern-B slot 8 (byte +32)
		// clrlwi r9,r3,24
		ctx.r9.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// bne cr6,0x8224d994
		if (ctx.r9.u32 == 0) {
			// lwz r8,0(r30)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r7,0(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 0, ctx, base);  // pattern-B slot 0 (byte +0)
			// mr r30,r29
			var_r30 = (uint32_t)(var_r29);
		}
	loc_8224D994:
		// li r3,16
		ctx.r3.s64 = 16;
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x8224d9cc
		if (ctx.r3.u32 != 0) {
			// lis r11,-32251
			// li r6,1
			ctx.r6.s64 = 1;
			// addi r11,r11,-2196
			ctx.r11.s64 = ctx.r11.s64 + -2196;
			// mr r31,r3
			var_r31 = ctx.r3.u32;
			// stb r6,5(r3)
			PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r6.u8);
			// stw r11,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
			// stw r29,8(r3)
			PPC_STORE_U32(ctx.r3.u32 + 8, var_r29);
			// sth r29,12(r3)
			PPC_STORE_U16(ctx.r3.u32 + 12, (uint16_t)var_r29);
			// sth r29,14(r3)
			PPC_STORE_U16(ctx.r3.u32 + 14, (uint16_t)var_r29);
			// b 0x8224d9d0
		} else {
		loc_8224D9CC:
			// mr r31,r29
			var_r31 = (uint32_t)(var_r29);
		}
	loc_8224D9D0:
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// mr r6,r26
		ctx.r6.u64 = var_r26;
		// mr r5,r27
		ctx.r5.u64 = var_r27;
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,32(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 8, ctx, base);  // pattern-B slot 8 (byte +32)
		// cmplwi cr6,r30,0
		// beq cr6,0x8224da70
		if (var_r30 != 0) {
			// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r8,52(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 13, ctx, base);  // pattern-B slot 13 (byte +52)
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,52(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 13, ctx, base);  // pattern-B slot 13 (byte +52)
			// cmpw cr6,r29,r3
			// li r4,1
			ctx.r4.s64 = 1;
			// bge cr6,0x8224da5c
			if ((int32_t)var_r29 < ctx.r3.s32) {
				// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r11,0(r5)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 0, ctx, base);  // pattern-B slot 0 (byte +0)
				// mr r31,r30
				var_r31 = (uint32_t)(var_r30);
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				return;
			}
		loc_8224DA5C:
			// lwz r10,0(r30)
  // [ph4a] vtable load collapsed
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r9,0(r10)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 0, ctx, base);  // pattern-B slot 0 (byte +0)
		}
	loc_8224DA70:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8224DA74:
	return;
}

__attribute__((alias("__imp__xe_DA80"))) PPC_WEAK_FUNC(xe_DA80);
PPC_FUNC_IMPL(__imp__xe_DA80) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	double var_f31 = 0.0;
	// FRAME: size=144, savegprlr_26
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// bl 0x8224d5b8
	xe_D5B8(ctx, base);
	// li r11,16
	ctx.r11.s64 = 16;
	// stb r31,0(r30)
	PPC_STORE_U8(var_r30 + 0, (uint8_t)var_r31);
	// li r3,12
	ctx.r3.s64 = 12;
	// sth r29,2(r30)
	PPC_STORE_U16(var_r30 + 2, (uint16_t)var_r29);
	// addi r27,r30,4
	var_r27 = (uint32_t)(var_r30 + 4);
	// stb r11,1(r30)
	PPC_STORE_U8(var_r30 + 1, ctx.r11.u8);
	// bl 0x820dec88
	xe_EC88(ctx, base);
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// cmplwi cr6,r26,0
	// beq cr6,0x8224daf4
	if (var_r26 != 0) {
		// mr r29,r26
		var_r29 = (uint32_t)(var_r26);
		// li r31,2
		var_r31 = 2;
	loc_8224DAD8:
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8224dfc0
		ke_DFC0(ctx, base);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpwi cr6,r31,0
		// bge cr6,0x8224dad8
		if ((int32_t)var_r31 >= 0) goto loc_8224DAD8;
		// b 0x8224daf8
	} else {
	loc_8224DAF4:
		// li r26,0
		var_r26 = 0;
	}
loc_8224DAF8:
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r26,0(r27)
	PPC_STORE_U32(var_r27 + 0, var_r26);
	// li r31,0
	var_r31 = 0;
	// sth r10,6(r27)
	PPC_STORE_U16(var_r27 + 6, ctx.r10.u16);
loc_8224DB08:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// li r6,3
	ctx.r6.s64 = 3;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lhz r5,4(r28)
	ctx.r5.u64 = PPC_LOAD_U16(var_r28 + 4);
	// add r4,r11,r31
	ctx.r4.u64 = ctx.r11.u64 + var_r31;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// bl 0x8224d850
	xe_D850(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x8224de98
	atSingleton_DE98_g(ctx, base);
	// addi r31,r31,4
	var_r31 = (uint32_t)(var_r31 + 4);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, var_r29);
	// cmpwi cr6,r31,12
	// blt cr6,0x8224db08
	if ((int32_t)var_r31 < 12) goto loc_8224DB08;
	return;
}

__attribute__((alias("__imp__xe_DB50"))) PPC_WEAK_FUNC(xe_DB50);
PPC_FUNC_IMPL(__imp__xe_DB50) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_26
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// bl 0x8224d5b8
	xe_D5B8(ctx, base);
	// li r11,1
	ctx.r11.s64 = 1;
	// stb r31,0(r26)
	PPC_STORE_U8(var_r26 + 0, (uint8_t)var_r31);
	// sth r30,2(r26)
	PPC_STORE_U16(var_r26 + 2, (uint16_t)var_r30);
	// stb r11,1(r26)
	PPC_STORE_U8(var_r26 + 1, ctx.r11.u8);
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r10,0(r13)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r9,4
	ctx.r9.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,32
	ctx.r4.s64 = 32;
	// lwzx r3,r9,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// lwz r7,4(r8)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r3,0
	// beq cr6,0x8224dbd8
	if (ctx.r3.u32 != 0) {
		// lis r11,-32251
		// li r6,9
		ctx.r6.s64 = 9;
		// addi r11,r11,-1764
		ctx.r11.s64 = ctx.r11.s64 + -1764;
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// stb r6,5(r3)
		PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r6.u8);
		// stw r11,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
		// b 0x8224dbdc
	} else {
	loc_8224DBD8:
		// mr r31,r30
		var_r31 = (uint32_t)(var_r30);
	}
loc_8224DBDC:
	// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 0);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r11,40(r5)
  // [ph4a] slot load collapsed
	// lhz r5,4(r29)
	ctx.r5.u64 = PPC_LOAD_U16(var_r29 + 4);
	// bctrl
	VCALL(var_r31, 10, ctx, base);  // pattern-B slot 10 (byte +40)
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	// beq cr6,0x8224dc24
	if (ctx.r10.u32 != 0) {
		// li r4,1
		ctx.r4.s64 = 1;
		// addi r3,r26,4
		ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 4;
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// stw r31,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, var_r31);
		return;
	}
loc_8224DC24:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,0(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 0, ctx, base);  // pattern-B slot 0 (byte +0)
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
	// lis r10,-32253
	// lfs f0,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,-12016(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f10.f64 = double(temp.f32);
	// fcmpu cr6,f0,f10
	// bge cr6,0x8224dc84
	if (ctx.f0.f64 < ctx.f10.f64) {
		// lfs f13,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// lfs f11,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f11.f64 = double(temp.f32);
		// fneg f12,f13
		ctx.f12.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// lfs f9,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f9.f64 = double(temp.f32);
		// fneg f8,f11
		ctx.f8.u64 = ctx.f11.u64 ^ 0x8000000000000000;
		// lfs f7,12(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		ctx.f7.f64 = double(temp.f32);
		// fneg f6,f9
		ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
		// fneg f5,f7
		ctx.f5.u64 = ctx.f7.u64 ^ 0x8000000000000000;
		// stfs f12,0(r11)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// stfs f8,4(r11)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// stfs f6,8(r11)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
		// stfs f5,12(r11)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	}
loc_8224DC84:
	// lhz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 4);
	// mr r9,r30
	ctx.r9.u64 = var_r30;
	// li r8,1
	ctx.r8.s64 = 1;
	// cmpwi cr6,r11,1
	// ble cr6,0x8224dd3c
	if (ctx.r11.s32 > 1) {
		// li r10,16
		ctx.r10.s64 = 16;
	loc_8224DC9C:
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// lfs f4,-8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		ctx.f4.f64 = double(temp.f32);
		// lfs f13,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f9,f4,f13
		ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// lfs f3,-12(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12);
		ctx.f3.f64 = double(temp.f32);
		// lfs f12,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// lfs f2,-4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		ctx.f2.f64 = double(temp.f32);
		// lfs f11,12(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		ctx.f11.f64 = double(temp.f32);
		// lfs f0,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// lfs f1,-16(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16);
		ctx.f1.f64 = double(temp.f32);
		// fmadds f8,f3,f12,f9
		ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 + ctx.f9.f64));
		// fmadds f7,f2,f11,f8
		ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f11.f64 + ctx.f8.f64));
		// fmadds f6,f0,f1,f7
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f7.f64));
		// fcmpu cr6,f6,f10
		// bge cr6,0x8224dcfc
		if (ctx.f6.f64 < ctx.f10.f64) {
			// fneg f5,f0
			ctx.f5.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// stfs f5,0(r11)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
			// fneg f4,f12
			ctx.f4.u64 = ctx.f12.u64 ^ 0x8000000000000000;
			// stfs f4,4(r11)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
			// fneg f3,f13
			ctx.f3.u64 = ctx.f13.u64 ^ 0x8000000000000000;
			// stfs f3,8(r11)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
			// fneg f2,f11
			ctx.f2.u64 = ctx.f11.u64 ^ 0x8000000000000000;
			// stfs f2,12(r11)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
		}
	loc_8224DCFC:
		// clrlwi r7,r9,24
		ctx.r7.u64 = ctx.r9.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// bne cr6,0x8224dd20
		if (ctx.r7.u32 == 0) {
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// mr r9,r30
			ctx.r9.u64 = var_r30;
			// add r6,r10,r11
			ctx.r6.u64 = ctx.r10.u64 + ctx.r11.u64;
			// lfs f1,12(r6)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
			ctx.f1.f64 = double(temp.f32);
			// fcmpu cr6,f1,f10
			// bge cr6,0x8224dd24
			if (ctx.f1.f64 >= ctx.f10.f64) goto loc_8224DD24;
		}
	loc_8224DD20:
		// li r9,1
		ctx.r9.s64 = 1;
	loc_8224DD24:
		// lhz r11,4(r29)
		ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 4);
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// clrlwi r9,r9,24
		ctx.r9.u64 = ctx.r9.u32 & 0xFF;
		// addi r10,r10,16
		ctx.r10.s64 = ctx.r10.s64 + 16;
		// cmpw cr6,r8,r11
		// blt cr6,0x8224dc9c
		if (ctx.r8.s32 < ctx.r11.s32) goto loc_8224DC9C;
	}
loc_8224DD3C:
	// clrlwi r11,r9,24
	ctx.r11.u64 = ctx.r9.u32 & 0xFF;
	// li r31,4
	var_r31 = 4;
	// cmplwi cr6,r11,0
	// bne cr6,0x8224dd54
	if (ctx.r11.u32 == 0) {
		// li r31,3
		var_r31 = 3;
		// beq cr6,0x8224dd5c
		if (ctx.cr6.eq) goto loc_8224DD5C;
	}
loc_8224DD54:
	// li r11,16
	ctx.r11.s64 = 16;
	// b 0x8224dd60
	goto loc_8224DD60;
loc_8224DD5C:
	// li r11,32
	ctx.r11.s64 = 32;
loc_8224DD60:
	// lbz r5,1(r26)
	ctx.r5.u64 = PPC_LOAD_U8(var_r26 + 1);
	// addi r27,r26,4
	var_r27 = (uint32_t)(var_r26 + 4);
	// cmplwi cr6,r31,0
	// or r4,r5,r11
	ctx.r4.u64 = ctx.r5.u64 | ctx.r11.u64;
	// stb r4,1(r26)
	PPC_STORE_U8(var_r26 + 1, ctx.r4.u8);
	// beq cr6,0x8224dd88
	if (var_r31 != 0) {
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8224dde8
		atSingleton_DDE8_g(ctx, base);
		// b 0x8224dd90
	} else {
	loc_8224DD88:
		// stw r30,0(r27)
		PPC_STORE_U32(var_r27 + 0, var_r30);
		// sth r30,6(r27)
		PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r30);
	}
loc_8224DD90:
	// cmpwi cr6,r31,0
	// ble cr6,0x8224ddd8
while (var_r31 != 0) {
	loc_8224DD98:
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// li r6,3
		ctx.r6.s64 = 3;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// lhz r5,4(r29)
		ctx.r5.u64 = PPC_LOAD_U16(var_r29 + 4);
		// add r4,r30,r11
		ctx.r4.u64 = var_r30 + ctx.r11.u64;
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// bl 0x8224d850
		xe_D850(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// li r4,16
		ctx.r4.s64 = 16;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8224de98
		atSingleton_DE98_g(ctx, base);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// stw r28,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, var_r28);
		// cmplwi cr6,r31,0
		// bne cr6,0x8224dd98
}
loc_8224DDD8:
	return;
}

__attribute__((alias("__imp__atSingleton_DDE8_g"))) PPC_WEAK_FUNC(atSingleton_DDE8_g);
PPC_FUNC_IMPL(__imp__atSingleton_DDE8_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,16383
	ctx.r11.s64 = 1073676288;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// ori r10,r11,65535
	ctx.r10.u64 = ctx.r11.u64 | 65535;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// cmplw cr6,r28,r10
	// rlwinm r31,r28,2,0,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC);
	// ble cr6,0x8224de14
	if (var_r28 > ctx.r10.u32) {
		// li r31,-1
		var_r31 = (uint32_t)(-1);
	}
loc_8224DE14:
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r9,0(r13)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwzx r3,r8,r9
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// lwz r6,4(r7)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmplwi cr6,r29,0
	// beq cr6,0x8224de80
	if (var_r29 != 0) {
		// addi r31,r28,-1
		var_r31 = (uint32_t)(var_r28 + -1);
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// cmpwi cr6,r31,0
		// blt cr6,0x8224de84
		if ((int32_t)var_r31 < 0) {
			// stw r29,0(r27)
			PPC_STORE_U32(var_r27 + 0, var_r29);
			// sth r28,6(r27)
			PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r28);
			return;
		}
	loc_8224DE58:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8224dfc0
		ke_DFC0(ctx, base);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpwi cr6,r31,0
		// bge cr6,0x8224de58
		if ((int32_t)var_r31 >= 0) goto loc_8224DE58;
		// stw r29,0(r27)
		PPC_STORE_U32(var_r27 + 0, var_r29);
		// sth r28,6(r27)
		PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r28);
		return;
	}
loc_8224DE80:
	// li r29,0
	var_r29 = 0;
loc_8224DE84:
	// stw r29,0(r27)
	PPC_STORE_U32(var_r27 + 0, var_r29);
	// sth r28,6(r27)
	PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r28);
	return;
}

__attribute__((alias("__imp__atSingleton_DE98_g"))) PPC_WEAK_FUNC(atSingleton_DE98_g);
PPC_FUNC_IMPL(__imp__atSingleton_DE98_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lhz r11,6(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 6);
	// lhz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 4);
	// cmplw cr6,r10,r11
	// bne cr6,0x8224df9c
	if (ctx.r10.u32 == ctx.r11.u32) {
		// clrlwi r10,r4,16
		ctx.r10.u64 = ctx.r4.u32 & 0xFFFF;
		// lis r9,16383
		ctx.r9.s64 = 1073676288;
		// add r7,r10,r11
		ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
		// ori r8,r9,65535
		ctx.r8.u64 = ctx.r9.u64 | 65535;
		// clrlwi r11,r7,16
		ctx.r11.u64 = ctx.r7.u32 & 0xFFFF;
		// mr r30,r11
		var_r30 = ctx.r11.u32;
		// cmplw cr6,r30,r8
		// sth r11,6(r31)
		PPC_STORE_U16(var_r31 + 6, ctx.r11.u16);
		// rlwinm r3,r30,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
		// ble cr6,0x8224dee4
		if (var_r30 > ctx.r8.u32) {
			// li r3,-1
		}
	loc_8224DEE4:
		// bl 0x820dec88
		xe_EC88(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// cmplwi cr6,r28,0
		// beq cr6,0x8224df20
		if (var_r28 != 0) {
			// addi r30,r30,-1
			var_r30 = (uint32_t)(var_r30 + -1);
			// mr r29,r28
			var_r29 = (uint32_t)(var_r28);
			// cmpwi cr6,r30,0
			// blt cr6,0x8224df24
			if ((int32_t)var_r30 < 0) goto loc_8224DF24;
		loc_8224DF04:
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x8224dfc0
			ke_DFC0(ctx, base);
			// addi r30,r30,-1
			var_r30 = (uint32_t)(var_r30 + -1);
			// addi r29,r29,4
			var_r29 = (uint32_t)(var_r29 + 4);
			// cmpwi cr6,r30,0
			// bge cr6,0x8224df04
			if ((int32_t)var_r30 >= 0) goto loc_8224DF04;
			// b 0x8224df24
		} else {
		loc_8224DF20:
			// li r28,0
			var_r28 = 0;
		}
	loc_8224DF24:
		// lhz r6,4(r31)
		ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 4);
		// cmplwi cr6,r6,0
		// beq cr6,0x8224df58
		if (ctx.r6.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		loc_8224DF34:
			// lwz r4,0(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
			// rlwinm r10,r11,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r5,r11,1
			ctx.r5.s64 = ctx.r11.s64 + 1;
			// clrlwi r11,r5,16
			ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
			// lwzx r3,r10,r4
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
			// stwx r3,r10,r28
			PPC_STORE_U32(ctx.r10.u32 + var_r28, ctx.r3.u32);
			// lhz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 4);
			// cmplw cr6,r11,r10
			// blt cr6,0x8224df34
			if (ctx.r11.u32 < ctx.r10.u32) goto loc_8224DF34;
		}
	loc_8224DF58:
		// lwz r30,0(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */);
		// cmplwi cr6,r30,0
		// beq cr6,0x8224df98
		if (var_r30 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x820f90d0
			atSingleton_Find_90D0(ctx, base);
			// clrlwi r9,r3,24
			ctx.r9.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r9,0
			// bne cr6,0x8224df98
			if (ctx.r9.u32 != 0) goto loc_8224DF98;
			// lwz r8,0(r13)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
			// li r7,4
			ctx.r7.s64 = 4;
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// lwzx r3,r7,r8
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
			// lwz r5,8(r6)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		}
	loc_8224DF98:
		// stw r28,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ var_r28);
	}
loc_8224DF9C:
	// lhz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 4);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
	// addi r4,r11,1
	ctx.r4.s64 = ctx.r11.s64 + 1;
	// rotlwi r10,r11,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 2);
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// sth r4,4(r31)
	PPC_STORE_U16(var_r31 + 4, ctx.r4.u16);
	return;
}

__attribute__((alias("__imp__ke_DFC0"))) PPC_WEAK_FUNC(ke_DFC0);
PPC_FUNC_IMPL(__imp__ke_DFC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// lis r30,-32161
	var_r30 = (uint32_t)(-2107703296);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,23208(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
	// cmplwi cr6,r3,0
	// bne cr6,0x8224dff0
	if (ctx.r3.u32 == 0) {
		// bl 0x8258623c
		__imp__KeTlsAlloc(ctx, base);
		// stw r3,23208(r30)
		PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
	}
loc_8224DFF0:
	// bl 0x8258621c
	__imp__KeTlsGetValue(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x8224e0d4
	if (ctx.r3.u32 != 0) {
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmplwi cr6,r11,0
		// beq cr6,0x8224e0d4
		if (!(ctx.r11.u32 == 0)) {
			// lwz r3,23208(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
			// cmplwi cr6,r3,0
			// bne cr6,0x8224e01c
			if (ctx.r3.u32 == 0) {
			// bl 0x8258623c
			__imp__KeTlsAlloc(ctx, base);
			// stw r3,23208(r30)
			PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
			}
			loc_8224E01C:
			// bl 0x8258621c
			__imp__KeTlsGetValue(ctx, base);
			// lwz r11,0(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// cmplw cr6,r31,r11
			// blt cr6,0x8224e040
			if (var_r31 >= ctx.r11.u32) {
			// lwz r10,72(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
			// add r10,r10,r11
			ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
			// li r11,1
			ctx.r11.s64 = 1;
			// cmplw cr6,r31,r10
			// ble cr6,0x8224e044
			if (var_r31 <= ctx.r10.u32) goto loc_8224E044;
			}
			loc_8224E040:
			// li r11,0
			ctx.r11.s64 = 0;
			loc_8224E044:
			// clrlwi r8,r11,24
			ctx.r8.u64 = ctx.r11.u32 & 0xFF;
		} else {
			if (!(ctx.r8.u32 == 0)) {
				// lwz r3,23208(r30)
				ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
				// cmplwi cr6,r3,0
				// bne cr6,0x8224e064
				if (ctx.r3.u32 == 0) {
				// bl 0x8258623c
				__imp__KeTlsAlloc(ctx, base);
				// stw r3,23208(r30)
				PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
				}
				loc_8224E064:
				// bl 0x8258621c
				__imp__KeTlsGetValue(ctx, base);
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
				// cmplwi cr6,r11,0
				// beq cr6,0x8224e09c
				if (ctx.r11.u32 != 0) {
				// lwz r7,4(r3)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
				// lwz r6,76(r3)
				ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
				// subf r5,r7,r11
				ctx.r5.s64 = ctx.r11.s64 - ctx.r7.s64;
				// twllei r6,0
				if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
				// divwu r10,r5,r6
				ctx.r10.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
				// addi r4,r10,2
				ctx.r4.s64 = ctx.r10.s64 + 2;
				// rlwinm r10,r4,2,0,29
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
				// lwzx r10,r10,r3
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
				// add r9,r10,r11
				ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
				// stw r9,0(r31)
				PPC_STORE_U32(var_r31 + 0, ctx.r9.u32);
				}
				loc_8224E09C:
				// lwz r3,23208(r30)
				ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 23208);
				// cmplwi cr6,r3,0
				// bne cr6,0x8224e0b0
				if (ctx.r3.u32 == 0) {
				// bl 0x8258623c
				__imp__KeTlsAlloc(ctx, base);
				// stw r3,23208(r30)
				PPC_STORE_U32(var_r30 + 23208, ctx.r3.u32);
				}
				loc_8224E0B0:
				// bl 0x8258621c
				__imp__KeTlsGetValue(ctx, base);
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
				// mr r4,r3
				ctx.r4.u64 = ctx.r3.u64;
				// mr r3,r11
				ctx.r3.u64 = ctx.r11.u64;
				// lwz r7,4(r8)
				// bctrl
				VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
				// b 0x8224e0dc
				} else {
			}
		}
	loc_8224E0D4:
		// li r6,0
		ctx.r6.s64 = 0;
		// stw r6,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r6.u32);
	}
loc_8224E0DC:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimPlayer_vfn_0"))) PPC_WEAK_FUNC(crAnimPlayer_vfn_0);
PPC_FUNC_IMPL(__imp__crAnimPlayer_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// rlwinm r11,r4,0,30,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x2;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r11,0
	// beq cr6,0x8224e170
	if (ctx.r11.u32 != 0) {
		// addi r30,r31,-16
		var_r30 = (uint32_t)(var_r31 + -16);
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// mulli r10,r11,368
		ctx.r10.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(368));
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// add r10,r10,r31
		ctx.r10.u64 = ctx.r10.u64 + var_r31;
		// cmpwi cr6,r11,0
		// blt cr6,0x8224e154
		if (ctx.r11.s32 >= 0) {
			// lis r9,-32251
			// addi r9,r9,-768
			ctx.r9.s64 = ctx.r9.s64 + -768;
		loc_8224E140:
			// addi r10,r10,-368
			ctx.r10.s64 = ctx.r10.s64 + -368;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r9,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
			// bge cr6,0x8224e140
			if (ctx.r11.s32 >= 0) goto loc_8224E140;
		}
	loc_8224E154:
		// clrlwi r10,r4,31
		ctx.r10.u64 = ctx.r4.u32 & 0x1;
		// cmplwi cr6,r10,0
		// beq cr6,0x8224e168
		if (ctx.r10.u32 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		}
	loc_8224E168:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// b 0x8224e194
	} else {
	loc_8224E170:
		// lis r11,-32251
		// clrlwi r9,r4,31
		ctx.r9.u64 = ctx.r4.u32 & 0x1;
		// addi r11,r11,-768
		ctx.r11.s64 = ctx.r11.s64 + -768;
		// cmplwi cr6,r9,0
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// beq cr6,0x8224e190
		if (ctx.r9.u32 != 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		}
	loc_8224E190:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8224E194:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_E1B0_h"))) PPC_WEAK_FUNC(phArticulatedCollider_E1B0_h);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_E1B0_h) {
	PPC_FUNC_PROLOGUE();
	// add r11,r4,r3
	ctx.r11.u64 = ctx.r4.u64 + ctx.r3.u64;
	// lbz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 176);
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8224e1c8
	if (ctx.r10.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8224E1C8:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_3"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_3);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_3) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r5,464(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r11,4(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpwi cr6,r11,0
	// blelr cr6
	if (ctx.r11.s32 <= 0) return;
	// addi r10,r5,40
	ctx.r10.s64 = ctx.r5.s64 + 40;
loc_8224E1E8:
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r8,r11,272
	ctx.r8.s64 = ctx.r11.s64 + 272;
	// addi r7,r11,288
	ctx.r7.s64 = ctx.r11.s64 + 288;
	// addi r11,r11,304
	ctx.r11.s64 = ctx.r11.s64 + 304;
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r8,4(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmpw cr6,r9,r8
	// blt cr6,0x8224e1e8
	if (ctx.r9.s32 < ctx.r8.s32) goto loc_8224E1E8;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_46"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_46);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_46) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,472(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 472);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_56"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_56);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_56) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,508(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 508);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_60"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_60);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,492(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 492);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_61"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_61);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_61) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,500(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 500);
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_62"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_62);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_62) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=96, manual
	// lwz r10,476(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 476);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,464(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r9,484(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// addi r7,r10,42
	ctx.r7.s64 = ctx.r10.s64 + 42;
	// lwzx r4,r9,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r6,r8
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r8.u32);
	// lwz r10,32(r11)
	// bctrl
	VCALL(ctx.r3.u32, 8, ctx, base);  // vtable slot 8 (byte +32)
	// lfs f1,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_63"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_63);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_63) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82255fe0
	phArticulatedCollider_5FE0(ctx, base);
	// lwz r4,464(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 464);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r11,4(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// cmpwi cr6,r11,0
	// ble cr6,0x8224e380
	if (ctx.r11.s32 > 0) {
		// addi r9,r4,40
		ctx.r9.s64 = ctx.r4.s64 + 40;
	loc_8224E30C:
		// lwz r10,0(r9)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// addi r11,r10,304
		ctx.r11.s64 = ctx.r10.s64 + 304;
		// addi r7,r10,1040
		ctx.r7.s64 = ctx.r10.s64 + 1040;
		// addi r31,r11,16
		var_r31 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82050010
		// addi r3,r7,16
		ctx.r3.s64 = ctx.r7.s64 + 16;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r10,272
		ctx.r6.s64 = ctx.r10.s64 + 272;
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r10,288
		ctx.r10.s64 = ctx.r10.s64 + 288;
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v11,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
		// stvx v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v10,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
		// stvx v10,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
		// stvx v9,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,4(r4)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// cmpw cr6,r8,r11
		// blt cr6,0x8224e30c
		if (ctx.r8.s32 < ctx.r11.s32) goto loc_8224E30C;
	}
loc_8224E380:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_E398_p33"))) PPC_WEAK_FUNC(phArticulatedCollider_E398_p33);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_E398_p33) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,8(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	// ble cr6,0x8224e418
	if (ctx.f0.f64 > ctx.f13.f64) {
		// lis r11,-32253
		// addi r11,r11,-12024
		ctx.r11.s64 = ctx.r11.s64 + -12024;
		// lfs f13,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f0,f13
		// bne cr6,0x8224e3f0
		if (ctx.f0.f64 == ctx.f13.f64) {
			// fmr f0,f13
			ctx.f0.f64 = ctx.f13.f64;
			// b 0x8224e3fc
		} else {
		loc_8224E3F0:
			// fsqrts f0,f0
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
			// lfs f13,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f13.f64 = double(temp.f32);
			// fdivs f0,f13,f0
			ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
		}
	loc_8224E3FC:
		// stfs f0,-16(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
		// addi r10,r1,-16
		ctx.r10.s64 = ctx.r1.s64 + -16;
		// lvx128 v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vmulfp128 v11,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v11,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	}
loc_8224E418:
	// lis r11,-32163
	// addi r11,r11,-18528
	ctx.r11.s64 = ctx.r11.s64 + -18528;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phPolytree_E430_w"))) PPC_WEAK_FUNC(phPolytree_E430_w);
PPC_FUNC_IMPL(__imp__phPolytree_E430_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=160, savegprlr_24
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x822ca808
	phPolytree_A808_w(ctx, base);
	// lis r10,-32251
	// li r31,0
	var_r31 = 0;
	// addi r10,r10,-652
	ctx.r10.s64 = ctx.r10.s64 + -652;
	// addi r3,r30,476
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 476;
	// addi r27,r30,484
	var_r27 = (uint32_t)(var_r30 + 484);
	// addi r26,r30,492
	var_r26 = (uint32_t)(var_r30 + 492);
	// addi r25,r30,500
	var_r25 = (uint32_t)(var_r30 + 500);
	// addi r24,r30,508
	var_r24 = (uint32_t)(var_r30 + 508);
	// stw r10,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r10.u32);
	// addi r29,r30,772
	var_r29 = (uint32_t)(var_r30 + 772);
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, var_r31);
	// sth r31,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, (uint16_t)var_r31);
	// addi r28,r30,780
	var_r28 = (uint32_t)(var_r30 + 780);
	// sth r31,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, (uint16_t)var_r31);
	// addi r11,r30,516
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 516;
	// stw r31,0(r27)
	PPC_STORE_U32(var_r27 + 0, var_r31);
	// mr r9,r31
	ctx.r9.u64 = var_r31;
	// sth r31,4(r27)
	PPC_STORE_U16(var_r27 + 4, (uint16_t)var_r31);
	// li r10,64
	ctx.r10.s64 = 64;
	// sth r31,6(r27)
	PPC_STORE_U16(var_r27 + 6, (uint16_t)var_r31);
	// stw r31,0(r26)
	PPC_STORE_U32(var_r26 + 0, var_r31);
	// sth r31,4(r26)
	PPC_STORE_U16(var_r26 + 4, (uint16_t)var_r31);
	// sth r31,6(r26)
	PPC_STORE_U16(var_r26 + 6, (uint16_t)var_r31);
	// stw r31,0(r25)
	PPC_STORE_U32(var_r25 + 0, var_r31);
	// sth r31,4(r25)
	PPC_STORE_U16(var_r25 + 4, (uint16_t)var_r31);
	// sth r31,6(r25)
	PPC_STORE_U16(var_r25 + 6, (uint16_t)var_r31);
	// stw r31,0(r24)
	PPC_STORE_U32(var_r24 + 0, var_r31);
	// sth r31,4(r24)
	PPC_STORE_U16(var_r24 + 4, (uint16_t)var_r31);
	// sth r31,6(r24)
	PPC_STORE_U16(var_r24 + 6, (uint16_t)var_r31);
	// stw r31,0(r29)
	PPC_STORE_U32(var_r29 + 0, var_r31);
	// sth r31,4(r29)
	PPC_STORE_U16(var_r29 + 4, (uint16_t)var_r31);
	// sth r31,6(r29)
	PPC_STORE_U16(var_r29 + 6, (uint16_t)var_r31);
	// stw r31,0(r28)
	PPC_STORE_U32(var_r28 + 0, var_r31);
	// sth r31,4(r28)
	PPC_STORE_U16(var_r28 + 4, (uint16_t)var_r31);
	// sth r31,6(r28)
	PPC_STORE_U16(var_r28 + 6, (uint16_t)var_r31);
	// stw r31,472(r30)
	PPC_STORE_U32(var_r30 + 472, var_r31);
	// stb r31,468(r30)
	PPC_STORE_U8(var_r30 + 468, (uint8_t)var_r31);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_8224E4DC:
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x8224e4dc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8224E4DC;
	// li r4,93
	ctx.r4.s64 = 93;
	// bl 0x8240a250
	xe_A250(ctx, base);
	// li r4,93
	ctx.r4.s64 = 93;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x8240a250
	xe_A250(ctx, base);
	// li r4,93
	ctx.r4.s64 = 93;
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	// bl 0x8240a250
	xe_A250(ctx, base);
	// li r4,93
	ctx.r4.s64 = 93;
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// bl 0x8240a250
	xe_A250(ctx, base);
	// li r4,93
	ctx.r4.s64 = 93;
	// mr r3,r24
	ctx.r3.u64 = var_r24;
	// bl 0x8240a250
	xe_A250(ctx, base);
	// li r4,256
	ctx.r4.s64 = 256;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// stb r31,788(r30)
	PPC_STORE_U8(var_r30 + 788, (uint8_t)var_r31);
	// bl 0x8240a250
	xe_A250(ctx, base);
	// li r4,256
	ctx.r4.s64 = 256;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// bl 0x8240a250
	xe_A250(ctx, base);
	// lhz r10,776(r30)
	ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 776);
	// mr r11,r31
	ctx.r11.u64 = var_r31;
	// cmpwi cr6,r10,0
	// ble cr6,0x8224e574
	if (ctx.r10.s32 > 0) {
		// li r9,-1
	loc_8224E550:
		// lwz r10,0(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// stwx r9,r10,r31
		PPC_STORE_U32(ctx.r10.u32 + var_r31, ctx.r9.u32);
		// lwz r8,0(r28)
		ctx.r8.u64 = PPC_LOAD_U32(var_r28 + 0);
		// stwx r9,r31,r8
		PPC_STORE_U32(var_r31 + ctx.r8.u32, ctx.r9.u32);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// lhz r10,776(r30)
		ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 776);
		// cmpw cr6,r11,r10
		// blt cr6,0x8224e550
		if (ctx.r11.s32 < ctx.r10.s32) goto loc_8224E550;
	}
loc_8224E574:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_0"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_0);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lhz r11,786(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 786);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224e5b0
	if (ctx.r11.u32 != 0) {
		// lwz r3,780(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 780);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E5B0:
	// lhz r10,778(r31)
	ctx.r10.u64 = PPC_LOAD_U16(var_r31 + 778);
	// cmplwi cr6,r10,0
	// beq cr6,0x8224e5c4
	if (ctx.r10.u32 != 0) {
		// lwz r3,772(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 772);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E5C4:
	// lhz r9,514(r31)
	ctx.r9.u64 = PPC_LOAD_U16(var_r31 + 514);
	// cmplwi cr6,r9,0
	// beq cr6,0x8224e5d8
	if (ctx.r9.u32 != 0) {
		// lwz r3,508(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 508);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E5D8:
	// lhz r8,506(r31)
	ctx.r8.u64 = PPC_LOAD_U16(var_r31 + 506);
	// cmplwi cr6,r8,0
	// beq cr6,0x8224e5ec
	if (ctx.r8.u32 != 0) {
		// lwz r3,500(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 500);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E5EC:
	// lhz r7,498(r31)
	ctx.r7.u64 = PPC_LOAD_U16(var_r31 + 498);
	// cmplwi cr6,r7,0
	// beq cr6,0x8224e600
	if (ctx.r7.u32 != 0) {
		// lwz r3,492(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 492);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E600:
	// lhz r6,490(r31)
	ctx.r6.u64 = PPC_LOAD_U16(var_r31 + 490);
	// cmplwi cr6,r6,0
	// beq cr6,0x8224e614
	if (ctx.r6.u32 != 0) {
		// lwz r3,484(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 484);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E614:
	// lhz r5,482(r31)
	ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 482);
	// cmplwi cr6,r5,0
	// beq cr6,0x8224e628
	if (ctx.r5.u32 != 0) {
		// lwz r3,476(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 476);
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
	}
loc_8224E628:
	// lis r11,-32250
	// clrlwi r4,r30,31
	ctx.r4.u64 = var_r30 & 0x1;
	// addi r11,r11,-25540
	ctx.r11.s64 = ctx.r11.s64 + -25540;
	// cmplwi cr6,r4,0
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x8224e64c
	if (ctx.r4.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8224E64C:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_E668"))) PPC_WEAK_FUNC(phArticulatedCollider_E668);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_E668) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32163
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r6,r11,11412
	ctx.r6.s64 = ctx.r11.s64 + 11412;
	// lis r11,-32163
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 16);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r5,r11,11384
	ctx.r5.s64 = ctx.r11.s64 + 11384;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r3,12(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// bl 0x82430978
	SinglesNetworkClient_0978_g(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// bl 0x8228f5c8
	pongCreatureInst_F5C8(ctx, base);
	// addi r8,r3,129
	ctx.r8.s64 = ctx.r3.s64 + 129;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r7,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r31);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_1"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_1);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x822cabe0
	phCollider_vfn_1(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// stw r11,472(r31)
	PPC_STORE_U32(var_r31 + 472, ctx.r11.u32);
	// stb r11,468(r31)
	PPC_STORE_U8(var_r31 + 468, ctx.r11.u8);
	// bl 0x822557f0
	phArticulatedCollider_57F0_fw(ctx, base);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82255d58
	phArticulatedCollider_5D58(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_2"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_2);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r31,256
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 256;
	// addi r10,r31,272
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 272;
	// addi r9,r31,224
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 224;
	// addi r8,r31,240
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 240;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,16(r11)
	// bctrl
	phArticulatedCollider_vfn_4(ctx, base);  // vtable slot 4 (byte +16)  // phArticulatedCollider::vfn_4
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82255a40
	phArticulatedCollider_5A40_wrh(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_4"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_4);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_4) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// bl 0x822cad78
	phCollider_vfn_4(ctx, base);
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// bl 0x82255b50
	phArticulatedCollider_5B50_wrh(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_5"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_5);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f87c
	ctx.lr = 0x8224E7D0;
	__savegprlr_21(ctx, base);
	// stfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.f31.u64);
	// ld r12,-4096(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// ld r12,-8192(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8192);
	// ld r12,-12288(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -12288);
	// ld r12,-16384(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16384);
	// stwu r1,-17280(r1)
	ea = -17280 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x822caf38
	phCollider_vfn_5(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,464(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 464);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// bl 0x82256518
	phArticulatedCollider_6518(ctx, base);
	// lbz r11,788(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 788);
	// cmplwi cr6,r11,0
	// beq cr6,0x8224ea18
	if (ctx.r11.u32 != 0) {
		// lwz r10,16(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 16);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// lwz r9,4(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// lwz r28,12(r9)
		var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + 12));
		// bl 0x82294330
		util_4330(ctx, base);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x822943e8
		util_43E8(ctx, base);
		// lhz r11,776(r30)
		ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 776);
		// li r24,0
		var_r24 = 0;
		// cmpwi cr6,r11,0
		// mr r21,r24
		var_r21 = (uint32_t)(var_r24);
		// ble cr6,0x8224ea18
		if (ctx.r11.s32 <= 0) {
			// addi r1,r1,17280
			ctx.r1.s64 = ctx.r1.s64 + 17280;
			// lfd f31,-104(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
			// b 0x8242f8cc
			__restgprlr_21(ctx, base);
			return;
		}
		// mr r23,r24
		var_r23 = (uint32_t)(var_r24);
		// lis r22,-32161
		var_r22 = (uint32_t)(-2107703296);
	loc_8224E848:
		// lwz r8,772(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 772);
		// lwzx r26,r23,r8
		var_r26 = (uint32_t)(PPC_LOAD_U32(var_r23 + ctx.r8.u32));
		// cmpwi cr6,r26,-1
		// beq cr6,0x8224ea18
		if ((int32_t)var_r26 == -1) {
			// addi r1,r1,17280
			ctx.r1.s64 = ctx.r1.s64 + 17280;
			// lfd f31,-104(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
			// b 0x8242f8cc
			__restgprlr_21(ctx, base);
			return;
		}
		// lwz r11,112(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 112);
		// rlwinm r7,r26,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(var_r26 | (var_r26 << 32), 2) & 0xFFFFFFFC;
		// lwzx r25,r7,r11
		var_r25 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32));
		// cmplwi cr6,r25,0
		// beq cr6,0x8224ea04
		if (var_r25 != 0) {
			// lwz r6,780(r30)
			ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 780);
			// lwzx r27,r6,r23
			var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r6.u32 + var_r23));
			// rlwinm r5,r27,2,0,29
			ctx.r5.u64 = __builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0xFFFFFFFC;
			// lwzx r6,r5,r11
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r11.u32);
			// cmplwi cr6,r6,0
			// beq cr6,0x8224ea04
			if (ctx.r6.u32 == 0) goto loc_8224EA04;
			// lwz r11,124(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 124);
			// rlwinm r10,r26,6,0,25
			ctx.r10.u64 = __builtin_rotateleft64(var_r26 | (var_r26 << 32), 6) & 0xFFFFFFC0;
			// lwz r7,120(r28)
			ctx.r7.u64 = PPC_LOAD_U32(var_r28 + 120);
			// addi r5,r1,17008
			ctx.r5.s64 = ctx.r1.s64 + 17008;
			// add r11,r10,r11
			ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
			// lwz r9,16(r30)
			ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 16);
			// add r10,r10,r7
			ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
			// stw r24,17144(r1)
			PPC_STORE_U32(ctx.r1.u32 + 17144, var_r24);
			// stw r25,17136(r1)
			PPC_STORE_U32(ctx.r1.u32 + 17136, var_r25);
			// addi r4,r11,16
			ctx.r4.s64 = ctx.r11.s64 + 16;
			// addi r7,r11,32
			ctx.r7.s64 = ctx.r11.s64 + 32;
			// rlwinm r8,r27,6,0,25
			ctx.r8.u64 = __builtin_rotateleft64(var_r27 | (var_r27 << 32), 6) & 0xFFFFFFC0;
			// stw r9,17152(r1)
			PPC_STORE_U32(ctx.r1.u32 + 17152, ctx.r9.u32);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stw r10,16992(r1)
			PPC_STORE_U32(ctx.r1.u32 + 16992, ctx.r10.u32);
			// lvx128 v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v0,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r10,r1,17024
			ctx.r10.s64 = ctx.r1.s64 + 17024;
			// lvx128 v13,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r11,48
			ctx.r11.s64 = ctx.r11.s64 + 48;
			// stvx v13,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v12,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r7,r1,17040
			ctx.r7.s64 = ctx.r1.s64 + 17040;
			// stvx v12,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r5,r1,17056
			ctx.r5.s64 = ctx.r1.s64 + 17056;
			// lvx128 v11,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v11,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r10,120(r28)
			ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 120);
			// lwz r11,124(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 124);
			// add r4,r8,r10
			ctx.r4.u64 = ctx.r8.u64 + ctx.r10.u64;
			// stw r9,17156(r1)
			PPC_STORE_U32(ctx.r1.u32 + 17156, ctx.r9.u32);
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
			// stw r6,17140(r1)
			PPC_STORE_U32(ctx.r1.u32 + 17140, ctx.r6.u32);
			// addi r8,r1,17072
			ctx.r8.s64 = ctx.r1.s64 + 17072;
			// stw r24,17148(r1)
			PPC_STORE_U32(ctx.r1.u32 + 17148, var_r24);
			// addi r10,r11,16
			ctx.r10.s64 = ctx.r11.s64 + 16;
			// addi r9,r11,32
			ctx.r9.s64 = ctx.r11.s64 + 32;
			// stw r4,16996(r1)
			PPC_STORE_U32(ctx.r1.u32 + 16996, ctx.r4.u32);
			// addi r7,r11,48
			ctx.r7.s64 = ctx.r11.s64 + 48;
			// lvx128 v10,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v10,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r6,r1,17088
			ctx.r6.s64 = ctx.r1.s64 + 17088;
			// lvx128 v9,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v9,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r5,r1,17104
			ctx.r5.s64 = ctx.r1.s64 + 17104;
			// lvx128 v8,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v8,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r1,17120
			ctx.r4.s64 = ctx.r1.s64 + 17120;
			// lvx128 v7,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v7,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x82294600
			phBoundGeometry_4600_p45(ctx, base);
			// lwz r3,80(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// stw r3,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
			// lwz r3,17136(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 17136);
			// lwz r10,108(r11)
			// bctrl
			VCALL(ctx.r3.u32, 27, ctx, base);  // vtable slot 27 (byte +108)
			// clrlwi r9,r3,24
			ctx.r9.u64 = ctx.r3.u32 & 0xFF;
			// cmplwi cr6,r9,0
			// beq cr6,0x8224ea04
			if (ctx.r9.u32 == 0) goto loc_8224EA04;
			// lwz r11,84(r1)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			// lwz r8,80(r1)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			// cmpw cr6,r11,r8
			// bge cr6,0x8224ea04
			if (ctx.r11.s32 >= ctx.r8.s32) goto loc_8224EA04;
			// mulli r10,r11,176
			ctx.r10.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(176));
			// addi r9,r1,108
			ctx.r9.s64 = ctx.r1.s64 + 108;
			// subf r29,r11,r8
			var_r29 = (uint32_t)(ctx.r8.s64 - ctx.r11.s64);
			// add r31,r10,r9
			var_r31 = (uint32_t)(ctx.r10.u64 + ctx.r9.u64);
		loc_8224E99C:
			// lwz r11,16(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
			// addi r3,r31,-12
			ctx.r3.s64 = (int64_t)(int32_t)var_r31 + -12;
			// addi r4,r11,16
			ctx.r4.s64 = ctx.r11.s64 + 16;
			// bl 0x824112e0
			atSingleton_12E0_p46(ctx, base);
			// lwz r11,16(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
			// stw r11,148(r31)
			PPC_STORE_U32(var_r31 + 148, ctx.r11.u32);
			// stw r11,152(r31)
			PPC_STORE_U32(var_r31 + 152, ctx.r11.u32);
			// lwz r8,17136(r1)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 17136);
			// cmplw cr6,r8,r25
			// bne cr6,0x8224e9d0
			if (ctx.r8.u32 == var_r25) {
				// stw r26,0(r31)
				PPC_STORE_U32(var_r31 + 0, var_r26);
				// stw r27,4(r31)
				PPC_STORE_U32(var_r31 + 4, var_r27);
				// b 0x8224e9d8
			} else {
			loc_8224E9D0:
				// stw r27,0(r31)
				PPC_STORE_U32(var_r31 + 0, var_r27);
				// stw r26,4(r31)
				PPC_STORE_U32(var_r31 + 4, var_r26);
			}
		loc_8224E9D8:
			// lwz r11,23504(r22)
			ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 23504);
			// mr r6,r3
			ctx.r6.u64 = ctx.r3.u64;
			// lwz r5,16(r30)
			ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 16);
			// li r7,0
			ctx.r7.s64 = 0;
			// mr r4,r5
			ctx.r4.u64 = ctx.r5.u64;
			// lwz r3,8(r11)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			// bl 0x822d5710
			util_5710(ctx, base);
			// addi r29,r29,-1
			var_r29 = (uint32_t)(var_r29 + -1);
			// addi r31,r31,176
			var_r31 = (uint32_t)(var_r31 + 176);
			// cmplwi cr6,r29,0
			// bne cr6,0x8224e99c
			if (var_r29 != 0) goto loc_8224E99C;
		}
	loc_8224EA04:
		// lhz r11,776(r30)
		ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 776);
		// addi r21,r21,1
		var_r21 = (uint32_t)(var_r21 + 1);
		// addi r23,r23,4
		var_r23 = (uint32_t)(var_r23 + 4);
		// cmpw cr6,r21,r11
		// blt cr6,0x8224e848
		if ((int32_t)var_r21 < ctx.r11.s32) goto loc_8224E848;
	}
loc_8224EA18:
	// addi r1,r1,17280
	ctx.r1.s64 = ctx.r1.s64 + 17280;
	// lfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x8242f8cc
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_6"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_6);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_6) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82256968
	phBoundCapsule_6968_g(ctx, base);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82258450
	phArticulatedCollider_8450(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224f0e0
	phArticulatedCollider_F0E0(ctx, base);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82255d58
	phArticulatedCollider_5D58(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_7"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_7);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_7) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=208, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x822cb740
	phCollider_vfn_7(ctx, base);
	// lwz r11,464(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 464);
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r11,40(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// addi r9,r11,48
	ctx.r9.s64 = ctx.r11.s64 + 48;
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v11,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// stvx v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// stvx v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8223b218
	LocomotionStateAnim_B218_g(ctx, base);
	// addi r4,r31,160
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 160;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8223b218
	LocomotionStateAnim_B218_g(ctx, base);
	// lfs f7,92(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// lfs f11,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lfs f8,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lfs f12,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f5,f8,f7
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// fmuls f6,f12,f11
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f9,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f3,f10,f9,f5
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 - ctx.f5.f64));
	// fmadds f4,f0,f13,f6
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f6.f64));
	// fnmsubs f1,f0,f11,f3
	ctx.f1.f64 = double(float(-(ctx.f0.f64 * ctx.f11.f64 - ctx.f3.f64)));
	// fmadds f2,f8,f10,f4
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 + ctx.f4.f64));
	// fmuls f4,f12,f7
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// fmuls f3,f0,f7
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// fmadds f5,f13,f12,f1
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f1.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmadds f6,f7,f9,f2
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + ctx.f2.f64));
	// stfs f6,92(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmsubs f2,f11,f9,f4
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f9.f64 - ctx.f4.f64));
	// fmsubs f1,f13,f9,f3
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f9.f64 - ctx.f3.f64));
	// fnmsubs f13,f13,f8,f2
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f8.f64 - ctx.f2.f64)));
	// fnmsubs f12,f12,f10,f1
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f10.f64 - ctx.f1.f64)));
	// fmadds f0,f0,f10,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 + ctx.f13.f64));
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmadds f11,f11,f8,f12
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f12.f64));
	// stfs f11,88(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x8224e398
	phArticulatedCollider_E398_p33(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x820c4340
	util_4340(ctx, base);
	// stfs f1,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// lvx128 v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// vmulfp128 v7,v9,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82256c28
	phBoundCapsule_6C28_fw(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_9"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_9);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lwz r11,464(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpwi cr6,r10,0
	// ble cr6,0x8224ec3c
	if (ctx.r10.s32 > 0) {
		// addi r31,r11,168
		var_r31 = (uint32_t)(ctx.r11.s64 + 168);  // addr:0x825d00a8
		// mr r30,r10
		var_r30 = ctx.r10.u32;
	loc_8224EBF8:
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lwz r11,20(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
		// cmpwi cr6,r11,0
		// li r11,1
		ctx.r11.s64 = 1;
		// bne cr6,0x8224ec10
		if (ctx.r11.s32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8224EC10:
		// clrlwi r9,r11,24
		ctx.r9.u64 = ctx.r11.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x8224ec2c
		if (ctx.r9.u32 != 0) {
			// lwz r7,72(r8)
			// bctrl
			VCALL(ctx.r3.u32, 18, ctx, base);  // vtable slot 18 (byte +72)
		}
	loc_8224EC2C:
		// addi r30,r30,-1
		var_r30 = (uint32_t)(var_r30 + -1);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmplwi cr6,r30,0
		// bne cr6,0x8224ebf8
		if (var_r30 != 0) goto loc_8224EBF8;
	}
loc_8224EC3C:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_8"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_8);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	double var_f29 = 0.0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=304, savegprlr_25
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f29,f1
	var_f29 = ctx.f1.f64;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x822567d8
	phArticulatedCollider_67D8_h(ctx, base);
	// lbz r11,468(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 468);
	// cmplwi cr6,r11,0
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r25,r11,-11560
	var_r25 = (uint32_t)(ctx.r11.s64 + -11560);  // lbl_8202D2D8 @ 0x8202d2d8
	// bne cr6,0x8224ee64
	if (ctx.r11.u32 == 0) {
		// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r9,188(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 47, ctx, base);  // pattern-B slot 47 (byte +188)
		// lwz r10,472(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 472);
		// cmpwi cr6,r10,1
		// ble cr6,0x8224ee54
		if (ctx.r10.s32 > 1) {
			// lis r11,-32161
			ctx.r11.s64 = -2107703296;
			// addi r30,r11,14376
			var_r30 = (uint32_t)(ctx.r11.s64 + 14376);  // lbl_825F3828 @ 0x825f3828
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// stw r10,36(r30)
			PPC_STORE_U32(var_r30 + 36, ctx.r10.u32);
			// bl 0x82351a58
			phJoint3Dof_351A58(ctx, base);
			// lwz r8,472(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 472);
			// li r28,0
			var_r28 = 0;
			// cmpwi cr6,r8,0
			// ble cr6,0x8224edb4
			if (ctx.r8.s32 > 0) {
				// lfs f30,-456(r25)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(var_r25 + -456);
				var_f30 = double(temp.f32);
				// li r27,0
				var_r27 = 0;
				// li r26,-1
				var_r26 = (uint32_t)(-1);
			loc_8224ECE4:
				// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
				// mr r4,r28
				ctx.r4.u64 = var_r28;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r6,204(r7)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 51, ctx, base);  // pattern-B slot 51 (byte +204)
				// lwz r11,32(r30)
				ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 32);
				// mr r4,r28
				ctx.r4.u64 = var_r28;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// stwx r26,r27,r11
				PPC_STORE_U32(var_r27 + ctx.r11.u32, var_r26);
				// lwz r11,12(r30)
				ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 12);
				// stfsx f30,r27,r11
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(var_f30);
				PPC_STORE_U32(var_r27 + ctx.r11.u32, temp.u32);
				// lwz r11,224(r5)
				// bctrl
				phArticulatedCollider_vfn_56(ctx, base);  // vtable slot 56 (byte +224)  // phArticulatedCollider::vfn_56
				// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
				// fmr f31,f1
				ctx.fpscr.disableFlushMode();
				var_f31 = ctx.f1.f64;
				// mr r4,r28
				ctx.r4.u64 = var_r28;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r9,228(r10)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 57, ctx, base);  // pattern-B slot 57 (byte +228)
				// fadds f0,f1,f31
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = double(float(ctx.f1.f64 + var_f31));
				// lwz r11,4(r30)
				ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
				// li r29,0
				var_r29 = 0;
				// fneg f13,f0
				ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
				// stfsx f13,r27,r11
				temp.f32 = float(ctx.f13.f64);
				PPC_STORE_U32(var_r27 + ctx.r11.u32, temp.u32);
				// lwz r8,472(r31)
				ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 472);
				// cmpwi cr6,r8,0
				// ble cr6,0x8224eda0
			while ((int32_t)var_r29 < ctx.r3.s32) {
				loc_8224ED60:
					// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
					// mr r4,r29
					ctx.r4.u64 = var_r29;
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// lwz r6,200(r7)
  // [ph4a] slot load collapsed
					// bctrl
					VCALL(var_r31, 50, ctx, base);  // pattern-B slot 50 (byte +200)
					// lwz r11,36(r30)
					ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 36);
					// mullw r11,r11,r29
					ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t((int32_t)var_r29);
					// add r5,r11,r28
					ctx.r5.u64 = ctx.r11.u64 + var_r28;
					// lwz r11,0(r30)
					ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
					// addi r29,r29,1
					var_r29 = (uint32_t)(var_r29 + 1);
					// rlwinm r4,r5,2,0,29
					ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
					// stfsx f1,r4,r11
					ctx.fpscr.disableFlushMode();
					temp.f32 = float(ctx.f1.f64);
					PPC_STORE_U32(ctx.r4.u32 + ctx.r11.u32, temp.u32);
					// lwz r3,472(r31)
					ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 472);
					// cmpw cr6,r29,r3
					// blt cr6,0x8224ed60
			}
			loc_8224EDA0:
				// lwz r11,472(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 472);
				// addi r28,r28,1
				var_r28 = (uint32_t)(var_r28 + 1);
				// addi r27,r27,4
				var_r27 = (uint32_t)(var_r27 + 4);
				// cmpw cr6,r28,r11
				// blt cr6,0x8224ece4
				if ((int32_t)var_r28 < ctx.r11.s32) goto loc_8224ECE4;
			}
		loc_8224EDB4:
			// lbz r11,44(r30)
			ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 44);
			// li r5,32
			ctx.r5.s64 = 32;
			// lfs f1,-332(r25)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r25 + -332);
			ctx.f1.f64 = double(temp.f32);
			// lwz r10,20(r30)
			ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 20);
			// lwz r9,8(r30)
			ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 8);
			// lwz r8,16(r30)
			ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 16);
			// lwz r7,12(r30)
			ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 12);
			// stb r11,103(r1)
			PPC_STORE_U8(ctx.r1.u32 + 103, ctx.r11.u8);
			// lwz r11,28(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 28);
			// stw r5,116(r1)
			PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
			// lwz r6,32(r30)
			ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 32);
			// lwz r5,4(r30)
			ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
			// lwz r4,0(r30)
			ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 0);
			// stw r11,92(r1)
			PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
			// lwz r11,24(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 24);
			// lwz r3,36(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 36);
			// stw r11,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
			// bl 0x823501d8
			phArticulatedCollider_01D8_h(ctx, base);
			// li r11,0
			ctx.r11.s64 = 0;
			// li r29,0
			var_r29 = 0;
			// stb r11,44(r30)
			PPC_STORE_U8(var_r30 + 44, ctx.r11.u8);
			// lwz r4,472(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 472);
			// cmpwi cr6,r4,0
			// ble cr6,0x8224ee64
			if (ctx.r4.s32 <= 0) goto loc_8224EE64;
			// li r28,0
			var_r28 = 0;
		loc_8224EE18:
			// lwz r11,16(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lfsx f0,r28,r11
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r11.u32);
			ctx.f0.f64 = double(temp.f32);
			// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
			// fneg f1,f0
			ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// lwz r10,232(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 58, ctx, base);  // pattern-B slot 58 (byte +232)
			// lwz r9,472(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 472);
			// addi r29,r29,1
			var_r29 = (uint32_t)(var_r29 + 1);
			// addi r28,r28,4
			var_r28 = (uint32_t)(var_r28 + 4);
			// cmpw cr6,r29,r9
			// blt cr6,0x8224ee18
			if ((int32_t)var_r29 < ctx.r9.s32) goto loc_8224EE18;
			// b 0x8224ee64
		} else {
		loc_8224EE54:
			// bne cr6,0x8224ee64
			if (!ctx.cr6.eq) goto loc_8224EE64;
			// lwz r3,464(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
			// fmr f1,f29
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f29;
			// bl 0x82256d30
			phArticulatedCollider_6D30_h(ctx, base);
		}
	}
loc_8224EE64:
	// lis r11,-32248
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// li r8,0
	ctx.r8.s64 = 0;
	// lfs f1,0(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r25 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmr f3,f29
	ctx.f3.f64 = var_f29;
	// lfs f2,-24328(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24328);
	ctx.f2.f64 = double(temp.f32);
	// stb r8,468(r31)
	PPC_STORE_U8(var_r31 + 468, ctx.r8.u8);
	// bl 0x822560f8
	phArticulatedCollider_60F8(ctx, base);
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 16);
	// lis r11,-32163
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r11,11412
	ctx.r6.s64 = ctx.r11.s64 + 11412;
	// lis r11,-32163
	// lwz r3,4(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// addi r5,r11,11384
	ctx.r5.s64 = ctx.r11.s64 + 11384;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,12(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// bl 0x82430978
	SinglesNetworkClient_0978_g(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8228f5c8
	pongCreatureInst_F5C8(ctx, base);
	// lfs f12,100(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 100);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,128(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r9,464(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 464);
	// addi r11,r31,224
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 224;
	// addi r7,r10,129
	ctx.r7.s64 = ctx.r10.s64 + 129;
	// addi r30,r31,240
	var_r30 = (uint32_t)(var_r31 + 240);
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r31,256
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 256;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
	// addi r7,r10,10
	ctx.r7.s64 = ctx.r10.s64 + 10;
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lwzx r10,r6,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	// addi r9,r10,272
	ctx.r9.s64 = ctx.r10.s64 + 272;
	// addi r7,r10,288
	ctx.r7.s64 = ctx.r10.s64 + 288;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v0,v11,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822cb8b0
	phCollider_vfn_13(ctx, base);
	// addi r6,r1,176
	ctx.r6.s64 = ctx.r1.s64 + 176;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// addi r11,r31,272
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 272;
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// lvx128 v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v12,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v11,v9,v10
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v13,v9,v10
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v12,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrglw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v0,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v11,v0,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_36"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_36);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_36) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r10,136(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_37"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_37);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_37) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// lwz r10,152(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 152);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_38"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_38);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_38) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r10,r3,10
	ctx.r10.s64 = ctx.r3.s64 + 10;
	// lwz r11,464(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 464);
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r9,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_F040_g"))) PPC_WEAK_FUNC(phArticulatedCollider_F040_g);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_F040_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// lwz r30,464(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 464));
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82255fe0
	phArticulatedCollider_5FE0(ctx, base);
	// lis r11,-32253
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// fmr f3,f31
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = var_f31;
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32248
	// fdivs f0,f0,f31
	ctx.f0.f64 = double(float(ctx.f0.f64 / var_f31));
	// lfs f13,-24568(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24568);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32248
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f13,-24332(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24332);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// bl 0x822560f8
	phArticulatedCollider_60F8(ctx, base);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// bl 0x82256968
	phBoundCapsule_6968_g(ctx, base);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82256b40
	phArticulatedCollider_6B40_wrh(ctx, base);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82258450
	phArticulatedCollider_8450(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8224f0e0
	phArticulatedCollider_F0E0(ctx, base);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82255d58
	phArticulatedCollider_5D58(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_F0E0"))) PPC_WEAK_FUNC(phArticulatedCollider_F0E0);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_F0E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r11,464(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r10,r3,160
	ctx.r10.s64 = ctx.r3.s64 + 160;
	// addi r8,r3,208
	ctx.r8.s64 = ctx.r3.s64 + 208;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// addi r5,r10,32
	ctx.r5.s64 = ctx.r10.s64 + 32;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r7,40(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// addi r4,r7,192
	ctx.r4.s64 = ctx.r7.s64 + 192;
	// addi r11,r7,144
	ctx.r11.s64 = ctx.r7.s64 + 144;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r11,48
	ctx.r4.s64 = ctx.r11.s64 + 48;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v9,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// vmrghw v8,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v12,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// vmrghw v10,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrglw v11,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v8,v9,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r6,464(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lwz r10,4(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpwi cr6,r10,0
	// blelr cr6
	if (ctx.r10.s32 <= 0) return;
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r6,40
	ctx.r8.s64 = ctx.r6.s64 + 40;
	// lfs f11,-8(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
loc_8224F190:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,192
	ctx.r7.s64 = ctx.r10.s64 + 192;
	// addi r5,r10,304
	ctx.r5.s64 = ctx.r10.s64 + 304;
	// addi r11,r10,320
	ctx.r11.s64 = ctx.r10.s64 + 320;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lvx128 v7,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f13,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f13,f11
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f3,f12,f10
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fmsubs f2,f0,f10,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 - ctx.f5.f64));
	// fmsubs f1,f12,f11,f4
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f11.f64 - ctx.f4.f64));
	// fmsubs f0,f9,f13,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f3.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f8,f0,f6
	ctx.f8.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f8,8(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lwz r3,4(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmpw cr6,r9,r3
	// blt cr6,0x8224f190
	if (ctx.r9.s32 < ctx.r3.s32) goto loc_8224F190;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_59"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_59);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_59) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// b 0x82256d30
	phArticulatedCollider_6D30_h(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_22"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_22);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_22) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// b 0x82255c60
	phArticulatedCollider_5C60_sp(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_23"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_23);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_23) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r24 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f884
	ctx.lr = 0x8224F230;
	__savegprlr_23(ctx, base);
	// stfd f29,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.f29.u64);
	// stfd f30,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f30.u64);
	// stfd f31,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f31.u64);
	// li r12,-144
	ctx.r12.s64 = -144;
	// stvx128 v126,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r12,-128
	ctx.r12.s64 = -128;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	var_r24 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// lwz r11,16(r24)
	ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 16);
	// lwz r28,4(r11)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 4));
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// lwz r9,64(r10)
	// bctrl
	VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8224f84c
	if (ctx.r8.u32 != 0) {
		// lis r11,-32253
		// addi r11,r11,-12024
		ctx.r11.s64 = ctx.r11.s64 + -12024;
		// lfs f30,8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		var_f30 = double(temp.f32);
		// fcmpu cr6,f31,f30
		// beq cr6,0x8224f2a0
		if (var_f31 != var_f30) {
			// lfs f0,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f0.f64 = double(temp.f32);
			// fdivs f29,f0,f31
			var_f29 = double(float(ctx.f0.f64 / var_f31));
			// b 0x8224f2a8
		} else {
		loc_8224F2A0:
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfs f29,-25896(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);  /* glob:lbl_82079AD8 @ 0x82079ad8 */
			var_f29 = double(temp.f32);
		}
	loc_8224F2A8:
		// lwz r7,464(r24)
		ctx.r7.u64 = PPC_LOAD_U32(var_r24 + 464);
		// lwz r11,4(r7)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
		// addi r26,r11,-1
		var_r26 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8207ffff
		// cmpwi cr6,r26,0
		// blt cr6,0x8224f84c
		if ((int32_t)var_r26 < 0) goto loc_8224F84C;
		// lis r11,-32160
		// addi r6,r26,10
		ctx.r6.s64 = (int64_t)(int32_t)var_r26 + 10;
		// addi r23,r11,26448
		var_r23 = (uint32_t)(ctx.r11.s64 + 26448);  // lbl_82606750 @ 0x82606750
		// lis r11,-32160
		// rlwinm r27,r6,2,0,29
		var_r27 = (uint32_t)(__builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC);
		// addi r25,r11,26432
		var_r25 = (uint32_t)(ctx.r11.s64 + 26432);  // lbl_82606740 @ 0x82606740
	loc_8224F2D4:
		// lwz r5,464(r24)
		ctx.r5.u64 = PPC_LOAD_U32(var_r24 + 464);
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lvx128 v126,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// li r4,2
		ctx.r4.s64 = 2;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwzx r29,r27,r5
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r27 + ctx.r5.u32));
		// stvx128 v126,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r31,r29,144
		var_r31 = (uint32_t)(var_r29 + 144);
		// addi r11,r29,272
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 272;
		// addi r10,r31,32
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 32;
		// addi r9,r31,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v13,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v12,v12,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v11,v11,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmrghw v0,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// vmrghw v13,v11,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// vmrghw128 v127,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v127.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// stvx128 v127,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8224e1b0
		phArticulatedCollider_E1B0_h(ctx, base);
		// clrlwi r7,r3,24
		ctx.r7.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8224f394
		if (ctx.r7.u32 != 0) {
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// addi r5,r28,112
			ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 112;
			// stvx128 v127,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f0,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f0.f64 = double(temp.f32);
			// lfs f13,84(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f13.f64 = double(temp.f32);
			// fabs f11,f0
			ctx.f11.u64 = ctx.f0.u64 & ~0x8000000000000000;
			// lfs f12,88(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f12.f64 = double(temp.f32);
			// fabs f10,f13
			ctx.f10.u64 = ctx.f13.u64 & ~0x8000000000000000;
			// fabs f9,f12
			ctx.f9.u64 = ctx.f12.u64 & ~0x8000000000000000;
			// stfs f11,80(r1)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// stfs f10,84(r1)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// stfs f9,88(r1)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lvx128 v13,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// lvx128 v0,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmulfp128 v0,v0,v127
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32)));
			// vmulfp128 v126,v0,v13
			simde_mm_store_ps(ctx.v126.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
			// stvx v0,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r1,80
			ctx.r11.s64 = ctx.r1.s64 + 80;
			// stvx128 v126,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_8224F394:
		// li r4,1
		ctx.r4.s64 = 1;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x8224e1b0
		phArticulatedCollider_E1B0_h(ctx, base);
		// clrlwi r10,r3,24
		ctx.r10.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x8224f3cc
		if (ctx.r10.u32 != 0) {
			// addi r9,r28,96
			ctx.r9.s64 = (int64_t)(int32_t)var_r28 + 96;
			// addi r8,r1,96
			ctx.r8.s64 = ctx.r1.s64 + 96;
			// addi r7,r1,80
			ctx.r7.s64 = ctx.r1.s64 + 80;
			// lvx128 v0,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmulfp128 v0,v0,v127
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32)));
			// vaddfp128 v13,v126,v0
			simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v126.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v0,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_8224F3CC:
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x8224e1b0
		phArticulatedCollider_E1B0_h(ctx, base);
		// clrlwi r6,r3,24
		ctx.r6.u64 = ctx.r3.u32 & 0xFF;
		// lfs f10,120(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
		ctx.f10.f64 = double(temp.f32);
		// lfs f11,116(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
		ctx.f11.f64 = double(temp.f32);
		// cmplwi cr6,r6,0
		// lfs f12,112(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f12.f64 = double(temp.f32);
		// beq cr6,0x8224f46c
		if (ctx.r6.u32 != 0) {
			// addi r5,r28,80
			ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 80;
			// fcmpu cr6,f12,f30
			// addi r4,r1,96
			ctx.r4.s64 = ctx.r1.s64 + 96;
			// lvx128 v12,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v12,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// ble cr6,0x8224f410
			if (ctx.f12.f64 > var_f30) {
				// lfs f0,96(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
				ctx.f0.f64 = double(temp.f32);
				// b 0x8224f418
			} else {
			loc_8224F410:
				// lfs f8,96(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
				ctx.f8.f64 = double(temp.f32);
				// fneg f0,f8
				ctx.f0.u64 = ctx.f8.u64 ^ 0x8000000000000000;
			}
		loc_8224F418:
			// lfs f7,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f7.f64 = double(temp.f32);
			// fcmpu cr6,f11,f30
			// fadds f6,f0,f7
			ctx.f6.f64 = double(float(ctx.f0.f64 + ctx.f7.f64));
			// stfs f6,80(r1)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// ble cr6,0x8224f434
			if (ctx.f11.f64 > var_f30) {
				// lfs f0,100(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
				ctx.f0.f64 = double(temp.f32);
				// b 0x8224f43c
			} else {
			loc_8224F434:
				// lfs f5,100(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
				ctx.f5.f64 = double(temp.f32);
				// fneg f0,f5
				ctx.f0.u64 = ctx.f5.u64 ^ 0x8000000000000000;
			}
		loc_8224F43C:
			// lfs f4,84(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f4.f64 = double(temp.f32);
			// fcmpu cr6,f10,f30
			// fadds f3,f0,f4
			ctx.f3.f64 = double(float(ctx.f0.f64 + ctx.f4.f64));
			// stfs f3,84(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// ble cr6,0x8224f458
			if (ctx.f10.f64 > var_f30) {
				// lfs f0,104(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
				ctx.f0.f64 = double(temp.f32);
				// b 0x8224f460
			} else {
			loc_8224F458:
				// lfs f2,104(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
				ctx.f2.f64 = double(temp.f32);
				// fneg f0,f2
				ctx.f0.u64 = ctx.f2.u64 ^ 0x8000000000000000;
			}
		loc_8224F460:
			// lfs f1,88(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f1.f64 = double(temp.f32);
			// fadds f0,f0,f1
			ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f1.f64));
			// stfs f0,88(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		}
	loc_8224F46C:
		// lfs f13,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// fabs f0,f13
		ctx.f0.u64 = ctx.f13.u64 & ~0x8000000000000000;
		// fabs f13,f12
		ctx.f13.u64 = ctx.f12.u64 & ~0x8000000000000000;
		// fmuls f9,f0,f31
		ctx.f9.f64 = double(float(ctx.f0.f64 * var_f31));
		// fcmpu cr6,f9,f13
		// ble cr6,0x8224f48c
		if (ctx.f9.f64 > ctx.f13.f64) {
			// fmuls f8,f12,f29
			ctx.f8.f64 = double(float(ctx.f12.f64 * var_f29));
			// stfs f8,80(r1)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		}
	loc_8224F48C:
		// lfs f7,84(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f7.f64 = double(temp.f32);
		// fabs f13,f11
		ctx.f13.u64 = ctx.f11.u64 & ~0x8000000000000000;
		// fabs f0,f7
		ctx.f0.u64 = ctx.f7.u64 & ~0x8000000000000000;
		// fmuls f6,f0,f31
		ctx.f6.f64 = double(float(ctx.f0.f64 * var_f31));
		// fcmpu cr6,f6,f13
		// ble cr6,0x8224f4ac
		if (ctx.f6.f64 > ctx.f13.f64) {
			// fmuls f5,f11,f29
			ctx.f5.f64 = double(float(ctx.f11.f64 * var_f29));
			// stfs f5,84(r1)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		}
	loc_8224F4AC:
		// lfs f4,88(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f4.f64 = double(temp.f32);
		// fabs f13,f10
		ctx.f13.u64 = ctx.f10.u64 & ~0x8000000000000000;
		// fabs f0,f4
		ctx.f0.u64 = ctx.f4.u64 & ~0x8000000000000000;
		// fmuls f3,f0,f31
		ctx.f3.f64 = double(float(ctx.f0.f64 * var_f31));
		// fcmpu cr6,f3,f13
		// ble cr6,0x8224f4cc
		if (ctx.f3.f64 > ctx.f13.f64) {
			// fmuls f2,f10,f29
			ctx.f2.f64 = double(float(ctx.f10.f64 * var_f29));
			// stfs f2,88(r1)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		}
	loc_8224F4CC:
		// lfs f1,112(r29)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r29 + 112);
		ctx.f1.f64 = double(temp.f32);
		// addi r3,r31,32
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 32;
		// fneg f0,f1
		ctx.f0.u64 = ctx.f1.u64 ^ 0x8000000000000000;
		// stfs f0,128(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r31,48
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 48;
		// addi r10,r31,16
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lvx128 v13,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r29,192
		ctx.r5.s64 = (int64_t)(int32_t)var_r29 + 192;
		// vmrghw v9,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// addi r3,r1,144
		ctx.r3.s64 = ctx.r1.s64 + 144;
		// lvx128 v7,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrglw v13,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r29,352
		var_r30 = (uint32_t)(var_r29 + 352);
		// lvx128 v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r29,336
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 336;
		// vmrghw v8,v11,v12
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// vmrglw v12,v11,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// addi r10,r29,288
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 288;
		// addi r7,r31,16
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lfs f8,0(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 0);
		ctx.f8.f64 = double(temp.f32);
		// vmrghw v11,v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// vmrghw v13,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// addi r9,r1,128
		ctx.r9.s64 = ctx.r1.s64 + 128;
		// addi r8,r1,128
		ctx.r8.s64 = ctx.r1.s64 + 128;
		// lvx128 v10,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,160
		ctx.r9.s64 = ctx.r1.s64 + 160;
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vmulfp128 v0,v7,v10
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stvx v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrglw v10,v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// addi r8,r31,32
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 32;
		// vmsum3fp128 v12,v0,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v0,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// vmsum3fp128 v11,v0,v10
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// vxor128 v126,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_setzero_si128());
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// stvx128 v126,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v13,v12,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
		// vmrghw v12,v11,v13
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// vmrghw v13,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// lvx128 v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,148(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,152(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,144(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
		ctx.f12.f64 = double(temp.f32);
		// lfs f11,168(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,164(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f6,f12,f11
		ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// lfs f9,160(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f7,f10,f13
		ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f5,f9,f0
		ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f3,f9,f13,f6
		ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f6.f64));
		// lfs f13,8(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fmsubs f4,f11,f0,f7
		ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f7.f64));
		// lfs f0,4(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 4);
		ctx.f0.f64 = double(temp.f32);
		// fmsubs f2,f12,f10,f5
		ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f5.f64));
		// fadds f12,f3,f0
		ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f0.f64));
		// stfs f12,4(r30)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r30 + 4, temp.u32);
		// fadds f1,f4,f8
		ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
		// stfs f1,0(r30)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r30 + 0, temp.u32);
		// fadds f11,f2,f13
		ctx.f11.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
		// stfs f11,8(r30)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r30 + 8, temp.u32);
		// lvx128 v6,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v5,v6,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v5,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v13,v0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v12,v12,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v11,v11,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmrghw v0,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// vmrghw v13,v11,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// li r4,5
		ctx.r4.s64 = 5;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// vmrghw128 v127,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v127.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// stvx128 v127,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8224e1b0
		phArticulatedCollider_E1B0_h(ctx, base);
		// clrlwi r4,r3,24
		ctx.r4.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r4,0
		// beq cr6,0x8224f68c
		if (ctx.r4.u32 != 0) {
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// addi r11,r28,160
			ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 160;
			// stvx128 v127,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f10,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f10.f64 = double(temp.f32);
			// lfs f9,84(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f9.f64 = double(temp.f32);
			// fabs f7,f10
			ctx.f7.u64 = ctx.f10.u64 & ~0x8000000000000000;
			// lfs f8,88(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f8.f64 = double(temp.f32);
			// fabs f6,f9
			ctx.f6.u64 = ctx.f9.u64 & ~0x8000000000000000;
			// fabs f5,f8
			ctx.f5.u64 = ctx.f8.u64 & ~0x8000000000000000;
			// stfs f7,80(r1)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// stfs f6,84(r1)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// stfs f5,88(r1)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
			// addi r10,r1,80
			ctx.r10.s64 = ctx.r1.s64 + 80;
			// lvx128 v13,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,80
			ctx.r9.s64 = ctx.r1.s64 + 80;
			// lvx128 v4,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmulfp128 v0,v4,v127
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v127.f32)));
			// vmulfp128 v126,v0,v13
			simde_mm_store_ps(ctx.v126.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
			// stvx v0,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,80
			ctx.r8.s64 = ctx.r1.s64 + 80;
			// stvx128 v126,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_8224F68C:
		// li r4,4
		ctx.r4.s64 = 4;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x8224e1b0
		phArticulatedCollider_E1B0_h(ctx, base);
		// clrlwi r7,r3,24
		ctx.r7.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8224f6c4
		if (ctx.r7.u32 != 0) {
			// addi r6,r28,144
			ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 144;
			// addi r5,r1,96
			ctx.r5.s64 = ctx.r1.s64 + 96;
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lvx128 v0,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmulfp128 v0,v0,v127
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32)));
			// vaddfp128 v3,v126,v0
			simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v126.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v0,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v3,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_8224F6C4:
		// li r4,3
		ctx.r4.s64 = 3;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// bl 0x8224e1b0
		phArticulatedCollider_E1B0_h(ctx, base);
		// clrlwi r3,r3,24
		ctx.r3.u64 = ctx.r3.u32 & 0xFF;
		// lfs f10,120(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
		ctx.f10.f64 = double(temp.f32);
		// lfs f11,116(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
		ctx.f11.f64 = double(temp.f32);
		// cmplwi cr6,r3,0
		// lfs f12,112(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f12.f64 = double(temp.f32);
		// beq cr6,0x8224f764
		if (ctx.r3.u32 != 0) {
			// addi r11,r28,128
			ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 128;
			// fcmpu cr6,f12,f30
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// lvx128 v2,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v2,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// ble cr6,0x8224f708
			if (ctx.f12.f64 > var_f30) {
				// lfs f0,96(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
				ctx.f0.f64 = double(temp.f32);
				// b 0x8224f710
			} else {
			loc_8224F708:
				// lfs f4,96(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
				ctx.f4.f64 = double(temp.f32);
				// fneg f0,f4
				ctx.f0.u64 = ctx.f4.u64 ^ 0x8000000000000000;
			}
		loc_8224F710:
			// lfs f3,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f3.f64 = double(temp.f32);
			// fcmpu cr6,f11,f30
			// fadds f2,f0,f3
			ctx.f2.f64 = double(float(ctx.f0.f64 + ctx.f3.f64));
			// stfs f2,80(r1)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// ble cr6,0x8224f72c
			if (ctx.f11.f64 > var_f30) {
				// lfs f0,100(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
				ctx.f0.f64 = double(temp.f32);
				// b 0x8224f734
			} else {
			loc_8224F72C:
				// lfs f1,100(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
				ctx.f1.f64 = double(temp.f32);
				// fneg f0,f1
				ctx.f0.u64 = ctx.f1.u64 ^ 0x8000000000000000;
			}
		loc_8224F734:
			// lfs f13,84(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f10,f30
			// fadds f9,f0,f13
			ctx.f9.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
			// stfs f9,84(r1)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// ble cr6,0x8224f750
			if (ctx.f10.f64 > var_f30) {
				// lfs f0,104(r1)
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
				ctx.f0.f64 = double(temp.f32);
				// b 0x8224f758
			} else {
			loc_8224F750:
				// lfs f8,104(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
				ctx.f8.f64 = double(temp.f32);
				// fneg f0,f8
				ctx.f0.u64 = ctx.f8.u64 ^ 0x8000000000000000;
			}
		loc_8224F758:
			// lfs f7,88(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f7.f64 = double(temp.f32);
			// fadds f6,f0,f7
			ctx.f6.f64 = double(float(ctx.f0.f64 + ctx.f7.f64));
			// stfs f6,88(r1)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		}
	loc_8224F764:
		// lfs f5,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f5.f64 = double(temp.f32);
		// fabs f13,f12
		ctx.f13.u64 = ctx.f12.u64 & ~0x8000000000000000;
		// fabs f0,f5
		ctx.f0.u64 = ctx.f5.u64 & ~0x8000000000000000;
		// fmuls f4,f0,f31
		ctx.f4.f64 = double(float(ctx.f0.f64 * var_f31));
		// fcmpu cr6,f4,f13
		// ble cr6,0x8224f784
		if (ctx.f4.f64 > ctx.f13.f64) {
			// fmuls f3,f12,f29
			ctx.f3.f64 = double(float(ctx.f12.f64 * var_f29));
			// stfs f3,80(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		}
	loc_8224F784:
		// lfs f2,84(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f2.f64 = double(temp.f32);
		// fabs f13,f11
		ctx.f13.u64 = ctx.f11.u64 & ~0x8000000000000000;
		// fabs f0,f2
		ctx.f0.u64 = ctx.f2.u64 & ~0x8000000000000000;
		// fmuls f1,f0,f31
		ctx.f1.f64 = double(float(ctx.f0.f64 * var_f31));
		// fcmpu cr6,f1,f13
		// ble cr6,0x8224f7a4
		if (ctx.f1.f64 > ctx.f13.f64) {
			// fmuls f0,f11,f29
			ctx.f0.f64 = double(float(ctx.f11.f64 * var_f29));
			// stfs f0,84(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		}
	loc_8224F7A4:
		// lfs f13,88(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f13.f64 = double(temp.f32);
		// fabs f0,f13
		ctx.f0.u64 = ctx.f13.u64 & ~0x8000000000000000;
		// fabs f13,f10
		ctx.f13.u64 = ctx.f10.u64 & ~0x8000000000000000;
		// fmuls f12,f0,f31
		ctx.f12.f64 = double(float(ctx.f0.f64 * var_f31));
		// fcmpu cr6,f12,f13
		// ble cr6,0x8224f7c4
		if (ctx.f12.f64 > ctx.f13.f64) {
			// fmuls f11,f10,f29
			ctx.f11.f64 = double(float(ctx.f10.f64 * var_f29));
			// stfs f11,88(r1)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		}
	loc_8224F7C4:
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r23
		ea = (var_r23) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r29,128
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 128;
		// lvx128 v31,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r31,32
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 32;
		// addi r6,r31,48
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
		// addi r5,r31,16
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lvx128 v1,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r26,r26,-1
		var_r26 = (uint32_t)(var_r26 + -1);
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r27,-4
		var_r27 = (uint32_t)(var_r27 + -4);
		// vmulfp128 v9,v1,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v13.f32)));
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// cmpwi cr6,r26,0
		ctx.cr6.compare<int32_t>((int32_t)var_r26, 0, ctx.xer);
		// lvx128 v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v8,v10,v11
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vsubfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmrghw v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// vmrglw v13,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// vmrglw v12,v10,v11
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vmrghw v11,v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// vmrglw v10,v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// vmrghw v13,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// vmsum3fp128 v12,v0,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// vmsum3fp128 v11,v0,v10
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// vmsum3fp128 v0,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// vmrghw v0,v12,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
		// vmrghw v13,v11,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// vmrghw v0,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vaddfp v30,v31,v0
		simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v30,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bge cr6,0x8224f2d4
		if (!ctx.cr6.lt) goto loc_8224F2D4;
	}
loc_8224F84C:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// li r0,-144
	ctx.r0.s64 = -144;
	// lvx128 v126,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r0,-128
	ctx.r0.s64 = -128;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfd f29,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f30,-96(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8242f8d4
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_40"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_40);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=112, manual
	// lfs f0,100(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,224
	ctx.r11.s64 = ctx.r3.s64 + 224;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r3,256
	ctx.r10.s64 = ctx.r3.s64 + 256;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v0,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82258a30
	phArticulatedCollider_8A30(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_41"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_41);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_41) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r31,256
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 256;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822ccdf0
	phCollider_CDF0_p39(ctx, base);
	// addi r4,r31,224
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 224;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82258a30
	phArticulatedCollider_8A30(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_42"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_42);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_42) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x822ccd28
	phCollider_vfn_42(ctx, base);
	// addi r4,r31,240
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 240;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82258b10
	phArticulatedCollider_8B10(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_43"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_43);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_43) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r31,272
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 272;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822cce58
	game_CE58(ctx, base);
	// addi r4,r31,240
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 240;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82258b10
	phArticulatedCollider_8B10(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_34"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_34);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_34) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x8224F998;
	__savegprlr_29(ctx, base);
	// li r12,-48
	ctx.r12.s64 = -48;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r11,r31,208
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 208;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v127,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v127.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lwz r10,464(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 464);
	// addi r8,r3,10
	ctx.r8.s64 = ctx.r3.s64 + 10;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(var_r29, 0, ctx.xer);
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx128 v127,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r10,r7,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// addi r11,r10,384
	ctx.r11.s64 = ctx.r10.s64 + 384;
	// addi r10,r10,368
	ctx.r10.s64 = ctx.r10.s64 + 368;
	// lfs f2,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f12,f11
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f13
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f4,f9,f13,f7
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f7.f64));
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f5,f11,f0,f8
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f8.f64));
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f3,f12,f10,f6
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f6.f64));
	// fadds f12,f4,f0
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f0.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f1,f5,f2
	ctx.f1.f64 = double(float(ctx.f5.f64 + ctx.f2.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f3,f13
	ctx.f11.f64 = double(float(ctx.f3.f64 + ctx.f13.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x8224fa84
	if (!(ctx.cr6.eq)) {
		// lwz r3,0(r31)
  // [ph4a] vtable load collapsed
		// lis r11,-32160
		// li r6,0
		ctx.r6.s64 = 0;
		// addi r5,r11,26432
		ctx.r5.s64 = ctx.r11.s64 + 26432;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// lwz r11,96(r3)
  // [ph4a] slot load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bctrl
		VCALL(var_r31, 24, ctx, base);  // pattern-B slot 24 (byte +96)
	}
loc_8224FA84:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// li r0,-48
	ctx.r0.s64 = -48;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_31"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_31);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_31) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f898
	ctx.lr = 0x8224FAA0;
	__savegprlr_28(ctx, base);
	// li r12,-64
	ctx.r12.s64 = -64;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 16);
	// lwz r10,64(r11)
	// bctrl
	VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
	// addi r9,r31,208
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 208;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v127,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v127.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lwz r8,464(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 464);
	// addi r6,r3,10
	ctx.r6.s64 = ctx.r3.s64 + 10;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx128 v127,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r10,r5,r8
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r8.u32);
	// addi r11,r10,352
	ctx.r11.s64 = ctx.r10.s64 + 352;
	// addi r10,r10,336
	ctx.r10.s64 = ctx.r10.s64 + 336;
	// lfs f2,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f12,f11
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f13
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f4,f9,f13,f7
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f7.f64));
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f5,f11,f0,f8
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f8.f64));
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f3,f12,f10,f6
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f6.f64));
	// fadds f12,f4,f0
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f0.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f1,f5,f2
	ctx.f1.f64 = double(float(ctx.f5.f64 + ctx.f2.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f3,f13
	ctx.f11.f64 = double(float(ctx.f3.f64 + ctx.f13.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_32"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_32);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_32) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lis r11,-32160
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r29,r11,26432
	var_r29 = (uint32_t)(ctx.r11.s64 + 26432);  // lbl_82606740 @ 0x82606740
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,16(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 16);
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// lwz r10,64(r11)
	// bctrl
	VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,464(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 464);
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,40(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// addi r11,r10,352
	ctx.r11.s64 = ctx.r10.s64 + 352;
	// addi r10,r10,336
	ctx.r10.s64 = ctx.r10.s64 + 336;
	// lfs f5,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f12,f11
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f13
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f3,f9,f13,f7
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f7.f64));
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f4,f11,f0,f8
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f8.f64));
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f2,f12,f10,f6
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f6.f64));
	// fadds f12,f3,f0
	ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f0.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f1,f4,f5
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f2,f13
	ctx.f11.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_35"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_35);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_35) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lis r11,-32160
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r29,r11,26432
	var_r29 = (uint32_t)(ctx.r11.s64 + 26432);  // lbl_82606740 @ 0x82606740
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,16(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 16);
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// lwz r10,60(r11)
	// bctrl
	VCALL(ctx.r3.u32, 15, ctx, base);  // vtable slot 15 (byte +60)
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,464(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 464);
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,40(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// addi r11,r10,384
	ctx.r11.s64 = ctx.r10.s64 + 384;
	// addi r10,r10,368
	ctx.r10.s64 = ctx.r10.s64 + 368;
	// lfs f5,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f12,f11
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f10,f13
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f3,f9,f13,f7
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f7.f64));
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f4,f11,f0,f8
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f8.f64));
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f2,f12,f10,f6
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f6.f64));
	// fadds f12,f3,f0
	ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f0.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f1,f4,f5
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f2,f13
	ctx.f11.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_33"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_33);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_33) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r11,464(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,40(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// addi r11,r11,352
	ctx.r11.s64 = ctx.r11.s64 + 352;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_25"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_25);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_25) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// b 0x82256c28
	phBoundCapsule_6C28_fw(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_27"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_27);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// lis r11,-32253
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f13,100(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fmuls f12,f13,f1
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,128(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 128);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_28"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_28);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,464(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r9,132(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	// lwz r11,40(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r6,r11,48
	ctx.r6.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v13,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,40(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f0,128(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f1
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v12,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_29"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_29);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,464(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r9,132(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	// lwz r11,40(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r6,r11,48
	ctx.r6.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrglw v13,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,40(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f0,136(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f1
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v12,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_30"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_30);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,464(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r9,132(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	// lwz r11,40(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r6,r11,48
	ctx.r6.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v13,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,40(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f0,132(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f1
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v12,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_12"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_12);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82256518
	phArticulatedCollider_6518(ctx, base);
	// lwz r9,464(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 464);
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r10,r9,40
	ctx.r10.s64 = ctx.r9.s64 + 40;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// addi r9,r9,1088
	ctx.r9.s64 = ctx.r9.s64 + 1088;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v0,v0,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// ble cr6,0x82250018
while (ctx.cr6.lt) {
	loc_8224FFCC:
		// lwz r9,0(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r7,r9,288
		ctx.r7.s64 = ctx.r9.s64 + 288;
		// addi r9,r9,1072
		ctx.r9.s64 = ctx.r9.s64 + 1072;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpw cr6,r11,r8
		ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// vmsum3fp128 v12,v0,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v11,v13,v13
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// stvx v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// fadds f12,f13,f0
		ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
		// lfs f11,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f11.f64 = double(temp.f32);
		// fadds f0,f12,f11
		ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
		// blt cr6,0x8224ffcc
}
loc_82250018:
	// addi r5,r8,-1
	ctx.r5.s64 = ctx.r8.s64 + -1;
	// extsw r4,r5
	ctx.r4.s64 = ctx.r5.s32;
	// std r4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
	// lfd f10,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fmuls f7,f8,f31
	ctx.f7.f64 = double(float(ctx.f8.f64 * var_f31));
	// fdivs f1,f0,f7
	ctx.f1.f64 = double(float(ctx.f0.f64 / ctx.f7.f64));
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__game_vtFD90_63"))) PPC_WEAK_FUNC(game_vtFD90_63);
PPC_FUNC_IMPL(__imp__game_vtFD90_63) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// FRAME: size=112, manual
	// addi r11,r3,208
	ctx.r11.s64 = ctx.r3.s64 + 208;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82258538
	phJoint3Dof_8538_w(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__game_vtFD8C_63"))) PPC_WEAK_FUNC(game_vtFD8C_63);
PPC_FUNC_IMPL(__imp__game_vtFD8C_63) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// FRAME: size=128, manual
	// addi r11,r3,208
	ctx.r11.s64 = ctx.r3.s64 + 208;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lwz r3,464(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vsubfp v10,v12,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82258788
	phArticulatedCollider_8788_w(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_13"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_13);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_13) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r7,r3,10
	ctx.r7.s64 = ctx.r3.s64 + 10;
	// lwz r11,464(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 464);
	// addi r10,r31,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r31,32
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 32;
	// addi r8,r31,48
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 48;
	// lwzx r11,r6,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// addi r11,r11,720
	ctx.r11.s64 = ctx.r11.s64 + 720;
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// addi r4,r11,32
	ctx.r4.s64 = ctx.r11.s64 + 32;
	// addi r3,r11,48
	ctx.r3.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_14"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_14);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r7,r3,10
	ctx.r7.s64 = ctx.r3.s64 + 10;
	// lwz r11,464(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 464);
	// addi r10,r31,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r31,32
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 32;
	// addi r8,r31,48
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 48;
	// lwzx r11,r6,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// addi r11,r11,784
	ctx.r11.s64 = ctx.r11.s64 + 784;
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// addi r4,r11,32
	ctx.r4.s64 = ctx.r11.s64 + 32;
	// addi r3,r11,48
	ctx.r3.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_15"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_15);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_15) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=128, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// cmplwi cr6,r6,0
	// beq cr6,0x8225022c
	if (ctx.r6.u32 != 0) {
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// b 0x8225023c
	} else {
	loc_8225022C:
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r8,r7
		ctx.r8.u64 = ctx.r7.u64;
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_8225023C:
	// addi r9,r31,208
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 208;
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// bl 0x82257a30
	phJoint_7A30_fw(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_18"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_18);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// li r12,-48
	ctx.r12.s64 = -48;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r30,208
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 208;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v127,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v127.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r10,464(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 464);
	// addi r8,r3,10
	ctx.r8.s64 = ctx.r3.s64 + 10;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx128 v127,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r7,r10
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// addi r6,r11,320
	ctx.r6.s64 = ctx.r11.s64 + 320;
	// addi r5,r11,304
	ctx.r5.s64 = ctx.r11.s64 + 304;
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f6.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f9,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 4, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// li r0,-48
	ctx.r0.s64 = -48;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_19"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_19);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_19) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r10,r30,208
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 208;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lvx128 v13,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,464(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 464);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r8,r11,10
	ctx.r8.s64 = ctx.r11.s64 + 10;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// vsubfp v0,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwzx r11,r7,r9
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// addi r11,r11,1072
	ctx.r11.s64 = ctx.r11.s64 + 1072;
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(var_r31 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f11,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 4, temp.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_20"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_20);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r10,464(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 464);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r9,r11,10
	ctx.r9.s64 = ctx.r11.s64 + 10;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r8,r10
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// addi r7,r11,1072
	ctx.r7.s64 = ctx.r11.s64 + 1072;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_39"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_39);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_39) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r10,0
	ctx.r10.s64 = 0;
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,464(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r9,0
	// blelr cr6
	if (ctx.r9.s32 <= 0) return;
	// li r9,40
	ctx.r9.s64 = 40;
loc_822504CC:
	// stfs f1,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// lwzx r11,r9,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r8,r11,336
	ctx.r8.s64 = ctx.r11.s64 + 336;
	// addi r7,r11,368
	ctx.r7.s64 = ctx.r11.s64 + 368;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v0,v13,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v0,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,464(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpw cr6,r10,r8
	// blt cr6,0x822504cc
	if (ctx.r10.s32 < ctx.r8.s32) goto loc_822504CC;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_47"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_47);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_47) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r8,508(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 508);
	// lwz r7,500(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 500);
	// lwz r6,492(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 492);
	// lwz r5,484(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 484);
	// lwz r4,476(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 476);
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// stb r11,468(r31)
	PPC_STORE_U8(var_r31 + 468, ctx.r11.u8);
	// bl 0x82258c98
	phArticulatedCollider_8C98_wrh(ctx, base);
	// stw r3,472(r31)
	PPC_STORE_U32(var_r31 + 472, ctx.r3.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_50"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_50);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_50) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,476(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 476);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,464(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwz r9,484(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// lwzx r5,r10,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// addi r8,r5,42
	ctx.r8.s64 = ctx.r5.s64 + 42;
	// lwzx r6,r9,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r7,r4
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_17"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_17);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_17) {
	PPC_FUNC_PROLOGUE();
	// lwz r8,484(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,476(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 476);
	// lwz r9,464(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwzx r4,r8,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// addi r7,r11,42
	ctx.r7.s64 = ctx.r11.s64 + 42;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r6,r9
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_51"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_51);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_51) {
	PPC_FUNC_PROLOGUE();
	// lwz r9,476(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 476);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,464(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwz r10,484(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// lwzx r5,r9,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// addi r8,r5,42
	ctx.r8.s64 = ctx.r5.s64 + 42;
	// lwzx r6,r10,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r7,r4
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r4.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_48"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_48);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_48) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// li r12,-48
	ctx.r12.s64 = -48;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// addi r11,r30,208
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 208;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v127,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v127.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r10,464(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 464);
	// addi r8,r3,10
	ctx.r8.s64 = ctx.r3.s64 + 10;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx128 v127,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r7,r10
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r10,r6,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f6.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f9,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 4, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// li r0,-48
	ctx.r0.s64 = -48;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_49"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_49);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_49) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r10,r3,10
	ctx.r10.s64 = ctx.r3.s64 + 10;
	// lwz r11,464(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 464);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r9,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r10,r8,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_54"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_54);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_54) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// addi r9,r3,10
	ctx.r9.s64 = ctx.r3.s64 + 10;
	// lwz r10,464(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 464);
	// lis r11,-32253
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// lwzx r11,r8,r10
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// lfs f13,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,80(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,84(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,52(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,88(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,0(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r31 + 0, temp.u32);
	// stfs f13,4(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 4, temp.u32);
	// stfs f12,8(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// stfs f11,24(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// stfs f10,20(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// stfs f9,16(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// stfs f8,40(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f7,36(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f6,32(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 32, temp.u32);
	// stfs f0,48(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	// stfs f0,52(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 52, temp.u32);
	// stfs f0,56(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 56, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_52"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_52);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_52) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r11,r31,208
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 208;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// bl 0x822577f0
	phArticulatedCollider_77F0_w(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_53"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_53);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_53) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8224e668
	phArticulatedCollider_E668(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 464);
	// bl 0x82257918
	phArticulatedCollider_7918_w(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_55"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_55);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_55) {
	PPC_FUNC_PROLOGUE();
	// lwz r9,476(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 476);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// lwz r10,484(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// lwz r4,464(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwzx r5,r9,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// lwzx r6,r10,r11
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// addi r8,r5,42
	ctx.r8.s64 = ctx.r5.s64 + 42;
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r3,r4
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r4.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_57"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_57);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_57) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r10,-32164
	// lwz r9,500(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 500);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,492(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 492);
	// lfs f0,22580(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22580);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32253
	// lfsx f12,r9,r11
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfsx f10,r8,r11
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,-11896(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -11896);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmadds f1,f10,f0,f11
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f11.f64));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_vfn_58"))) PPC_WEAK_FUNC(phArticulatedCollider_vfn_58);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_vfn_58) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,476(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 476);
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,464(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 464);
	// lwz r9,484(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// addi r7,r10,42
	ctx.r7.s64 = ctx.r10.s64 + 42;
	// lwzx r4,r9,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r6,r8
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r8.u32);
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,28(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 28);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__game_vtFD80_63"))) PPC_WEAK_FUNC(game_vtFD80_63);
PPC_FUNC_IMPL(__imp__game_vtFD80_63) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// bl 0x822ccfa0
	game_vt9C48_63(ctx, base);
	// lwz r30,464(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 464));
	// li r31,0
	var_r31 = 0;
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// cmpwi cr6,r11,0
	// ble cr6,0x82250994
	if (ctx.r11.s32 > 0) {
		// addi r4,r30,40
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 40;
	loc_82250978:
		// lwz r3,0(r4)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		// bl 0x82259278
		phJoint1Dof_9278_p42(ctx, base);
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r4,r4,4
		ctx.r4.s64 = ctx.r4.s64 + 4;
		// cmpw cr6,r31,r10
		// blt cr6,0x82250978
		if ((int32_t)var_r31 < ctx.r10.s32) goto loc_82250978;
	}
loc_82250994:
	// blr
	return;
}

__attribute__((alias("__imp__phSimulator_09B0_v12"))) PPC_WEAK_FUNC(phSimulator_09B0_v12);
PPC_FUNC_IMPL(__imp__phSimulator_09B0_v12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r20 = 0;
	double var_f29 = 0.0;
	double var_f27 = 0.0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	double var_f28 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f870
	ctx.lr = 0x822509B8;
	__savegprlr_18(ctx, base);
	// addi r12,r1,-120
	ctx.r12.s64 = ctx.r1.s64 + -120;
	// bl 0x82436614
	__savefpr_27(ctx, base);
	// lis r11,-32253
	// li r8,-1
	// addi r25,r11,-12024
	var_r25 = (uint32_t)(ctx.r11.s64 + -12024);  // lbl_8202D108 @ 0x8202d108
	// addi r30,r3,8
	var_r30 = (uint32_t)(ctx.r3.s64 + 8);
	// li r23,0
	var_r23 = 0;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r31,r8
	var_r31 = ctx.r8.u32;
	// lfs f1,8(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r25 + 8);
	ctx.f1.f64 = double(temp.f32);
	// mr r11,r23
	ctx.r11.u64 = var_r23;
	// fmr f13,f1
	ctx.f13.f64 = ctx.f1.f64;
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// li r22,1
	var_r22 = 1;
	// li r24,2
	var_r24 = 2;
loc_822509F4:
	// lfs f0,-8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fabs f0,f0
	ctx.f0.u64 = ctx.f0.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f0
	// bge cr6,0x82250a10
	if (ctx.f13.f64 < ctx.f0.f64) {
		// fmr f13,f0
		ctx.f13.f64 = ctx.f0.f64;
		// mr r6,r11
		ctx.r6.u64 = ctx.r11.u64;
		// mr r31,r23
		var_r31 = (uint32_t)(var_r23);
	}
loc_82250A10:
	// lfs f12,-4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// fabs f0,f12
	ctx.f0.u64 = ctx.f12.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f0
	// bge cr6,0x82250a2c
	if (ctx.f13.f64 < ctx.f0.f64) {
		// fmr f13,f0
		ctx.f13.f64 = ctx.f0.f64;
		// mr r6,r11
		ctx.r6.u64 = ctx.r11.u64;
		// mr r31,r22
		var_r31 = (uint32_t)(var_r22);
	}
loc_82250A2C:
	// lfs f11,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fabs f0,f11
	ctx.f0.u64 = ctx.f11.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f0
	// bge cr6,0x82250a48
	if (ctx.f13.f64 < ctx.f0.f64) {
		// fmr f13,f0
		ctx.f13.f64 = ctx.f0.f64;
		// mr r6,r11
		ctx.r6.u64 = ctx.r11.u64;
		// mr r31,r24
		var_r31 = (uint32_t)(var_r24);
	}
loc_82250A48:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r11,3
	// blt cr6,0x822509f4
	if (ctx.r11.s32 < 3) goto loc_822509F4;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-24684(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24684);  /* glob:lbl_82079F94 @ 0x82079f94 */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	// blt cr6,0x82251174
	if (ctx.f13.f64 >= ctx.f0.f64) {
		// cmpwi cr6,r6,-1
		// beq cr6,0x82251174
		if (ctx.r6.s32 == -1) goto loc_82251174;
		// cmpwi cr6,r31,-1
		// beq cr6,0x82251174
		if ((int32_t)var_r31 == -1) goto loc_82251174;
		// lis r11,-32248
		// lfs f11,24(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
		ctx.f11.f64 = double(temp.f32);
		// lfs f9,32(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
		ctx.f9.f64 = double(temp.f32);
		// fmr f4,f1
		ctx.f4.f64 = ctx.f1.f64;
		// fmuls f2,f9,f11
		ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
		// lfs f8,0(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phSimulator::vtable@+0x0 */;
		ctx.f8.f64 = double(temp.f32);
		// lfs f10,16(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f29,f8,f9
		var_f29 = double(float(ctx.f8.f64 * ctx.f9.f64));
		// lfs f0,40(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f27,f8,f10
		var_f27 = double(float(ctx.f8.f64 * ctx.f10.f64));
		// lfs f12,-24688(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24688);  /* glob:lbl_82079F90 @ 0x82079f90 */
		ctx.f12.f64 = double(temp.f32);
		// lis r11,-32160
		// fmuls f5,f13,f12
		ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
		// lfs f12,36(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
		ctx.f12.f64 = double(temp.f32);
		// lfs f13,20(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f3,f11,f12
		ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
		// fmuls f31,f9,f13
		var_f31 = double(float(ctx.f9.f64 * ctx.f13.f64));
		// lfs f7,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phSimulator::flags@+0x4 */;
		ctx.f7.f64 = double(temp.f32);
		// fmuls f30,f8,f12
		var_f30 = double(float(ctx.f8.f64 * ctx.f12.f64));
		// addi r11,r11,26432
		ctx.r11.s64 = ctx.r11.s64 + 26432;
		// fmuls f28,f8,f13
		var_f28 = double(float(ctx.f8.f64 * ctx.f13.f64));
		// lfs f6,0(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phSimulator::vtable@+0x0 */;
		ctx.f6.f64 = double(temp.f32);
		// fmuls f9,f7,f9
		ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
		// stw r8,-288(r1)
		PPC_STORE_U32(ctx.r1.u32 + -288, ctx.r8.u32);
		// stw r8,-284(r1)
		PPC_STORE_U32(ctx.r1.u32 + -284, ctx.r8.u32);
		// mr r7,r23
		ctx.r7.u64 = var_r23;
		// stw r8,-280(r1)
		PPC_STORE_U32(ctx.r1.u32 + -280, ctx.r8.u32);
		// addi r9,r1,-216
		ctx.r9.s64 = ctx.r1.s64 + -216;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,-272
		ctx.r11.s64 = ctx.r1.s64 + -272;
		// fmsubs f8,f13,f0,f3
		ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 - ctx.f3.f64));
		// stfs f8,-224(r1)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + -224, temp.u32);
		// fnmsubs f3,f10,f0,f2
		ctx.f3.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f2.f64)));
		// stfs f3,-220(r1)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r1.u32 + -220, temp.u32);
		// fmsubs f2,f10,f12,f31
		ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 - var_f31));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// fmuls f10,f7,f10
		ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
		// stfs f2,-216(r1)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r1.u32 + -216, temp.u32);
		// fmsubs f31,f6,f0,f29
		var_f31 = double(float(ctx.f6.f64 * ctx.f0.f64 - var_f29));
		// stfs f31,-204(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + -204, temp.u32);
		// fnmsubs f31,f6,f11,f27
		var_f31 = double(float(-(ctx.f6.f64 * ctx.f11.f64 - var_f27)));
		// mr r11,r23
		ctx.r11.u64 = var_r23;
		// fnmsubs f0,f7,f0,f30
		ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - var_f30)));
		// stfs f31,-188(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + -188, temp.u32);
		// fnmsubs f12,f6,f12,f9
		ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f12.f64 - ctx.f9.f64)));
		// stfs f0,-208(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + -208, temp.u32);
		// fmsubs f11,f7,f11,f28
		ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 - var_f28));
		// stfs f12,-200(r1)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r1.u32 + -200, temp.u32);
		// stfs f11,-192(r1)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r1.u32 + -192, temp.u32);
		// fmsubs f10,f6,f13,f10
		ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f13.f64 - ctx.f10.f64));
		// stfs f10,-184(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + -184, temp.u32);
	loc_82250B44:
		// lfs f9,-8(r9)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -8);
		ctx.f9.f64 = double(temp.f32);
		// addi r10,r1,-272
		ctx.r10.s64 = ctx.r1.s64 + -272;
		// fabs f0,f9
		ctx.f0.u64 = ctx.f9.u64 & ~0x8000000000000000;
		// lfsx f7,r11,r10
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
		ctx.f7.f64 = double(temp.f32);
		// fcmpu cr6,f7,f0
		// bge cr6,0x82250b68
		if (ctx.f7.f64 < ctx.f0.f64) {
			// addi r29,r1,-288
			var_r29 = (uint32_t)(ctx.r1.s64 + -288);
			// stfsx f0,r11,r10
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
			// stwx r23,r11,r29
			PPC_STORE_U32(ctx.r11.u32 + var_r29, var_r23);
		}
	loc_82250B68:
		// lfs f0,-4(r9)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
		ctx.f0.f64 = double(temp.f32);
		// fabs f0,f0
		ctx.f0.u64 = ctx.f0.u64 & ~0x8000000000000000;
		// lfsx f13,r11,r10
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// bge cr6,0x82250b88
		if (ctx.f13.f64 < ctx.f0.f64) {
			// addi r29,r1,-288
			var_r29 = (uint32_t)(ctx.r1.s64 + -288);
			// stfsx f0,r11,r10
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
			// stwx r22,r11,r29
			PPC_STORE_U32(ctx.r11.u32 + var_r29, var_r22);
		}
	loc_82250B88:
		// lfs f12,0(r9)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		ctx.f12.f64 = double(temp.f32);
		// fabs f0,f12
		ctx.f0.u64 = ctx.f12.u64 & ~0x8000000000000000;
		// lfsx f11,r11,r10
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f11,f0
		// bge cr6,0x82250ba8
		if (ctx.f11.f64 < ctx.f0.f64) {
			// addi r29,r1,-288
			var_r29 = (uint32_t)(ctx.r1.s64 + -288);
			// stfsx f0,r11,r10
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, temp.u32);
			// stwx r24,r11,r29
			PPC_STORE_U32(ctx.r11.u32 + var_r29, var_r24);
		}
	loc_82250BA8:
		// lfsx f0,r11,r10
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f4,f0
		// bge cr6,0x82250bbc
		if (ctx.f4.f64 < ctx.f0.f64) {
			// fmr f4,f0
			ctx.f4.f64 = ctx.f0.f64;
			// mr r8,r7
			ctx.r8.u64 = ctx.r7.u64;
		}
	loc_82250BBC:
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// cmpwi cr6,r11,12
		// blt cr6,0x82250b44
		if (ctx.r11.s32 < 12) goto loc_82250B44;
		// lfs f10,4(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phSimulator::flags@+0x4 */;
		ctx.f10.f64 = double(temp.f32);
		// fmuls f0,f5,f5
		ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f5.f64));
		// fmuls f9,f10,f3
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f3.f64));
		// lfs f7,0(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phSimulator::vtable@+0x0 */;
		ctx.f7.f64 = double(temp.f32);
		// fmuls f13,f4,f5
		ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
		// lis r10,21845
		ctx.r10.s64 = 1431633920;
		// ori r11,r10,21846
		ctx.r11.u64 = ctx.r10.u64 | 21846;
		// fmuls f12,f0,f5
		ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
		// fmadds f3,f7,f2,f9
		ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f2.f64 + ctx.f9.f64));
		// fsubs f2,f12,f13
		ctx.f2.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
		// fmadds f10,f6,f8,f3
		ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 + ctx.f3.f64));
		// fsel f13,f2,f12,f13
		ctx.f13.f64 = ctx.f2.f64 >= 0.0 ? ctx.f12.f64 : ctx.f13.f64;
		// fabs f11,f10
		ctx.f11.u64 = ctx.f10.u64 & ~0x8000000000000000;
		// fcmpu cr6,f11,f13
		// ble cr6,0x82250ce4
		if (ctx.f11.f64 > ctx.f13.f64) {
			// addi r10,r6,1
			ctx.r10.s64 = ctx.r6.s64 + 1;
			// addi r30,r1,-272
			var_r30 = (uint32_t)(ctx.r1.s64 + -272);
			// mulhw r9,r10,r11
			ctx.r9.s64 = (int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32)) >> 32;
			// rlwinm r7,r9,1,31,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
			// add r9,r9,r7
			ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
			// rlwinm r7,r9,1,0,30
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
			// add r9,r9,r7
			ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
			// subf r7,r9,r10
			ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
			// rlwinm r10,r7,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// lfsx f13,r10,r30
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r30);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f13,f0
			// ble cr6,0x82250ce4
			if (ctx.f13.f64 <= ctx.f0.f64) goto loc_82250CE4;
			// addi r10,r6,2
			ctx.r10.s64 = ctx.r6.s64 + 2;
			// addi r30,r1,-272
			var_r30 = (uint32_t)(ctx.r1.s64 + -272);
			// mulhw r9,r10,r11
			ctx.r9.s64 = (int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32)) >> 32;
			// rlwinm r7,r9,1,31,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
			// add r9,r9,r7
			ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
			// rlwinm r7,r9,1,0,30
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
			// add r9,r9,r7
			ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
			// subf r7,r9,r10
			ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
			// rlwinm r10,r7,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// lfsx f12,r10,r30
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r30);
			ctx.f12.f64 = double(temp.f32);
			// fcmpu cr6,f12,f0
			// ble cr6,0x82250ce4
			if (ctx.f12.f64 <= ctx.f0.f64) goto loc_82250CE4;
			// addi r9,r1,-224
			ctx.r9.s64 = ctx.r1.s64 + -224;
			// lvx128 v0,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,-192
			ctx.r8.s64 = ctx.r1.s64 + -192;
			// stfs f10,-240(r1)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(ctx.r1.u32 + -240, temp.u32);
			// addi r7,r1,-208
			ctx.r7.s64 = ctx.r1.s64 + -208;
			// addi r6,r1,-240
			ctx.r6.s64 = ctx.r1.s64 + -240;
			// lis r11,-32163
			// lvx128 v13,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// li r3,1
			ctx.r3.s64 = 1;
			// lvx128 v12,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v13,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// vmsum3fp128 v11,v12,v0
			simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// lvx128 v10,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v10,v10,v0
			simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// lvx128 v9,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v12,v9,0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
			// addi r11,r11,-18496
			ctx.r11.s64 = ctx.r11.s64 + -18496;
			// vrefp v0,v12
			simde_mm_store_ps(ctx.v0.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
			// vmrghw v13,v13,v11
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
			// vmrghw v11,v10,v13
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
			// vmrghw v11,v13,v11
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
			// stvx v11,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v13,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vnmsubfp v13,v0,v12,v13
			simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmaddfp v0,v0,v13,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
			// vmulfp128 v8,v11,v0
			simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v8,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r12,r1,-120
			ctx.r12.s64 = ctx.r1.s64 + -120;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8c0
			__restgprlr_18(ctx, base);
			return;
		}
	loc_82250CE4:
		// rlwinm r10,r6,4,0,27
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
		// fcmpu cr6,f4,f0
		ctx.fpscr.disableFlushMode();
		// addi r9,r1,-256
		ctx.r9.s64 = ctx.r1.s64 + -256;
		// lvx128 v0,r10,r3
		ea = (ctx.r10.u32 + ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// ble cr6,0x82251064
		if (ctx.f4.f64 > ctx.f0.f64) {
			// addi r10,r8,1
			ctx.r10.s64 = ctx.r8.s64 + 1;
			// lvx128 v13,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r8,2
			ctx.r9.s64 = ctx.r8.s64 + 2;
			// vor v6,v13,v13
			simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
			// rlwinm r6,r8,2,0,29
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r8,r8,4,0,27
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
			// addi r31,r1,-224
			var_r31 = (uint32_t)(ctx.r1.s64 + -224);
			// mr r30,r11
			var_r30 = ctx.r11.u32;
			// addi r7,r1,-288
			ctx.r7.s64 = ctx.r1.s64 + -288;
			// mr r28,r11
			var_r28 = ctx.r11.u32;
			// mr r27,r11
			var_r27 = ctx.r11.u32;
			// lvx128 v0,r8,r31
			ea = (ctx.r8.u32 + var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// mulhw r31,r10,r30
			var_r31 = (uint32_t)((int64_t(ctx.r10.s32) * int64_t((int32_t)var_r30)) >> 32);
			// vmsum3fp128 v7,v0,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// vmsum3fp128 v5,v13,v0
			simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// lwzx r7,r6,r7
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
			// mr r29,r11
			var_r29 = ctx.r11.u32;
			// mulhw r30,r9,r28
			var_r30 = (uint32_t)((int64_t(ctx.r9.s32) * int64_t((int32_t)var_r28)) >> 32);
			// addi r11,r1,-256
			ctx.r11.s64 = ctx.r1.s64 + -256;
			// addi r28,r1,-240
			var_r28 = (uint32_t)(ctx.r1.s64 + -240);
			// addi r8,r7,2
			ctx.r8.s64 = ctx.r7.s64 + 2;
			// rlwinm r26,r31,1,31,31
			var_r26 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0x1);
			// add r31,r31,r26
			var_r31 = (uint32_t)(var_r31 + var_r26);
			// stvx v7,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r7,1
			ctx.r11.s64 = ctx.r7.s64 + 1;
			// stvx v5,r0,r28
			ea = (var_r28) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// mulhw r28,r8,r27
			var_r28 = (uint32_t)((int64_t(ctx.r8.s32) * int64_t((int32_t)var_r27)) >> 32);
			// lfs f11,-240(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -240);
			ctx.f11.f64 = double(temp.f32);
			// lfs f10,-256(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -256);
			ctx.f10.f64 = double(temp.f32);
			// fdivs f9,f11,f10
			ctx.f9.f64 = double(float(ctx.f11.f64 / ctx.f10.f64));
			// stfs f9,-240(r1)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r1.u32 + -240, temp.u32);
			// rlwinm r27,r30,1,31,31
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 1) & 0x1);
			// mulhw r29,r11,r29
			var_r29 = (uint32_t)((int64_t(ctx.r11.s32) * int64_t((int32_t)var_r29)) >> 32);
			// add r30,r30,r27
			var_r30 = (uint32_t)(var_r30 + var_r27);
			// rlwinm r27,r29,1,31,31
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 1) & 0x1);
			// rlwinm r26,r31,1,0,30
			var_r26 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0xFFFFFFFE);
			// add r29,r29,r27
			var_r29 = (uint32_t)(var_r29 + var_r27);
			// rlwinm r27,r28,1,31,31
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 1) & 0x1);
			// add r31,r31,r26
			var_r31 = (uint32_t)(var_r31 + var_r26);
			// add r28,r28,r27
			var_r28 = (uint32_t)(var_r28 + var_r27);
			// addi r27,r1,-240
			var_r27 = (uint32_t)(ctx.r1.s64 + -240);
			// subf r31,r31,r10
			var_r31 = (uint32_t)(ctx.r10.s64 - (int64_t)(int32_t)var_r31);
			// rlwinm r10,r29,1,0,30
			ctx.r10.u64 = __builtin_rotateleft64(var_r29 | (var_r29 << 32), 1) & 0xFFFFFFFE;
			// add r10,r29,r10
			ctx.r10.u64 = var_r29 + ctx.r10.u64;
			// lvx128 v4,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// rlwinm r27,r30,1,0,30
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 1) & 0xFFFFFFFE);
			// subf r11,r10,r11
			ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
			// vspltw v13,v4,0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.u32), 0xFF));
			// add r30,r30,r27
			var_r30 = (uint32_t)(var_r30 + var_r27);
			// rlwinm r10,r28,1,0,30
			ctx.r10.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 1) & 0xFFFFFFFE;
			// subf r30,r30,r9
			var_r30 = (uint32_t)(ctx.r9.s64 - (int64_t)(int32_t)var_r30);
			// add r9,r28,r10
			ctx.r9.u64 = var_r28 + ctx.r10.u64;
			// vnmsubfp v6,v0,v13,v6
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v6.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v6.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// subf r10,r9,r8
			ctx.r10.s64 = ctx.r8.s64 - ctx.r9.s64;
			// rlwinm r8,r31,4,0,27
			ctx.r8.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 4) & 0xFFFFFFF0;
			// rlwinm r9,r30,4,0,27
			ctx.r9.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 4) & 0xFFFFFFF0;
			// lvx128 v3,r8,r3
			ea = (ctx.r8.u32 + ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,-256
			ctx.r8.s64 = ctx.r1.s64 + -256;
			// lvx128 v2,r9,r3
			ea = (ctx.r9.u32 + ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,-288
			ctx.r9.s64 = ctx.r1.s64 + -288;
			// stvx v3,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,-240
			ctx.r8.s64 = ctx.r1.s64 + -240;
			// stvx v2,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r11,8
			ctx.r9.s64 = ctx.r11.s64 + 8;
			// rlwinm r29,r9,2,0,29
			var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC);
			// rlwinm r9,r11,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// stvx v6,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r11,4
			ctx.r8.s64 = ctx.r11.s64 + 4;
			// rlwinm r8,r8,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// lfsx f8,r29,r3
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r3.u32);
			ctx.f8.f64 = double(temp.f32);
			// lfsx f6,r9,r3
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
			ctx.f6.f64 = double(temp.f32);
			// stfs f8,-264(r1)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r1.u32 + -264, temp.u32);
			// stfs f6,-272(r1)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r1.u32 + -272, temp.u32);
			// lfsx f7,r8,r3
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r3.u32);
			ctx.f7.f64 = double(temp.f32);
			// addi r8,r10,8
			ctx.r8.s64 = ctx.r10.s64 + 8;
			// stfs f7,-268(r1)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r1.u32 + -268, temp.u32);
			// addi r29,r1,-272
			var_r29 = (uint32_t)(ctx.r1.s64 + -272);
			// rlwinm r28,r8,2,0,29
			var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC);
			// lvx128 v0,r0,r29
			ea = (var_r29) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r29,r10,4
			var_r29 = (uint32_t)(ctx.r10.s64 + 4);  // addr:0x82030004
			// rlwinm r29,r29,2,0,29
			var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC);
			// rlwinm r8,r10,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// lfsx f3,r28,r3
			temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r3.u32);
			ctx.f3.f64 = double(temp.f32);
			// addi r26,r1,-256
			var_r26 = (uint32_t)(ctx.r1.s64 + -256);
			// lfsx f2,r29,r3
			temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r3.u32);
			ctx.f2.f64 = double(temp.f32);
			// addi r21,r1,-288
			var_r21 = (uint32_t)(ctx.r1.s64 + -288);
			// stfs f3,-264(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + -264, temp.u32);
			// addi r27,r1,-256
			var_r27 = (uint32_t)(ctx.r1.s64 + -256);
			// stfs f2,-268(r1)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r1.u32 + -268, temp.u32);
			// addi r18,r1,-240
			var_r18 = (uint32_t)(ctx.r1.s64 + -240);
			// vpermwi128 v12,v0,99
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
			// addi r19,r1,-240
			var_r19 = (uint32_t)(ctx.r1.s64 + -240);
			// lfsx f4,r8,r3
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r3.u32);
			ctx.f4.f64 = double(temp.f32);
			// lfsx f11,r8,r26
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r26);
			ctx.f11.f64 = double(temp.f32);
			// addi r20,r1,-288
			var_r20 = (uint32_t)(ctx.r1.s64 + -288);
			// lfsx f13,r9,r21
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r21);
			ctx.f13.f64 = double(temp.f32);
			// vpermwi128 v11,v0,135
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
			// fmuls f8,f13,f11
			ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
			// lfsx f0,r9,r27
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r27);
			ctx.f0.f64 = double(temp.f32);
			// lfsx f9,r8,r18
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r18);
			ctx.f9.f64 = double(temp.f32);
			// rlwinm r31,r31,2,0,29
			var_r31 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC);
			// lfsx f12,r9,r19
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r19);
			ctx.f12.f64 = double(temp.f32);
			// fmuls f7,f0,f9
			ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
			// fmuls f6,f13,f9
			ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
			// stfs f4,-272(r1)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r1.u32 + -272, temp.u32);
			// lfsx f10,r8,r20
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r20);
			ctx.f10.f64 = double(temp.f32);
			// addi r9,r1,-272
			ctx.r9.s64 = ctx.r1.s64 + -272;
			// stfsx f1,r6,r5
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r6.u32 + ctx.r5.u32, temp.u32);
			// rlwinm r30,r30,2,0,29
			var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC);
			// addi r8,r1,-240
			ctx.r8.s64 = ctx.r1.s64 + -240;
			// addi r6,r1,-256
			ctx.r6.s64 = ctx.r1.s64 + -256;
			// addi r27,r1,-208
			var_r27 = (uint32_t)(ctx.r1.s64 + -208);
			// lvx128 v0,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r28,r1,-192
			var_r28 = (uint32_t)(ctx.r1.s64 + -192);
			// fmsubs f4,f10,f0,f8
			ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 - ctx.f8.f64));
			// vpermwi128 v10,v0,135
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
			// vpermwi128 v9,v0,99
			simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
			// addi r29,r1,-208
			var_r29 = (uint32_t)(ctx.r1.s64 + -208);
			// fmsubs f3,f11,f12,f7
			ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 - ctx.f7.f64));
			// fmsubs f2,f10,f12,f6
			ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 - ctx.f6.f64));
			// lfs f12,0(r25)
			temp.u32 = PPC_LOAD_U32(var_r25 + 0);
			ctx.f12.f64 = double(temp.f32);
			// vmulfp128 v0,v10,v12
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
			// fdivs f0,f12,f4
			ctx.fpscr.disableFlushModeUnconditional();
			ctx.f0.f64 = double(float(ctx.f12.f64 / ctx.f4.f64));
			// vnmsubfp v0,v9,v11,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// fmuls f1,f3,f0
			ctx.fpscr.disableFlushModeUnconditional();
			ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
			// fmuls f0,f2,f0
			ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
			// stfsx f0,r31,r5
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r31 + ctx.r5.u32, temp.u32);
			// addi r31,r1,-224
			var_r31 = (uint32_t)(ctx.r1.s64 + -224);
			// vmsum3fp128 v30,v0,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v30.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// fneg f13,f1
			ctx.fpscr.disableFlushModeUnconditional();
			ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
			// stfsx f13,r30,r5
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(var_r30 + ctx.r5.u32, temp.u32);
			// lvx128 v13,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r30,r1,-192
			var_r30 = (uint32_t)(ctx.r1.s64 + -192);
			// vmsum3fp128 v31,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v31.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// vor v1,v13,v13
			simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
			// stvx v30,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r6,r1,-224
			ctx.r6.s64 = ctx.r1.s64 + -224;
			// lfs f10,-256(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -256);
			ctx.f10.f64 = double(temp.f32);
			// stvx v31,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// rlwinm r8,r10,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// lfs f11,-240(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -240);
			ctx.f11.f64 = double(temp.f32);
			// fdivs f9,f11,f10
			ctx.f9.f64 = double(float(ctx.f11.f64 / ctx.f10.f64));
			// stfs f9,-240(r1)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r1.u32 + -240, temp.u32);
			// addi r9,r1,-240
			ctx.r9.s64 = ctx.r1.s64 + -240;
			// lfsx f6,r8,r29
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r29);
			ctx.f6.f64 = double(temp.f32);
			// lfsx f4,r8,r30
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r30);
			ctx.f4.f64 = double(temp.f32);
			// fabs f11,f6
			ctx.f11.u64 = ctx.f6.u64 & ~0x8000000000000000;
			// lfsx f2,r8,r6
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
			ctx.f2.f64 = double(temp.f32);
			// fabs f10,f4
			ctx.f10.u64 = ctx.f4.u64 & ~0x8000000000000000;
			// lvx128 v29,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// rlwinm r9,r11,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// vspltw v13,v29,0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v29.u32), 0xFF));
			// lfsx f8,r9,r27
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r27);
			ctx.f8.f64 = double(temp.f32);
			// vnmsubfp v1,v0,v13,v1
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v1.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v1.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// lfsx f7,r9,r28
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r28);
			ctx.f7.f64 = double(temp.f32);
			// fabs f0,f8
			ctx.f0.u64 = ctx.f8.u64 & ~0x8000000000000000;
			// fabs f13,f7
			ctx.f13.u64 = ctx.f7.u64 & ~0x8000000000000000;
			// lfsx f3,r9,r31
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r31);
			ctx.f3.f64 = double(temp.f32);
			// fabs f9,f3
			ctx.f9.u64 = ctx.f3.u64 & ~0x8000000000000000;
			// fabs f8,f2
			ctx.f8.u64 = ctx.f2.u64 & ~0x8000000000000000;
			// stvx v1,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fadds f1,f13,f0
			ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// fadds f0,f10,f11
			ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
			// fadds f13,f1,f9
			ctx.f13.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
			// fadds f11,f0,f8
			ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f8.f64));
			// fcmpu cr6,f13,f11
			// bge cr6,0x82250f9c
			if (ctx.f13.f64 < ctx.f11.f64) {
				// mr r11,r10
				ctx.r11.u64 = ctx.r10.u64;
			}
		loc_82250F9C:
			// addi r8,r7,4
			ctx.r8.s64 = ctx.r7.s64 + 4;
			// rlwinm r9,r7,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r6,r8,2,0,29
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r7,r7,8
			ctx.r7.s64 = ctx.r7.s64 + 8;
			// mr r8,r24
			ctx.r8.u64 = var_r24;
			// rlwinm r7,r7,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// lfsx f8,r9,r3
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
			ctx.f8.f64 = double(temp.f32);
			// lfsx f10,r6,r3
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r3.u32);
			ctx.f10.f64 = double(temp.f32);
			// fabs f13,f8
			ctx.f13.u64 = ctx.f8.u64 & ~0x8000000000000000;
			// fabs f0,f10
			ctx.f0.u64 = ctx.f10.u64 & ~0x8000000000000000;
			// lfsx f9,r7,r3
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r3.u32);
			ctx.f9.f64 = double(temp.f32);
			// fabs f11,f9
			ctx.f11.u64 = ctx.f9.u64 & ~0x8000000000000000;
			// fcmpu cr6,f13,f0
			// ble cr6,0x82250fe4
			if (ctx.f13.f64 > ctx.f0.f64) {
				// fcmpu cr6,f13,f11
				// ble cr6,0x82250ff0
				if (ctx.f13.f64 <= ctx.f11.f64) goto loc_82250FF0;
				// mr r8,r23
				ctx.r8.u64 = var_r23;
				// b 0x82250ff0
			} else {
			loc_82250FE4:
				// fcmpu cr6,f0,f11
				ctx.fpscr.disableFlushMode();
				// ble cr6,0x82250ff0
				if (ctx.f0.f64 <= ctx.f11.f64) goto loc_82250FF0;
				// mr r8,r22
				ctx.r8.u64 = var_r22;
			}
		loc_82250FF0:
			// rlwinm r10,r8,4,0,27
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
			// lfsx f7,r9,r4
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r4.u32);
			ctx.f7.f64 = double(temp.f32);
			// rlwinm r11,r11,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// add r10,r10,r3
			ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
			// lis r8,-32248
			// lfsx f3,r11,r4
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r4.u32);
			ctx.f3.f64 = double(temp.f32);
			// lfsx f6,r10,r9
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
			ctx.f6.f64 = double(temp.f32);
			// lfsx f4,r11,r10
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
			ctx.f4.f64 = double(temp.f32);
			// fmuls f13,f3,f6
			ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
			// fmuls f0,f7,f4
			ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
			// lfd f11,-25848(r8)
			ctx.f11.u64 = PPC_LOAD_U64(ctx.r8.u32 + -25848);
			// fsubs f10,f13,f0
			ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
			// fabs f0,f0
			ctx.f0.u64 = ctx.f0.u64 & ~0x8000000000000000;
			// fabs f13,f13
			ctx.f13.u64 = ctx.f13.u64 & ~0x8000000000000000;
			// fabs f10,f10
			ctx.f10.u64 = ctx.f10.u64 & ~0x8000000000000000;
			// fsubs f2,f13,f0
			ctx.f2.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
			// fsel f0,f2,f13,f0
			ctx.f0.f64 = ctx.f2.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
			// fsubs f1,f12,f0
			ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// fsel f0,f1,f11,f0
			ctx.f0.f64 = ctx.f1.f64 >= 0.0 ? ctx.f11.f64 : ctx.f0.f64;
			// fmuls f0,f0,f5
			ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
			// fcmpu cr6,f10,f0
			// bgt cr6,0x822511ec
			if (ctx.f10.f64 > ctx.f0.f64) goto loc_822511EC;
			// mr r11,r22
			ctx.r11.u64 = var_r22;
			// bns cr6,0x822511f0
			// UNIMPLEMENTED: bns
			PPC_UNIMPLEMENTED(0x8225104C, "bns");
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// clrlwi r3,r11,24
			ctx.r3.u64 = ctx.r11.u32 & 0xFF;
			// addi r12,r1,-120
			ctx.r12.s64 = ctx.r1.s64 + -120;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8c0
			__restgprlr_18(ctx, base);
			return;
		}
	loc_82251064:
		// lvx128 v28,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v25,v0,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v25.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v26,v28,v0
		simde_mm_store_ps(ctx.v26.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// addi r6,r31,8
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 8;
		// addi r7,r1,-240
		ctx.r7.s64 = ctx.r1.s64 + -240;
		// vor v27,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// rlwinm r9,r6,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lfs f2,-252(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -252);
		ctx.f2.f64 = double(temp.f32);
		// addi r6,r1,-288
		ctx.r6.s64 = ctx.r1.s64 + -288;
		// lfs f12,-256(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -256);
		ctx.f12.f64 = double(temp.f32);
		// addi r11,r31,4
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 4;
		// addi r8,r1,-240
		ctx.r8.s64 = ctx.r1.s64 + -240;
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r11,r31,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
		// lfsx f11,r9,r3
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
		ctx.f11.f64 = double(temp.f32);
		// stfs f11,-264(r1)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r1.u32 + -264, temp.u32);
		// lfs f11,-248(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -248);
		ctx.f11.f64 = double(temp.f32);
		// lfsx f10,r10,r3
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
		ctx.f10.f64 = double(temp.f32);
		// lfsx f13,r11,r3
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
		ctx.f13.f64 = double(temp.f32);
		// stvx v25,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v26,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f9,-240(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -240);
		ctx.f9.f64 = double(temp.f32);
		// lfs f8,-288(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -288);
		ctx.f8.f64 = double(temp.f32);
		// fdivs f7,f9,f8
		ctx.f7.f64 = double(float(ctx.f9.f64 / ctx.f8.f64));
		// stfs f7,-240(r1)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r1.u32 + -240, temp.u32);
		// addi r9,r1,-240
		ctx.r9.s64 = ctx.r1.s64 + -240;
		// stfs f13,-272(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + -272, temp.u32);
		// stfs f10,-268(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + -268, temp.u32);
		// addi r3,r1,-272
		ctx.r3.s64 = ctx.r1.s64 + -272;
		// addi r7,r1,-240
		ctx.r7.s64 = ctx.r1.s64 + -240;
		// addi r10,r1,-272
		ctx.r10.s64 = ctx.r1.s64 + -272;
		// lvx128 v23,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v23,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v23.u32), 0xFF));
		// lvx128 v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v24,v0,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v24.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmulfp128 v22,v27,v13
		simde_mm_store_ps(ctx.v22.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v24,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v22,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfsx f13,r11,r8
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
		ctx.f13.f64 = double(temp.f32);
		// lfs f6,-272(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -272);
		ctx.f6.f64 = double(temp.f32);
		// fdivs f5,f13,f6
		ctx.f5.f64 = double(float(ctx.f13.f64 / ctx.f6.f64));
		// stfs f5,-240(r1)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r1.u32 + -240, temp.u32);
		// addi r6,r1,-240
		ctx.r6.s64 = ctx.r1.s64 + -240;
		// lvx128 v21,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v21,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v21.u32), 0xFF));
		// vmulfp128 v20,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v20.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v20,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,0(r4)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f1,f13,f2
		ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
		// lfs f4,4(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		ctx.f4.f64 = double(temp.f32);
		// fmuls f10,f13,f11
		ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
		// lfs f3,8(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
		ctx.f3.f64 = double(temp.f32);
		// fmsubs f13,f4,f12,f1
		ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f1.f64));
		// fmsubs f12,f3,f12,f10
		ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 - ctx.f10.f64));
		// fabs f13,f13
		ctx.f13.u64 = ctx.f13.u64 & ~0x8000000000000000;
		// fcmpu cr6,f13,f0
		// bgt cr6,0x822511ec
		if (ctx.f13.f64 > ctx.f0.f64) goto loc_822511EC;
		// bso cr6,0x822511ec
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82251148, "bso");
		// fabs f13,f12
		ctx.f13.u64 = ctx.f12.u64 & ~0x8000000000000000;
		// fcmpu cr6,f13,f0
		// bgt cr6,0x822511ec
		if (ctx.f13.f64 > ctx.f0.f64) goto loc_822511EC;
		// mr r11,r22
		ctx.r11.u64 = var_r22;
		// bns cr6,0x822511f0
		// UNIMPLEMENTED: bns
		PPC_UNIMPLEMENTED(0x8225115C, "bns");
		// mr r11,r23
		ctx.r11.u64 = var_r23;
		// clrlwi r3,r11,24
		ctx.r3.u64 = ctx.r11.u32 & 0xFF;
		// addi r12,r1,-120
		ctx.r12.s64 = ctx.r1.s64 + -120;
		// bl 0x82436660
		__restfpr_27(ctx, base);
		// b 0x8242f8c0
		__restgprlr_18(ctx, base);
		return;
	}
loc_82251174:
	// lis r11,-32158
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v18,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_setzero_si128());
	// stfs f0,-240(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -240, temp.u32);
	// addi r11,r11,-25632
	ctx.r11.s64 = ctx.r11.s64 + -25632;
	// addi r3,r1,-240
	ctx.r3.s64 = ctx.r1.s64 + -240;
	// stvx v18,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v17,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// lvx128 v19,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,26432
	ctx.r11.s64 = ctx.r11.s64 + 26432;
	// vspltw v13,v19,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v19.u32), 0xFF));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vand v12,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vand v0,v17,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v11,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v13,v12,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vcmpgefp. v16,v0,v11
	simde_mm_store_ps(ctx.v16.f32, simde_mm_cmpge_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	ctx.cr6.setFromMask(simde_mm_load_ps(ctx.v16.f32), 0xF);
	// mfocrf r11,2
	ctx.r11.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// rlwinm r9,r11,0,24,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80;
	// vcmpgefp. v15,v13,v0
	simde_mm_store_ps(ctx.v15.f32, simde_mm_cmpge_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	ctx.cr6.setFromMask(simde_mm_load_ps(ctx.v15.f32), 0xF);
	// mfocrf r10,2
	ctx.r10.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// cmplwi cr6,r9,0
	// beq cr6,0x822511ec
	if (ctx.r9.u32 != 0) {
		// rlwinm r8,r11,0,24,24
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80;
		// mr r11,r22
		ctx.r11.u64 = var_r22;
		// cmplwi cr6,r8,0
		// bne cr6,0x822511f0
		if (ctx.r8.u32 != 0) {
			// clrlwi r3,r11,24
			ctx.r3.u64 = ctx.r11.u32 & 0xFF;
			// addi r12,r1,-120
			ctx.r12.s64 = ctx.r1.s64 + -120;
			// bl 0x82436660
			__restfpr_27(ctx, base);
			// b 0x8242f8c0
			__restgprlr_18(ctx, base);
			return;
		}
	}
loc_822511EC:
	// mr r11,r23
	ctx.r11.u64 = var_r23;
loc_822511F0:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// addi r12,r1,-120
	ctx.r12.s64 = ctx.r1.s64 + -120;
	// bl 0x82436660
	__restfpr_27(ctx, base);
	// b 0x8242f8c0
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_13"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_13);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_13) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x8225aff8
	phJoint1Dof_AFF8_p42(ctx, base);
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82251388
	phJoint_1388(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_14"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_14);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// li r11,0
	ctx.r11.s64 = 0;
	// stfs f1,720(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 720, temp.u32);
	// stfs f1,724(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 724, temp.u32);
	// stfs f1,728(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 728, temp.u32);
	// stfs f1,732(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 732, temp.u32);
	// stfs f2,736(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 736, temp.u32);
	// stb r11,744(r3)
	PPC_STORE_U8(ctx.r3.u32 + 744, ctx.r11.u8);
	// lis r11,-32248
	// stfs f2,740(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 740, temp.u32);
	// lfs f0,-25896(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f2,f0
	// bne cr6,0x822512a0
	if (ctx.f2.f64 == ctx.f0.f64) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822512A0:
	// stb r11,745(r3)
	PPC_STORE_U8(ctx.r3.u32 + 745, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint_12A8_fw"))) PPC_WEAK_FUNC(phJoint_12A8_fw);
PPC_FUNC_IMPL(__imp__phJoint_12A8_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f29.u64);
	// stfd f30,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f30.u64);
	// stfd f31,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vpermwi128 v12,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v10,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mr r6,r11
	ctx.r6.u64 = ctx.r11.u64;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// fmr f30,f2
	var_f30 = ctx.f2.f64;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// vpermwi128 v11,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v13,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// fmr f29,f3
	var_f29 = ctx.f3.f64;
	// vmulfp128 v0,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vnmsubfp v0,v13,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82251388
	phJoint_1388(ctx, base);
	// stfs f31,720(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 720, temp.u32);
	// fcmpu cr6,f31,f30
	// stfs f31,724(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 724, temp.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// stfs f30,728(r31)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(var_r31 + 728, temp.u32);
	// stfs f30,732(r31)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(var_r31 + 732, temp.u32);
	// bne cr6,0x8225133c
	if (var_f31 == var_f30) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8225133C:
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// stfs f29,736(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f29);
	PPC_STORE_U32(var_r31 + 736, temp.u32);
	// lis r11,-32248
	// stfs f29,740(r31)
	temp.f32 = float(var_f29);
	PPC_STORE_U32(var_r31 + 740, temp.u32);
	// stb r10,744(r31)
	PPC_STORE_U8(var_r31 + 744, ctx.r10.u8);
	// lfs f0,-25896(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f29,f0
	// bne cr6,0x82251364
	if (var_f29 == ctx.f0.f64) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82251364:
	// stb r11,745(r31)
	PPC_STORE_U8(var_r31 + 745, ctx.r11.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f30,-32(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint_1388"))) PPC_WEAK_FUNC(phJoint_1388);
PPC_FUNC_IMPL(__imp__phJoint_1388) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=224, savegprlr_25
	// lis r10,-32253
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r9
	var_r30 = ctx.r9.u32;
	// addi r11,r8,144
	ctx.r11.s64 = ctx.r8.s64 + 144;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// lfs f0,-12016(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r8,192
	ctx.r10.s64 = ctx.r8.s64 + 192;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stw r8,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r8.u32);
	// stw r30,28(r31)
	PPC_STORE_U32(var_r31 + 28, var_r30);
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// stfs f0,144(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// addi r3,r31,576
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 576;
	// stfs f0,148(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stfs f0,152(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// addi r28,r3,16
	var_r28 = (uint32_t)(ctx.r3.s64 + 16);
	// lfs f0,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r27,r3,32
	var_r27 = (uint32_t)(ctx.r3.s64 + 32);
	// lfs f13,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r26,r3,48
	var_r26 = (uint32_t)(ctx.r3.s64 + 48);
	// lfs f11,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// addi r25,r3,48
	var_r25 = (uint32_t)(ctx.r3.s64 + 48);
	// lfs f10,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// lfs f7,8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// addi r7,r11,48
	ctx.r7.s64 = ctx.r11.s64 + 48;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r31,560
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 560;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v9,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v12,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// lfs f12,0(r5)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// vmrghw v8,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// lfs f9,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// vmrghw v11,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// vmrglw v10,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// stfs f12,104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r11,48
	ctx.r5.s64 = ctx.r11.s64 + 48;
	// vmsum3fp128 v13,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stfs f11,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// vmsum3fp128 v12,v0,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// stfs f10,116(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// vmsum3fp128 v11,v0,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// stfs f9,120(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f8,128(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f7,132(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f6,136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// vmrghw v0,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v8,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v7,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v6,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v8,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r11,r30,144
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 144;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r30,192
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 192;
	// addi r3,r11,32
	ctx.r3.s64 = ctx.r11.s64 + 32;
	// addi r10,r11,48
	ctx.r10.s64 = ctx.r11.s64 + 48;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// addi r8,r31,640
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 640;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// vsubfp v0,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r31,656
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 656;
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v8,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v9,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// vmrglw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r5,r11,48
	ctx.r5.s64 = ctx.r11.s64 + 48;
	// vmrglw v12,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// addi r30,r3,48
	var_r30 = (uint32_t)(ctx.r3.s64 + 48);
	// vmrghw v11,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// vmrglw v10,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmsum3fp128 v12,v0,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmsum3fp128 v11,v0,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,32
	ctx.r8.s64 = ctx.r3.s64 + 32;
	// lvx128 v5,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r3,32
	ctx.r11.s64 = ctx.r3.s64 + 32;
	// stvx v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r3,48
	ctx.r7.s64 = ctx.r3.s64 + 48;
	// stvx v4,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v9,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrglw v13,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v1,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v1.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v31,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v31.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v30,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v30.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v1,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25896(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);  /* glob:lbl_82079AD8 @ 0x82079ad8 */
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,1668(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 1668, temp.u32);
	// stfs f0,1672(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 1672, temp.u32);
	return;
}

__attribute__((alias("__imp__phJoint_1618_g"))) PPC_WEAK_FUNC(phJoint_1618_g);
PPC_FUNC_IMPL(__imp__phJoint_1618_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f898
	ctx.lr = 0x82251620;
	__savegprlr_28(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// li r12,-80
	ctx.r12.s64 = -80;
	// stvx128 v126,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r12,-64
	ctx.r12.s64 = -64;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r3,r31,944
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 944;
	// addi r4,r31,576
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 576;
	// addi r7,r3,16
	ctx.r7.s64 = ctx.r3.s64 + 16;
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r6,r3,32
	ctx.r6.s64 = ctx.r3.s64 + 32;
	// addi r5,r3,48
	ctx.r5.s64 = ctx.r3.s64 + 48;
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// addi r9,r11,32
	ctx.r9.s64 = ctx.r11.s64 + 32;
	// addi r8,r11,48
	ctx.r8.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lfs f0,664(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 664);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,680(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 680);
	ctx.f13.f64 = double(temp.f32);
	// addi r30,r31,880
	var_r30 = (uint32_t)(var_r31 + 880);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// lfs f12,696(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 696);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// lfs f11,952(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 952);
	ctx.f11.f64 = double(temp.f32);
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// lfs f10,968(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 968);
	ctx.f10.f64 = double(temp.f32);
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r28,r11,-12020
	var_r28 = (uint32_t)(ctx.r11.s64 + -12020);  // lbl_8202D10C @ 0x8202d10c
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lfs f9,984(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(var_r31 + 984);
	ctx.f9.f64 = double(temp.f32);
	// vmsum3fp128 v10,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stfs f11,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f9,88(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lfs f31,4(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 4);
	var_f31 = double(temp.f32);
	// lvx128 v126,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vpermwi128 v12,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmrghw v11,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw128 v127,v0,v11
	simde_mm_store_si128((simde__m128i*)ctx.v127.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vpermwi128 v0,v127,135
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x78));
	// vpermwi128 v11,v127,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x9C));
	// vmulfp128 v0,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vnmsubfp v0,v11,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v0,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f31
	// bne cr6,0x82251770
	if (ctx.f1.f64 == var_f31) {
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// stfs f31,772(r31)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r31 + 772, temp.u32);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x8225b128
		game_B128(ctx, base);
		// addi r11,r31,896
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 896;
	loc_82251760:
		// lfs f0,-4(r28)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r28 + -4);
		ctx.f0.f64 = double(temp.f32);
		// stvx128 v126,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stfs f0,928(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 928, temp.u32);
		// b 0x82251874
		goto loc_82251874;
	}
loc_82251770:
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vmsum3fp128 v9,v126,v127
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v126.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lvx128 v8,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lis r11,-32163
	ctx.r11.s64 = -2107834368;
	// addi r29,r11,-18496
	var_r29 = (uint32_t)(ctx.r11.s64 + -18496);  // lbl_825CB7C0 @ 0x825cb7c0
	// lvx128 v7,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v7,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrefp v13,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vnmsubfp v0,v13,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v13,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v6,v8,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f2,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// bl 0x82430fe0
	phBoundCapsule_0FE0_g(ctx, base);
	// addi r11,r31,896
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 896;
	// frsp f8,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f1.f64));
	// stfs f8,772(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 772, temp.u32);
	// lis r10,-32158
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r10,r10,-25616
	ctx.r10.s64 = ctx.r10.s64 + -25616;
	// stvx128 v126,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v4,v5,v127
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v127.f32)));
	// stvx v4,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v0,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32160
	// lvx128 v13,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,26448
	ctx.r10.s64 = ctx.r10.s64 + 26448;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32163
	// addi r10,r10,-18480
	ctx.r10.s64 = ctx.r10.s64 + -18480;
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrsqrtefp v11,v12
	simde_mm_store_ps(ctx.v11.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v12.f32))));
	// vcmpeqfp v0,v11,v10
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsel v0,v11,v9,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
	// vmulfp128 v11,v0,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v10,v8,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v13,v12,v11,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v13,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v3,v12,v0
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v3,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,928(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 928, temp.u32);
	// fcmpu cr6,f0,f31
	// beq cr6,0x82251760
	if (ctx.f0.f64 == var_f31) goto loc_82251760;
	// lfs f13,120(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 120);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// lfs f13,0(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,928(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 928, temp.u32);
	// lvx128 v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmuls f7,f0,f13
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lvx128 v1,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v1,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.u32), 0xFF));
	// vmulfp128 v31,v2,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_82251874:
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r31,912
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 912;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v11,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// lfs f1,772(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 772);
	ctx.f1.f64 = double(temp.f32);
	// vpermwi128 v10,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmulfp128 v0,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vnmsubfp v0,v10,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// li r0,-80
	ctx.r0.s64 = -80;
	// lvx128 v126,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_18C0_g"))) PPC_WEAK_FUNC(phBoundCapsule_18C0_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_18C0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x822518C8;
	__savegprlr_26(ctx, base);
	// stfd f31,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f31.u64);
	// li r12,-96
	ctx.r12.s64 = -96;
	// stvx128 v126,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r12,-80
	ctx.r12.s64 = -80;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r28,r31,944
	var_r28 = (uint32_t)(var_r31 + 944);
	// addi r4,r31,576
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 576;
	// addi r7,r28,16
	ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r6,r28,32
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 32;
	// addi r5,r28,48
	ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 48;
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// addi r9,r11,32
	ctx.r9.s64 = ctx.r11.s64 + 32;
	// addi r8,r11,48
	ctx.r8.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lfs f0,664(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 664);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,680(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 680);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r31,880
	var_r29 = (uint32_t)(var_r31 + 880);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// lfs f12,696(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 696);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// lfs f11,952(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 952);
	ctx.f11.f64 = double(temp.f32);
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// lfs f10,968(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 968);
	ctx.f10.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r26,r11,-12020
	var_r26 = (uint32_t)(ctx.r11.s64 + -12020);  // lbl_8202D10C @ 0x8202d10c
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lfs f9,984(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(var_r31 + 984);
	ctx.f9.f64 = double(temp.f32);
	// vmsum3fp128 v11,v11,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stfs f11,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f31,4(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 4);
	var_f31 = double(temp.f32);
	// lvx128 v127,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v127,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v9,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmrghw v0,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v12,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw128 v126,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v126.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vpermwi128 v0,v126,135
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v126.u32), 0x78));
	// vpermwi128 v12,v126,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v126.u32), 0x9C));
	// stvx128 v126,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v0,v0,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vnmsubfp v0,v12,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v0,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f31
	// bne cr6,0x82251aec
	if (ctx.f1.f64 == var_f31) {
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// stfs f31,772(r31)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r31 + 772, temp.u32);
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// bl 0x8225b128
		game_B128(ctx, base);
		// addi r6,r31,896
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 896;
		// lfs f0,-4(r26)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r26 + -4);
		ctx.f0.f64 = double(temp.f32);
		// vpermwi128 v12,v127,135
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x78));
		// vpermwi128 v11,v127,99
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x9C));
		// addi r5,r31,912
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 912;
		// stvx128 v127,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,28(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
		// stfs f0,928(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 928, temp.u32);
		// lvx128 v0,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,144
		ctx.r11.s64 = ctx.r11.s64 + 144;
		// vpermwi128 v10,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// lfs f8,656(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 656);
		ctx.f8.f64 = double(temp.f32);
		// vpermwi128 v9,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// lfs f7,672(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 672);
		ctx.f7.f64 = double(temp.f32);
		// lfs f6,688(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 688);
		ctx.f6.f64 = double(temp.f32);
		// addi r3,r11,32
		ctx.r3.s64 = ctx.r11.s64 + 32;
		// stfs f8,80(r1)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// vmulfp128 v0,v12,v10
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stfs f7,84(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// stfs f6,88(r1)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// lfs f5,960(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 960);
		ctx.f5.f64 = double(temp.f32);
		// lfs f11,964(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 964);
		ctx.f11.f64 = double(temp.f32);
		// lfs f4,976(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 976);
		ctx.f4.f64 = double(temp.f32);
		// lfs f9,980(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 980);
		ctx.f9.f64 = double(temp.f32);
		// lvx128 v13,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f3,0(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + 0);
		ctx.f3.f64 = double(temp.f32);
		// lfs f8,948(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 948);
		ctx.f8.f64 = double(temp.f32);
		// vnmsubfp v0,v11,v9,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v0,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// vmsum3fp128 v12,v12,v13
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// lvx128 v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v11,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// vmrghw v0,v0,v12
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vmrghw v13,v13,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// vmrghw v9,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// stvx v9,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,84(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f2,f5,f13
		ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
		// lfs f0,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f10,f11,f13
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
		// lfs f12,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f12.f64 = double(temp.f32);
		// fmadds f1,f4,f0,f2
		ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f2.f64));
		// fmadds f7,f9,f0,f10
		ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f10.f64));
		// fmadds f2,f12,f3,f1
		ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f3.f64 + ctx.f1.f64));
		// fmadds f1,f8,f12,f7
		ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f7.f64));
		// bl 0x82430fe0
		phBoundCapsule_0FE0_g(ctx, base);
		// frsp f6,f1
		ctx.fpscr.disableFlushMode();
		ctx.f6.f64 = double(float(ctx.f1.f64));
		// stfs f6,776(r31)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(var_r31 + 776, temp.u32);
		// b 0x82251dbc
	} else {
	loc_82251AEC:
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f1,f1
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = double(float(ctx.f1.f64));
		// stfs f1,96(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// vmsum3fp128 v8,v127,v126
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v126.f32), 0xEF));
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// lvx128 v7,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// lis r11,-32163
		ctx.r11.s64 = -2107834368;
		// addi r27,r11,-18496
		var_r27 = (uint32_t)(ctx.r11.s64 + -18496);  // lbl_825CB7C0 @ 0x825cb7c0
		// lvx128 v6,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// lvx128 v0,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vrefp v13,v12
		simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
		// stvx v8,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vnmsubfp v0,v13,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v5,v7,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v5,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f2,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f2.f64 = double(temp.f32);
		// bl 0x82430fe0
		phBoundCapsule_0FE0_g(ctx, base);
		// addi r30,r31,896
		var_r30 = (uint32_t)(var_r31 + 896);
		// frsp f5,f1
		ctx.fpscr.disableFlushMode();
		ctx.f5.f64 = double(float(ctx.f1.f64));
		// stfs f5,772(r31)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(var_r31 + 772, temp.u32);
		// lis r11,-32158
		// addi r6,r1,112
		ctx.r6.s64 = ctx.r1.s64 + 112;
		// addi r11,r11,-25616
		ctx.r11.s64 = ctx.r11.s64 + -25616;
		// stvx128 v127,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v4,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp128 v3,v4,v126
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v126.f32)));
		// stvx v3,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v12,v0,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32160
		// lvx128 v13,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32163
		// addi r11,r11,-18480
		ctx.r11.s64 = ctx.r11.s64 + -18480;
		// lvx128 v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vrsqrtefp v11,v12
		simde_mm_store_ps(ctx.v11.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v12.f32))));
		// vcmpeqfp v0,v11,v10
		simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vsel v0,v11,v9,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
		// vmulfp128 v11,v0,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v10,v8,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vnmsubfp v13,v12,v11,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v10,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v2,v12,v0
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v2,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,112(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,928(r31)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r31 + 928, temp.u32);
		// fcmpu cr6,f13,f31
		// bne cr6,0x82251bf8
		if (ctx.f13.f64 == var_f31) {
			// lfs f0,-4(r26)
			temp.u32 = PPC_LOAD_U32(var_r26 + -4);
			ctx.f0.f64 = double(temp.f32);
			// vpermwi128 v13,v127,135
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x78));
			// stvx128 v127,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vpermwi128 v12,v127,99
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x9C));
			// stfs f0,928(r31)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r31 + 928, temp.u32);
			// addi r5,r31,912
			ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 912;
			// stfs f31,776(r31)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(var_r31 + 776, temp.u32);
			// lvx128 v0,r0,r29
			ea = (var_r29) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vpermwi128 v11,v0,99
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
			// vpermwi128 v10,v0,135
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
			// vmulfp128 v0,v13,v11
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
			// vnmsubfp v0,v12,v10,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// stvx v0,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// b 0x82251dbc
		} else {
		loc_82251BF8:
			// lfs f0,120(r26)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r26 + 120);
			ctx.f0.f64 = double(temp.f32);
			// addi r27,r31,912
			var_r27 = (uint32_t)(var_r31 + 912);
			// fdivs f0,f0,f13
			ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
			// lfs f13,0(r26)
			temp.u32 = PPC_LOAD_U32(var_r26 + 0);
			ctx.f13.f64 = double(temp.f32);
			// stfs f0,928(r31)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r31 + 928, temp.u32);
			// addi r7,r1,96
			ctx.r7.s64 = ctx.r1.s64 + 96;
			// lvx128 v1,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fmuls f4,f0,f13
			ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
			// stfs f4,112(r1)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
			// addi r4,r1,112
			ctx.r4.s64 = ctx.r1.s64 + 112;
			// addi r6,r1,112
			ctx.r6.s64 = ctx.r1.s64 + 112;
			// lvx128 v31,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v0,v31,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v31.u32), 0xFF));
			// vmulfp128 v30,v1,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v30,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v29,r0,r29
			ea = (var_r29) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v29,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v0,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v13,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vpermwi128 v12,v0,135
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
			// vpermwi128 v11,v13,99
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
			// vpermwi128 v10,v0,99
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
			// vpermwi128 v13,v13,135
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
			// vmulfp128 v0,v12,v11
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
			// vnmsubfp v0,v10,v13,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// stvx v0,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,28(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
			// lfs f1,688(r31)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(var_r31 + 688);
			ctx.f1.f64 = double(temp.f32);
			// addi r11,r11,144
			ctx.r11.s64 = ctx.r11.s64 + 144;
			// lfs f3,656(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 656);
			ctx.f3.f64 = double(temp.f32);
			// lfs f2,672(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 672);
			ctx.f2.f64 = double(temp.f32);
			// stfs f1,88(r1)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
			// addi r10,r11,32
			ctx.r10.s64 = ctx.r11.s64 + 32;
			// stfs f3,80(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// addi r9,r11,16
			ctx.r9.s64 = ctx.r11.s64 + 16;
			// stfs f2,84(r1)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// lvx128 v13,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,80
			ctx.r8.s64 = ctx.r1.s64 + 80;
			// lvx128 v10,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v12,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f1,772(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 772);
			ctx.f1.f64 = double(temp.f32);
			// lvx128 v0,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v11,v13,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// lvx128 v13,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v12,v12,v0
			simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// vmsum3fp128 v13,v13,v0
			simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
			// vmrghw v0,v11,v12
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
			// vmrghw v13,v13,v0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
			// vmrghw128 v127,v0,v13
			simde_mm_store_si128((simde__m128i*)ctx.v127.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
			// lvx128 v0,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v28,v10,v127
			simde_mm_store_ps(ctx.v28.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
			// vmsum3fp128 v27,v0,v127
			simde_mm_store_ps(ctx.v27.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
			// stvx128 v127,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v28,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v27,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x824302b0
			phBoundCapsule_02B0_g(ctx, base);
			// lfs f0,772(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 772);
			ctx.f0.f64 = double(temp.f32);
			// frsp f31,f1
			var_f31 = double(float(ctx.f1.f64));
			// fmr f1,f0
			ctx.f1.f64 = ctx.f0.f64;
			// bl 0x824301d8
			phBoundCapsule_01D8_g(ctx, base);
			// lfs f13,96(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			ctx.f13.f64 = double(temp.f32);
			// frsp f12,f1
			ctx.f12.f64 = double(float(ctx.f1.f64));
			// fmuls f11,f13,f31
			ctx.f11.f64 = double(float(ctx.f13.f64 * var_f31));
			// lfs f0,112(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
			ctx.f0.f64 = double(temp.f32);
			// addi r5,r28,32
			ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 32;
			// lvx128 v0,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r28,48
			ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 48;
			// lvx128 v7,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r3,r28,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 16;
			// lvx128 v13,r0,r28
			ea = (var_r28) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v12,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v11,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v10,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fmuls f10,f13,f12
			ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
			// fmadds f12,f0,f12,f11
			ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f11.f64));
			// fsubs f9,f12,f13
			ctx.f9.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
			// stfs f9,112(r1)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
			// fmsubs f13,f0,f31,f10
			ctx.f13.f64 = double(float(ctx.f0.f64 * var_f31 - ctx.f10.f64));
			// fsubs f8,f13,f0
			ctx.f8.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
			// stfs f8,96(r1)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
			// addi r11,r1,112
			ctx.r11.s64 = ctx.r1.s64 + 112;
			// vmrghw v9,v13,v12
			simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// vmrghw v8,v10,v11
			simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
			// addi r9,r1,80
			ctx.r9.s64 = ctx.r1.s64 + 80;
			// vmrglw v13,v13,v12
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
			// vmrglw v12,v10,v11
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
			// lvx128 v26,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmrghw v11,v9,v8
			simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
			// vspltw v6,v26,0
			simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v26.u32), 0xFF));
			// lvx128 v25,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v5,v25,0
			simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v25.u32), 0xFF));
			// vmrglw v10,v9,v8
			simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
			// vmrghw v13,v13,v12
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
			// vmaddcfp128 v0,v6,v0,v127
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v127.f32)));
			// stvx v0,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmaddfp v0,v7,v5,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v5.f32)), simde_mm_load_ps(ctx.v0.f32)));
			// addi r8,r1,80
			ctx.r8.s64 = ctx.r1.s64 + 80;
			// vmsum3fp128 v12,v0,v11
			simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
			// stvx v0,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v11,v0,v10
			simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
			// addi r7,r1,80
			ctx.r7.s64 = ctx.r1.s64 + 80;
			// vmsum3fp128 v0,v0,v13
			simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
			// vmrghw v0,v12,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
			// vmrghw v13,v11,v0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
			// vmrghw v24,v0,v13
			simde_mm_store_si128((simde__m128i*)ctx.v24.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
			// stvx v24,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f2,80(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f2.f64 = double(temp.f32);
			// lfs f1,84(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f1.f64 = double(temp.f32);
			// bl 0x82430fe0
			phBoundCapsule_0FE0_g(ctx, base);
			// frsp f7,f1
			ctx.fpscr.disableFlushMode();
			ctx.f7.f64 = double(float(ctx.f1.f64));
			// stfs f7,776(r31)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(var_r31 + 776, temp.u32);
		}
	}
loc_82251DBC:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// li r0,-96
	ctx.r0.s64 = -96;
	// lvx128 v126,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r0,-80
	ctx.r0.s64 = -80;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfd f31,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_1DD8"))) PPC_WEAK_FUNC(phJoint3Dof_1DD8);
PPC_FUNC_IMPL(__imp__phJoint3Dof_1DD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,744(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 744);
	// cmplwi cr6,r11,0
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// bne cr6,0x82251e88
	if (ctx.r11.u32 == 0) {
		// lfs f13,772(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 772);
		ctx.f13.f64 = double(temp.f32);
		// addi r11,r11,-12024
		ctx.r11.s64 = ctx.r11.s64 + -12024;
		// lfs f0,724(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 724);
		ctx.f0.f64 = double(temp.f32);
		// fsubs f12,f13,f0
		ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// lfs f13,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f12,f13
		// blt cr6,0x82251e80
		if (ctx.f12.f64 < ctx.f13.f64) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x82251f88
			// addi r1,r1,160
			ctx.r1.s64 = ctx.r1.s64 + 160;
			// lwz r12,-8(r1)
			ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
			// mtlr r12
			ctx.lr = ctx.r12.u64;
			// lfd f30,-40(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
			// lfd f31,-32(r1)
			ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
			// ld r30,-24(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
			// ld r31,-16(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// blr
			return;
		}
		// bso cr6,0x82251e80
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82251E24, "bso");
		// lwz r10,28(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
		// addi r9,r31,880
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 880;
		// lwz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
		// lfs f11,720(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 720);
		ctx.f11.f64 = double(temp.f32);
		// addi r8,r10,288
		ctx.r8.s64 = ctx.r10.s64 + 288;
		// stfs f11,748(r31)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r31 + 748, temp.u32);
		// addi r7,r11,288
		ctx.r7.s64 = ctx.r11.s64 + 288;
		// stfs f0,752(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 752, temp.u32);
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// vmsum3fp128 v12,v0,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f10,80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f9.f64 = double(temp.f32);
		// fsubs f8,f10,f9
		ctx.f8.f64 = double(float(ctx.f10.f64 - ctx.f9.f64));
		// stfs f8,764(r31)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r31 + 764, temp.u32);
		// b 0x82251f84
		goto loc_82251F84;
	loc_82251E80:
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x82251f88
		// addi r1,r1,160
		ctx.r1.s64 = ctx.r1.s64 + 160;
		// lwz r12,-8(r1)
		ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
		// mtlr r12
		ctx.lr = ctx.r12.u64;
		// lfd f30,-40(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// lfd f31,-32(r1)
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
		// ld r30,-24(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
		// ld r31,-16(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// blr
		return;
	}
loc_82251E88:
	// lfs f0,884(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 884);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r31,880
	var_r30 = (uint32_t)(var_r31 + 880);
	// lfs f7,964(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 964);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f10,f7,f0
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f6,960(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 960);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,948(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 948);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f9,f6,f0
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f13,888(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 888);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phJoint3Dof::vtable@+0x0 */;
	ctx.f12.f64 = double(temp.f32);
	// lfs f4,944(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 944);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,980(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 980);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,724(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 724);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,976(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 976);
	ctx.f2.f64 = double(temp.f32);
	// lfs f11,732(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 732);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,772(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 772);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f8,f5,f12,f10
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f12.f64 + ctx.f10.f64));
	// fmadds f7,f4,f12,f9
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fmadds f31,f3,f13,f8
	var_f31 = double(float(ctx.f3.f64 * ctx.f13.f64 + ctx.f8.f64));
	// fmadds f30,f2,f13,f7
	var_f30 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f7.f64));
	// fdivs f12,f31,f1
	ctx.f12.f64 = double(float(var_f31 / ctx.f1.f64));
	// fdivs f13,f30,f11
	ctx.f13.f64 = double(float(var_f30 / ctx.f11.f64));
	// fmuls f6,f12,f12
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmadds f5,f13,f13,f6
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f6.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f1,f4,f0
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// blt cr6,0x82251e80
	if (ctx.f1.f64 < ctx.f0.f64) {
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x82251f88
		// addi r1,r1,160
		ctx.r1.s64 = ctx.r1.s64 + 160;
		// lwz r12,-8(r1)
		ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
		// mtlr r12
		ctx.lr = ctx.r12.u64;
		// lfd f30,-40(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// lfd f31,-32(r1)
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
		// ld r30,-24(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
		// ld r31,-16(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// blr
		return;
	}
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// lfs f3,720(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 720);
	ctx.f3.f64 = double(temp.f32);
	// fdivs f13,f31,f3
	ctx.f13.f64 = double(float(var_f31 / ctx.f3.f64));
	// lfs f2,728(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 728);
	ctx.f2.f64 = double(temp.f32);
	// fdivs f12,f30,f2
	ctx.f12.f64 = double(float(var_f30 / ctx.f2.f64));
	// lfs f0,772(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 772);
	ctx.f0.f64 = double(temp.f32);
	// frsp f1,f1
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// fmuls f13,f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// fdivs f11,f0,f1
	ctx.f11.f64 = double(float(ctx.f0.f64 / ctx.f1.f64));
	// stfs f11,752(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 752, temp.u32);
	// fmadds f10,f12,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f13.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f1,f9,f0
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f7,f1
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f1.f64));
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lfs f8,772(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 772);
	ctx.f8.f64 = double(temp.f32);
	// addi r4,r10,288
	ctx.r4.s64 = ctx.r10.s64 + 288;
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,288
	ctx.r11.s64 = ctx.r11.s64 + 288;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// fdivs f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 / ctx.f7.f64));
	// stfs f6,748(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 748, temp.u32);
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v0,v12
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f5,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// fsubs f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// stfs f3,764(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 764, temp.u32);
loc_82251F84:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82251F88:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_1FA8_wrh"))) PPC_WEAK_FUNC(phJoint3Dof_1FA8_wrh);
PPC_FUNC_IMPL(__imp__phJoint3Dof_1FA8_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_24
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r24,r10
	var_r24 = ctx.r10.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// mr r26,r8
	var_r26 = ctx.r8.u32;
	// mr r25,r9
	var_r25 = ctx.r9.u32;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// addi r9,r31,880
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 880;
	// addi r8,r31,896
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 896;
	// lwz r6,260(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lbz r7,746(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 746);
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stb r7,0(r30)
	PPC_STORE_U8(var_r30 + 0, ctx.r7.u8);
	// stvx v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lbz r5,746(r31)
	ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 746);
	// cmplwi cr6,r5,0
	// beq cr6,0x82252038
	if (ctx.r5.u32 != 0) {
		// lfs f0,772(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 772);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,748(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 748);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,752(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 752);
		ctx.f12.f64 = double(temp.f32);
		// fsubs f10,f0,f13
		ctx.f10.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
		// lfs f11,764(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 764);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f9,f0,f12
		ctx.f9.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// stfs f11,0(r26)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r26 + 0, temp.u32);
		// stfs f10,0(r28)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r28 + 0, temp.u32);
		// stfs f9,0(r27)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
	}
loc_82252038:
	// lbz r11,747(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 747);
	// cmplwi cr6,r11,0
	// stb r11,0(r25)
	PPC_STORE_U8(var_r25 + 0, ctx.r11.u8);
	// beq cr6,0x82252098
	if (ctx.r11.u32 != 0) {
		// lwz r11,244(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
		// lfs f0,776(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 776);
		ctx.f0.f64 = double(temp.f32);
		// lfs f8,756(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 756);
		ctx.f8.f64 = double(temp.f32);
		// lwz r10,252(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
		// fsubs f7,f0,f8
		ctx.f7.f64 = double(float(ctx.f0.f64 - ctx.f8.f64));
		// lfs f13,760(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 760);
		ctx.f13.f64 = double(temp.f32);
		// fsubs f6,f0,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
		// stfs f7,0(r11)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// lis r11,-32253
		// stfs f6,0(r10)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// lfs f0,-12016(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		ctx.f0.f64 = double(temp.f32);
		// li r11,-1
		// fcmpu cr6,f13,f0
		// blt cr6,0x82252084
		if (ctx.f13.f64 >= ctx.f0.f64) {
			// li r11,1
			ctx.r11.s64 = 1;
		}
	loc_82252084:
		// lwz r9,268(r1)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
		// lfs f5,768(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 768);
		ctx.f5.f64 = double(temp.f32);
		// stw r11,0(r24)
		PPC_STORE_U32(var_r24 + 0, ctx.r11.u32);
		// stfs f5,0(r9)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// b 0x822520a0
	} else {
	loc_82252098:
		// li r8,0
		ctx.r8.s64 = 0;
		// stw r8,0(r24)
		PPC_STORE_U32(var_r24 + 0, ctx.r8.u32);
	}
loc_822520A0:
	// cmpwi cr6,r3,0
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x822520b0
	if (ctx.r3.s32 <= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822520B0:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_3"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_3);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_3) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,745(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 745);
	// cmplwi cr6,r11,0
	// bne cr6,0x822521bc
	if (ctx.r11.u32 == 0) {
		// lwz r10,20(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
		// li r11,1
		ctx.r11.s64 = 1;
		// cmpwi cr6,r10,0
		// bne cr6,0x822520f4
		if (ctx.r10.s32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_822520F4:
		// clrlwi r8,r11,24
		ctx.r8.u64 = ctx.r11.u32 & 0xFF;
		// li r11,0
		ctx.r11.s64 = 0;
		// cmplwi cr6,r8,0
		// beq cr6,0x82252108
		if (ctx.r8.u32 != 0) {
			// li r11,1
			ctx.r11.s64 = 1;
		}
	loc_82252108:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// cmplwi cr6,r6,0
		// beq cr6,0x8225217c
		if (ctx.r6.u32 != 0) {
			// bl 0x822518c0
			phBoundCapsule_18C0_g(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82251dd8
			phJoint3Dof_1DD8(ctx, base);
			// lwz r10,28(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
			// lwz r11,24(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
			// addi r5,r31,896
			ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 896;
			// addi r4,r10,288
			ctx.r4.s64 = ctx.r10.s64 + 288;
			// stb r3,746(r31)
			PPC_STORE_U8(var_r31 + 746, ctx.r3.u8);
			// addi r11,r11,288
			ctx.r11.s64 = ctx.r11.s64 + 288;
			// lfs f0,928(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 928);
			ctx.f0.f64 = double(temp.f32);
			// addi r10,r1,80
			ctx.r10.s64 = ctx.r1.s64 + 80;
			// addi r9,r1,96
			ctx.r9.s64 = ctx.r1.s64 + 96;
			// lvx128 v0,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v13,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v12,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vmsum3fp128 v13,v0,v13
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
			// vmsum3fp128 v12,v0,v12
			simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
			// stvx v13,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v12,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lfs f13,80(r1)
			ctx.fpscr.disableFlushModeUnconditional();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f13.f64 = double(temp.f32);
			// lfs f12,96(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			ctx.f12.f64 = double(temp.f32);
			// fsubs f11,f13,f12
			ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
			// fmuls f10,f11,f0
			ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
			// stfs f10,768(r31)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(var_r31 + 768, temp.u32);
			// b 0x8225218c
		} else {
		loc_8225217C:
			// bl 0x82251618
			phJoint_1618_g(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82251dd8
			phJoint3Dof_1DD8(ctx, base);
			// stb r3,746(r31)
			PPC_STORE_U8(var_r31 + 746, ctx.r3.u8);
		}
	loc_8225218C:
		// li r7,0
		ctx.r7.s64 = 0;
		// lbz r8,746(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 746);
		// li r3,2
		ctx.r3.s64 = 2;
		// cmplwi cr6,r8,0
		// stb r7,747(r31)
		PPC_STORE_U8(var_r31 + 747, ctx.r7.u8);
		// bne cr6,0x822522cc
		if (ctx.r8.u32 != 0) {
			// blr
			return;
		}
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_822521BC:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822518c0
	phBoundCapsule_18C0_g(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82251dd8
	phJoint3Dof_1DD8(ctx, base);
	// clrlwi r6,r3,24
	ctx.r6.u64 = ctx.r3.u32 & 0xFF;
	// stb r3,746(r31)
	PPC_STORE_U8(var_r31 + 746, ctx.r3.u8);
	// li r3,2
	ctx.r3.s64 = 2;
	// cmplwi cr6,r6,0
	// bne cr6,0x822521e4
	if (ctx.r6.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_822521E4:
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r5,r31,896
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 896;
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r4,r10,288
	ctx.r4.s64 = ctx.r10.s64 + 288;
	// lfs f13,776(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 776);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,288
	ctx.r11.s64 = ctx.r11.s64 + 288;
	// lfs f0,740(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 740);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// fsubs f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f9,928(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f9.f64 = double(temp.f32);
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v0,v12
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f12,-12016(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f11,f12
	ctx.cr6.compare(ctx.f11.f64, ctx.f12.f64);
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// fmuls f5,f6,f9
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// stfs f5,768(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r31 + 768, temp.u32);
	// blt cr6,0x82252280
	if (!(ctx.cr6.lt)) {
		// bso cr6,0x82252280
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82252250, "bso");
		// li r8,1
		ctx.r8.s64 = 1;
		// lfs f4,736(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 736);
		ctx.f4.f64 = double(temp.f32);
		// stfs f4,756(r31)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(var_r31 + 756, temp.u32);
		// addi r3,r3,1
		ctx.r3.s64 = ctx.r3.s64 + 1;
		// stfs f0,760(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 760, temp.u32);
		// stb r8,747(r31)
		PPC_STORE_U8(var_r31 + 747, ctx.r8.u8);
		// blr
		return;
	}
loc_82252280:
	// fadds f13,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// fcmpu cr6,f13,f12
	// bgt cr6,0x822522c4
	if (ctx.f13.f64 <= ctx.f12.f64) {
		// bso cr6,0x822522c4
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8225228C, "bso");
		// li r7,1
		ctx.r7.s64 = 1;
		// lfs f3,736(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 736);
		ctx.f3.f64 = double(temp.f32);
		// fneg f2,f0
		ctx.f2.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// addi r3,r3,1
		ctx.r3.s64 = ctx.r3.s64 + 1;
		// fneg f1,f3
		ctx.f1.u64 = ctx.f3.u64 ^ 0x8000000000000000;
		// stfs f2,760(r31)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(var_r31 + 760, temp.u32);
		// stfs f1,756(r31)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r31 + 756, temp.u32);
		// stb r7,747(r31)
		PPC_STORE_U8(var_r31 + 747, ctx.r7.u8);
		// blr
		return;
	}
loc_822522C4:
	// li r6,0
	ctx.r6.s64 = 0;
	// stb r6,747(r31)
	PPC_STORE_U8(var_r31 + 747, ctx.r6.u8);
loc_822522CC:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_4"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_4);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_4) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lbz r11,746(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 746);
	// cmplwi cr6,r11,0
	// li r11,2
	ctx.r11.s64 = 2;
	// bne cr6,0x822522f4
	if (ctx.r11.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_822522F4:
	// cmpw cr6,r4,r11
	// bne cr6,0x82252378
	if (ctx.r4.s32 == ctx.r11.s32) {
		// lis r11,-32253
		// lfs f13,760(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 760);
		ctx.f13.f64 = double(temp.f32);
		// lfs f0,-12016(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// ble cr6,0x8225233c
		if (ctx.f13.f64 > ctx.f0.f64) {
			// li r10,2
			ctx.r10.s64 = 2;
			// stw r10,0(r5)
			PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
			// lfs f12,776(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 776);
			ctx.f12.f64 = double(temp.f32);
			// stfs f12,0(r6)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
			// lfs f11,768(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 768);
			ctx.f11.f64 = double(temp.f32);
			// stfs f11,0(r7)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
			// lfs f10,756(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 756);
			ctx.f10.f64 = double(temp.f32);
			// stfs f10,0(r8)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// lfs f9,760(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 760);
			ctx.f9.f64 = double(temp.f32);
			// stfs f9,0(r9)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
			// blr
			return;
		}
	loc_8225233C:
		// li r4,3
		ctx.r4.s64 = 3;
		// stw r4,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r4.u32);
		// lfs f8,776(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 776);
		ctx.f8.f64 = double(temp.f32);
		// fneg f7,f8
		ctx.f7.u64 = ctx.f8.u64 ^ 0x8000000000000000;
		// stfs f7,0(r6)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// lfs f6,768(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 768);
		ctx.f6.f64 = double(temp.f32);
		// fneg f5,f6
		ctx.f5.u64 = ctx.f6.u64 ^ 0x8000000000000000;
		// stfs f5,0(r7)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// lfs f4,756(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 756);
		ctx.f4.f64 = double(temp.f32);
		// fneg f3,f4
		ctx.f3.u64 = ctx.f4.u64 ^ 0x8000000000000000;
		// stfs f3,0(r8)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// lfs f2,760(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 760);
		ctx.f2.f64 = double(temp.f32);
		// fneg f1,f2
		ctx.f1.u64 = ctx.f2.u64 ^ 0x8000000000000000;
		// stfs f1,0(r9)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// blr
		return;
	}
loc_82252378:
	// cmpwi cr6,r4,0
	// bne cr6,0x822523ac
	if (ctx.r4.s32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
		// lfs f0,772(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 772);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r6)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// lfs f13,764(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 764);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,0(r7)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// lfs f12,748(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 748);
		ctx.f12.f64 = double(temp.f32);
		// stfs f12,0(r8)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// lfs f11,752(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 752);
		ctx.f11.f64 = double(temp.f32);
		// stfs f11,0(r9)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// blr
		return;
	}
loc_822523AC:
	// lis r10,-32253
	// addi r11,r3,912
	ctx.r11.s64 = ctx.r3.s64 + 912;
	// addi r4,r1,-32
	ctx.r4.s64 = ctx.r1.s64 + -32;
	// lfs f0,-12016(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// stfs f0,0(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r3,r1,-16
	ctx.r3.s64 = ctx.r1.s64 + -16;
	// addi r6,r6,288
	ctx.r6.s64 = ctx.r6.s64 + 288;
	// addi r5,r10,288
	ctx.r5.s64 = ctx.r10.s64 + 288;
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f13,f10,f9
	ctx.f13.f64 = double(float(ctx.f10.f64 - ctx.f9.f64));
	// stfs f13,0(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// fcmpu cr6,f13,f0
	// bge cr6,0x82252430
	if (ctx.f13.f64 < ctx.f0.f64) {
		// lis r10,-32160
		// fneg f8,f13
		ctx.f8.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// stfs f8,0(r7)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// addi r10,r10,26448
		ctx.r10.s64 = ctx.r10.s64 + 26448;
		// lvx128 v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v10,v0,v11
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvx v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_82252430:
	// stfs f0,0(r8)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f0,0(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_5"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_5);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// cmplwi cr6,r6,3
	// bgt cr6,0x822524f8
	if (ctx.r6.u32 > 3) {
		// blr
		return;
	}
	// lis r12,-32219
	// addi r12,r12,9336
	ctx.r12.s64 = ctx.r12.s64 + 9336;
	// rlwinm r0,r6,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r6.u64) {
	case 0:
		// addi r5,r11,880
		ctx.r5.s64 = ctx.r11.s64 + 880;
		// bl 0x822574f0
		phJoint1Dof_74F0_v12(ctx, base);
		// blr
		return;
	case 1:
		// addi r5,r11,912
		ctx.r5.s64 = ctx.r11.s64 + 912;
		// bl 0x822574f0
		phJoint1Dof_74F0_v12(ctx, base);
		// blr
		return;
	case 2:
		// addi r5,r11,896
		ctx.r5.s64 = ctx.r11.s64 + 896;
		// bl 0x822574f0
		phJoint1Dof_74F0_v12(ctx, base);
		// blr
		return;
	case 3:
		goto loc_822524D0;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82252488:
	// addi r5,r11,880
	ctx.r5.s64 = ctx.r11.s64 + 880;
	// bl 0x822574f0
	phJoint1Dof_74F0_v12(ctx, base);
	// blr
	return;
loc_822524D0:
	// addi r10,r11,896
	ctx.r10.s64 = ctx.r11.s64 + 896;
	// lis r11,-32160
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822574f0
	phJoint1Dof_74F0_v12(ctx, base);
loc_822524F8:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_6"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_6);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// cmplwi cr6,r6,3
	// bgt cr6,0x822526bc
	if (ctx.r6.u32 > 3) {
		// lis r11,-32253
		ctx.r11.s64 = -2113732608;
		// lfs f1,-12016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
		ctx.f1.f64 = double(temp.f32);
		// blr
		return;
	}
	// lis r12,-32219
	// addi r12,r12,9512
	ctx.r12.s64 = ctx.r12.s64 + 9512;
	// rlwinm r0,r6,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r6.u64) {
	case 0:
		goto loc_82252538;
	case 1:
		goto loc_82252594;
	case 2:
		goto loc_822525F0;
	case 3:
		goto loc_82252654;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82252538:
	// addi r11,r5,42
	ctx.r11.s64 = ctx.r5.s64 + 42;
	// addi r10,r3,880
	ctx.r10.s64 = ctx.r3.s64 + 880;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r1,-16
	ctx.r5.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r9,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r4.u32);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r11,28(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r9,r7,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v12,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
loc_82252594:
	// addi r11,r5,42
	ctx.r11.s64 = ctx.r5.s64 + 42;
	// addi r10,r3,912
	ctx.r10.s64 = ctx.r3.s64 + 912;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r9,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r4.u32);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r11,28(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r9,r7,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v7,v8
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmsum3fp128 v6,v9,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v6,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
loc_822525F0:
	// addi r5,r5,42
	ctx.r5.s64 = ctx.r5.s64 + 42;
	// lfs f0,928(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 928);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r3,896
	ctx.r3.s64 = ctx.r3.s64 + 896;
	// rlwinm r11,r5,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// lvx128 v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r11,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r4.u32);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r11,28(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r9,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r9,r7,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// lvx128 v4,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v3,v4
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vmsum3fp128 v2,v5,v0
	simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v2,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// blr
	return;
loc_82252654:
	// addi r5,r5,42
	ctx.r5.s64 = ctx.r5.s64 + 42;
	// lfs f12,928(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 928);
	ctx.f12.f64 = double(temp.f32);
	// addi r3,r3,896
	ctx.r3.s64 = ctx.r3.s64 + 896;
	// rlwinm r11,r5,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// lvx128 v1,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r11,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r4.u32);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r11,28(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r9,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r9,r7,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// lvx128 v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v30,v31
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v31.f32)));
	// vmsum3fp128 v29,v1,v0
	simde_mm_store_ps(ctx.v29.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v29,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f11,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fneg f1,f10
	ctx.f1.u64 = ctx.f10.u64 ^ 0x8000000000000000;
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_1"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_1);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r4,3
	// bgt cr6,0x82252774
	if (ctx.r4.u32 > 3) {
		// lis r11,-32253
		ctx.r11.s64 = -2113732608;
		// lfs f1,-12016(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
		ctx.f1.f64 = double(temp.f32);
		// blr
		return;
	}
	// lis r12,-32219
	// addi r12,r12,9980
	ctx.r12.s64 = ctx.r12.s64 + 9980;
	// rlwinm r0,r4,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r4.u64) {
	case 0:
		// addi r4,r31,880
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 880;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8225ae38
		phJoint1Dof_AE38(ctx, base);
		// blr
		return;
	case 1:
		// addi r4,r31,912
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 912;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8225ae38
		phJoint1Dof_AE38(ctx, base);
		// blr
		return;
	case 2:
		goto loc_8225274C;
	case 3:
		goto loc_8225274C;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_8225270C:
	// addi r4,r31,880
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 880;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// blr
	return;
loc_8225274C:
	// addi r4,r31,896
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 896;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// lfs f0,928(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f0,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_7"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_7);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_7) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// cmplwi cr6,r4,3
	// bgtlr cr6
	if (ctx.r4.u32 > 3) return;
	// lis r12,-32219
	// addi r12,r12,10160
	ctx.r12.s64 = ctx.r12.s64 + 10160;
	// rlwinm r0,r4,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r4.u64) {
	case 0:
		goto loc_822527C0;
	case 1:
		goto loc_82252818;
	case 2:
		goto loc_82252870;
	case 3:
		goto loc_822528C8;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_822527C0:
	// stfs f1,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r10,r3,880
	ctx.r10.s64 = ctx.r3.s64 + 880;
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v11,v12,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
loc_82252818:
	// stfs f1,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r8,r3,912
	ctx.r8.s64 = ctx.r3.s64 + 912;
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lvx128 v8,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v5,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v4,v5,v0
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v4,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
loc_82252870:
	// stfs f1,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r6,r3,896
	ctx.r6.s64 = ctx.r3.s64 + 896;
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,-16
	ctx.r5.s64 = ctx.r1.s64 + -16;
	// lvx128 v3,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v3,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v1,v2,v0
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v1,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v30,v31,v0
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
loc_822528C8:
	// fneg f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r4,r3,896
	ctx.r4.s64 = ctx.r3.s64 + 896;
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v28,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lvx128 v29,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v29,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v29.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v27,v28,v0
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v27,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v26,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v25,v26,v0
	simde_mm_store_ps(ctx.v25.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v25,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_8"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_8);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// cmplwi cr6,r4,3
	// bgtlr cr6
	if (ctx.r4.u32 > 3) return;
	// lis r12,-32219
	// addi r12,r12,10568
	ctx.r12.s64 = ctx.r12.s64 + 10568;
	// rlwinm r0,r4,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r4.u64) {
	case 0:
		goto loc_82252958;
	case 1:
		goto loc_822529A4;
	case 2:
		goto loc_822529F0;
	case 3:
		goto loc_82252A48;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82252958:
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r3,880
	ctx.r11.s64 = ctx.r3.s64 + 880;
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// addi r9,r9,288
	ctx.r9.s64 = ctx.r9.s64 + 288;
	// addi r8,r10,288
	ctx.r8.s64 = ctx.r10.s64 + 288;
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// blr
	return;
loc_822529A4:
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r3,912
	ctx.r11.s64 = ctx.r3.s64 + 912;
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r4,r9,288
	ctx.r4.s64 = ctx.r9.s64 + 288;
	// addi r3,r10,288
	ctx.r3.s64 = ctx.r10.s64 + 288;
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v0,v12
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f11,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// stfs f9,0(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// blr
	return;
loc_822529F0:
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r3,896
	ctx.r11.s64 = ctx.r3.s64 + 896;
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// addi r9,r9,288
	ctx.r9.s64 = ctx.r9.s64 + 288;
	// addi r8,r10,288
	ctx.r8.s64 = ctx.r10.s64 + 288;
	// addi r6,r1,-32
	ctx.r6.s64 = ctx.r1.s64 + -32;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v9,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v8,v0,v12
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f0,f8,f7
	ctx.f0.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f6,928(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 928);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,0(r5)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// blr
	return;
loc_82252A48:
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r3,896
	ctx.r11.s64 = ctx.r3.s64 + 896;
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// addi r4,r9,288
	ctx.r4.s64 = ctx.r9.s64 + 288;
	// addi r10,r10,288
	ctx.r10.s64 = ctx.r10.s64 + 288;
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v6,v0,v12
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f4,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f0,f4,f3
	ctx.f0.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f2,928(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 928);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fneg f0,f1
	ctx.f0.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_9"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_9);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r10,8(r11)
	// bctrl
	VCALL(ctx.r3.u32, 2, ctx, base);  // phJoint3Dof::vfn_2 (unnamed)  // vtable slot 2 (byte +8)
	// cmplwi cr6,r29,3
	// bgt cr6,0x82252b60
	if (var_r29 > 3) {
		return;
	}
	// lis r12,-32219
	// addi r12,r12,10996
	ctx.r12.s64 = ctx.r12.s64 + 10996;
	// rlwinm r0,r29,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (var_r29) {
	case 0:
		// addi r9,r31,880
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 880;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	case 1:
		// addi r8,r31,912
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 912;
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	case 2:
		// addi r7,r31,896
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 896;
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	case 3:
		goto loc_82252B40;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82252B04:
	// addi r9,r31,880
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 880;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
loc_82252B40:
	// lis r11,-32160
	// addi r6,r31,896
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 896;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v11,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_82252B60:
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_10"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_10);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmplwi cr6,r4,3
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bgtlr cr6
	if (ctx.r4.u32 > 3) return;
	// lis r12,-32219
	// addi r12,r12,11156
	ctx.r12.s64 = ctx.r12.s64 + 11156;
	// rlwinm r0,r4,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r4.u64) {
	case 0:
		// addi r11,r3,880
		ctx.r11.s64 = ctx.r3.s64 + 880;
		// addi r10,r5,16
		ctx.r10.s64 = ctx.r5.s64 + 16;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	case 1:
		// addi r9,r3,912
		ctx.r9.s64 = ctx.r3.s64 + 912;
		// addi r8,r5,16
		ctx.r8.s64 = ctx.r5.s64 + 16;
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	case 2:
		// addi r7,r3,896
		ctx.r7.s64 = ctx.r3.s64 + 896;
		// addi r6,r5,16
		ctx.r6.s64 = ctx.r5.s64 + 16;
		// lvx128 v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	case 3:
		goto loc_82252BE0;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82252BA4:
	// addi r11,r3,880
	ctx.r11.s64 = ctx.r3.s64 + 880;
	// addi r10,r5,16
	ctx.r10.s64 = ctx.r5.s64 + 16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
loc_82252BE0:
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lis r10,-32160
	// addi r5,r3,896
	ctx.r5.s64 = ctx.r3.s64 + 896;
	// addi r10,r10,26448
	ctx.r10.s64 = ctx.r10.s64 + 26448;
	// lvx128 v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v8,v0,v9
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_11"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_11);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	double var_f27 = 0.0;
	double var_f26 = 0.0;
	double var_f28 = 0.0;
	double var_f25 = 0.0;
	double var_f24 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// addi r12,r1,-24
	ctx.r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82436608
	__savefpr_24(ctx, base);
	// stwu r1,-480(r1)
	ea = -480 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	var_f29 = ctx.f1.f64;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r30,r11,-12024
	var_r30 = (uint32_t)(ctx.r11.s64 + -12024);  // lbl_8202D108 @ 0x8202d108
	// li r9,0
	ctx.r9.s64 = 0;
	// lbz r10,745(r31)
	ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 745);
	// lfs f31,8(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	var_f31 = double(temp.f32);
	// fmr f27,f31
	var_f27 = var_f31;
	// stw r9,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r9.u32);
	// fmr f26,f31
	var_f26 = var_f31;
	// stfs f27,268(r1)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r1.u32 + 268, temp.u32);
	// fmr f28,f31
	var_f28 = var_f31;
	// stfs f26,280(r1)
	temp.f32 = float(var_f26);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// stfs f28,260(r1)
	temp.f32 = float(var_f28);
	PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
	// cmplwi cr6,r10,0
	// bne cr6,0x82252c90
	if (ctx.r10.u32 == 0) {
		// lwz r8,20(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 20);
		// li r11,1
		ctx.r11.s64 = 1;
		// cmpwi cr6,r8,0
		// bne cr6,0x82252c80
		if (ctx.r8.s32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82252C80:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// li r11,0
		ctx.r11.s64 = 0;
		// cmplwi cr6,r6,0
		// beq cr6,0x82252c94
		if (ctx.r6.u32 == 0) goto loc_82252C94;
	}
loc_82252C90:
	// li r11,1
	ctx.r11.s64 = 1;
loc_82252C94:
	// clrlwi r4,r11,24
	ctx.r4.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r4,0
	// bne cr6,0x82252d24
	if (ctx.r4.u32 == 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82251618
		phJoint_1618_g(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82251dd8
		phJoint3Dof_1DD8(ctx, base);
		// clrlwi r11,r3,24
		ctx.r11.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// beq cr6,0x82252cf8
		if (ctx.r11.u32 != 0) {
			// addi r3,r31,880
			ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 880;
			// lfs f0,772(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 772);
			ctx.f0.f64 = double(temp.f32);
			// addi r10,r31,896
			ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 896;
			// lfs f13,748(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 748);
			ctx.f13.f64 = double(temp.f32);
			// addi r9,r1,160
			ctx.r9.s64 = ctx.r1.s64 + 160;
			// lfs f11,752(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 752);
			ctx.f11.f64 = double(temp.f32);
			// addi r8,r1,176
			ctx.r8.s64 = ctx.r1.s64 + 176;
			// lfs f12,764(r31)
			temp.u32 = PPC_LOAD_U32(var_r31 + 764);
			ctx.f12.f64 = double(temp.f32);
			// fsubs f9,f13,f0
			ctx.f9.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
			// lvx128 v0,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// fsubs f10,f11,f0
			ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
			// lvx128 v13,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v0,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v13,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// b 0x82252d14
		} else {
		loc_82252CF8:
			// addi r7,r1,160
			ctx.r7.s64 = ctx.r1.s64 + 160;
			// lfs f12,276(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
			ctx.f12.f64 = double(temp.f32);
			// addi r6,r1,176
			ctx.r6.s64 = ctx.r1.s64 + 176;
			// lfs f10,264(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
			ctx.f10.f64 = double(temp.f32);
			// lfs f9,256(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
			ctx.f9.f64 = double(temp.f32);
			// lvx128 v0,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v13,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_82252D14:
		// cmplwi cr6,r11,0
		// beq cr6,0x82252da0
		if (ctx.r11.u32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x82253338
			// addi r1,r1,480
			ctx.r1.s64 = ctx.r1.s64 + 480;
			// addi r12,r1,-24
			ctx.r12.s64 = ctx.r1.s64 + -24;
			// bl 0x82436654
			__restfpr_24(ctx, base);
			// lwz r12,-8(r1)
			ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
			// mtlr r12
			ctx.lr = ctx.r12.u64;
			// ld r30,-24(r1)
			var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
			// ld r31,-16(r1)
			var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
			// blr
			return;
		}
		// li r9,1
		ctx.r9.s64 = 1;
		// b 0x82252dd0
		goto loc_82252DD0;
	}
loc_82252D24:
	// addi r5,r1,260
	ctx.r5.s64 = ctx.r1.s64 + 260;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r3,r1,280
	ctx.r3.s64 = ctx.r1.s64 + 280;
	// addi r11,r1,268
	ctx.r11.s64 = ctx.r1.s64 + 268;
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// stw r5,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
	// addi r9,r1,145
	ctx.r9.s64 = ctx.r1.s64 + 145;
	// stw r4,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r4.u32);
	// addi r8,r1,276
	ctx.r8.s64 = ctx.r1.s64 + 276;
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// addi r6,r1,264
	ctx.r6.s64 = ctx.r1.s64 + 264;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// addi r5,r1,256
	ctx.r5.s64 = ctx.r1.s64 + 256;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82251fa8
	phJoint3Dof_1FA8_wrh(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	// beq cr6,0x82252da0
	if (ctx.r10.u32 != 0) {
		// lbz r8,144(r1)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 144);
		// li r9,2
		ctx.r9.s64 = 2;
		// cmplwi cr6,r8,0
		// bne cr6,0x82252d88
		if (ctx.r8.u32 == 0) {
			// li r9,0
			ctx.r9.s64 = 0;
		}
	loc_82252D88:
		// lbz r6,145(r1)
		ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + 145);
		// cmplwi cr6,r6,0
		// beq cr6,0x82252d98
		if (ctx.r6.u32 != 0) {
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
		}
	loc_82252D98:
		// cmpwi cr6,r9,0
		// bne cr6,0x82252da8
		if (ctx.r9.s32 != 0) goto loc_82252DA8;
	}
loc_82252DA0:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82253338
	// addi r1,r1,480
	ctx.r1.s64 = ctx.r1.s64 + 480;
	// addi r12,r1,-24
	ctx.r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82436654
	__restfpr_24(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
loc_82252DA8:
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// lfs f27,268(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	var_f27 = double(temp.f32);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// lfs f26,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	var_f26 = double(temp.f32);
	// lfs f28,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	var_f28 = double(temp.f32);
	// lfs f12,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f10.f64 = double(temp.f32);
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f9,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f9.f64 = double(temp.f32);
loc_82252DD0:
	// lis r10,-32248
	// lfs f7,128(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 128);
	ctx.f7.f64 = double(temp.f32);
	// lis r11,-32164
	// lfs f25,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phJoint3Dof::vtable@+0x0 */;
	var_f25 = double(temp.f32);
	// rlwinm r3,r9,0,30,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	// fmr f24,f31
	var_f24 = var_f31;
	// fmr f30,f31
	var_f30 = var_f31;
	// cmpwi cr6,r3,0
	// lfd f6,-25856(r10)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25856);
	// lfs f8,22580(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22580);
	ctx.f8.f64 = double(temp.f32);
	// beq cr6,0x82252ecc
	if (ctx.r3.s32 != 0) {
		// addi r7,r1,208
		ctx.r7.s64 = ctx.r1.s64 + 208;
		// vpermwi128 v11,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vpermwi128 v12,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// lwz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
		// vpermwi128 v9,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// lwz r10,28(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
		// vpermwi128 v10,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// addi r8,r11,288
		ctx.r8.s64 = ctx.r11.s64 + 288;
		// addi r10,r10,288
		ctx.r10.s64 = ctx.r10.s64 + 288;
		// fmr f11,f25
		ctx.f11.f64 = var_f25;
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v0,v12,v11
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
		// addi r6,r1,208
		ctx.r6.s64 = ctx.r1.s64 + 208;
		// fcmpu cr6,f12,f31
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.cr6.compare(ctx.f12.f64, var_f31);
		// addi r5,r1,192
		ctx.r5.s64 = ctx.r1.s64 + 192;
		// addi r4,r1,128
		ctx.r4.s64 = ctx.r1.s64 + 128;
		// li r11,0
		ctx.r11.s64 = 0;
		// vnmsubfp v0,v10,v9,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v12,v0,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
		// vmsum3fp128 v11,v0,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f5,192(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f5.f64 = double(temp.f32);
		// lfs f4,128(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f4.f64 = double(temp.f32);
		// fsubs f24,f5,f4
		var_f24 = double(float(ctx.f5.f64 - ctx.f4.f64));
		// ble cr6,0x82252e84
		if (ctx.cr6.gt) {
			// fmuls f3,f12,f29
			ctx.f3.f64 = double(float(ctx.f12.f64 * var_f29));
			// fmuls f13,f3,f31
			ctx.f13.f64 = double(float(ctx.f3.f64 * var_f31));
			// fsubs f0,f9,f13
			ctx.f0.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
			// fsubs f13,f10,f13
			ctx.f13.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
			// b 0x82252e98
		} else {
		loc_82252E84:
			// fmuls f13,f12,f29
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = double(float(ctx.f12.f64 * var_f29));
			// li r11,1
			ctx.r11.s64 = 1;
			// fmr f11,f31
			ctx.f11.f64 = var_f31;
			// fadds f0,f13,f9
			ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
			// fadds f13,f13,f10
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f10.f64));
		}
	loc_82252E98:
		// fcmpu cr6,f13,f31
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x82252eb8
		if (ctx.f13.f64 <= var_f31) {
			// bso cr6,0x82252eb8
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82252EA0, "bso");
			// clrlwi r3,r11,24
			ctx.r3.u64 = ctx.r11.u32 & 0xFF;
			// fmr f13,f31
			ctx.f13.f64 = var_f31;
			// cmplwi cr6,r3,0
			// beq cr6,0x82252eb8
			if (ctx.r3.u32 == 0) goto loc_82252EB8;
			// clrlwi r9,r9,31
			ctx.r9.u64 = ctx.r9.u32 & 0x1;
		}
	loc_82252EB8:
		// fmuls f2,f11,f12
		ctx.fpscr.disableFlushMode();
		ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
		// fsel f0,f0,f0,f6
		ctx.f0.f64 = ctx.f0.f64 >= 0.0 ? ctx.f0.f64 : ctx.f6.f64;
		// fneg f1,f2
		ctx.f1.u64 = ctx.f2.u64 ^ 0x8000000000000000;
		// fnmsubs f0,f0,f8,f1
		ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f8.f64 - ctx.f1.f64)));
		// fnmsubs f30,f13,f7,f0
		var_f30 = double(float(-(ctx.f13.f64 * ctx.f7.f64 - ctx.f0.f64)));
	}
loc_82252ECC:
	// clrlwi r11,r9,31
	ctx.r11.u64 = ctx.r9.u32 & 0x1;
	// cmpwi cr6,r11,0
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// addi r30,r11,26448
	var_r30 = (uint32_t)(ctx.r11.s64 + 26448);  // lbl_82606750 @ 0x82606750
	// beq cr6,0x82252f78
	if (ctx.r11.s32 != 0) {
		// lwz r10,272(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
		// li r11,0
		ctx.r11.s64 = 0;
		// cmpwi cr6,r10,-1
		// bne cr6,0x82252f0c
		if (ctx.r10.s32 == -1) {
			// lvx128 v0,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,176
			ctx.r8.s64 = ctx.r1.s64 + 176;
			// vsubfp v10,v0,v13
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
			// fneg f27,f27
			ctx.fpscr.disableFlushModeUnconditional();
			ctx.f27.u64 = ctx.f27.u64 ^ 0x8000000000000000;
			// fneg f26,f26
			ctx.f26.u64 = ctx.f26.u64 ^ 0x8000000000000000;
			// fneg f28,f28
			ctx.f28.u64 = ctx.f28.u64 ^ 0x8000000000000000;
			// stvx v10,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_82252F0C:
		// fmr f12,f25
		ctx.fpscr.disableFlushMode();
		ctx.f12.f64 = var_f25;
		// fcmpu cr6,f28,f31
		// fmuls f13,f28,f29
		ctx.f13.f64 = double(float(var_f28 * var_f29));
		// ble cr6,0x82252f2c
		if (var_f28 > var_f31) {
			// fmuls f13,f13,f31
			ctx.f13.f64 = double(float(ctx.f13.f64 * var_f31));
			// fsubs f0,f27,f13
			ctx.f0.f64 = double(float(var_f27 - ctx.f13.f64));
			// fsubs f13,f26,f13
			ctx.f13.f64 = double(float(var_f26 - ctx.f13.f64));
			// b 0x82252f3c
		} else {
		loc_82252F2C:
			// fadds f0,f13,f27
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(ctx.f13.f64 + var_f27));
			// li r11,1
			ctx.r11.s64 = 1;
			// fmr f12,f31
			ctx.f12.f64 = var_f31;
			// fadds f13,f13,f26
			ctx.f13.f64 = double(float(ctx.f13.f64 + var_f26));
		}
	loc_82252F3C:
		// fcmpu cr6,f13,f31
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x82252f5c
		if (ctx.f13.f64 <= var_f31) {
			// bso cr6,0x82252f5c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82252F44, "bso");
			// clrlwi r7,r11,24
			ctx.r7.u64 = ctx.r11.u32 & 0xFF;
			// fmr f13,f31
			ctx.f13.f64 = var_f31;
			// cmplwi cr6,r7,0
			// beq cr6,0x82252f5c
			if (ctx.r7.u32 == 0) goto loc_82252F5C;
			// rlwinm r9,r9,0,30,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
		}
	loc_82252F5C:
		// fmuls f10,f12,f28
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = double(float(ctx.f12.f64 * var_f28));
		// lfs f11,928(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 928);
		ctx.f11.f64 = double(temp.f32);
		// fsel f0,f0,f0,f6
		ctx.f0.f64 = ctx.f0.f64 >= 0.0 ? ctx.f0.f64 : ctx.f6.f64;
		// fneg f9,f10
		ctx.f9.u64 = ctx.f10.u64 ^ 0x8000000000000000;
		// fnmsubs f8,f0,f8,f9
		ctx.f8.f64 = double(float(-(ctx.f0.f64 * ctx.f8.f64 - ctx.f9.f64)));
		// fnmsubs f7,f13,f7,f8
		ctx.f7.f64 = double(float(-(ctx.f13.f64 * ctx.f7.f64 - ctx.f8.f64)));
		// fdivs f31,f7,f11
		var_f31 = double(float(ctx.f7.f64 / ctx.f11.f64));
	}
loc_82252F78:
	// cmplwi cr6,r9,3
	// bgt cr6,0x822532f4
	if (!(ctx.r9.u32 > 3)) {
		// lis r12,-32219
		// addi r12,r12,12184
		ctx.r12.s64 = ctx.r12.s64 + 12184;
		// rlwinm r0,r9,2,0,29
		ctx.r0.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r0,r12,r0
		ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
		// mtctr r0
		ctx.ctr.u64 = ctx.r0.u64;
		// bctr
		switch (ctx.r9.u64) {
		case 0:
		goto loc_82252FA8;
		case 1:
		goto loc_82252FC0;
		case 2:
		goto loc_82253004;
		case 3:
		goto loc_82253100;
		default:
		__builtin_trap(); // Switch case out of range
		}
		loc_82252FA8:
		// addi r6,r1,112
		ctx.r6.s64 = ctx.r1.s64 + 112;
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
		// stvx v9,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	} else {
		loc_82252FC0:
		// addi r4,r1,176
		ctx.r4.s64 = ctx.r1.s64 + 176;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8225ae38
		phJoint1Dof_AE38(ctx, base);
		// fdivs f0,f25,f1
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = double(float(var_f25 / ctx.f1.f64));
		// addi r4,r1,176
		ctx.r4.s64 = ctx.r1.s64 + 176;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// lvx128 v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// fmuls f0,f0,f31
		ctx.f0.f64 = double(float(ctx.f0.f64 * var_f31));
		// stfs f0,128(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// addi r11,r1,128
		ctx.r11.s64 = ctx.r1.s64 + 128;
		// lvx128 v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vmulfp128 v7,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		loc_82253004:
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// addi r4,r1,160
		ctx.r4.s64 = ctx.r1.s64 + 160;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8225ac88
		phJoint1Dof_AC88_p39(ctx, base);
		// addi r9,r1,112
		ctx.r9.s64 = ctx.r1.s64 + 112;
		// addi r8,r1,160
		ctx.r8.s64 = ctx.r1.s64 + 160;
		// addi r7,r1,208
		ctx.r7.s64 = ctx.r1.s64 + 208;
		// addi r6,r1,128
		ctx.r6.s64 = ctx.r1.s64 + 128;
		// addi r11,r1,192
		ctx.r11.s64 = ctx.r1.s64 + 192;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lvx128 v6,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,208
		ctx.r4.s64 = ctx.r1.s64 + 208;
		// lvx128 v5,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v6,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v0,v5,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x8225ac88
		phJoint1Dof_AC88_p39(ctx, base);
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// lfs f13,128(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f13.f64 = double(temp.f32);
		// addi r9,r1,160
		ctx.r9.s64 = ctx.r1.s64 + 160;
		// lfs f12,192(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f12.f64 = double(temp.f32);
		// addi r8,r1,208
		ctx.r8.s64 = ctx.r1.s64 + 208;
		// fneg f0,f24
		ctx.f0.u64 = ctx.f24.u64 ^ 0x8000000000000000;
		// addi r6,r1,128
		ctx.r6.s64 = ctx.r1.s64 + 128;
		// fmuls f6,f12,f30
		ctx.f6.f64 = double(float(ctx.f12.f64 * var_f30));
		// addi r5,r1,192
		ctx.r5.s64 = ctx.r1.s64 + 192;
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,112
		ctx.r7.s64 = ctx.r1.s64 + 112;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v4,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// lvx128 v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v3,v12,v13
		simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,112
		ctx.r11.s64 = ctx.r1.s64 + 112;
		// fmsubs f5,f13,f0,f6
		ctx.fpscr.disableFlushModeUnconditional();
		ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 - ctx.f6.f64));
		// stvx v4,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v3,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f11,128(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f4,f11,f12
		ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
		// lfs f12,192(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f3,f11,f0
		ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// fmsubs f2,f12,f13,f4
		ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 - ctx.f4.f64));
		// fmsubs f1,f12,f30,f3
		ctx.f1.f64 = double(float(ctx.f12.f64 * var_f30 - ctx.f3.f64));
		// fdivs f0,f25,f2
		ctx.f0.f64 = double(float(var_f25 / ctx.f2.f64));
		// fmuls f13,f5,f0
		ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
		// stfs f13,192(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
		// fmuls f0,f1,f0
		ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
		// stfs f0,128(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// addi r3,r1,192
		ctx.r3.s64 = ctx.r1.s64 + 192;
		// addi r4,r1,128
		ctx.r4.s64 = ctx.r1.s64 + 128;
		// lvx128 v1,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v2,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v11,v1,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.u32), 0xFF));
		// vspltw v13,v2,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
		// vmulfp128 v0,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmaddfp v31,v12,v11,v0
		simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// stvx v31,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		loc_82253100:
		// addi r5,r1,240
		ctx.r5.s64 = ctx.r1.s64 + 240;
		// addi r4,r1,160
		ctx.r4.s64 = ctx.r1.s64 + 160;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8225ac88
		phJoint1Dof_AC88_p39(ctx, base);
		// addi r9,r1,240
		ctx.r9.s64 = ctx.r1.s64 + 240;
		// addi r8,r1,160
		ctx.r8.s64 = ctx.r1.s64 + 160;
		// addi r7,r1,208
		ctx.r7.s64 = ctx.r1.s64 + 208;
		// addi r6,r1,176
		ctx.r6.s64 = ctx.r1.s64 + 176;
		// addi r11,r1,128
		ctx.r11.s64 = ctx.r1.s64 + 128;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,192
		ctx.r10.s64 = ctx.r1.s64 + 192;
		// lvx128 v30,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,224
		ctx.r9.s64 = ctx.r1.s64 + 224;
		// lvx128 v29,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v30,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v28,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v12,v29,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v0,v28,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// addi r5,r1,240
		ctx.r5.s64 = ctx.r1.s64 + 240;
		// addi r4,r1,208
		ctx.r4.s64 = ctx.r1.s64 + 208;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,128(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,192(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,224(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
		ctx.f12.f64 = double(temp.f32);
		// stfs f0,320(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 320, temp.u32);
		// stfs f13,336(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
		// stfs f12,352(r1)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r1.u32 + 352, temp.u32);
		// bl 0x8225ac88
		phJoint1Dof_AC88_p39(ctx, base);
		// addi r8,r1,240
		ctx.r8.s64 = ctx.r1.s64 + 240;
		// addi r7,r1,160
		ctx.r7.s64 = ctx.r1.s64 + 160;
		// addi r6,r1,208
		ctx.r6.s64 = ctx.r1.s64 + 208;
		// addi r11,r1,176
		ctx.r11.s64 = ctx.r1.s64 + 176;
		// addi r10,r1,224
		ctx.r10.s64 = ctx.r1.s64 + 224;
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,128
		ctx.r9.s64 = ctx.r1.s64 + 128;
		// lvx128 v27,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,192
		ctx.r8.s64 = ctx.r1.s64 + 192;
		// lvx128 v26,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v13,v27,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v25,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v12,v26,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v0,v25,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// addi r5,r1,240
		ctx.r5.s64 = ctx.r1.s64 + 240;
		// addi r4,r1,176
		ctx.r4.s64 = ctx.r1.s64 + 176;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f11,224(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,128(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,192(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f9.f64 = double(temp.f32);
		// stfs f11,324(r1)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
		// stfs f10,340(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
		// stfs f9,356(r1)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r1.u32 + 356, temp.u32);
		// bl 0x8225ac88
		phJoint1Dof_AC88_p39(ctx, base);
		// addi r7,r1,240
		ctx.r7.s64 = ctx.r1.s64 + 240;
		// fneg f8,f24
		ctx.fpscr.disableFlushMode();
		ctx.f8.u64 = ctx.f24.u64 ^ 0x8000000000000000;
		// addi r6,r1,160
		ctx.r6.s64 = ctx.r1.s64 + 160;
		// stfs f30,288(r1)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
		// addi r11,r1,208
		ctx.r11.s64 = ctx.r1.s64 + 208;
		// stfs f8,292(r1)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
		// addi r10,r1,176
		ctx.r10.s64 = ctx.r1.s64 + 176;
		// stfs f31,296(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
		// addi r9,r1,224
		ctx.r9.s64 = ctx.r1.s64 + 224;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,128
		ctx.r8.s64 = ctx.r1.s64 + 128;
		// lvx128 v24,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,192
		ctx.r7.s64 = ctx.r1.s64 + 192;
		// lvx128 v23,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v21,v24,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v21.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v24.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// vmsum3fp128 v20,v23,v0
		simde_mm_store_ps(ctx.v20.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v23.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// lvx128 v22,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,304
		ctx.r5.s64 = ctx.r1.s64 + 304;
		// vmsum3fp128 v19,v22,v0
		simde_mm_store_ps(ctx.v19.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v22.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// addi r4,r1,288
		ctx.r4.s64 = ctx.r1.s64 + 288;
		// addi r3,r1,320
		ctx.r3.s64 = ctx.r1.s64 + 320;
		// stvx v21,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v20,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v19,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f7,224(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
		ctx.f7.f64 = double(temp.f32);
		// lfs f6,128(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
		ctx.f6.f64 = double(temp.f32);
		// stfs f7,328(r1)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r1.u32 + 328, temp.u32);
		// stfs f6,344(r1)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r1.u32 + 344, temp.u32);
		// lfs f5,192(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f5.f64 = double(temp.f32);
		// stfs f5,360(r1)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
		// bl 0x822509b0
		phSimulator_09B0_v12(ctx, base);
		// addi r6,r1,304
		ctx.r6.s64 = ctx.r1.s64 + 304;
		// addi r5,r1,240
		ctx.r5.s64 = ctx.r1.s64 + 240;
		// addi r4,r1,160
		ctx.r4.s64 = ctx.r1.s64 + 160;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// addi r8,r1,208
		ctx.r8.s64 = ctx.r1.s64 + 208;
		// lvx128 v18,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,176
		ctx.r7.s64 = ctx.r1.s64 + 176;
		// stvx v18,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f4,240(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
		ctx.f4.f64 = double(temp.f32);
		// stfs f4,224(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
		// lfs f3,244(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
		ctx.f3.f64 = double(temp.f32);
		// stfs f3,128(r1)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// lfs f2,248(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
		ctx.f2.f64 = double(temp.f32);
		// lvx128 v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,112
		ctx.r9.s64 = ctx.r1.s64 + 112;
		// lvx128 v15,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v14,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,224
		ctx.r11.s64 = ctx.r1.s64 + 224;
		// addi r10,r1,128
		ctx.r10.s64 = ctx.r1.s64 + 128;
		// lvx128 v17,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stfs f2,224(r1)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
		// vspltw v13,v17,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v17.u32), 0xFF));
		// lvx128 v16,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v12,v16,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v16.u32), 0xFF));
		// vmulfp128 v0,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmaddfp v0,v15,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v15.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,112
		ctx.r4.s64 = ctx.r1.s64 + 112;
		// addi r6,r1,224
		ctx.r6.s64 = ctx.r1.s64 + 224;
		// lvx128 v63,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw128 v13,v63,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v63.u32), 0xFF));
		// vmaddfp v0,v14,v13,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v14.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_822532F4:
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v13,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v10
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v8,v9,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_82253338:
	// addi r1,r1,480
	ctx.r1.s64 = ctx.r1.s64 + 480;
	// addi r12,r1,-24
	ctx.r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82436654
	__restfpr_24(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_12"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_12);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// cmplwi cr6,r11,3
	// bgt cr6,0x8225344c
	if (ctx.r11.u32 > 3) {
		// blr
		return;
	}
	// lis r12,-32219
	// addi r12,r12,13212
	ctx.r12.s64 = ctx.r12.s64 + 13212;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		// addi r5,r31,880
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 880;
		// bl 0x82257bd0
		phJoint1Dof_7BD0_v12(ctx, base);
		// blr
		return;
	case 1:
		// addi r5,r31,912
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 912;
		// bl 0x82257bd0
		phJoint1Dof_7BD0_v12(ctx, base);
		// blr
		return;
	case 2:
		goto loc_822533E4;
	case 3:
		goto loc_82253420;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_822533AC:
	// addi r5,r31,880
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 880;
	// bl 0x82257bd0
	phJoint1Dof_7BD0_v12(ctx, base);
	// blr
	return;
loc_822533E4:
	// addi r5,r31,896
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 896;
	// bl 0x82257bd0
	phJoint1Dof_7BD0_v12(ctx, base);
	// lfs f0,928(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v11,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
loc_82253420:
	// addi r5,r31,896
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 896;
	// bl 0x82257bd0
	phJoint1Dof_7BD0_v12(ctx, base);
	// lfs f13,928(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f13
	ctx.f12.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvx128 v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vmulfp128 v8,v10,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_8225344C:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_18"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_18);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=208, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x822518c0
	phBoundCapsule_18C0_g(ctx, base);
	// lfs f0,944(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 944);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r31,912
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 912;
	// lfs f12,960(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 960);
	ctx.f12.f64 = double(temp.f32);
	// fneg f9,f0
	ctx.f9.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f11,976(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 976);
	ctx.f11.f64 = double(temp.f32);
	// fneg f7,f12
	ctx.f7.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// lfs f10,948(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 948);
	ctx.f10.f64 = double(temp.f32);
	// fneg f5,f11
	ctx.f5.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f8,964(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 964);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r31,880
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 880;
	// lfs f6,980(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 980);
	ctx.f6.f64 = double(temp.f32);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r7,r31,896
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 896;
	// stfs f9,112(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// stfs f8,84(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f7,116(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f5,120(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// vor v10,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v8,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r10,r10,288
	ctx.r10.s64 = ctx.r10.s64 + 288;
	// vmsum3fp128 v7,v13,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
	// addi r9,r9,288
	ctx.r9.s64 = ctx.r9.s64 + 288;
	// vmsum3fp128 v6,v13,v8
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
	// lfs f13,928(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r1,176
	ctx.r8.s64 = ctx.r1.s64 + 176;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v3,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v2,v0,v11
	simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmsum3fp128 v5,v10,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v4,v10,v11
	simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// stvx v7,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// stvx v5,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f4,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f0,f4,f3
	ctx.f0.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f0,764(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 764, temp.u32);
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v1,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v1.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v31,v0,v12
	simde_mm_store_ps(ctx.v31.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// lfs f2,128(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f2.f64 = double(temp.f32);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lfs f1,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f1.f64 = double(temp.f32);
	// fsubs f12,f2,f1
	ctx.f12.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
	// lfs f1,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f0,f1
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// stvx v31,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v1,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// fdivs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f13.f64));
	// fmuls f7,f1,f12
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// lfs f10,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// fmuls f11,f9,f13
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f13,772(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 772);
	ctx.f13.f64 = double(temp.f32);
	// stfs f11,768(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 768, temp.u32);
	// addi r11,r11,26432
	ctx.r11.s64 = ctx.r11.s64 + 26432;
	// fmsubs f10,f0,f2,f7
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f2.f64 - ctx.f7.f64));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmuls f0,f13,f1
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// fmadds f12,f2,f12,f8
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f12.f64 + ctx.f8.f64));
	// cmpwi cr6,r10,2
	// fmuls f13,f13,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x822535ec
	if (ctx.r10.s32 != 2) {
		// cmpwi cr6,r10,3
		// li r11,0
		ctx.r11.s64 = 0;
		// bne cr6,0x822535f0
		if (ctx.r10.s32 != 3) goto loc_822535F0;
	}
loc_822535EC:
	// li r11,1
	ctx.r11.s64 = 1;
loc_822535F0:
	// clrlwi r6,r11,24
	ctx.r6.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x82253694
	if (ctx.r6.u32 != 0) {
		// addi r5,r31,816
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 816;
		// lbz r4,745(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 745);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// lfs f6,776(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 776);
		ctx.f6.f64 = double(temp.f32);
		// cmplwi cr6,r4,0
		ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
		// lvx128 v30,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v30,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f5,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f5.f64 = double(temp.f32);
		// lfs f3,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f3.f64 = double(temp.f32);
		// fsubs f4,f5,f0
		ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
		// fsubs f0,f3,f13
		ctx.f0.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
		// lfs f13,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f13.f64 = double(temp.f32);
		// stfs f0,84(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// fsubs f0,f13,f6
		ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f6.f64));
		// stfs f4,80(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// stfs f0,88(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		// bne cr6,0x82253680
		if (ctx.cr6.eq) {
			// lis r11,-32248
			ctx.r11.s64 = -2113404928;
			// lfs f13,-25808(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25808);  /* glob:lbl_82079B30 @ 0x82079b30 */
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f0,f13
			// ble cr6,0x82253660
			if (ctx.f0.f64 > ctx.f13.f64) {
				// lis r11,-32253
				ctx.r11.s64 = -2113732608;
				// lfs f13,-16340(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16340);  /* glob:lbl_8202C02C @ 0x8202c02c */
				ctx.f13.f64 = double(temp.f32);
				// fsubs f0,f0,f13
				ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
				// b 0x8225367c
			} else {
			loc_82253660:
				// lis r11,-32248
				ctx.r11.s64 = -2113404928;
				// lfs f13,-25812(r11)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25812);  /* glob:lbl_82079B2C @ 0x82079b2c */
				ctx.f13.f64 = double(temp.f32);
				// fcmpu cr6,f0,f13
				// bge cr6,0x8225367c
				if (ctx.f0.f64 >= ctx.f13.f64) goto loc_8225367C;
				// lis r11,-32253
				ctx.r11.s64 = -2113732608;
				// lfs f13,-16340(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16340);  /* glob:lbl_8202C02C @ 0x8202c02c */
				ctx.f13.f64 = double(temp.f32);
				// fadds f0,f0,f13
				ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
			}
		loc_8225367C:
			// stfs f0,88(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		}
	loc_82253680:
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// addi r11,r31,784
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 784;
		// lvx128 v29,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v28,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v13,v29,v28
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v28.f32)));
	}
loc_82253694:
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r10,1
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x822536b0
	if (ctx.r10.s32 != 1) {
		// cmpwi cr6,r10,3
		// li r11,0
		ctx.r11.s64 = 0;
		// bne cr6,0x822536b4
		if (ctx.r10.s32 != 3) goto loc_822536B4;
	}
loc_822536B0:
	// li r11,1
	ctx.r11.s64 = 1;
loc_822536B4:
	// clrlwi r6,r11,24
	ctx.r6.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x82253708
	if (ctx.r6.u32 != 0) {
		// addi r5,r31,832
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 832;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r11,r31,800
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 800;
		// lvx128 v27,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v27,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f9,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f9.f64 = double(temp.f32);
		// lfs f7,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f7.f64 = double(temp.f32);
		// fsubs f8,f9,f12
		ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
		// lfs f5,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f5.f64 = double(temp.f32);
		// fsubs f6,f7,f10
		ctx.f6.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
		// fsubs f4,f5,f11
		ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f11.f64));
		// stfs f8,80(r1)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// stfs f6,84(r1)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
		// stfs f4,88(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// lvx128 v25,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v26,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v0,v26,v25
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v25.f32)));
	}
loc_82253708:
	// vaddfp v24,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r11,r1,176
	ctx.r11.s64 = ctx.r1.s64 + 176;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stvx v24,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82253780
	phJoint1Dof_3780_fw(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,352
	ctx.r11.s64 = ctx.r11.s64 + 352;
	// lvx128 v23,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v22,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v21,v22,v23
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v21.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v22.f32), simde_mm_load_ps(ctx.v23.f32)));
	// stvx v21,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v20,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// vsubfp v0,v0,v20
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v20.f32)));
	// addi r11,r11,352
	ctx.r11.s64 = ctx.r11.s64 + 352;
	// lvx128 v19,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v18,v19,v0
	simde_mm_store_ps(ctx.v18.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v19.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v18,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_3780_fw"))) PPC_WEAK_FUNC(phJoint1Dof_3780_fw);
PPC_FUNC_IMPL(__imp__phJoint1Dof_3780_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	double var_f24 = 0.0;
	double var_f29 = 0.0;
	double var_f26 = 0.0;
	double var_f27 = 0.0;
	double var_f25 = 0.0;
	double var_f28 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f894
	ctx.lr = 0x82253788;
	__savegprlr_27(ctx, base);
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82436608
	__savefpr_24(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	var_f31 = ctx.f1.f64;
	// lfs f12,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmr f30,f2
	var_f30 = ctx.f2.f64;
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lfs f11,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// addi r29,r31,880
	var_r29 = (uint32_t)(var_r31 + 880);
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// lfs f13,928(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f24,f11,f13
	var_f24 = double(float(ctx.f11.f64 / ctx.f13.f64));
	// fmuls f10,f12,f31
	ctx.f10.f64 = double(float(ctx.f12.f64 * var_f31));
	// fmuls f9,f12,f30
	ctx.f9.f64 = double(float(ctx.f12.f64 * var_f30));
	// fmsubs f8,f0,f30,f10
	ctx.f8.f64 = double(float(ctx.f0.f64 * var_f30 - ctx.f10.f64));
	// fmadds f29,f0,f31,f9
	var_f29 = double(float(ctx.f0.f64 * var_f31 + ctx.f9.f64));
	// fmuls f26,f8,f13
	var_f26 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f27,-25512(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	var_f27 = double(temp.f32);
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// fcmpu cr6,f1,f27
	// lfs f25,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);  /* glob:lbl_8202D110 @ 0x8202d110 */
	var_f25 = double(temp.f32);
	// ble cr6,0x822537f8
	if (ctx.f1.f64 > var_f27) {
		// fdivs f28,f29,f1
		var_f28 = double(float(var_f29 / ctx.f1.f64));
		// b 0x822537fc
	} else {
	loc_822537F8:
		// fmr f28,f25
		ctx.fpscr.disableFlushMode();
		var_f28 = var_f25;
	}
loc_822537FC:
	// addi r27,r31,912
	var_r27 = (uint32_t)(var_r31 + 912);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// fcmpu cr6,f1,f27
	ctx.fpscr.disableFlushMode();
	// ble cr6,0x8225381c
	if (ctx.f1.f64 > var_f27) {
		// fdivs f29,f26,f1
		var_f29 = double(float(var_f26 / ctx.f1.f64));
		// b 0x82253820
	} else {
	loc_8225381C:
		// fmr f29,f25
		ctx.fpscr.disableFlushMode();
		var_f29 = var_f25;
	}
loc_82253820:
	// addi r28,r31,896
	var_r28 = (uint32_t)(var_r31 + 896);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// fcmpu cr6,f1,f27
	ctx.fpscr.disableFlushMode();
	// ble cr6,0x82253840
	if (ctx.f1.f64 > var_f27) {
		// fdivs f9,f24,f1
		ctx.f9.f64 = double(float(var_f24 / ctx.f1.f64));
		// b 0x82253844
	} else {
	loc_82253840:
		// fmr f9,f25
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = var_f25;
	}
loc_82253844:
	// fmuls f5,f29,f30
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(var_f29 * var_f30));
	// lfs f12,872(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 872);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f29,f31
	ctx.f4.f64 = double(float(var_f29 * var_f31));
	// lfs f0,864(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 864);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,868(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 868);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,852(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 852);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,856(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 856);
	ctx.f6.f64 = double(temp.f32);
	// lfs f8,848(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 848);
	ctx.f8.f64 = double(temp.f32);
	// lvx128 v12,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmadds f11,f28,f31,f5
	ctx.f11.f64 = double(float(var_f28 * var_f31 + ctx.f5.f64));
	// fmsubs f10,f28,f30,f4
	ctx.f10.f64 = double(float(var_f28 * var_f30 - ctx.f4.f64));
	// fsubs f5,f9,f12
	ctx.f5.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
	// fsubs f4,f11,f0
	ctx.f4.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// fsubs f3,f10,f13
	ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fsel f2,f5,f9,f12
	ctx.f2.f64 = ctx.f5.f64 >= 0.0 ? ctx.f9.f64 : ctx.f12.f64;
	// fsel f12,f5,f9,f12
	ctx.f12.f64 = ctx.f5.f64 >= 0.0 ? ctx.f9.f64 : ctx.f12.f64;
	// fsel f9,f4,f11,f0
	ctx.f9.f64 = ctx.f4.f64 >= 0.0 ? ctx.f11.f64 : ctx.f0.f64;
	// fsel f0,f4,f11,f0
	ctx.f0.f64 = ctx.f4.f64 >= 0.0 ? ctx.f11.f64 : ctx.f0.f64;
	// fsel f11,f3,f10,f13
	ctx.f11.f64 = ctx.f3.f64 >= 0.0 ? ctx.f10.f64 : ctx.f13.f64;
	// fsel f13,f3,f10,f13
	ctx.f13.f64 = ctx.f3.f64 >= 0.0 ? ctx.f10.f64 : ctx.f13.f64;
	// fsub f3,f2,f6
	ctx.f3.f64 = ctx.f2.f64 - ctx.f6.f64;
	// fsub f2,f9,f8
	ctx.f2.f64 = ctx.f9.f64 - ctx.f8.f64;
	// fsub f1,f11,f7
	ctx.f1.f64 = ctx.f11.f64 - ctx.f7.f64;
	// fsel f12,f3,f6,f12
	ctx.f12.f64 = ctx.f3.f64 >= 0.0 ? ctx.f6.f64 : ctx.f12.f64;
	// fsel f0,f2,f8,f0
	ctx.f0.f64 = ctx.f2.f64 >= 0.0 ? ctx.f8.f64 : ctx.f0.f64;
	// fsel f13,f1,f7,f13
	ctx.f13.f64 = ctx.f1.f64 >= 0.0 ? ctx.f7.f64 : ctx.f13.f64;
	// fmuls f11,f13,f30
	ctx.f11.f64 = double(float(ctx.f13.f64 * var_f30));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * var_f31));
	// fmadds f9,f0,f31,f11
	ctx.f9.f64 = double(float(ctx.f0.f64 * var_f31 + ctx.f11.f64));
	// stfs f9,80(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmsubs f0,f0,f30,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * var_f30 - ctx.f10.f64));
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// vspltw v13,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vmulfp128 v0,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v0,v9,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lvx128 v8,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// vmaddfp v6,v7,v13,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82436654
	__restfpr_24(ctx, base);
	// b 0x8242f8e4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_31"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_31);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_31) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82251618
	phJoint_1618_g(ctx, base);
	// addi r4,r31,896
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 896;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// lfs f0,928(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 928);
	ctx.f0.f64 = double(temp.f32);
	// addi r4,r31,880
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 880;
	// fmuls f13,f0,f1
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stfs f13,1672(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 1672, temp.u32);
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// addi r4,r31,912
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 912;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	var_f31 = ctx.f1.f64;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// fmuls f12,f31,f31
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(var_f31 * var_f31));
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// lfs f0,-12020(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12020);  /* glob:lbl_8202D10C @ 0x8202d10c */
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f1,f1,f12
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f1.f64 + ctx.f12.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f10,f1
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// stfs f10,1668(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 1668, temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_20"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_20);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r31,640
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 640;
	// addi r5,r31,1008
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 1008;
	// addi r4,r31,1216
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 1216;
	// addi r3,r31,240
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 240;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r11,192
	ctx.r8.s64 = ctx.r11.s64 + 192;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v12,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82254568
	phJoint3Dof_4568(ctx, base);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r6,128(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 128);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
	// clrlwi r5,r3,24
	ctx.r5.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r5,0
	// beq cr6,0x82253aa0
	if (ctx.r5.u32 != 0) {
		// lis r11,-32160
		// addi r10,r31,1600
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 1600;
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// addi r4,r10,16
		ctx.r4.s64 = ctx.r10.s64 + 16;
		// addi r3,r10,32
		ctx.r3.s64 = ctx.r10.s64 + 32;
		// addi r9,r31,1472
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 1472;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r9,32
		ctx.r8.s64 = ctx.r9.s64 + 32;
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r9,16
		ctx.r10.s64 = ctx.r9.s64 + 16;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	}
loc_82253AA0:
	// lis r11,-32253
	// lfs f13,1384(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 1384);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,1364(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 1364);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r31,1344
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 1344;
	// fadds f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f0,16(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r31,1472
	var_r30 = (uint32_t)(var_r31 + 1472);
	// addi r8,r10,32
	ctx.r8.s64 = ctx.r10.s64 + 32;
	// lfs f13,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32163
	// fsubs f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lfs f9,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r30,16
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r6,r30,32
	ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 32;
	// addi r29,r10,48
	var_r29 = (uint32_t)(ctx.r10.s64 + 48);  // addr:0x82080030
	// addi r5,r30,48
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 48;
	// addi r9,r31,1600
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 1600;
	// fadds f13,f11,f9
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// fdivs f8,f0,f10
	ctx.f8.f64 = double(float(ctx.f0.f64 / ctx.f10.f64));
	// lfs f0,-32660(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32660);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f6,1664(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 1664, temp.u32);
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f5,1664(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 1664);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phJoint3Dof::vtable@+0x0 */;
	ctx.f4.f64 = double(temp.f32);
	// fadds f3,f4,f5
	ctx.f3.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// stfs f3,0(r30)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r30 + 0,/* phJoint3Dof::vtable@+0x0 */ temp.u32);
	// lfs f2,1664(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 1664);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,20(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 20);
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f2,f1
	ctx.f0.f64 = double(float(ctx.f2.f64 + ctx.f1.f64));
	// stfs f0,20(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r30 + 20, temp.u32);
	// lfs f13,1664(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 1664);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,40(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 40);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// stfs f11,40(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r30 + 40, temp.u32);
	// bl 0x8225b7e0
	phJoint3Dof_B7E0(ctx, base);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// addi r6,r30,16
	ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v6,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r9,32
	ctx.r5.s64 = ctx.r9.s64 + 32;
	// stvx v6,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// addi r11,r30,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 32;
	// addi r10,r9,48
	ctx.r10.s64 = ctx.r9.s64 + 48;
	// lvx128 v5,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stvx v5,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r30,48
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 48;
	// lvx128 v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r4,r31,1280
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 1280;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_19"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_19);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_19) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r5,r30,1008
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 1008;
	// addi r4,r30,1024
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 1024;
	// addi r3,r30,48
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 48;
	// bl 0x82254568
	phJoint3Dof_4568(ctx, base);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 128);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// addi r10,r30,1536
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 1536;
	// cmplwi cr6,r9,0
	// beq cr6,0x82253c54
	if (ctx.r9.u32 != 0) {
		// lis r11,-32160
		// addi r8,r10,16
		ctx.r8.s64 = ctx.r10.s64 + 16;
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// addi r7,r10,32
		ctx.r7.s64 = ctx.r10.s64 + 32;
		// addi r9,r30,1408
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 1408;
		// addi r6,r9,16
		ctx.r6.s64 = ctx.r9.s64 + 16;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r9,32
		ctx.r5.s64 = ctx.r9.s64 + 32;
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// b 0x82253d20
	} else {
	loc_82253C54:
		// addi r9,r30,1152
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 1152;
		// addi r31,r30,1408
		var_r31 = (uint32_t)(var_r30 + 1408);
		// addi r3,r9,16
		ctx.r3.s64 = ctx.r9.s64 + 16;
		// addi r11,r31,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
		// addi r8,r9,32
		ctx.r8.s64 = ctx.r9.s64 + 32;
		// addi r7,r31,32
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 32;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r9,48
		ctx.r6.s64 = ctx.r9.s64 + 48;
		// stvx v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r31,48
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 48;
		// lvx128 v13,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r4,r10
		ctx.r4.u64 = ctx.r10.u64;
		// lvx128 v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,1664(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 1664);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,0(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phJoint3Dof::vtable@+0x0 */;
		ctx.f13.f64 = double(temp.f32);
		// fadds f12,f0,f13
		ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
		// stfs f12,0(r31)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r31 + 0,/* phJoint3Dof::vtable@+0x0 */ temp.u32);
		// lfs f11,1664(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 1664);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,20(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 20);
		ctx.f10.f64 = double(temp.f32);
		// fadds f9,f10,f11
		ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
		// stfs f9,20(r31)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r31 + 20, temp.u32);
		// lfs f8,40(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 40);
		ctx.f8.f64 = double(temp.f32);
		// lfs f7,1664(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 1664);
		ctx.f7.f64 = double(temp.f32);
		// fadds f6,f8,f7
		ctx.f6.f64 = double(float(ctx.f8.f64 + ctx.f7.f64));
		// stfs f6,40(r31)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(var_r31 + 40, temp.u32);
		// bl 0x8225b7e0
		phJoint3Dof_B7E0(ctx, base);
		// mr r4,r9
		ctx.r4.u64 = ctx.r9.u64;
		// addi r11,r10,16
		ctx.r11.s64 = ctx.r10.s64 + 16;
		// lvx128 v10,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r31,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 16;
		// stvx v10,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r10,32
		ctx.r8.s64 = ctx.r10.s64 + 32;
		// addi r7,r31,32
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 32;
		// addi r6,r10,48
		ctx.r6.s64 = ctx.r10.s64 + 48;
		// addi r5,r31,48
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 48;
		// lvx128 v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v9,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r3,r10
		ctx.r3.u64 = ctx.r10.u64;
		// lvx128 v8,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v7,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x820c3bc8
		phJoint_3BC8_g(ctx, base);
		// addi r4,r30,1088
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 1088;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820c3bc8
		phJoint_3BC8_g(ctx, base);
	}
loc_82253D20:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_21"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_21);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_21) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t ea{};
	// FRAME: size=416, savegprlr_26
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lwz r10,132(r11)
	// bctrl
	VCALL(ctx.r3.u32, 33, ctx, base);  // phJoint3Dof::vfn_33 (unnamed)  // vtable slot 33 (byte +132)
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x82253eb0
	if (ctx.r9.u32 != 0) {
		// addi r11,r29,1472
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1472;
		// addi r30,r31,16
		var_r30 = (uint32_t)(var_r31 + 16);
		// addi r8,r11,16
		ctx.r8.s64 = ctx.r11.s64 + 16;
		// addi r7,r11,32
		ctx.r7.s64 = ctx.r11.s64 + 32;
		// addi r28,r31,32
		var_r28 = (uint32_t)(var_r31 + 32);
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r11,48
		ctx.r6.s64 = ctx.r11.s64 + 48;
		// stvx v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r31,48
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 48;
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r29,1280
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 1280;
		// stvx v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,288
		ctx.r4.s64 = ctx.r1.s64 + 288;
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,304
		ctx.r7.s64 = ctx.r1.s64 + 304;
		// stvx v12,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r10,32
		ctx.r5.s64 = ctx.r10.s64 + 32;
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r10,48
		ctx.r11.s64 = ctx.r10.s64 + 48;
		// stvx v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r10,16
		ctx.r8.s64 = ctx.r10.s64 + 16;
		// lvx128 v10,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v10,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,320
		ctx.r6.s64 = ctx.r1.s64 + 320;
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v10,v13,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// lvx128 v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrglw v0,v13,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// lvx128 v9,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrglw v13,v11,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// stvx v9,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v9,v11,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// addi r5,r1,336
		ctx.r5.s64 = ctx.r1.s64 + 336;
		// lvx128 v7,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v4,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vmrghw v6,v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vmrglw v5,v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// stvx v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,288
		ctx.r11.s64 = ctx.r1.s64 + 288;
		// stvx v6,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v5,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v4,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x820c3bc8
		phJoint_3BC8_g(ctx, base);
		// lis r11,-32160
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r29,1216
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 1216;
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// addi r9,r10,16
		ctx.r9.s64 = ctx.r10.s64 + 16;
		// addi r8,r10,32
		ctx.r8.s64 = ctx.r10.s64 + 32;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v3,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v3,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v2,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v1,v0,v2
		simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// stvx v1,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v31,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v30,v0,v31
		simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v31.f32)));
		// stvx v30,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v29,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v28,v29,v0
		simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v28,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v27,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v26,v27,v0
		simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v26,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v25,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v24,v25,v0
		simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v24,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x82254800
		phJoint3Dof_4800(ctx, base);
		return;
	}
loc_82253EB0:
	// addi r28,r29,1280
	var_r28 = (uint32_t)(var_r29 + 1280);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// addi r7,r28,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 32;
	// addi r6,r28,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 48;
	// addi r5,r28,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r26,r29,1472
	var_r26 = (uint32_t)(var_r29 + 1472);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r26
	ctx.r4.u64 = var_r26;
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v23,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v22,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v22.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v21,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v23,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v22,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx v21,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lis r10,-32160
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r27,r10,26448
	var_r27 = (uint32_t)(ctx.r10.s64 + 26448);  // lbl_82606750 @ 0x82606750
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lvx128 v20,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r29,1216
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1216;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r30,r29,1344
	var_r30 = (uint32_t)(var_r29 + 1344);
	// vsubfp v13,v0,v20
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v20.f32)));
	// lvx128 v19,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v19
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v19.f32)));
	// lvx128 v18,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v18
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v18.f32)));
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// lvx128 v14,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r30,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 32;
	// lvx128 v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r30,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v62,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// vaddfp v17,v13,v11
	simde_mm_store_ps(ctx.v17.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v16,v12,v10
	simde_mm_store_ps(ctx.v16.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// lvx128 v63,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v15,v0,v13
	simde_mm_store_ps(ctx.v15.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// addi r4,r29,1600
	ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 1600;
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stvx v17,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r9,r30,48
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 48;
	// stvx v16,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v61,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v15,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stvx v14,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// stvx128 v63,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,240
	ctx.r7.s64 = ctx.r1.s64 + 240;
	// stvx128 v62,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// stvx128 v61,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// lvx128 v60,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v60.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v59,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v59.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// lvx128 v11,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// vor128 v57,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v57.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// addi r9,r30,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r30,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
	// vsubfp128 v13,v0,v60
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v60.f32)));
	// addi r8,r30,32
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 32;
	// vsubfp128 v12,v0,v59
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v59.f32)));
	// lvx128 v58,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v58.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r30,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 32;
	// stvx128 v57,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v57.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// lvx128 v56,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v56.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r30,48
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 48;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v0,v0,v58
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v58.f32)));
	// lvx128 v55,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v55.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r26
	ctx.r4.u64 = var_r26;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// stvx128 v56,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v56.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,176
	ctx.r11.s64 = ctx.r1.s64 + 176;
	// lvx128 v54,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v54.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v53,v13,v11
	simde_mm_store_ps(ctx.v53.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx128 v55,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v55.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// vaddfp128 v52,v12,v10
	simde_mm_store_ps(ctx.v52.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp128 v51,v0,v9
	simde_mm_store_ps(ctx.v51.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx128 v54,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v54.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,240
	ctx.r7.s64 = ctx.r1.s64 + 240;
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,208
	ctx.r6.s64 = ctx.r1.s64 + 208;
	// stvx128 v53,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v53.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// stvx128 v52,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v52.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,240
	ctx.r11.s64 = ctx.r1.s64 + 240;
	// stvx128 v51,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v51.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// lvx128 v10,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// addi r11,r29,1008
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1008;
	// addi r6,r28,16
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lvx128 v50,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v50.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// vsubfp128 v13,v0,v50
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v50.f32)));
	// lvx128 v49,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v49.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v12,v0,v49
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v49.f32)));
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,176
	ctx.r7.s64 = ctx.r1.s64 + 176;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// vaddfp128 v47,v13,v10
	simde_mm_store_ps(ctx.v47.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp128 v46,v12,v9
	simde_mm_store_ps(ctx.v46.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// lvx128 v48,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v48.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v11,v0,v48
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v48.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r28,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 32;
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// vsubfp128 v44,v13,v0
	simde_mm_store_ps(ctx.v44.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// vaddfp128 v45,v11,v8
	simde_mm_store_ps(ctx.v45.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stvx128 v47,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v47.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// stvx128 v46,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v46.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,176
	ctx.r6.s64 = ctx.r1.s64 + 176;
	// stvx128 v45,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v45.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,272
	ctx.r11.s64 = ctx.r1.s64 + 272;
	// stvx128 v44,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v44.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82254568
	phJoint3Dof_4568(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_22"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_22);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_22) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t ea{};
	// FRAME: size=416, savegprlr_26
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lwz r10,132(r11)
	// bctrl
	VCALL(ctx.r3.u32, 33, ctx, base);  // phJoint3Dof::vfn_33 (unnamed)  // vtable slot 33 (byte +132)
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x822542c8
	if (ctx.r9.u32 != 0) {
		// addi r11,r29,1408
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1408;
		// addi r30,r31,16
		var_r30 = (uint32_t)(var_r31 + 16);
		// addi r8,r11,16
		ctx.r8.s64 = ctx.r11.s64 + 16;
		// addi r7,r11,32
		ctx.r7.s64 = ctx.r11.s64 + 32;
		// addi r28,r31,32
		var_r28 = (uint32_t)(var_r31 + 32);
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r11,48
		ctx.r6.s64 = ctx.r11.s64 + 48;
		// stvx v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r31,48
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 48;
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r29,1088
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 1088;
		// stvx v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,288
		ctx.r4.s64 = ctx.r1.s64 + 288;
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,304
		ctx.r7.s64 = ctx.r1.s64 + 304;
		// stvx v12,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r10,32
		ctx.r5.s64 = ctx.r10.s64 + 32;
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r10,48
		ctx.r11.s64 = ctx.r10.s64 + 48;
		// stvx v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r10,16
		ctx.r8.s64 = ctx.r10.s64 + 16;
		// lvx128 v10,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v10,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,320
		ctx.r6.s64 = ctx.r1.s64 + 320;
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v10,v13,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// lvx128 v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrglw v0,v13,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
		// lvx128 v9,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrglw v13,v11,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// stvx v9,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v9,v11,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// addi r5,r1,336
		ctx.r5.s64 = ctx.r1.s64 + 336;
		// lvx128 v7,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v4,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vmrghw v6,v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vmrglw v5,v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// stvx v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,288
		ctx.r11.s64 = ctx.r1.s64 + 288;
		// stvx v6,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v5,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v4,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x820c3bc8
		phJoint_3BC8_g(ctx, base);
		// lis r11,-32160
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r29,1024
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 1024;
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// addi r9,r10,16
		ctx.r9.s64 = ctx.r10.s64 + 16;
		// addi r8,r10,32
		ctx.r8.s64 = ctx.r10.s64 + 32;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v3,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v3,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v2,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v1,v0,v2
		simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// stvx v1,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v31,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v30,v0,v31
		simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v31.f32)));
		// stvx v30,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v29,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v28,v29,v0
		simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v28,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v27,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v26,v27,v0
		simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v26,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v25,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v24,v25,v0
		simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v24,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x82254800
		phJoint3Dof_4800(ctx, base);
		return;
	}
loc_822542C8:
	// addi r28,r29,1088
	var_r28 = (uint32_t)(var_r29 + 1088);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// addi r7,r28,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 32;
	// addi r6,r28,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 48;
	// addi r5,r28,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r26,r29,1408
	var_r26 = (uint32_t)(var_r29 + 1408);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r26
	ctx.r4.u64 = var_r26;
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v23,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v22,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v22.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v21,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v23,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v22,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx v21,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lis r10,-32160
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r27,r10,26448
	var_r27 = (uint32_t)(ctx.r10.s64 + 26448);  // lbl_82606750 @ 0x82606750
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lvx128 v20,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r29,1024
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1024;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r30,r29,1152
	var_r30 = (uint32_t)(var_r29 + 1152);
	// vsubfp v13,v0,v20
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v20.f32)));
	// lvx128 v19,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v19
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v19.f32)));
	// lvx128 v18,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v18
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v18.f32)));
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// lvx128 v14,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r30,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 32;
	// lvx128 v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r30,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v62,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// vaddfp v17,v13,v11
	simde_mm_store_ps(ctx.v17.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v16,v12,v10
	simde_mm_store_ps(ctx.v16.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// lvx128 v63,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v15,v0,v13
	simde_mm_store_ps(ctx.v15.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// addi r4,r29,1536
	ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 1536;
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stvx v17,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r9,r30,48
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 48;
	// stvx v16,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v61,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v15,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// stvx v14,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// stvx128 v63,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,240
	ctx.r7.s64 = ctx.r1.s64 + 240;
	// stvx128 v62,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,256
	ctx.r6.s64 = ctx.r1.s64 + 256;
	// stvx128 v61,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// lvx128 v60,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v60.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v59,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v59.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// lvx128 v11,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// vor128 v57,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v57.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// addi r9,r30,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r30,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
	// vsubfp128 v13,v0,v60
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v60.f32)));
	// addi r8,r30,32
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 32;
	// vsubfp128 v12,v0,v59
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v59.f32)));
	// lvx128 v58,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v58.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r30,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 32;
	// stvx128 v57,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v57.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// lvx128 v56,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v56.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r30,48
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 48;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v0,v0,v58
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v58.f32)));
	// lvx128 v55,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v55.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r26
	ctx.r4.u64 = var_r26;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// stvx128 v56,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v56.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,176
	ctx.r11.s64 = ctx.r1.s64 + 176;
	// lvx128 v54,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v54.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v53,v13,v11
	simde_mm_store_ps(ctx.v53.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx128 v55,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v55.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// vaddfp128 v52,v12,v10
	simde_mm_store_ps(ctx.v52.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp128 v51,v0,v9
	simde_mm_store_ps(ctx.v51.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx128 v54,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v54.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,208
	ctx.r9.s64 = ctx.r1.s64 + 208;
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,224
	ctx.r8.s64 = ctx.r1.s64 + 224;
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,240
	ctx.r7.s64 = ctx.r1.s64 + 240;
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,208
	ctx.r6.s64 = ctx.r1.s64 + 208;
	// stvx128 v53,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v53.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// stvx128 v52,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v52.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,240
	ctx.r11.s64 = ctx.r1.s64 + 240;
	// stvx128 v51,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v51.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// lvx128 v10,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// addi r11,r29,1008
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1008;
	// addi r6,r28,16
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lvx128 v50,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v50.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// vsubfp128 v13,v0,v50
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v50.f32)));
	// lvx128 v49,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v49.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v12,v0,v49
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v49.f32)));
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,176
	ctx.r7.s64 = ctx.r1.s64 + 176;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// vaddfp128 v47,v13,v10
	simde_mm_store_ps(ctx.v47.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp128 v46,v12,v9
	simde_mm_store_ps(ctx.v46.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// lvx128 v48,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v48.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v11,v0,v48
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v48.f32)));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r28,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 32;
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// vsubfp128 v44,v13,v0
	simde_mm_store_ps(ctx.v44.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// vaddfp128 v45,v11,v8
	simde_mm_store_ps(ctx.v45.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stvx128 v47,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v47.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// stvx128 v46,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v46.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,176
	ctx.r6.s64 = ctx.r1.s64 + 176;
	// stvx128 v45,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v45.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,272
	ctx.r11.s64 = ctx.r1.s64 + 272;
	// stvx128 v44,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v44.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82254568
	phJoint3Dof_4568(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint3Dof_4568"))) PPC_WEAK_FUNC(phJoint3Dof_4568);
PPC_FUNC_IMPL(__imp__phJoint3Dof_4568) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=192, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r10,r29,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 16;
	// addi r9,r30,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r8,r29,32
	ctx.r8.s64 = (int64_t)(int32_t)var_r29 + 32;
	// addi r7,r30,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 32;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r29,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r29 + 48;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r30,48
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 48;
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// stvx v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r31,r30,64
	var_r31 = (uint32_t)(var_r30 + 64);
	// lfs f0,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r31,16
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 16;
	// stvx v10,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r31,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 32;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r6,r31,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
	// stfs f0,100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stfs f0,120(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// lfs f0,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fneg f11,f0
	ctx.f11.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fneg f9,f13
	ctx.f9.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fneg f10,f12
	ctx.f10.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// stvx v9,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r31,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
	// stfs f9,112(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f12,116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r31,32
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 32;
	// stvx v7,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v5,v0,v6
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
	// stvx v5,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v3,v0,v4
	simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v1,v0,v2
	simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
	// stvx v1,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r28,r29,64
	var_r28 = (uint32_t)(var_r29 + 64);
	// lvx128 v31,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// addi r5,r28,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 16;
	// addi r10,r31,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 32;
	// addi r4,r28,32
	ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 32;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v30,v31,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v30,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v29,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v28,v29,v0
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v28,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r30,r30,128
	var_r30 = (uint32_t)(var_r30 + 128);
	// lvx128 v27,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// vaddfp v26,v27,v0
	simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r8,r30,16
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r7,r30,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 32;
	// addi r9,r31,48
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 48;
	// addi r6,r30,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 48;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stvx v26,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r31,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 32;
	// lvx128 v25,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v25,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v24,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v24,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v23,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v23,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v22,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v22,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v11,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r28,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 32;
	// lvx128 v21,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r28,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r28 + 16;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r30,32
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 32;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v8,v0,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v6,v0,v10
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v5,v0,v9
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v13,v11
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// lvx128 v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v3,v13,v10
	simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// addi r10,r30,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 16;
	// vmsum3fp128 v4,v13,v9
	simde_mm_store_ps(ctx.v4.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
	// addi r11,r29,128
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 128;
	// vmsum3fp128 v11,v12,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// addi r8,r30,16
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 16;
	// vmsum3fp128 v10,v12,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// vmsum3fp128 v9,v12,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
	// addi r7,r30,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 32;
	// addi r5,r11,32
	ctx.r5.s64 = ctx.r11.s64 + 32;
	// vmrghw v8,v8,v6
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// vmrghw v0,v5,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v5.u32)));
	// vmrghw v7,v7,v3
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
	// vmrghw v13,v4,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v4.u32)));
	// vmrghw v0,v8,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// vmrghw v11,v11,v10
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v9,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v13,v7,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
	// vsubfp v20,v21,v0
	simde_mm_store_ps(ctx.v20.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v21.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmrghw v12,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// stvx v20,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v19,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v18,v19,v13
	simde_mm_store_ps(ctx.v18.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v19.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v18,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v17,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v16,v17,v12
	simde_mm_store_ps(ctx.v16.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v17.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v16,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v15,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v14,v15,v0
	simde_mm_store_ps(ctx.v14.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v15.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v14,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v63,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v62,v63,v0
	simde_mm_store_ps(ctx.v62.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v63.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx128 v62,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v61,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v60,v61,v0
	simde_mm_store_ps(ctx.v60.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v61.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx128 v60,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v60.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phJoint3Dof_4800"))) PPC_WEAK_FUNC(phJoint3Dof_4800);
PPC_FUNC_IMPL(__imp__phJoint3Dof_4800) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, manual
	// addi r11,r3,1008
	ctx.r11.s64 = ctx.r3.s64 + 1008;
	// lis r10,-32253
	// addi r31,r4,128
	var_r31 = (uint32_t)(ctx.r4.s64 + 128);  // addr:0x82410080
	// addi r30,r4,64
	var_r30 = (uint32_t)(ctx.r4.s64 + 64);  // addr:0x82410040
	// addi r8,r31,32
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 32;
	// lfs f12,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// addi r9,r30,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f12
	ctx.f12.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// lfs f0,-12016(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
	// stfs f13,8(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// addi r7,r30,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 32;
	// stfs f0,0(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 0,/* phJoint3Dof::vtable@+0x0 */ temp.u32);
	// addi r6,r31,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 4,/* phJoint3Dof::flags@+0x4 */ temp.u32);
	// addi r5,r30,48
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 48;
	// lfs f11,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f10,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fneg f13,f11
	ctx.f13.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f10,16(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// stfs f0,20(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// stfs f13,24(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// lfs f9,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fneg f8,f9
	ctx.f8.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// stfs f0,40(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f8,32(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 32, temp.u32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v10,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// addi r10,r31,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 32;
	// addi r8,r31,48
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 48;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lvx128 v7,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// stvx v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820d4f88
	util_4F88(ctx, base);
	// lis r11,-32160
	// lvx128 v6,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r31,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r9,r31,32
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 32;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v5,v0,v6
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
	// stvx v5,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v3,v0,v4
	simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v1,v0,v2
	simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
	// stvx v1,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_24"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_24);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// addi r9,r3,1008
	ctx.r9.s64 = ctx.r3.s64 + 1008;
	// addi r11,r3,1600
	ctx.r11.s64 = ctx.r3.s64 + 1600;
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r11,48
	ctx.r7.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f7,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,1472
	ctx.r10.s64 = ctx.r3.s64 + 1472;
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lfs f13,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f10,f13
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f12,f11
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fmuls f3,f9,f0
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f11,f0,f5
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f5.f64));
	// fmsubs f1,f9,f13,f4
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f4.f64));
	// fmsubs f0,f12,f10,f3
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f3.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r8,-32160
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// vmrghw v10,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r8,r8,26448
	ctx.r8.s64 = ctx.r8.s64 + 26448;
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r7,r10,32
	ctx.r7.s64 = ctx.r10.s64 + 32;
	// addi r6,r10,48
	ctx.r6.s64 = ctx.r10.s64 + 48;
	// vmrghw v12,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v11,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmsum3fp128 v12,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v11,v0,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmrghw v0,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v12,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v11,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v10,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmsum3fp128 v12,v0,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vmsum3fp128 v11,v0,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f12,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f9,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_23"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_23);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// addi r9,r3,1008
	ctx.r9.s64 = ctx.r3.s64 + 1008;
	// addi r11,r3,1536
	ctx.r11.s64 = ctx.r3.s64 + 1536;
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r11,48
	ctx.r7.s64 = ctx.r11.s64 + 48;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f7,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,1408
	ctx.r10.s64 = ctx.r3.s64 + 1408;
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lfs f13,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f10,f13
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f12,f11
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fmuls f3,f9,f0
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f11,f0,f5
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f5.f64));
	// fmsubs f1,f9,f13,f4
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f4.f64));
	// fmsubs f0,f12,f10,f3
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f3.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r8,-32160
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// vmrghw v10,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r8,r8,26448
	ctx.r8.s64 = ctx.r8.s64 + 26448;
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r7,r10,32
	ctx.r7.s64 = ctx.r10.s64 + 32;
	// addi r6,r10,48
	ctx.r6.s64 = ctx.r10.s64 + 48;
	// vmrghw v12,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v11,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmsum3fp128 v12,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v11,v0,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmrghw v0,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v12,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v11,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v10,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmsum3fp128 v12,v0,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vmsum3fp128 v11,v0,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f12,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f9,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_25"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_25);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_25) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,96(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 96);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_26"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_26);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_26) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 92);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_28"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_28);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r7,r4,16
	ctx.r7.s64 = ctx.r4.s64 + 16;
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// addi r8,r3,1008
	ctx.r8.s64 = ctx.r3.s64 + 1008;
	// addi r10,r3,1600
	ctx.r10.s64 = ctx.r3.s64 + 1600;
	// addi r9,r3,1472
	ctx.r9.s64 = ctx.r3.s64 + 1472;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r10,32
	ctx.r6.s64 = ctx.r10.s64 + 32;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f12,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f12,f11
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f10,f13
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f13,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f10,f11,f4
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32160
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v10,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r9,32
	ctx.r6.s64 = ctx.r9.s64 + 32;
	// addi r3,r9,16
	ctx.r3.s64 = ctx.r9.s64 + 16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,26448
	ctx.r10.s64 = ctx.r10.s64 + 26448;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// vmrghw v0,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v12,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmsum3fp128 v10,v9,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v12,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v13,v11,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vaddfp v13,v0,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vsubfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f9,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_27"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_27);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r7,r4,16
	ctx.r7.s64 = ctx.r4.s64 + 16;
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// addi r8,r3,1008
	ctx.r8.s64 = ctx.r3.s64 + 1008;
	// addi r10,r3,1536
	ctx.r10.s64 = ctx.r3.s64 + 1536;
	// addi r9,r3,1408
	ctx.r9.s64 = ctx.r3.s64 + 1408;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r10,32
	ctx.r6.s64 = ctx.r10.s64 + 32;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f12,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f12,f11
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f10,f13
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f13,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f10,f11,f4
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32160
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v10,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r9,32
	ctx.r6.s64 = ctx.r9.s64 + 32;
	// addi r3,r9,16
	ctx.r3.s64 = ctx.r9.s64 + 16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,26448
	ctx.r10.s64 = ctx.r10.s64 + 26448;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// vmrghw v0,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v12,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmsum3fp128 v10,v9,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v12,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v13,v11,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vaddfp v13,v0,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vsubfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f9,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_29"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_29);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_29) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,112(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_30"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_30);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_30) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=128, manual
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phJoint3Dof::vtable@+0x0 */;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r10,108(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_0"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_0);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// li r12,-48
	ctx.r12.s64 = -48;
	// stvx128 v126,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v126.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r12,-32
	ctx.r12.s64 = -32;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r31,560
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 560;
	// addi r10,r31,640
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 640;
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r9,144
	ctx.r10.s64 = ctx.r9.s64 + 144;
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r6,r10,32
	ctx.r6.s64 = ctx.r10.s64 + 32;
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r9,192
	ctx.r9.s64 = ctx.r9.s64 + 192;
	// vmsum3fp128 v12,v12,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v8,v8,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v13
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v9,v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v13,v0
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v6,v13,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r4,r11,192
	ctx.r4.s64 = ctx.r11.s64 + 192;
	// vmrghw v13,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v0,v7,v8
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
	// vmrghw v12,v6,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v6.u32)));
	// vmrghw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vaddfp128 v126,v0,v10
	simde_mm_store_ps(ctx.v126.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmrghw v0,v9,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw128 v127,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v127.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vsubfp128 v0,v126,v127
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v126.f32), simde_mm_load_ps(ctx.v127.f32)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 28);
	// bl 0x8211d048
	util_D048(ctx, base);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// vpermwi128 v11,v127,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x78));
	// addi r3,r11,192
	ctx.r3.s64 = ctx.r11.s64 + 192;
	// vpermwi128 v10,v127,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v127.u32), 0x9C));
	// addi r9,r11,288
	ctx.r9.s64 = ctx.r11.s64 + 288;
	// addi r8,r10,288
	ctx.r8.s64 = ctx.r10.s64 + 288;
	// addi r7,r11,272
	ctx.r7.s64 = ctx.r11.s64 + 272;
	// addi r6,r10,272
	ctx.r6.s64 = ctx.r10.s64 + 272;
	// lvx128 v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v12,v126,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v126.f32), simde_mm_load_ps(ctx.v12.f32)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v8,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v7,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v0,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v6,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmulfp128 v0,v11,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vpermwi128 v13,v12,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x78));
	// vpermwi128 v12,v12,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x9C));
	// vmulfp128 v13,v13,v8
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vnmsubfp v0,v10,v6,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vnmsubfp v13,v12,v7,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vaddfp v13,v13,v9
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// li r0,-48
	ctx.r0.s64 = -48;
	// lvx128 v126,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v126.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r0,-32
	ctx.r0.s64 = -32;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_16"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_16);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_16) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// addi r11,r3,496
	ctx.r11.s64 = ctx.r3.s64 + 496;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_vfn_17"))) PPC_WEAK_FUNC(phJoint3Dof_vfn_17);
PPC_FUNC_IMPL(__imp__phJoint3Dof_vfn_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r11,r3,432
	ctx.r11.s64 = ctx.r3.s64 + 432;
	// lis r10,-32253
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// addi r9,r3,528
	ctx.r9.s64 = ctx.r3.s64 + 528;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lfs f0,-12016(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r3,464
	ctx.r10.s64 = ctx.r3.s64 + 464;
	// stfs f0,764(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 764, temp.u32);
	// stfs f0,768(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 768, temp.u32);
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v10,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
	// stvx v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v9,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
	// stvx v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_53B8_2h"))) PPC_WEAK_FUNC(phArticulatedCollider_53B8_2h);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_53B8_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v13,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// vpermwi128 v12,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v10,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vmulfp128 v0,v13,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vnmsubfp v0,v12,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f12,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f13,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v12,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vpermwi128 v10,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v13,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vmulfp128 v0,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vnmsubfp v0,v13,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5478_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_5478_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5478_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f88c
	ctx.lr = 0x82255480;
	__savegprlr_25(ctx, base);
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r4,16
	ctx.r5.s64 = ctx.r4.s64 + 16;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r3,16
	ctx.r7.s64 = ctx.r3.s64 + 16;
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r6,r3,32
	ctx.r6.s64 = ctx.r3.s64 + 32;
	// addi r29,r4,32
	var_r29 = (uint32_t)(ctx.r4.s64 + 32);  // addr:0x82410020
	// addi r11,r3,64
	ctx.r11.s64 = ctx.r3.s64 + 64;
	// addi r9,r4,64
	ctx.r9.s64 = ctx.r4.s64 + 64;
	// addi r8,r4,128
	ctx.r8.s64 = ctx.r4.s64 + 128;
	// addi r28,r9,16
	var_r28 = (uint32_t)(ctx.r9.s64 + 16);  // addr:0x82050010
	// addi r4,r11,32
	ctx.r4.s64 = ctx.r11.s64 + 32;
	// addi r27,r9,32
	var_r27 = (uint32_t)(ctx.r9.s64 + 32);  // addr:0x82050020
	// addi r10,r3,128
	ctx.r10.s64 = ctx.r3.s64 + 128;
	// addi r26,r8,16
	var_r26 = (uint32_t)(ctx.r8.s64 + 16);  // addr:0x82600010
	// addi r31,r10,16
	var_r31 = (uint32_t)(ctx.r10.s64 + 16);  // addr:0x82030010
	// addi r30,r10,32
	var_r30 = (uint32_t)(ctx.r10.s64 + 32);  // addr:0x82030020
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r25,r8,32
	var_r25 = (uint32_t)(ctx.r8.s64 + 32);  // addr:0x82600020
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v8,v9,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v4,v5,v0
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v2,v3,v0
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v2,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v1,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v31,v1,v0
	simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v30,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v29,v30,v0
	simde_mm_store_ps(ctx.v29.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v29,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v28,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v27,v28,v0
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v27,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8242f8dc
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint1Dof_5558"))) PPC_WEAK_FUNC(phJoint1Dof_5558);
PPC_FUNC_IMPL(__imp__phJoint1Dof_5558) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r25 = 0;
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f88c
	ctx.lr = 0x82255560;
	__savegprlr_25(ctx, base);
	// lis r11,-32160
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r7,r3,32
	ctx.r7.s64 = ctx.r3.s64 + 32;
	// addi r28,r4,16
	var_r28 = (uint32_t)(ctx.r4.s64 + 16);  // addr:0x82410010
	// addi r27,r4,32
	var_r27 = (uint32_t)(ctx.r4.s64 + 32);  // addr:0x82410020
	// addi r10,r3,128
	ctx.r10.s64 = ctx.r3.s64 + 128;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r4,128
	ctx.r6.s64 = ctx.r4.s64 + 128;
	// vsubfp v0,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r5,r10,16
	ctx.r5.s64 = ctx.r10.s64 + 16;
	// addi r31,r10,32
	var_r31 = (uint32_t)(ctx.r10.s64 + 32);  // addr:0x82030020
	// addi r30,r10,16
	var_r30 = (uint32_t)(ctx.r10.s64 + 16);  // addr:0x82030010
	// addi r26,r6,16
	var_r26 = (uint32_t)(ctx.r6.s64 + 16);  // addr:0x82410010
	// addi r29,r10,32
	var_r29 = (uint32_t)(ctx.r10.s64 + 32);  // addr:0x82030020
	// addi r25,r6,32
	var_r25 = (uint32_t)(ctx.r6.s64 + 32);  // addr:0x82410020
	// addi r9,r3,64
	ctx.r9.s64 = ctx.r3.s64 + 64;
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v10,v0,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v8,v9,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v8,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v4,v5,v0
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r9,32
	ctx.r7.s64 = ctx.r9.s64 + 32;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v2,v0,v3
	simde_mm_store_ps(ctx.v2.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v3.f32)));
	// stvx v2,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v31,v0,v1
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v1.f32)));
	// stvx v31,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r9,32
	ctx.r5.s64 = ctx.r9.s64 + 32;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v30,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v29,v0,v30
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v30.f32)));
	// stvx v29,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvx128 v28,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v27,v28,v0
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v27,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,64
	ctx.r10.s64 = ctx.r4.s64 + 64;
	// lvx128 v0,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v26,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// vaddfp v25,v26,v0
	simde_mm_store_ps(ctx.v25.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r3,r10,32
	ctx.r3.s64 = ctx.r10.s64 + 32;
	// stvx v25,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r25
	ea = (var_r25) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v24,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v23,v24,v0
	simde_mm_store_ps(ctx.v23.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v24.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v23,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v22,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v21,v0,v22
	simde_mm_store_ps(ctx.v21.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v22.f32)));
	// stvx v21,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v20,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v19,v0,v20
	simde_mm_store_ps(ctx.v19.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v20.f32)));
	// stvx v19,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v18,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v17,v0,v18
	simde_mm_store_ps(ctx.v17.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v18.f32)));
	// stvx v17,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v16,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v15,v16,v0
	simde_mm_store_ps(ctx.v15.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v16.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v15,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v14,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v63,v14,v0
	simde_mm_store_ps(ctx.v63.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v14.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx128 v63,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v62,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp128 v61,v62,v0
	simde_mm_store_ps(ctx.v61.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v62.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx128 v61,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8242f8dc
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__util_56E0"))) PPC_WEAK_FUNC(util_56E0);
PPC_FUNC_IMPL(__imp__util_56E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// addi r11,r3,64
	ctx.r11.s64 = ctx.r3.s64 + 64;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,80
	ctx.r10.s64 = ctx.r3.s64 + 80;
	// addi r9,r3,96
	ctx.r9.s64 = ctx.r3.s64 + 96;
	// addi r7,r3,112
	ctx.r7.s64 = ctx.r3.s64 + 112;
	// addi r8,r4,16
	ctx.r8.s64 = ctx.r4.s64 + 16;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r3,32
	ctx.r6.s64 = ctx.r3.s64 + 32;
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v8,v12,v10
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v9,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r7,r3,144
	ctx.r7.s64 = ctx.r3.s64 + 144;
	// vmrglw v13,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v12,v12,v10
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v11,v8,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// vmrghw v13,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrglw v12,v8,v9
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// vmsum3fp128 v11,v0,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmsum3fp128 v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmrghw v0,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v0,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r10,r3,128
	ctx.r10.s64 = ctx.r3.s64 + 128;
	// addi r9,r3,160
	ctx.r9.s64 = ctx.r3.s64 + 160;
	// vmrghw v0,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v12,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// li r12,16
	ctx.r12.s64 = 16;
	// stvx v12,r5,r12
	ea = (ctx.r5.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v9,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v10,v8,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v9,v7,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v11,v10
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v11,v9,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v0,v0,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vaddfp v6,v13,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v6,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v5,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v11,v4,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v3,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v3,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vaddfp v2,v12,v0
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// li r12,16
	ctx.r12.s64 = 16;
	// stvx v2,r5,r12
	ea = (ctx.r5.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_57F0_fw"))) PPC_WEAK_FUNC(phArticulatedCollider_57F0_fw);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_57F0_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// mr r30,r28
	var_r30 = (uint32_t)(var_r28);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// cmpwi cr6,r11,0
	// ble cr6,0x82255908
	if (ctx.r11.s32 > 0) {
		// addi r31,r29,40
		var_r31 = (uint32_t)(var_r29 + 40);
	loc_82255818:
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// bl 0x82126bd0
		phArticulatedCollider_6BD0_2hr(ctx, base);
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r9,r11,256
		ctx.r9.s64 = ctx.r11.s64 + 256;
		// addi r8,r11,192
		ctx.r8.s64 = ctx.r11.s64 + 192;
		// addi r7,r11,288
		ctx.r7.s64 = ctx.r11.s64 + 288;
		// addi r6,r11,304
		ctx.r6.s64 = ctx.r11.s64 + 304;
		// addi r5,r11,272
		ctx.r5.s64 = ctx.r11.s64 + 272;
		// addi r10,r11,320
		ctx.r10.s64 = ctx.r11.s64 + 320;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r11,208
		ctx.r9.s64 = ctx.r11.s64 + 208;
		// lvx128 v13,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,144
		ctx.r11.s64 = ctx.r11.s64 + 144;
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r9,16
		ctx.r4.s64 = ctx.r9.s64 + 16;
		// lvx128 v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r11,16
		ctx.r3.s64 = ctx.r11.s64 + 16;
		// stvx v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r9,32
		var_r27 = (uint32_t)(ctx.r9.s64 + 32);  // addr:0x82050020
		// lvx128 v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// lvx128 v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// lfs f8,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// addi r26,r11,32
		var_r26 = (uint32_t)(ctx.r11.s64 + 32);  // addr:0x82600020
		// stvx v10,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f13,100(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		ctx.f13.f64 = double(temp.f32);
		// lfs f0,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f0.f64 = double(temp.f32);
		// lfs f12,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f12.f64 = double(temp.f32);
		// lfs f11,104(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f7,f12,f11
		ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// lfs f9,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f5,f10,f13
		ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f6,f9,f0
		ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f4,f0,f13,f7
		ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f7.f64));
		// lfs f0,4(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		ctx.f0.f64 = double(temp.f32);
		// fmsubs f2,f9,f12,f5
		ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f5.f64));
		// lfs f13,8(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		ctx.f13.f64 = double(temp.f32);
		// fmsubs f3,f10,f11,f6
		ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f6.f64));
		// fadds f1,f4,f8
		ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
		// stfs f1,0(r10)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// fadds f11,f2,f13
		ctx.f11.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
		// stfs f11,8(r10)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
		// fadds f12,f3,f0
		ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f0.f64));
		// stfs f12,4(r10)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
		// lvx128 v9,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v7,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v7,r0,r26
		ea = (var_r26) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stw r28,380(r31)
		PPC_STORE_U32(var_r31 + 380, var_r28);
		// lwz r6,4(r29)
		ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 4);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// cmpw cr6,r30,r6
		// blt cr6,0x82255818
		if ((int32_t)var_r30 < ctx.r6.s32) goto loc_82255818;
	}
loc_82255908:
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r10,0
	// ble cr6,0x82255934
	if (ctx.r10.s32 > 0) {
		// addi r11,r29,8
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 8;
		// cmplwi cr6,r10,0
		// beq cr6,0x82255934
		if (ctx.r10.u32 == 0) {
			return;
		}
		// mtctr r10
		ctx.ctr.u64 = ctx.r10.u64;
	loc_82255928:
		// stb r28,0(r11)
		PPC_STORE_U8(ctx.r11.u32 + 0, (uint8_t)var_r28);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// bdnz 0x82255928
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_82255928;
	}
loc_82255934:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5940_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_5940_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5940_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r11,r31,11
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 11;
	// addi r9,r31,10
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 10;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
	// add r4,r10,r30
	ctx.r4.u64 = ctx.r10.u64 + var_r30;
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + var_r30;
	// subf r11,r31,r8
	ctx.r11.s64 = ctx.r8.s64 - (int64_t)(int32_t)var_r31;
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// rlwinm r5,r7,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8242ff70
	memmove(ctx, base);
	// cmpwi cr6,r31,0
	// beq cr6,0x82255a18
	if ((int32_t)var_r31 != 0) {
		// lwz r6,4(r30)
		ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 4);
		// addi r5,r31,42
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 42;
		// addi r4,r31,41
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 41;
		// subf r11,r31,r6
		ctx.r11.s64 = ctx.r6.s64 - (int64_t)(int32_t)var_r31;
		// rlwinm r10,r5,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r3,r11,-1
		ctx.r3.s64 = ctx.r11.s64 + -1;
		// rlwinm r11,r4,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r5,r3,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// add r4,r10,r30
		ctx.r4.u64 = ctx.r10.u64 + var_r30;
		// add r3,r11,r30
		ctx.r3.u64 = ctx.r11.u64 + var_r30;
		// bl 0x8242ff70
		memmove(ctx, base);
		// addi r11,r31,72
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 72;
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// mr r9,r31
		ctx.r9.u64 = var_r31;
		// rlwinm r11,r11,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r8,r10,-1
		ctx.r8.s64 = ctx.r10.s64 + -1;
		// add r10,r11,r30
		ctx.r10.u64 = ctx.r11.u64 + var_r30;
		// cmpw cr6,r31,r8
		ctx.cr6.compare<int32_t>((int32_t)var_r31, ctx.r8.s32, ctx.xer);
		// lwz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// bge cr6,0x82255a18
	while (ctx.r9.s32 < ctx.r7.s32) {
		loc_822559E0:
			// lwz r11,4(r10)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
			// cmpw cr6,r11,r31
			// bne cr6,0x822559f4
			if (ctx.r11.s32 == (int32_t)var_r31) {
				// mr r11,r8
				ctx.r11.u64 = ctx.r8.u64;
				// b 0x822559fc
			} else {
			loc_822559F4:
				// ble cr6,0x822559fc
				if (!ctx.cr6.gt) goto loc_822559FC;
				// addi r11,r11,-1
				ctx.r11.s64 = ctx.r11.s64 + -1;
			}
		loc_822559FC:
			// stw r11,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// lwz r11,4(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
			// addi r10,r10,4
			ctx.r10.s64 = ctx.r10.s64 + 4;
			// addi r7,r11,-1
			ctx.r7.s64 = ctx.r11.s64 + -1;
			// cmpw cr6,r9,r7
			// blt cr6,0x822559e0
	}
	}
loc_82255A18:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// addi r6,r11,-1
	ctx.r6.s64 = ctx.r11.s64 + -1;
	// stw r6,4(r30)
	PPC_STORE_U32(var_r30 + 4, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5A40_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_5A40_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5A40_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x82255b00
	if (ctx.r11.s32 > 0) {
		// addi r8,r3,40
		ctx.r8.s64 = ctx.r3.s64 + 40;
	loc_82255A68:
		// lwz r11,0(r8)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// addi r6,r11,272
		ctx.r6.s64 = ctx.r11.s64 + 272;
		// addi r5,r11,288
		ctx.r5.s64 = ctx.r11.s64 + 288;
		// addi r10,r11,304
		ctx.r10.s64 = ctx.r11.s64 + 304;
		// addi r9,r11,1008
		ctx.r9.s64 = ctx.r11.s64 + 1008;
		// addi r4,r10,16
		ctx.r4.s64 = ctx.r10.s64 + 16;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r31,r9,16
		var_r31 = (uint32_t)(ctx.r9.s64 + 16);  // addr:0x82050010
		// vxor v0,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
		// addi r11,r11,1040
		ctx.r11.s64 = ctx.r11.s64 + 1040;
		// addi r30,r11,16
		var_r30 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82600010
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v13,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
		// stvx v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// stvx v12,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v11,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
		// stvx v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v10,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
		// stvx v10,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
		// stvx v9,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v8,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_setzero_si128());
		// stvx v8,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v7,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_setzero_si128());
		// stvx v7,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r10,4(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// cmpw cr6,r7,r10
		// blt cr6,0x82255a68
		if (ctx.r7.s32 < ctx.r10.s32) goto loc_82255A68;
	}
loc_82255B00:
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r31,r11,-1
	var_r31 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825fffff
	// cmpwi cr6,r31,0
	// ble cr6,0x82255b38
	if ((int32_t)var_r31 > 0) {
		// addi r30,r3,168
		var_r30 = (uint32_t)(ctx.r3.s64 + 168);
	loc_82255B14:
		// lwz r3,0(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lwz r8,68(r9)
		// bctrl
		VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r31,0
		// bne cr6,0x82255b14
		if (var_r31 != 0) goto loc_82255B14;
	}
loc_82255B38:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5B50_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_5B50_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5B50_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x82255c10
	if (ctx.r11.s32 > 0) {
		// addi r7,r3,40
		ctx.r7.s64 = ctx.r3.s64 + 40;
	loc_82255B78:
		// lwz r11,0(r7)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// addi r6,r6,1
		ctx.r6.s64 = ctx.r6.s64 + 1;
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// addi r10,r11,336
		ctx.r10.s64 = ctx.r11.s64 + 336;
		// addi r9,r11,368
		ctx.r9.s64 = ctx.r11.s64 + 368;
		// addi r5,r10,16
		ctx.r5.s64 = ctx.r10.s64 + 16;
		// addi r4,r9,16
		ctx.r4.s64 = ctx.r9.s64 + 16;
		// addi r8,r11,976
		ctx.r8.s64 = ctx.r11.s64 + 976;
		// addi r11,r11,1072
		ctx.r11.s64 = ctx.r11.s64 + 1072;
		// addi r31,r8,16
		var_r31 = (uint32_t)(ctx.r8.s64 + 16);  // addr:0x82600010
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r11,16
		var_r30 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82600010
		// vxor v0,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v13,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// stvx v12,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v11,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
		// stvx v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v10,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
		// stvx v10,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
		// stvx v9,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v8,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_setzero_si128());
		// stvx v8,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v7,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_setzero_si128());
		// stvx v7,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r10,4(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// cmpw cr6,r6,r10
		// blt cr6,0x82255b78
		if (ctx.r6.s32 < ctx.r10.s32) goto loc_82255B78;
	}
loc_82255C10:
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r31,r11,-1
	var_r31 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825fffff
	// cmpwi cr6,r31,0
	// ble cr6,0x82255c48
	if ((int32_t)var_r31 > 0) {
		// addi r30,r3,168
		var_r30 = (uint32_t)(ctx.r3.s64 + 168);
	loc_82255C24:
		// lwz r3,0(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lwz r8,64(r9)
		// bctrl
		VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r31,0
		// bne cr6,0x82255c24
		if (var_r31 != 0) goto loc_82255C24;
	}
loc_82255C48:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5C60_sp"))) PPC_WEAK_FUNC(phArticulatedCollider_5C60_sp);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5C60_sp) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32163
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r11,r11,-17184
	ctx.r11.s64 = ctx.r11.s64 + -17184;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r10,0
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v13,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// blelr cr6
	if (ctx.r10.s32 <= 0) return;
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
loc_82255C98:
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r6,r1,-32
	ctx.r6.s64 = ctx.r1.s64 + -32;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,192
	ctx.r7.s64 = ctx.r10.s64 + 192;
	// addi r11,r10,352
	ctx.r11.s64 = ctx.r10.s64 + 352;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lfs f13,112(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,336
	ctx.r10.s64 = ctx.r10.s64 + 336;
	// stfs f13,-48(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// addi r5,r1,-48
	ctx.r5.s64 = ctx.r1.s64 + -48;
	// addi r4,r1,-48
	ctx.r4.s64 = ctx.r1.s64 + -48;
	// lfs f13,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f12.f64 = double(temp.f32);
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v0,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,-12(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f10,f13
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f12,f11
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fmuls f3,f9,f0
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f11,f0,f5
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f5.f64));
	// fmsubs f1,f9,f13,f4
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f4.f64));
	// fmsubs f0,f12,f10,f3
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f3.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v11,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r9,r6
	// blt cr6,0x82255c98
	if (ctx.r9.s32 < ctx.r6.s32) goto loc_82255C98;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5D58"))) PPC_WEAK_FUNC(phArticulatedCollider_5D58);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5D58) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	// FRAME: size=352, savegprlr_23
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r23,r30,40
	var_r23 = (uint32_t)(var_r30 + 40);
	// mr r29,r23
	var_r29 = (uint32_t)(var_r23);
	// lwz r31,4(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 4));
	// cmpwi cr6,r31,0
	// ble cr6,0x82255da4
while ((int32_t)var_r31 > 0) {
	loc_82255D7C:
		// lwz r8,0(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 0);
		// mr r3,r8
		ctx.r3.u64 = ctx.r8.u64;
		// bl 0x82258e48
		phArticulatedCollider_8E48_fw(ctx, base);
		// addi r4,r8,400
		ctx.r4.s64 = ctx.r8.s64 + 400;
		// addi r3,r8,592
		ctx.r3.s64 = ctx.r8.s64 + 592;
		// bl 0x82255f10
		phArticulatedCollider_5F10_wrh(ctx, base);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpwi cr6,r31,0
		// bgt cr6,0x82255d7c
}
loc_82255DA4:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// addi r10,r11,9
	ctx.r10.s64 = ctx.r11.s64 + 9;
	// addi r9,r11,40
	ctx.r9.s64 = ctx.r11.s64 + 40;
	// addi r7,r11,71
	ctx.r7.s64 = ctx.r11.s64 + 71;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r8,r30
	var_r28 = (uint32_t)(ctx.r8.u64 + var_r30);
	// add r29,r9,r30
	var_r29 = (uint32_t)(ctx.r9.u64 + var_r30);
	// add r27,r10,r30
	var_r27 = (uint32_t)(ctx.r10.u64 + var_r30);
	// cmpwi cr6,r11,1
	// ble cr6,0x82255e50
	if (ctx.r11.s32 > 1) {
		// addi r26,r11,-1
		var_r26 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825cffff
	loc_82255DD8:
		// lwz r10,0(r27)
		ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 0);
		// lwz r31,0(r29)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0));
		// addi r6,r10,10
		ctx.r6.s64 = ctx.r10.s64 + 10;
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// addi r3,r31,240
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 240;
		// rlwinm r5,r6,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r4,r11,592
		ctx.r4.s64 = ctx.r11.s64 + 592;
		// lwzx r24,r5,r30
		var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + var_r30));
		// bl 0x82255f10
		phArticulatedCollider_5F10_wrh(ctx, base);
		// lwz r4,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r11,80(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 20, ctx, base);  // pattern-B slot 20 (byte +80)
		// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
		// addi r25,r31,48
		var_r25 = (uint32_t)(var_r31 + 48);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// mr r4,r25
		ctx.r4.u64 = var_r25;
		// lwz r9,84(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 21, ctx, base);  // pattern-B slot 21 (byte +84)
		// mr r4,r25
		ctx.r4.u64 = var_r25;
		// addi r3,r24,592
		ctx.r3.s64 = (int64_t)(int32_t)var_r24 + 592;
		// bl 0x82255478
		phArticulatedCollider_5478_wrh(ctx, base);
		// addi r26,r26,-1
		var_r26 = (uint32_t)(var_r26 + -1);
		// addi r29,r29,-4
		var_r29 = (uint32_t)(var_r29 + -4);
		// addi r28,r28,-4
		var_r28 = (uint32_t)(var_r28 + -4);
		// addi r27,r27,-4
		var_r27 = (uint32_t)(var_r27 + -4);
		// cmplwi cr6,r26,0
		// bne cr6,0x82255dd8
		if (var_r26 != 0) goto loc_82255DD8;
	}
loc_82255E50:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// addi r28,r30,168
	var_r28 = (uint32_t)(var_r30 + 168);
	// cmpwi cr6,r11,1
	// ble cr6,0x82255ed8
	if (ctx.r11.s32 > 1) {
		// addi r29,r28,124
		var_r29 = (uint32_t)(var_r28 + 124);
		// addi r27,r11,-1
		var_r27 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825cffff
	loc_82255E68:
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r31,0(r28)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
		// addi r8,r11,10
		ctx.r8.s64 = ctx.r11.s64 + 10;
		// lwz r26,-248(r29)
		var_r26 = (uint32_t)(PPC_LOAD_U32(var_r29 + -248));
		// addi r3,r31,48
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 48;
		// rlwinm r7,r8,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r7,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r30);
		// addi r4,r11,592
		ctx.r4.s64 = ctx.r11.s64 + 592;
		// bl 0x82255558
		phJoint1Dof_5558(ctx, base);
		// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r5,76(r6)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 19, ctx, base);  // pattern-B slot 19 (byte +76)
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,88(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 22, ctx, base);  // pattern-B slot 22 (byte +88)
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r3,r26,592
		ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 592;
		// bl 0x82255478
		phArticulatedCollider_5478_wrh(ctx, base);
		// addi r27,r27,-1
		var_r27 = (uint32_t)(var_r27 + -1);
		// addi r28,r28,4
		var_r28 = (uint32_t)(var_r28 + 4);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmplwi cr6,r27,0
		// bne cr6,0x82255e68
		if (var_r27 != 0) goto loc_82255E68;
	}
loc_82255ED8:
	// lwz r31,4(r30)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 4));
	// mr r30,r23
	var_r30 = (uint32_t)(var_r23);
	// cmpwi cr6,r31,0
	// ble cr6,0x82255f08
while ((int32_t)var_r31 > 0) {
	loc_82255EE8:
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r4,r11,784
		ctx.r4.s64 = ctx.r11.s64 + 784;
		// addi r3,r11,592
		ctx.r3.s64 = ctx.r11.s64 + 592;
		// bl 0x8225b498
		phJoint3Dof_B498_h(ctx, base);
		// addi r31,r31,-1
		var_r31 = (uint32_t)(var_r31 + -1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpwi cr6,r31,0
		// bgt cr6,0x82255ee8
}
loc_82255F08:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5F10_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_5F10_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5F10_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r30);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// addi r7,r4,16
	ctx.r7.s64 = ctx.r4.s64 + 16;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r3,16
	ctx.r6.s64 = ctx.r3.s64 + 16;
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r4,32
	ctx.r5.s64 = ctx.r4.s64 + 32;
	// addi r11,r4,64
	ctx.r11.s64 = ctx.r4.s64 + 64;
	// addi r9,r4,128
	ctx.r9.s64 = ctx.r4.s64 + 128;
	// addi r31,r3,32
	var_r31 = (uint32_t)(ctx.r3.s64 + 32);
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r4,48
	ctx.r4.s64 = ctx.r4.s64 + 48;
	// stvx v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r30,r3,48
	var_r30 = (uint32_t)(ctx.r3.s64 + 48);
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,64
	ctx.r10.s64 = ctx.r3.s64 + 64;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// stvx v12,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lvx128 v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r11,32
	ctx.r5.s64 = ctx.r11.s64 + 32;
	// stvx v11,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r10,32
	ctx.r4.s64 = ctx.r10.s64 + 32;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,48
	ctx.r11.s64 = ctx.r11.s64 + 48;
	// stvx v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,128
	ctx.r8.s64 = ctx.r3.s64 + 128;
	// stvx v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v8,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r9,32
	ctx.r5.s64 = ctx.r9.s64 + 32;
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r8,32
	ctx.r4.s64 = ctx.r8.s64 + 32;
	// stvx v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r9,48
	ctx.r11.s64 = ctx.r9.s64 + 48;
	// lvx128 v6,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r8,48
	ctx.r10.s64 = ctx.r8.s64 + 48;
	// stvx v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r30,-16(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_5FE0"))) PPC_WEAK_FUNC(phArticulatedCollider_5FE0);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_5FE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82256300
	phArticulatedCollider_6300_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r7,r31,40
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 40;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x822560e0
while (ctx.r8.s32 < ctx.r3.s32) {
	loc_8225600C:
		// lwz r10,0(r7)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// addi r9,r10,304
		ctx.r9.s64 = ctx.r10.s64 + 304;
		// addi r6,r10,1008
		ctx.r6.s64 = ctx.r10.s64 + 1008;
		// addi r11,r10,272
		ctx.r11.s64 = ctx.r10.s64 + 272;
		// addi r5,r10,288
		ctx.r5.s64 = ctx.r10.s64 + 288;
		// addi r4,r10,320
		ctx.r4.s64 = ctx.r10.s64 + 320;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r10,192
		ctx.r3.s64 = ctx.r10.s64 + 192;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r9,16
		ctx.r10.s64 = ctx.r9.s64 + 16;
		// vaddfp v12,v13,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r6,r6,16
		ctx.r6.s64 = ctx.r6.s64 + 16;
		// stvx v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v10,v11,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v10,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v9,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v9,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// stvx v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v6,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lvx128 v7,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f8,0(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// stvx v6,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f7,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f7.f64 = double(temp.f32);
		// stvx v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f6,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f6.f64 = double(temp.f32);
		// lfs f11,104(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
		ctx.f11.f64 = double(temp.f32);
		// lfs f13,100(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f12.f64 = double(temp.f32);
		// lfs f0,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f5,f12,f11
		ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// lfs f10,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f3,f10,f13
		ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f4,f9,f0
		ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f2,f0,f13,f5
		ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f5.f64));
		// fmsubs f0,f9,f12,f3
		ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
		// fmsubs f1,f10,f11,f4
		ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f4.f64));
		// fadds f13,f2,f8
		ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
		// stfs f13,0(r11)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// fadds f11,f0,f6
		ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
		// stfs f11,8(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
		// fadds f12,f1,f7
		ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
		// stfs f12,4(r11)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// lwz r3,4(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpw cr6,r8,r3
		// blt cr6,0x8225600c
}
loc_822560E0:
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_60F8"))) PPC_WEAK_FUNC(phArticulatedCollider_60F8);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_60F8) {
	PPC_FUNC_PROLOGUE();
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	double var_f27 = 0.0;
	double var_f28 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82436614
	__savefpr_27(ctx, base);
	// lis r11,-32253
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r10,0
	// lfs f0,-12312(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12312);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32253
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// lfs f7,-8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f8,f3,f0,f7
	ctx.f8.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// ble cr6,0x822562e8
	if (ctx.r10.s32 > 0) {
		// lfs f6,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f6.f64 = double(temp.f32);
		// fmuls f4,f2,f2
		ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
		// fmuls f5,f1,f1
		ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
		// addi r6,r3,40
		ctx.r6.s64 = ctx.r3.s64 + 40;
	loc_82256140:
		// lwz r9,0(r6)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// addi r5,r1,-80
		ctx.r5.s64 = ctx.r1.s64 + -80;
		// addi r8,r9,288
		ctx.r8.s64 = ctx.r9.s64 + 288;
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v0,v0,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,-80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -80);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f4
		// ble cr6,0x822561a4
		if (ctx.f0.f64 > ctx.f4.f64) {
			// fcmpu cr6,f0,f6
			// bne cr6,0x82256174
			if (ctx.f0.f64 == ctx.f6.f64) {
				// fmr f0,f6
				ctx.f0.f64 = ctx.f6.f64;
				// b 0x8225617c
			} else {
			loc_82256174:
				// fsqrts f0,f0
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
				// fdivs f0,f7,f0
				ctx.f0.f64 = double(float(ctx.f7.f64 / ctx.f0.f64));
			}
		loc_8225617C:
			// fmuls f0,f0,f2
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
			// stfs f0,-176(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
			// lvx128 v12,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r1,-176
			ctx.r4.s64 = ctx.r1.s64 + -176;
			// addi r11,r1,-176
			ctx.r11.s64 = ctx.r1.s64 + -176;
			// lvx128 v13,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v0,v13,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
			// vmulfp128 v11,v12,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v11,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_822561A4:
		// stfs f8,-160(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
		// addi r10,r9,272
		ctx.r10.s64 = ctx.r9.s64 + 272;
		// lvx128 v9,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,-64
		ctx.r11.s64 = ctx.r1.s64 + -64;
		// addi r5,r1,-160
		ctx.r5.s64 = ctx.r1.s64 + -160;
		// addi r4,r1,-160
		ctx.r4.s64 = ctx.r1.s64 + -160;
		// lvx128 v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vmulfp128 v8,v9,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v0,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v7,v0,v0
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
		// stvx v7,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f0,-64(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -64);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f0,f5
		// ble cr6,0x82256228
		if (ctx.f0.f64 > ctx.f5.f64) {
			// fcmpu cr6,f0,f6
			// bne cr6,0x822561f8
			if (ctx.f0.f64 == ctx.f6.f64) {
				// fmr f0,f6
				ctx.f0.f64 = ctx.f6.f64;
				// b 0x82256200
			} else {
			loc_822561F8:
				// fsqrts f0,f0
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
				// fdivs f0,f7,f0
				ctx.f0.f64 = double(float(ctx.f7.f64 / ctx.f0.f64));
			}
		loc_82256200:
			// fmuls f13,f0,f1
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
			// stfs f13,-144(r1)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
			// lvx128 v5,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r5,r1,-144
			ctx.r5.s64 = ctx.r1.s64 + -144;
			// addi r4,r1,-144
			ctx.r4.s64 = ctx.r1.s64 + -144;
			// lvx128 v6,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v0,v6,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
			// vmulfp128 v4,v5,v0
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v0,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v4,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_82256228:
		// stfs f8,-128(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
		// addi r11,r9,320
		ctx.r11.s64 = ctx.r9.s64 + 320;
		// addi r5,r9,304
		ctx.r5.s64 = ctx.r9.s64 + 304;
		// lvx128 v2,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r9,192
		ctx.r4.s64 = ctx.r9.s64 + 192;
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// addi r6,r6,4
		ctx.r6.s64 = ctx.r6.s64 + 4;
		// addi r9,r1,-128
		ctx.r9.s64 = ctx.r1.s64 + -128;
		// lvx128 v3,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,-128
		ctx.r9.s64 = ctx.r1.s64 + -128;
		// vspltw v0,v3,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
		// vmulfp128 v1,v2,v0
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v1,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,-96
		ctx.r5.s64 = ctx.r1.s64 + -96;
		// lvx128 v31,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v31,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v30,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,-112
		ctx.r8.s64 = ctx.r1.s64 + -112;
		// lvx128 v29,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v29,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f3,0(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f3.f64 = double(temp.f32);
		// lfs f31,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		var_f31 = double(temp.f32);
		// stvx v30,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f30,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		var_f30 = double(temp.f32);
		// lfs f11,-88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -88);
		ctx.f11.f64 = double(temp.f32);
		// lfs f13,-92(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -92);
		ctx.f13.f64 = double(temp.f32);
		// lfs f9,-96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		ctx.f9.f64 = double(temp.f32);
		// lfs f12,-108(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -108);
		ctx.f12.f64 = double(temp.f32);
		// lfs f10,-112(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -112);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f29,f12,f11
		var_f29 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// lfs f0,-104(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -104);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f27,f10,f13
		var_f27 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f28,f9,f0
		var_f28 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f0,f0,f13,f29
		ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - var_f29));
		// fmsubs f12,f9,f12,f27
		ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - var_f27));
		// fmsubs f13,f10,f11,f28
		ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - var_f28));
		// fadds f11,f0,f3
		ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f3.f64));
		// stfs f11,0(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// fadds f9,f12,f30
		ctx.f9.f64 = double(float(ctx.f12.f64 + var_f30));
		// stfs f9,8(r11)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
		// fadds f10,f13,f31
		ctx.f10.f64 = double(float(ctx.f13.f64 + var_f31));
		// stfs f10,4(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// lwz r4,4(r3)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// cmpw cr6,r7,r4
		// blt cr6,0x82256140
		if (ctx.r7.s32 < ctx.r4.s32) goto loc_82256140;
	}
loc_822562E8:
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82436660
	__restfpr_27(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_6300_w"))) PPC_WEAK_FUNC(phArticulatedCollider_6300_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_6300_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t ea{};
	// FRAME: size=208, savegprlr_25
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// addi r27,r31,40
	var_r27 = (uint32_t)(var_r31 + 40);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// ble cr6,0x8225638c
while ((int32_t)var_r28 < ctx.r7.s32) {
	loc_82256324:
		// lwz r29,0(r27)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r27 + 0));
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// addi r30,r29,368
		var_r30 = (uint32_t)(var_r29 + 368);
		// addi r3,r29,784
		ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 784;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x822556e0
		util_56E0(ctx, base);
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// addi r11,r29,1008
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 1008;
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// addi r27,r27,4
		var_r27 = (uint32_t)(var_r27 + 4);
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r30,16
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 16;
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,128
		ctx.r8.s64 = ctx.r1.s64 + 128;
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// stvx v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v11,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
		// stvx v11,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r7,4(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpw cr6,r28,r7
		// blt cr6,0x82256324
}
loc_8225638C:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r6,r11,9
	ctx.r6.s64 = ctx.r11.s64 + 9;
	// addi r5,r11,40
	ctx.r5.s64 = ctx.r11.s64 + 40;
	// addi r4,r11,71
	ctx.r4.s64 = ctx.r11.s64 + 71;
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r8,r31
	var_r28 = (uint32_t)(ctx.r8.u64 + var_r31);
	// add r29,r9,r31
	var_r29 = (uint32_t)(ctx.r9.u64 + var_r31);
	// add r27,r10,r31
	var_r27 = (uint32_t)(ctx.r10.u64 + var_r31);
	// cmpwi cr6,r11,1
	// ble cr6,0x82256438
	if (ctx.r11.s32 > 1) {
		// addi r26,r11,-1
		var_r26 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8202ffff
	loc_822563C0:
		// lwz r3,0(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r10,0(r27)
		ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 0);
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// addi r30,r3,432
		var_r30 = (uint32_t)(ctx.r3.s64 + 432);
		// addi r10,r10,10
		ctx.r10.s64 = ctx.r10.s64 + 10;
		// addi r4,r11,1008
		ctx.r4.s64 = ctx.r11.s64 + 1008;
		// lwz r8,0(r3)
  // [ph4a] vtable load collapsed
		// rlwinm r9,r10,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// lwzx r25,r9,r31
		var_r25 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + var_r31));
		// lwz r7,108(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 27, ctx, base);  // pattern-B slot 27 (byte +108)
		// addi r11,r25,1008
		ctx.r11.s64 = (int64_t)(int32_t)var_r25 + 1008;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r30,16
		ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r26,r26,-1
		var_r26 = (uint32_t)(var_r26 + -1);
		// addi r29,r29,-4
		var_r29 = (uint32_t)(var_r29 + -4);
		// lvx128 v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r28,r28,-4
		var_r28 = (uint32_t)(var_r28 + -4);
		// vaddfp v9,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r27,r27,-4
		var_r27 = (uint32_t)(var_r27 + -4);
		// cmplwi cr6,r26,0
		// stvx v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v7,v8,v0
		simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x822563c0
		if (var_r26 != 0) goto loc_822563C0;
	}
loc_82256438:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r27,r31,168
	var_r27 = (uint32_t)(var_r31 + 168);
	// addi r28,r11,-1
	var_r28 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8202ffff
	// cmpwi cr6,r28,0
	// ble cr6,0x82256510
	if ((int32_t)var_r28 > 0) {
		// addi r30,r27,124
		var_r30 = (uint32_t)(var_r27 + 124);
	loc_82256450:
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// lwz r3,0(r27)
		ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 0);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r5,r11,10
		ctx.r5.s64 = ctx.r11.s64 + 10;
		// lwz r26,-248(r30)
		var_r26 = (uint32_t)(PPC_LOAD_U32(var_r30 + -248));
		// addi r10,r3,432
		ctx.r10.s64 = ctx.r3.s64 + 432;
		// rlwinm r11,r5,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r6,r10,16
		ctx.r6.s64 = ctx.r10.s64 + 16;
		// addi r29,r3,464
		var_r29 = (uint32_t)(ctx.r3.s64 + 464);
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// lwzx r11,r11,r31
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
		// addi r11,r11,1008
		ctx.r11.s64 = ctx.r11.s64 + 1008;
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vor v6,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// stvx v13,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v5,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v5,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v4,v6,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v4,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r8,112(r9)
		// bctrl
		VCALL(ctx.r3.u32, 28, ctx, base);  // vtable slot 28 (byte +112)
		// addi r11,r26,1008
		ctx.r11.s64 = (int64_t)(int32_t)var_r26 + 1008;
		// lvx128 v0,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r29,16
		ctx.r7.s64 = (int64_t)(int32_t)var_r29 + 16;
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// addi r27,r27,4
		var_r27 = (uint32_t)(var_r27 + 4);
		// lvx128 v3,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// vaddfp v2,v3,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32)));
		// cmpwi cr6,r28,0
		// stvx v2,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v1,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v31,v1,v0
		simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v31,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bgt cr6,0x82256450
		if ((int32_t)var_r28 > 0) goto loc_82256450;
	}
loc_82256510:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_6518"))) PPC_WEAK_FUNC(phArticulatedCollider_6518);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_6518) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=1264, savegprlr_27
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// bl 0x82256e90
	phArticulatedCollider_6E90(ctx, base);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// li r28,0
	var_r28 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x8225665c
	if (ctx.r11.s32 > 0) {
		// clrlwi r27,r31,24
		var_r27 = (uint32_t)(var_r31 & 0xFF);
		// addi r30,r29,40
		var_r30 = (uint32_t)(var_r29 + 40);
	loc_82256550:
		// lwz r31,0(r30)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// cmplwi cr6,r27,0
		// beq cr6,0x82256580
		if (var_r27 != 0) {
			// addi r11,r31,368
			ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 368;
			// addi r9,r1,80
			ctx.r9.s64 = ctx.r1.s64 + 80;
			// addi r10,r11,16
			ctx.r10.s64 = ctx.r11.s64 + 16;
			// lvx128 v8,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v8,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,96
			ctx.r8.s64 = ctx.r1.s64 + 96;
			// lvx128 v9,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v9,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// b 0x822565a8
		} else {
		loc_82256580:
			// addi r7,r1,96
			ctx.r7.s64 = ctx.r1.s64 + 96;
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// addi r5,r1,96
			ctx.r5.s64 = ctx.r1.s64 + 96;
			// lvx128 v0,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vxor v9,v0,v0
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
			// lvx128 v13,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vxor v8,v13,v13
			simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_setzero_si128());
			// stvx v9,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// stvx v8,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_822565A8:
		// stfs f31,160(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
		// addi r11,r31,336
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 336;
		// stfs f31,144(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
		// addi r10,r31,976
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 976;
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// addi r8,r10,16
		ctx.r8.s64 = ctx.r10.s64 + 16;
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r31,784
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 784;
		// vaddfp v10,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,80
		ctx.r9.s64 = ctx.r1.s64 + 80;
		// vaddfp v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
		// addi r7,r1,160
		ctx.r7.s64 = ctx.r1.s64 + 160;
		// addi r6,r1,144
		ctx.r6.s64 = ctx.r1.s64 + 144;
		// addi r11,r1,160
		ctx.r11.s64 = ctx.r1.s64 + 160;
		// addi r10,r1,144
		ctx.r10.s64 = ctx.r1.s64 + 144;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v0,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
		// lvx128 v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// vmaddfp v11,v10,v0,v8
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v8.f32)));
		// vmaddfp v10,v12,v13,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v9.f32)));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,96
		ctx.r8.s64 = ctx.r1.s64 + 96;
		// stvx v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x822556e0
		util_56E0(ctx, base);
		// addi r7,r1,112
		ctx.r7.s64 = ctx.r1.s64 + 112;
		// addi r11,r31,1072
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 1072;
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r6,r11,16
		ctx.r6.s64 = ctx.r11.s64 + 16;
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// lvx128 v9,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,128
		ctx.r5.s64 = ctx.r1.s64 + 128;
		// lvx128 v8,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r4,4(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 4);
		// cmpw cr6,r28,r4
		// blt cr6,0x82256550
		if ((int32_t)var_r28 < ctx.r4.s32) goto loc_82256550;
	}
loc_8225665C:
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// addi r28,r11,-1
	var_r28 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8202ffff
	// cmpwi cr6,r28,0
	// ble cr6,0x822566f8
	if ((int32_t)var_r28 > 0) {
		// addi r10,r1,208
		ctx.r10.s64 = ctx.r1.s64 + 208;
		// rlwinm r11,r28,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 5) & 0xFFFFFFE0;
		// addi r3,r28,72
		ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 72;
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
		// rlwinm r10,r3,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r30,r11,-32
		var_r30 = (uint32_t)(ctx.r11.s64 + -32);  // addr:0x8202ffe0
		// add r31,r10,r29
		var_r31 = (uint32_t)(ctx.r10.u64 + var_r29);
	loc_82256688:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// lwz r3,-124(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + -124);
		// addi r10,r11,10
		ctx.r10.s64 = ctx.r11.s64 + 10;
		// lwz r11,-248(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + -248);
		// rlwinm r8,r10,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r4,r11,1072
		ctx.r4.s64 = ctx.r11.s64 + 1072;
		// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
		// lwzx r27,r8,r29
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + var_r29));
		// lwz r7,108(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 27, ctx, base);  // pattern-B slot 27 (byte +108)
		// addi r11,r27,1072
		ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 1072;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r30,16
		ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// addi r31,r31,-4
		var_r31 = (uint32_t)(var_r31 + -4);
		// lvx128 v7,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r30,-32
		var_r30 = (uint32_t)(var_r30 + -32);
		// vaddfp v6,v7,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
		// cmpwi cr6,r28,0
		// stvx v6,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v5,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v4,v5,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v4,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bgt cr6,0x82256688
		if ((int32_t)var_r28 > 0) goto loc_82256688;
	}
loc_822566F8:
	// lwz r5,4(r29)
	ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 4);
	// li r28,1
	var_r28 = 1;
	// cmpwi cr6,r5,1
	// ble cr6,0x822567cc
	if (ctx.r5.s32 > 1) {
		// addi r30,r1,208
		var_r30 = (uint32_t)(ctx.r1.s64 + 208);
		// addi r31,r29,292
		var_r31 = (uint32_t)(var_r29 + 292);
	loc_82256710:
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r8,r1,176
		ctx.r8.s64 = ctx.r1.s64 + 176;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r30,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r11,r11,10
		ctx.r11.s64 = ctx.r11.s64 + 10;
		// lwz r3,-124(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + -124);
		// lwz r27,-248(r31)
		var_r27 = (uint32_t)(PPC_LOAD_U32(var_r31 + -248));
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// lvx128 v13,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,176
		ctx.r4.s64 = ctx.r1.s64 + 176;
		// lwzx r11,r10,r29
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r29);
		// addi r11,r11,1072
		ctx.r11.s64 = ctx.r11.s64 + 1072;
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,192
		ctx.r7.s64 = ctx.r1.s64 + 192;
		// vsubfp v3,v12,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvx128 v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v2,v11,v13
		simde_mm_store_ps(ctx.v2.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,176
		ctx.r6.s64 = ctx.r1.s64 + 176;
		// stvx v3,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,192
		ctx.r11.s64 = ctx.r1.s64 + 192;
		// stvx v2,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r9,112(r10)
		// bctrl
		VCALL(ctx.r3.u32, 28, ctx, base);  // vtable slot 28 (byte +112)
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// addi r11,r27,1072
		ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 1072;
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// lvx128 v1,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r30,32
		var_r30 = (uint32_t)(var_r30 + 32);
		// lvx128 v31,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v30,v31,v1
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v1.f32)));
		// stvx v30,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,128
		ctx.r7.s64 = ctx.r1.s64 + 128;
		// lvx128 v28,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v29,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v27,v28,v29
		simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v29.f32)));
		// stvx v27,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r6,4(r29)
		ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 4);
		// cmpw cr6,r28,r6
		// blt cr6,0x82256710
		if ((int32_t)var_r28 < ctx.r6.s32) goto loc_82256710;
	}
loc_822567CC:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_67D8_h"))) PPC_WEAK_FUNC(phArticulatedCollider_67D8_h);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_67D8_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=240, savegprlr_27
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x82256e90
	phArticulatedCollider_6E90(ctx, base);
	// lwz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 4);
	// addi r29,r28,40
	var_r29 = (uint32_t)(var_r28 + 40);
	// li r30,0
	var_r30 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x82256950
	if (ctx.r11.s32 > 0) {
		// fneg f30,f31
		ctx.fpscr.disableFlushMode();
		ctx.f30.u64 = ctx.f31.u64 ^ 0x8000000000000000;
	loc_82256810:
		// lwz r31,0(r29)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0));
		// addi r5,r1,144
		ctx.r5.s64 = ctx.r1.s64 + 144;
		// addi r4,r31,976
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 976;
		// addi r3,r31,784
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 784;
		// bl 0x822556e0
		util_56E0(ctx, base);
		// stfs f30,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// stfs f30,96(r1)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// addi r7,r1,160
		ctx.r7.s64 = ctx.r1.s64 + 160;
		// stfs f31,112(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// addi r9,r1,144
		ctx.r9.s64 = ctx.r1.s64 + 144;
		// stfs f31,128(r1)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
		// addi r10,r31,1008
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 1008;
		// addi r11,r31,336
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 336;
		// addi r8,r31,368
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 368;
		// lvx128 v9,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// lvx128 v10,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r31,304
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 304;
		// addi r31,r11,16
		var_r31 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82030010
		// addi r7,r9,16
		ctx.r7.s64 = ctx.r9.s64 + 16;
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// addi r27,r1,80
		var_r27 = (uint32_t)(ctx.r1.s64 + 80);
		// addi r4,r1,112
		ctx.r4.s64 = ctx.r1.s64 + 112;
		// addi r3,r1,128
		ctx.r3.s64 = ctx.r1.s64 + 128;
		// lvx128 v8,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r8,16
		ctx.r6.s64 = ctx.r8.s64 + 16;
		// vspltw v0,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// lvx128 v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// lvx128 v6,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r10,16
		ctx.r4.s64 = ctx.r10.s64 + 16;
		// vspltw v12,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// lvx128 v5,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v11,v5,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
		// addi r3,r10,16
		ctx.r3.s64 = ctx.r10.s64 + 16;
		// vmulfp128 v4,v9,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v0,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r1,96
		var_r27 = (uint32_t)(ctx.r1.s64 + 96);
		// vmulfp128 v0,v10,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v13,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r1,112
		var_r27 = (uint32_t)(ctx.r1.s64 + 112);
		// stvx v12,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r1,128
		var_r27 = (uint32_t)(ctx.r1.s64 + 128);
		// stvx v11,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r1,160
		var_r27 = (uint32_t)(ctx.r1.s64 + 160);
		// stvx v4,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r1,144
		var_r27 = (uint32_t)(ctx.r1.s64 + 144);
		// stvx v0,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r27,r1,160
		var_r27 = (uint32_t)(ctx.r1.s64 + 160);
		// lvx128 v3,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v3,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v2,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v1,v2,v0
		simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v1,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v31,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v30,v31,v0
		simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v30,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v29,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmaddfp v28,v0,v12,v29
		simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v29.f32)));
		// stvx v28,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v27,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmaddfp v26,v0,v11,v27
		simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v27.f32)));
		// stvx v26,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v25,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_setzero_si128());
		// stvx v25,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v24,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_setzero_si128());
		// stvx v24,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,4(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 4);
		// cmpw cr6,r30,r11
		// blt cr6,0x82256810
		if ((int32_t)var_r30 < ctx.r11.s32) goto loc_82256810;
	}
loc_82256950:
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// bl 0x82255fe0
	phArticulatedCollider_5FE0(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundCapsule_6968_g"))) PPC_WEAK_FUNC(phBoundCapsule_6968_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_6968_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f894
	ctx.lr = 0x82256970;
	__savegprlr_27(ctx, base);
	// stfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f30.u64);
	// stfd f31,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// li r12,-80
	ctx.r12.s64 = -80;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// li r28,0
	var_r28 = 0;
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	// cmpwi cr6,r11,0
	// ble cr6,0x82256b28
	if (ctx.r11.s32 > 0) {
		// lis r10,-32248
		// lis r11,-32163
		// addi r29,r30,40
		var_r29 = (uint32_t)(var_r30 + 40);
		// addi r27,r11,-18496
		var_r27 = (uint32_t)(ctx.r11.s64 + -18496);  // lbl_825CB7C0 @ 0x825cb7c0
		// lfs f30,-25840(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25840);
		var_f30 = double(temp.f32);
	loc_822569B0:
		// lwz r31,0(r29)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0)/* phBoundCapsule::vtable@+0x0 */);
		// stfs f31,112(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
		// addi r7,r1,192
		ctx.r7.s64 = ctx.r1.s64 + 192;
		// addi r10,r31,288
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 288;
		// addi r11,r31,192
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 192;
		// addi r9,r31,272
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 272;
		// addi r8,r1,96
		ctx.r8.s64 = ctx.r1.s64 + 96;
		// lvx128 v127,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v0,v127,v127
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx128 v127,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,112
		ctx.r6.s64 = ctx.r1.s64 + 112;
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v0,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vmaddfp v10,v13,v0,v12
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v12.f32)));
		// stvx v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f1,192(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
		ctx.f1.f64 = double(temp.f32);
		// fcmpu cr6,f1,f30
		// stvx v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// ble cr6,0x82256b14
		if (ctx.f1.f64 > var_f30) {
			// bl 0x824301d0
			phBoundCapsule_01D0_g(ctx, base);
			// frsp f0,f1
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(ctx.f1.f64));
			// stfs f0,80(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
			// addi r9,r1,96
			ctx.r9.s64 = ctx.r1.s64 + 96;
			// lvx128 v0,r0,r27
			ea = (var_r27) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r5,r1,96
			ctx.r5.s64 = ctx.r1.s64 + 96;
			// addi r4,r1,272
			ctx.r4.s64 = ctx.r1.s64 + 272;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// fmuls f1,f0,f31
			ctx.f1.f64 = double(float(ctx.f0.f64 * var_f31));
			// addi r11,r1,80
			ctx.r11.s64 = ctx.r1.s64 + 80;
			// addi r10,r1,80
			ctx.r10.s64 = ctx.r1.s64 + 80;
			// lvx128 v9,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vspltw v12,v9,0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
			// vrefp v13,v12
			ctx.fpscr.enableFlushModeUnconditional();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
			// vnmsubfp v0,v13,v12,v0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
			// vmaddfp v0,v13,v0,v13
			simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
			// vmulfp128 v8,v127,v0
			simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v0,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v8,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x82256d98
			phBoundCapsule_6D98_g(ctx, base);
			// addi r31,r31,144
			var_r31 = (uint32_t)(var_r31 + 144);
			// addi r6,r1,208
			ctx.r6.s64 = ctx.r1.s64 + 208;
			// addi r8,r31,16
			ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 16;
			// addi r7,r31,32
			ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 32;
			// addi r5,r31,48
			ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 48;
			// lvx128 v7,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r4,r1,208
			ctx.r4.s64 = ctx.r1.s64 + 208;
			// stvx v7,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r1,224
			ctx.r11.s64 = ctx.r1.s64 + 224;
			// lvx128 v6,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,272
			ctx.r8.s64 = ctx.r1.s64 + 272;
			// addi r6,r1,288
			ctx.r6.s64 = ctx.r1.s64 + 288;
			// addi r3,r1,128
			ctx.r3.s64 = ctx.r1.s64 + 128;
			// stvx v6,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r10,r1,240
			ctx.r10.s64 = ctx.r1.s64 + 240;
			// lvx128 v5,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r7,r1,128
			ctx.r7.s64 = ctx.r1.s64 + 128;
			// lvx128 v3,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r11,r1,304
			ctx.r11.s64 = ctx.r1.s64 + 304;
			// lvx128 v2,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v5,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,256
			ctx.r9.s64 = ctx.r1.s64 + 256;
			// lvx128 v4,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v3,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r5,r1,144
			ctx.r5.s64 = ctx.r1.s64 + 144;
			// lvx128 v1,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v4,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r9,r1,320
			ctx.r9.s64 = ctx.r1.s64 + 320;
			// stvx v2,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r10,r1,160
			ctx.r10.s64 = ctx.r1.s64 + 160;
			// lvx128 v31,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v1,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r8,r1,176
			ctx.r8.s64 = ctx.r1.s64 + 176;
			// stvx v31,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bl 0x820c3bc8
			phJoint_3BC8_g(ctx, base);
			// addi r7,r1,128
			ctx.r7.s64 = ctx.r1.s64 + 128;
			// addi r6,r31,16
			ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 16;
			// addi r4,r31,32
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 32;
			// lvx128 v30,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v30,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r5,r1,144
			ctx.r5.s64 = ctx.r1.s64 + 144;
			// lvx128 v29,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v29,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r3,r1,160
			ctx.r3.s64 = ctx.r1.s64 + 160;
			// lvx128 v28,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v28,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_82256B14:
		// lwz r11,4(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// cmpw cr6,r28,r11
		// blt cr6,0x822569b0
		if ((int32_t)var_r28 < ctx.r11.s32) goto loc_822569B0;
	}
loc_82256B28:
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// li r0,-80
	ctx.r0.s64 = -80;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lfd f31,-56(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x8242f8e4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_6B40_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_6B40_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_6B40_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	// ble cr6,0x82256c1c
	if (ctx.r11.s32 > 0) {
		// addi r8,r3,40
		ctx.r8.s64 = ctx.r3.s64 + 40;
	loc_82256B58:
		// lwz r10,0(r8)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// addi r6,r10,1040
		ctx.r6.s64 = ctx.r10.s64 + 1040;
		// addi r9,r10,304
		ctx.r9.s64 = ctx.r10.s64 + 304;
		// addi r31,r6,16
		var_r31 = (uint32_t)(ctx.r6.s64 + 16);  // addr:0x82410010
		// addi r5,r10,288
		ctx.r5.s64 = ctx.r10.s64 + 288;
		// addi r4,r10,320
		ctx.r4.s64 = ctx.r10.s64 + 320;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r9,16
		ctx.r6.s64 = ctx.r9.s64 + 16;
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r10,272
		ctx.r11.s64 = ctx.r10.s64 + 272;
		// lvx128 v13,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r10,192
		ctx.r10.s64 = ctx.r10.s64 + 192;
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,-32
		ctx.r4.s64 = ctx.r1.s64 + -32;
		// stvx v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v9,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,-48
		ctx.r5.s64 = ctx.r1.s64 + -48;
		// lvx128 v10,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f8,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// stvx v9,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f7,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f7.f64 = double(temp.f32);
		// stvx v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f6,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f6.f64 = double(temp.f32);
		// lfs f11,-24(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
		ctx.f11.f64 = double(temp.f32);
		// lfs f13,-28(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,-44(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -44);
		ctx.f12.f64 = double(temp.f32);
		// lfs f0,-40(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -40);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f5,f12,f11
		ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// lfs f10,-48(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,-32(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f3,f10,f13
		ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f4,f9,f0
		ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f2,f0,f13,f5
		ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f5.f64));
		// fmsubs f0,f9,f12,f3
		ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
		// fmsubs f1,f10,f11,f4
		ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f4.f64));
		// fadds f13,f2,f8
		ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
		// stfs f13,0(r11)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// fadds f11,f0,f6
		ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
		// stfs f11,8(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
		// fadds f12,f1,f7
		ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
		// stfs f12,4(r11)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// lwz r11,4(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// cmpw cr6,r7,r11
		// blt cr6,0x82256b58
		if (ctx.r7.s32 < ctx.r11.s32) goto loc_82256B58;
	}
loc_82256C1C:
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_6C28_fw"))) PPC_WEAK_FUNC(phBoundCapsule_6C28_fw);
PPC_FUNC_IMPL(__imp__phBoundCapsule_6C28_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x82256C30;
	__savegprlr_26(ctx, base);
	// li r12,-80
	ctx.r12.s64 = -80;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// li r28,0
	var_r28 = 0;
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 4);
	// cmpwi cr6,r11,0
	// ble cr6,0x82256d20
	if (ctx.r11.s32 > 0) {
		// addi r29,r26,40
		var_r29 = (uint32_t)(var_r26 + 40);
	loc_82256C58:
		// lwz r31,0(r29)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0)/* phBoundCapsule::vtable@+0x0 */);
		// lvx128 v0,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v13,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// mr r4,r27
		ctx.r4.u64 = var_r27;
		// addi r30,r31,192
		var_r30 = (uint32_t)(var_r31 + 192);
		// vpermwi128 v12,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v11,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vpermwi128 v10,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vmulfp128 v0,v11,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vnmsubfp v0,v10,v12,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vor128 v127,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// bl 0x82258fc0
		phBoundCapsule_8FC0_g(ctx, base);
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r31,304
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 304;
		// vaddfp128 v13,v0,v127
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v127.f32)));
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// addi r8,r1,96
		ctx.r8.s64 = ctx.r1.s64 + 96;
		// addi r11,r31,320
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 320;
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// stvx128 v127,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f8,0(r11)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// lfs f7,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f7.f64 = double(temp.f32);
		// lfs f6,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f6.f64 = double(temp.f32);
		// lfs f13,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f13.f64 = double(temp.f32);
		// lfs f0,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f0.f64 = double(temp.f32);
		// lfs f12,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f12.f64 = double(temp.f32);
		// lfs f10,100(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		ctx.f10.f64 = double(temp.f32);
		// lfs f11,104(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f5,f10,f13
		ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// lfs f9,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f4,f12,f11
		ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// fmuls f3,f9,f0
		ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f2,f11,f0,f5
		ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f5.f64));
		// fmsubs f1,f9,f13,f4
		ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 - ctx.f4.f64));
		// fmsubs f0,f12,f10,f3
		ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f3.f64));
		// fadds f13,f2,f8
		ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
		// stfs f13,0(r11)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// fadds f12,f1,f7
		ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
		// stfs f12,4(r11)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// fadds f11,f0,f6
		ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
		// stfs f11,8(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
		// lwz r7,4(r26)
		ctx.r7.u64 = PPC_LOAD_U32(var_r26 + 4);
		// cmpw cr6,r28,r7
		// blt cr6,0x82256c58
		if ((int32_t)var_r28 < ctx.r7.s32) goto loc_82256C58;
	}
loc_82256D20:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// li r0,-80
	ctx.r0.s64 = -80;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_6D30_h"))) PPC_WEAK_FUNC(phArticulatedCollider_6D30_h);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_6D30_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	// FRAME: size=128, savegprlr_28
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// li r31,0
	var_r31 = 0;
	// addi r29,r11,-1
	var_r29 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825cffff
	// cmpwi cr6,r29,0
	// ble cr6,0x82256d8c
	if ((int32_t)var_r29 > 0) {
		// addi r30,r3,168
		var_r30 = (uint32_t)(ctx.r3.s64 + 168);
		// addi r28,r3,8
		var_r28 = (uint32_t)(ctx.r3.s64 + 8);
	loc_82256D60:
		// lwz r3,0(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// lwz r10,44(r11)
		// bctrl
		VCALL(ctx.r3.u32, 11, ctx, base);  // vtable slot 11 (byte +44)
		// stbx r3,r28,r31
		PPC_STORE_U8(var_r28 + var_r31, ctx.r3.u8);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r31,r29
		// blt cr6,0x82256d60
		if ((int32_t)var_r31 < (int32_t)var_r29) goto loc_82256D60;
	}
loc_82256D8C:
	return;
}

__attribute__((alias("__imp__phBoundCapsule_6D98_g"))) PPC_WEAK_FUNC(phBoundCapsule_6D98_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_6D98_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x824302b0
	phBoundCapsule_02B0_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// frsp f31,f0
	var_f31 = double(float(ctx.f0.f64));
	// bl 0x824301d8
	phBoundCapsule_01D8_g(ctx, base);
	// lis r11,-32253
	// lfs f13,4(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f6,f13,f13
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// lfs f11,0(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f11.f64 = double(temp.f32);
	// fmuls f7,f12,f13
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f5,f12,f12
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmuls f4,f11,f11
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// frsp f9,f1
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// lfs f0,-8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f0,f0,f31
	ctx.f0.f64 = double(float(ctx.f0.f64 - var_f31));
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmadds f6,f6,f0,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + var_f31));
	// fmadds f5,f5,f0,f31
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + var_f31));
	// fmadds f3,f4,f0,f31
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + var_f31));
	// stfs f3,0(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// fmuls f11,f11,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f0,f13,f8
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// fmuls f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fmuls f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
	// fmuls f13,f13,f9
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fsubs f1,f0,f12
	ctx.f1.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// stfs f1,4(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// fadds f2,f13,f8
	ctx.f2.f64 = double(float(ctx.f13.f64 + ctx.f8.f64));
	// stfs f2,8(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// stfs f6,20(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// fsubs f9,f8,f13
	ctx.f9.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// stfs f0,16(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// fsubs f12,f7,f11
	ctx.f12.f64 = double(float(ctx.f7.f64 - ctx.f11.f64));
	// stfs f12,24(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// fadds f8,f11,f7
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f7.f64));
	// stfs f9,32(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 32, temp.u32);
	// stfs f8,36(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f5,40(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f10,48(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	// stfs f10,52(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 52, temp.u32);
	// stfs f10,56(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 56, temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_6E90"))) PPC_WEAK_FUNC(phArticulatedCollider_6E90);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_6E90) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t ea{};
	// FRAME: size=240, savegprlr_22
	// mr r23,r3
	var_r23 = ctx.r3.u32;
	// addi r28,r23,40
	var_r28 = (uint32_t)(var_r23 + 40);
	// lwz r29,4(r23)
	var_r29 = (uint32_t)(PPC_LOAD_U32(var_r23 + 4));
	// cmpwi cr6,r29,0
	// ble cr6,0x82256f20
while (ctx.cr6.gt) {
	loc_82256EB0:
		// lwz r31,0(r28)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// addi r30,r31,304
		var_r30 = (uint32_t)(var_r31 + 304);
		// addi r3,r31,400
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 400;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x822556e0
		util_56E0(ctx, base);
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r30,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,128
		ctx.r9.s64 = ctx.r1.s64 + 128;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x822553b8
		phArticulatedCollider_53B8_2h(ctx, base);
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// addi r11,r31,976
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 976;
		// addi r29,r29,-1
		var_r29 = (uint32_t)(var_r29 + -1);
		// addi r7,r11,16
		ctx.r7.s64 = ctx.r11.s64 + 16;
		// addi r28,r28,4
		var_r28 = (uint32_t)(var_r28 + 4);
		// lvx128 v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// cmpwi cr6,r29,0
		ctx.cr6.compare<int32_t>((int32_t)var_r29, 0, ctx.xer);
		// stvx v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,128
		ctx.r6.s64 = ctx.r1.s64 + 128;
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bgt cr6,0x82256eb0
}
loc_82256F20:
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 4);
	// addi r22,r23,168
	var_r22 = (uint32_t)(var_r23 + 168);
	// mr r9,r22
	ctx.r9.u64 = var_r22;
	// cmpwi cr6,r11,1
	// ble cr6,0x82256f6c
	if (ctx.r11.s32 > 1) {
		// addi r10,r11,-1
		ctx.r10.s64 = ctx.r11.s64 + -1;
	loc_82256F38:
		// lwz r11,0(r9)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// addi r11,r11,496
		ctx.r11.s64 = ctx.r11.s64 + 496;
		// cmplwi cr6,r10,0
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
		// addi r8,r11,16
		ctx.r8.s64 = ctx.r11.s64 + 16;
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v10,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
		// stvx v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
		// stvx v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x82256f38
		if (!ctx.cr6.eq) goto loc_82256F38;
	}
loc_82256F6C:
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 4);
	// addi r5,r11,9
	ctx.r5.s64 = ctx.r11.s64 + 9;
	// addi r4,r11,40
	ctx.r4.s64 = ctx.r11.s64 + 40;
	// addi r3,r11,71
	ctx.r3.s64 = ctx.r11.s64 + 71;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r8,r23
	var_r29 = (uint32_t)(ctx.r8.u64 + var_r23);
	// add r28,r9,r23
	var_r28 = (uint32_t)(ctx.r9.u64 + var_r23);
	// add r27,r10,r23
	var_r27 = (uint32_t)(ctx.r10.u64 + var_r23);
	// cmpwi cr6,r11,1
	// ble cr6,0x82257090
	if (ctx.r11.s32 > 1) {
		// addi r26,r11,-1
		var_r26 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8202ffff
	loc_82256FA0:
		// lwz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
		// lwz r31,0(r28)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
		// addi r11,r11,10
		ctx.r11.s64 = ctx.r11.s64 + 10;
		// lwz r25,0(r29)
		var_r25 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0));
		// addi r30,r31,528
		var_r30 = (uint32_t)(var_r31 + 528);
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r8,r30,16
		ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r4,r25,304
		ctx.r4.s64 = (int64_t)(int32_t)var_r25 + 304;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwzx r24,r10,r23
		var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r23));
		// addi r11,r24,304
		ctx.r11.s64 = (int64_t)(int32_t)var_r24 + 304;
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// lvx128 v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v7,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v7,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x822553b8
		phArticulatedCollider_53B8_2h(ctx, base);
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// addi r3,r31,240
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 240;
		// bl 0x822556e0
		util_56E0(ctx, base);
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// addi r11,r25,976
		ctx.r11.s64 = (int64_t)(int32_t)var_r25 + 976;
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// addi r30,r31,496
		var_r30 = (uint32_t)(var_r31 + 496);
		// lvx128 v6,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// vaddfp v4,v6,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvx128 v5,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// stvx v4,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v3,v5,v0
		simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v3,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r7,96(r8)
		// bctrl
		VCALL(ctx.r3.u32, 24, ctx, base);  // vtable slot 24 (byte +96)
		// addi r11,r24,976
		ctx.r11.s64 = (int64_t)(int32_t)var_r24 + 976;
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r30,16
		ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r26,r26,-1
		var_r26 = (uint32_t)(var_r26 + -1);
		// addi r29,r29,-4
		var_r29 = (uint32_t)(var_r29 + -4);
		// lvx128 v2,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r28,r28,-4
		var_r28 = (uint32_t)(var_r28 + -4);
		// vaddfp v1,v2,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r27,r27,-4
		var_r27 = (uint32_t)(var_r27 + -4);
		// cmplwi cr6,r26,0
		// stvx v1,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v31,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v30,v31,v0
		simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v30,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x82256fa0
		if (var_r26 != 0) goto loc_82256FA0;
	}
loc_82257090:
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 4);
	// mr r27,r22
	var_r27 = (uint32_t)(var_r22);
	// addi r28,r11,-1
	var_r28 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8202ffff
	// cmpwi cr6,r28,0
	// ble cr6,0x822571c0
	if ((int32_t)var_r28 > 0) {
		// lis r11,-32160
		// addi r29,r27,124
		var_r29 = (uint32_t)(var_r27 + 124);
		// addi r25,r11,26448
		var_r25 = (uint32_t)(ctx.r11.s64 + 26448);  // lbl_82606750 @ 0x82606750
	loc_822570B0:
		// lwz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lwz r31,0(r27)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r27 + 0));
		// addi r4,r11,10
		ctx.r4.s64 = ctx.r11.s64 + 10;
		// lwz r30,-248(r29)
		var_r30 = (uint32_t)(PPC_LOAD_U32(var_r29 + -248));
		// rlwinm r3,r4,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r4,r31,528
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 528;
		// lwzx r26,r3,r23
		var_r26 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + var_r23));
		// addi r3,r31,48
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 48;
		// bl 0x822556e0
		util_56E0(ctx, base);
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// addi r30,r30,976
		var_r30 = (uint32_t)(var_r30 + 976);
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lvx128 v29,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r26,976
		ctx.r11.s64 = (int64_t)(int32_t)var_r26 + 976;
		// vsubfp v13,v0,v29
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v29.f32)));
		// lvx128 v28,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v0,v0,v28
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v28.f32)));
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// addi r10,r31,496
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 496;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r8,r10,16
		ctx.r8.s64 = ctx.r10.s64 + 16;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stvx v13,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r30,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
		// vaddfp v13,v13,v12
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// stvx v13,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v0,v0,v12
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// vsubfp v27,v13,v12
		simde_mm_store_ps(ctx.v27.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// stvx v27,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// vsubfp v26,v0,v13
		simde_mm_store_ps(ctx.v26.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v26,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v25,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v25,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,128
		ctx.r7.s64 = ctx.r1.s64 + 128;
		// lvx128 v24,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v24,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,104(r6)
		// bctrl
		VCALL(ctx.r3.u32, 26, ctx, base);  // vtable slot 26 (byte +104)
		// addi r10,r1,112
		ctx.r10.s64 = ctx.r1.s64 + 112;
		// addi r9,r30,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 16;
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// addi r27,r27,4
		var_r27 = (uint32_t)(var_r27 + 4);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// lvx128 v23,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// cmpwi cr6,r28,0
		// stvx v23,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r1,128
		ctx.r8.s64 = ctx.r1.s64 + 128;
		// lvx128 v22,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v22,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bgt cr6,0x822570b0
		if ((int32_t)var_r28 > 0) goto loc_822570B0;
	}
loc_822571C0:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_71C8_w"))) PPC_WEAK_FUNC(phArticulatedCollider_71C8_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_71C8_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// vpermwi128 v10,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r30,10
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 10;
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v0,v11,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lwzx r28,r9,r31
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + var_r31));
	// addi r3,r28,784
	ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 784;
	// vnmsubfp v0,v10,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r6,0(r28)
	ctx.r6.u64 = PPC_LOAD_U32(var_r28 + 0);
	// rlwinm r11,r6,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + var_r28;
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// bl 0x822556e0
	util_56E0(ctx, base);
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r27,1
	var_r27 = 1;
	// cmpwi cr6,r5,1
	// mr r11,r27
	ctx.r11.u64 = var_r27;
	// ble cr6,0x82257270
	if (ctx.r5.s32 > 1) {
		// addi r10,r31,424
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 424;
		// li r9,0
		ctx.r9.s64 = 0;
	loc_82257258:
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// lwz r4,4(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpw cr6,r11,r4
		// blt cr6,0x82257258
		if (ctx.r11.s32 < ctx.r4.s32) goto loc_82257258;
	}
loc_82257270:
	// cmpwi cr6,r30,0
	// ble cr6,0x822572e4
while (ctx.cr6.gt) {
	loc_82257278:
		// addi r10,r30,72
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 72;
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// addi r3,r30,41
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 41;
		// rlwinm r8,r10,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r11,r11,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
		// rlwinm r9,r3,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// lwzx r29,r8,r31
		var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + var_r31));
		// addi r4,r11,16
		ctx.r4.s64 = ctx.r11.s64 + 16;
		// addi r7,r29,10
		ctx.r7.s64 = (int64_t)(int32_t)var_r29 + 10;
		// lwzx r3,r9,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r31);
		// rlwinm r6,r7,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r28,r6,r31
		var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r6.u32 + var_r31));
		// lwz r5,0(r28)
		ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
		// rlwinm r11,r5,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// lwz r10,108(r11)
		// bctrl
		VCALL(ctx.r3.u32, 27, ctx, base);  // vtable slot 27 (byte +108)
		// addi r9,r30,105
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 105;
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// cmpwi cr6,r29,0
		ctx.cr6.compare<int32_t>((int32_t)var_r29, 0, ctx.xer);
		// stwx r27,r8,r31
		PPC_STORE_U32(ctx.r8.u32 + var_r31, var_r27);
		// bgt cr6,0x82257278
}
loc_822572E4:
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r7,1
	// ble cr6,0x8225735c
	if (ctx.r7.s32 > 1) {
		// addi r30,r31,168
		var_r30 = (uint32_t)(var_r31 + 168);
	loc_822572F4:
		// lwz r6,256(r30)
		ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 256);
		// cmpwi cr6,r6,0
		// bne cr6,0x82257348
		if (ctx.r6.s32 == 0) {
			// lwz r10,124(r30)
			ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 124);
			// lwz r11,-124(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + -124);
			// addi r5,r10,10
			ctx.r5.s64 = ctx.r10.s64 + 10;
			// lwz r3,0(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
			// rlwinm r4,r5,2,0,29
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			// lwz r8,0(r11)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r7,0(r3)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// lwzx r10,r4,r31
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + var_r31);
			// lwz r6,112(r7)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 112);
			// lwz r9,0(r10)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
			// rlwinm r9,r9,5,0,26
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
			// add r10,r9,r10
			ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
			// addi r4,r10,16
			ctx.r4.s64 = ctx.r10.s64 + 16;
			// rlwinm r10,r8,5,0,26
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
			// add r11,r10,r11
			ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// bctrl
			PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
		}
	loc_82257348:
		// lwz r5,4(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r27,r27,1
		var_r27 = (uint32_t)(var_r27 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r27,r5
		// blt cr6,0x822572f4
		if ((int32_t)var_r27 < ctx.r5.s32) goto loc_822572F4;
	}
loc_8225735C:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_7368_w"))) PPC_WEAK_FUNC(phArticulatedCollider_7368_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_7368_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r11,r30,10
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 10;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lwzx r28,r9,r31
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + var_r31));
	// addi r3,r28,784
	ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 784;
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r6,0(r28)
	ctx.r6.u64 = PPC_LOAD_U32(var_r28 + 0);
	// rlwinm r11,r6,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + var_r28;
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// bl 0x822556e0
	util_56E0(ctx, base);
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r27,1
	var_r27 = 1;
	// cmpwi cr6,r5,1
	// mr r11,r27
	ctx.r11.u64 = var_r27;
	// ble cr6,0x822573f8
	if (ctx.r5.s32 > 1) {
		// addi r10,r31,424
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 424;
		// li r9,0
		ctx.r9.s64 = 0;
	loc_822573E0:
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// lwz r4,4(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpw cr6,r11,r4
		// blt cr6,0x822573e0
		if (ctx.r11.s32 < ctx.r4.s32) goto loc_822573E0;
	}
loc_822573F8:
	// cmpwi cr6,r30,0
	// ble cr6,0x8225746c
while (ctx.cr6.gt) {
	loc_82257400:
		// addi r3,r30,72
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 72;
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// addi r10,r30,41
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 41;
		// rlwinm r9,r3,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r11,r11,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
		// rlwinm r8,r10,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// lwzx r29,r9,r31
		var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + var_r31));
		// addi r4,r11,16
		ctx.r4.s64 = ctx.r11.s64 + 16;
		// addi r7,r29,10
		ctx.r7.s64 = (int64_t)(int32_t)var_r29 + 10;
		// lwzx r3,r8,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
		// rlwinm r5,r7,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// lwz r6,0(r3)
  // [ph4a] vtable load collapsed
		// lwzx r28,r5,r31
		var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + var_r31));
		// lwz r10,108(r6)
  // [ph4a] slot load collapsed
		// lwz r9,0(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 0);
		// rlwinm r11,r9,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// bctrl
		VCALL(ctx.r3.u32, 27, ctx, base);  // pattern-B slot 27 (byte +108)
		// addi r8,r30,105
		ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 105;
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// rlwinm r7,r8,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// cmpwi cr6,r29,0
		ctx.cr6.compare<int32_t>((int32_t)var_r29, 0, ctx.xer);
		// stwx r27,r7,r31
		PPC_STORE_U32(ctx.r7.u32 + var_r31, var_r27);
		// bgt cr6,0x82257400
}
loc_8225746C:
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r6,1
	// ble cr6,0x822574e4
	if (ctx.r6.s32 > 1) {
		// addi r30,r31,168
		var_r30 = (uint32_t)(var_r31 + 168);
	loc_8225747C:
		// lwz r5,256(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 256);
		// cmpwi cr6,r5,0
		// bne cr6,0x822574d0
		if (ctx.r5.s32 == 0) {
			// lwz r10,124(r30)
			ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 124);
			// lwz r11,-124(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + -124);
			// addi r4,r10,10
			ctx.r4.s64 = ctx.r10.s64 + 10;
			// lwz r3,0(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
			// rlwinm r10,r4,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// lwz r9,0(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r8,0(r3)
  // [ph4a] vtable load collapsed
			// rlwinm r9,r9,5,0,26
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
			// lwzx r10,r10,r31
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
			// add r11,r9,r11
			ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
			// addi r5,r11,16
			ctx.r5.s64 = ctx.r11.s64 + 16;
			// lwz r7,112(r8)
  // [ph4a] slot load collapsed
			// lwz r6,0(r10)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
			// rlwinm r11,r6,5,0,26
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
			// add r11,r11,r10
			ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
			// addi r4,r11,16
			ctx.r4.s64 = ctx.r11.s64 + 16;
			// bctrl
			VCALL(ctx.r3.u32, 28, ctx, base);  // pattern-B slot 28 (byte +112)
		}
	loc_822574D0:
		// lwz r5,4(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r27,r27,1
		var_r27 = (uint32_t)(var_r27 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r27,r5
		// blt cr6,0x8225747c
		if ((int32_t)var_r27 < ctx.r5.s32) goto loc_8225747C;
	}
loc_822574E4:
	return;
}

__attribute__((alias("__imp__phJoint1Dof_74F0_v12"))) PPC_WEAK_FUNC(phJoint1Dof_74F0_v12);
PPC_FUNC_IMPL(__imp__phJoint1Dof_74F0_v12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t ea{};
	// FRAME: size=176, savegprlr_25
	// mr r25,r4
	var_r25 = ctx.r4.u32;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r25,42
	ctx.r11.s64 = (int64_t)(int32_t)var_r25 + 42;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r30,r10,r31
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r31));
	// lwz r11,28(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 28);
	// lwz r26,24(r30)
	var_r26 = (uint32_t)(PPC_LOAD_U32(var_r30 + 24));
	// addi r10,r11,784
	ctx.r10.s64 = ctx.r11.s64 + 784;
	// addi r9,r11,848
	ctx.r9.s64 = ctx.r11.s64 + 848;
	// addi r7,r10,32
	ctx.r7.s64 = ctx.r10.s64 + 32;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r4,r9,32
	ctx.r4.s64 = ctx.r9.s64 + 32;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// vmsum3fp128 v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r3,r9,16
	ctx.r3.s64 = ctx.r9.s64 + 16;
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// add r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r27,r11,16
	var_r27 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82600010
	// addi r11,r26,784
	ctx.r11.s64 = (int64_t)(int32_t)var_r26 + 784;
	// addi r10,r26,848
	ctx.r10.s64 = (int64_t)(int32_t)var_r26 + 848;
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// addi r6,r10,32
	ctx.r6.s64 = ctx.r10.s64 + 32;
	// vmrghw v0,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r9,r27,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r27 + 16;
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r3,0(r26)
	ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 0);
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r11,r3,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 5) & 0xFFFFFFE0;
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// add r11,r11,r26
	ctx.r11.u64 = ctx.r11.u64 + var_r26;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// addi r29,r11,16
	var_r29 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82600010
	// lis r11,-32160
	// addi r28,r29,16
	var_r28 = (uint32_t)(var_r29 + 16);
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v10,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// vmrghw v0,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v12,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v13,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v11,v0,v12
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v11,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,108(r11)
	// bctrl
	VCALL(ctx.r3.u32, 27, ctx, base);  // vtable slot 27 (byte +108)
	// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
	// mr r5,r27
	ctx.r5.u64 = var_r27;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r8,116(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r30, 29, ctx, base);  // pattern-B slot 29 (byte +116)
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r27,1
	var_r27 = 1;
	// mr r11,r27
	ctx.r11.u64 = var_r27;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v8,v9,v10
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v10.f32)));
	// stvx v8,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvx128 v6,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v5,v6,v7
	simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v7.f32)));
	// stvx v5,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r5,1
	// ble cr6,0x822576d8
	if (ctx.r5.s32 > 1) {
		// addi r10,r31,424
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 424;
		// li r9,0
		ctx.r9.s64 = 0;
	loc_822576C0:
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// lwz r4,4(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpw cr6,r11,r4
		// blt cr6,0x822576c0
		if (ctx.r11.s32 < ctx.r4.s32) goto loc_822576C0;
	}
loc_822576D8:
	// addi r3,r25,73
	ctx.r3.s64 = (int64_t)(int32_t)var_r25 + 73;
	// addi r11,r25,106
	ctx.r11.s64 = (int64_t)(int32_t)var_r25 + 106;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r28,r26
	var_r28 = (uint32_t)(var_r26);
	// lwzx r30,r10,r31
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r31));
	// stwx r27,r9,r31
	PPC_STORE_U32(ctx.r9.u32 + var_r31, var_r27);
	// cmpwi cr6,r30,0
	// ble cr6,0x82257768
while (ctx.cr6.gt) {
	loc_822576FC:
		// addi r8,r30,72
		ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 72;
		// lwz r7,0(r28)
		ctx.r7.u64 = PPC_LOAD_U32(var_r28 + 0);
		// addi r6,r30,41
		ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 41;
		// rlwinm r5,r8,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r11,r7,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
		// rlwinm r4,r6,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// lwzx r29,r5,r31
		var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + var_r31));
		// addi r10,r29,10
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 10;
		// lwzx r3,r4,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + var_r31);
		// addi r4,r11,16
		ctx.r4.s64 = ctx.r11.s64 + 16;
		// rlwinm r9,r10,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// lwz r8,0(r3)
  // [ph4a] vtable load collapsed
		// lwzx r28,r9,r31
		var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + var_r31));
		// lwz r6,108(r8)
  // [ph4a] slot load collapsed
		// lwz r7,0(r28)
		ctx.r7.u64 = PPC_LOAD_U32(var_r28 + 0);
		// rlwinm r11,r7,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// bctrl
		VCALL(ctx.r3.u32, 27, ctx, base);  // pattern-B slot 27 (byte +108)
		// addi r5,r30,105
		ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 105;
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// rlwinm r4,r5,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// cmpwi cr6,r29,0
		ctx.cr6.compare<int32_t>((int32_t)var_r29, 0, ctx.xer);
		// stwx r27,r4,r31
		PPC_STORE_U32(ctx.r4.u32 + var_r31, var_r27);
		// bgt cr6,0x822576fc
}
loc_82257768:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r29,r31,292
	var_r29 = (uint32_t)(var_r31 + 292);
	// addi r28,r11,-1
	var_r28 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825fffff
	// cmpwi cr6,r28,0
	// ble cr6,0x822577e8
	if ((int32_t)var_r28 > 0) {
		// addi r30,r29,-248
		var_r30 = (uint32_t)(var_r29 + -248);
	loc_82257780:
		// lwz r3,380(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 380);
		// cmpwi cr6,r3,0
		// bne cr6,0x822577d4
		if (ctx.r3.s32 == 0) {
			// lwz r11,0(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
			// lwz r10,0(r29)
			ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
			// lwz r3,124(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 124);
			// addi r10,r10,10
			ctx.r10.s64 = ctx.r10.s64 + 10;
			// lwz r9,0(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// rlwinm r8,r10,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r10,r9,5,0,26
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
			// lwz r7,0(r3)
  // [ph4a] vtable load collapsed
			// add r10,r10,r11
			ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
			// lwzx r11,r8,r31
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
			// addi r5,r10,16
			ctx.r5.s64 = ctx.r10.s64 + 16;
			// lwz r9,112(r7)
  // [ph4a] slot load collapsed
			// lwz r6,0(r11)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// rlwinm r10,r6,5,0,26
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
			// add r11,r10,r11
			ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
			// addi r4,r11,16
			ctx.r4.s64 = ctx.r11.s64 + 16;
			// bctrl
			VCALL(ctx.r3.u32, 28, ctx, base);  // pattern-B slot 28 (byte +112)
		}
	loc_822577D4:
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpwi cr6,r28,0
		// bgt cr6,0x82257780
		if ((int32_t)var_r28 > 0) goto loc_82257780;
	}
loc_822577E8:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_77F0_w"))) PPC_WEAK_FUNC(phArticulatedCollider_77F0_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_77F0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r30,r31,40
	var_r30 = (uint32_t)(var_r31 + 40);
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// li r27,0
	var_r27 = 0;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x8225783c
while (!ctx.cr6.lt) {
	loc_82257824:
		// lwz r9,0(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
		// stw r27,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, var_r27);
		// bge cr6,0x82257824
}
loc_8225783C:
	// lis r11,-32163
	// mr r6,r28
	ctx.r6.u64 = var_r28;
	// addi r5,r11,-18544
	ctx.r5.s64 = ctx.r11.s64 + -18544;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822571c8
	phArticulatedCollider_71C8_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x82257884
	if (ctx.r11.s32 >= 0) {
		// li r9,1
		ctx.r9.s64 = 1;
	loc_8225786C:
		// lwz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		// stw r9,0(r8)
		PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
		// bge cr6,0x8225786c
		if (ctx.r11.s32 >= 0) goto loc_8225786C;
	}
loc_82257884:
	// lis r11,-32163
	// mr r6,r28
	ctx.r6.u64 = var_r28;
	// addi r5,r11,-18528
	ctx.r5.s64 = ctx.r11.s64 + -18528;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822571c8
	phArticulatedCollider_71C8_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x822578cc
	if (ctx.r11.s32 >= 0) {
		// li r9,2
		ctx.r9.s64 = 2;
	loc_822578B4:
		// lwz r7,0(r10)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		// stw r9,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r9.u32);
		// bge cr6,0x822578b4
		if (ctx.r11.s32 >= 0) goto loc_822578B4;
	}
loc_822578CC:
	// lis r11,-32163
	// mr r6,r28
	ctx.r6.u64 = var_r28;
	// addi r5,r11,-18512
	ctx.r5.s64 = ctx.r11.s64 + -18512;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822571c8
	phArticulatedCollider_71C8_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x82257910
while (!ctx.cr6.lt) {
	loc_822578F8:
		// lwz r6,0(r10)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
		// stw r27,0(r6)
		PPC_STORE_U32(ctx.r6.u32 + 0, var_r27);
		// bge cr6,0x822578f8
}
loc_82257910:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_7918_w"))) PPC_WEAK_FUNC(phArticulatedCollider_7918_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_7918_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r30,r31,40
	var_r30 = (uint32_t)(var_r31 + 40);
	// li r28,0
	var_r28 = 0;
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x82257960
while (!ctx.cr6.lt) {
	loc_82257948:
		// lwz r9,0(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
		// stw r28,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, var_r28);
		// bge cr6,0x82257948
}
loc_82257960:
	// lis r11,-32163
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// addi r5,r11,-18544
	ctx.r5.s64 = ctx.r11.s64 + -18544;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82257368
	phArticulatedCollider_7368_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x822579a4
	if (ctx.r11.s32 >= 0) {
		// li r9,1
		ctx.r9.s64 = 1;
	loc_8225798C:
		// lwz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		// stw r9,0(r8)
		PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
		// bge cr6,0x8225798c
		if (ctx.r11.s32 >= 0) goto loc_8225798C;
	}
loc_822579A4:
	// lis r11,-32163
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// addi r5,r11,-18528
	ctx.r5.s64 = ctx.r11.s64 + -18528;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82257368
	phArticulatedCollider_7368_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x822579e8
	if (ctx.r11.s32 >= 0) {
		// li r9,2
		ctx.r9.s64 = 2;
	loc_822579D0:
		// lwz r7,0(r10)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		// stw r9,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r9.u32);
		// bge cr6,0x822579d0
		if (ctx.r11.s32 >= 0) goto loc_822579D0;
	}
loc_822579E8:
	// lis r11,-32163
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// addi r5,r11,-18512
	ctx.r5.s64 = ctx.r11.s64 + -18512;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82257368
	phArticulatedCollider_7368_w(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	// blt cr6,0x82257a28
while (!ctx.cr6.lt) {
	loc_82257A10:
		// lwz r6,0(r10)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,0
		ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
		// stw r28,0(r6)
		PPC_STORE_U32(ctx.r6.u32 + 0, var_r28);
		// bge cr6,0x82257a10
}
loc_82257A28:
	return;
}

__attribute__((alias("__imp__phJoint_7A30_fw"))) PPC_WEAK_FUNC(phJoint_7A30_fw);
PPC_FUNC_IMPL(__imp__phJoint_7A30_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=192, savegprlr_28
	// lis r11,-32253
	// lfs f12,8(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// mr r30,r6
	var_r30 = ctx.r6.u32;
	// lfs f13,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f12
	ctx.f12.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r29,r30,16
	var_r29 = (uint32_t)(var_r30 + 16);
	// addi r28,r30,32
	var_r28 = (uint32_t)(var_r30 + 32);
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r4,10
	ctx.r11.s64 = ctx.r4.s64 + 10;
	// stfs f13,8(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stfs f0,0(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r30 + 0,/* phJoint::vtable@+0x0 */ temp.u32);
	// stfs f12,4(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r30 + 4,/* phJoint::flags@+0x4 */ temp.u32);
	// lis r11,-32160
	// lfs f11,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f10,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fneg f13,f11
	ctx.f13.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// stfs f10,0(r29)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r29 + 0,/* phJoint::vtable@+0x0 */ temp.u32);
	// stfs f0,4(r29)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r29 + 4,/* phJoint::flags@+0x4 */ temp.u32);
	// stfs f13,8(r29)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r29 + 8, temp.u32);
	// lfs f9,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fneg f8,f9
	ctx.f8.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// stfs f0,8(r28)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r28 + 8, temp.u32);
	// stfs f8,0(r28)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r28 + 0, temp.u32);
	// stfs f13,4(r28)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r28 + 4, temp.u32);
	// lvx128 v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v10,v0,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx v10,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r31,r9,r10
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32));
	// lfs f13,24(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(var_r31 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,56(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 56);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,88(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 88);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,84(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 84);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,52(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 52);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 20);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,80(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 80);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f0,132(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f0,136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f13,112(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f12,116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f11,120(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f7,88(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f6,84(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lfs f0,72(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 72);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,104(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 104);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,96(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 96);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,64(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 64);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,100(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 100);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,68(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 68);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,32(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f4.f64 = double(temp.f32);
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f0,116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f13,120(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// lvx128 v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v9,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stfs f10,104(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v7,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f7,112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// stvx v6,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v3,v4,v7
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v7.f32)));
	// lvx128 v5,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v1,v2,v5
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v5.f32)));
	// stvx v1,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phJoint1Dof_7BD0_v12"))) PPC_WEAK_FUNC(phJoint1Dof_7BD0_v12);
PPC_FUNC_IMPL(__imp__phJoint1Dof_7BD0_v12) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r10,r4,42
	ctx.r10.s64 = ctx.r4.s64 + 42;
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32253
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// lwzx r10,r9,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-12(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -12, temp.u32);
	// stfs f0,-8(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -8, temp.u32);
	// addi r8,r1,-16
	ctx.r8.s64 = ctx.r1.s64 + -16;
	// lwz r11,28(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// lvx128 v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,80(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,-64(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f13,-60(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// stfs f12,-56(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// addi r7,r1,-64
	ctx.r7.s64 = ctx.r1.s64 + -64;
	// lfs f11,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,52(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,84(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f11,-48(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// stfs f10,-44(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f9,-40(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// addi r5,r1,-48
	ctx.r5.s64 = ctx.r1.s64 + -48;
	// lfs f7,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,88(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,48(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f8,-32(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// stfs f7,-28(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// stfs f6,-24(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// addi r4,r1,-32
	ctx.r4.s64 = ctx.r1.s64 + -32;
	// lfs f3,80(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,52(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 52);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,84(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f5,-64(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// stfs f4,-60(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// stfs f3,-56(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// addi r3,r1,-64
	ctx.r3.s64 = ctx.r1.s64 + -64;
	// lfs f13,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,56(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 56);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,88(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	ctx.f11.f64 = double(temp.f32);
	// lvx128 v10,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f2,-48(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// vsubfp v13,v13,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// stfs f1,-44(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// stfs f0,-40(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// addi r11,r1,-48
	ctx.r11.s64 = ctx.r1.s64 + -48;
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f13,-32(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// vsubfp v12,v12,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stfs f12,-28(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// stfs f11,-24(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -24, temp.u32);
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// lvx128 v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v11,v11,v7
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmrghw v10,v12,v9
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrglw v9,v12,v9
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v12,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v13,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v11,v12,v10
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v13,v9
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v12,v12,v10
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmsum3fp128 v11,v0,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmsum3fp128 v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmrghw v0,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v6,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v6,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phSimulator_7D10_2hr"))) PPC_WEAK_FUNC(phSimulator_7D10_2hr);
PPC_FUNC_IMPL(__imp__phSimulator_7D10_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phSimulator::flags@+0x4 */;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r7,r3,40
	ctx.r7.s64 = ctx.r3.s64 + 40;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// cmpwi cr6,r11,0
	// ble cr6,0x82257d8c
	if (ctx.r11.s32 > 0) {
		// addi r8,r3,420
		ctx.r8.s64 = ctx.r3.s64 + 420;
	loc_82257D2C:
		// stw r4,0(r8)
		PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r4.u32);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// lwz r10,0(r7)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// addi r11,r10,16
		ctx.r11.s64 = ctx.r10.s64 + 16;
		// addi r10,r10,48
		ctx.r10.s64 = ctx.r10.s64 + 48;
		// addi r6,r11,16
		ctx.r6.s64 = ctx.r11.s64 + 16;
		// addi r5,r10,16
		ctx.r5.s64 = ctx.r10.s64 + 16;
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v0,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v13,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
		// stvx v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v11,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
		// stvx v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r10,4(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phSimulator::flags@+0x4 */;
		// cmpw cr6,r9,r10
		// blt cr6,0x82257d2c
		if (ctx.r9.s32 < ctx.r10.s32) goto loc_82257D2C;
	}
loc_82257D8C:
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phSimulator::flags@+0x4 */;
	// addi r8,r3,168
	ctx.r8.s64 = ctx.r3.s64 + 168;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addic. r6,r7,-1
	ctx.xer.ca = ctx.r7.u32 > 0;
	ctx.r6.s64 = ctx.r7.s64 + -1;
	// ble 0x82257dfc
while (ctx.r9.s32 < ctx.r5.s32) {
	loc_82257DA0:
		// lwz r10,0(r8)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// addi r11,r10,432
		ctx.r11.s64 = ctx.r10.s64 + 432;
		// addi r10,r10,464
		ctx.r10.s64 = ctx.r10.s64 + 464;
		// addi r7,r11,16
		ctx.r7.s64 = ctx.r11.s64 + 16;
		// addi r6,r10,16
		ctx.r6.s64 = ctx.r10.s64 + 16;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v10,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
		// stvx v10,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
		// stvx v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v8,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_setzero_si128());
		// stvx v8,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v7,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_setzero_si128());
		// stvx v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,4(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* phSimulator::flags@+0x4 */;
		// addi r5,r11,-1
		ctx.r5.s64 = ctx.r11.s64 + -1;
		// cmpw cr6,r9,r5
		// blt cr6,0x82257da0
}
loc_82257DFC:
	// li r11,-1
	ctx.r11.s64 = -1;
	// stw r11,552(r3)
	PPC_STORE_U32(ctx.r3.u32 + 552, ctx.r11.u32);
	// stw r11,548(r3)
	PPC_STORE_U32(ctx.r3.u32 + 548, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phSimulator_7E10_2h"))) PPC_WEAK_FUNC(phSimulator_7E10_2h);
PPC_FUNC_IMPL(__imp__phSimulator_7E10_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=192, savegprlr_29
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r10,r11,42
	ctx.r10.s64 = ctx.r11.s64 + 42;
	// addi r9,r11,73
	ctx.r9.s64 = ctx.r11.s64 + 73;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// addi r30,r11,1
	var_r30 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82030001
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lwzx r3,r8,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
	// lwzx r29,r7,r31
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + var_r31));
	// lwz r11,40(r6)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// fneg f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = ctx.f31.u64 ^ 0x8000000000000000;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v0,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vmulfp128 v8,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v7,v11,v13
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// stvx v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82257f10
	phSimulator_7F10(ctx, base);
	// lis r11,-32160
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lvx128 v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// vsubfp v4,v0,v6
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
	// lvx128 v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v3,v0,v5
	simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v5.f32)));
	// stvx v4,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// stvx v3,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82257f10
	phSimulator_7F10(ctx, base);
	return;
}

__attribute__((alias("__imp__phSimulator_7F10"))) PPC_WEAK_FUNC(phSimulator_7F10);
PPC_FUNC_IMPL(__imp__phSimulator_7F10) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r30,10
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 10;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// rlwinm r29,r11,2,0,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwzx r11,r29,r31
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + var_r31);
	// addi r3,r11,784
	ctx.r3.s64 = ctx.r11.s64 + 784;
	// bl 0x822556e0
	util_56E0(ctx, base);
	// lwz r10,548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 548);
	// cmpwi cr6,r10,-1
	// bne cr6,0x82257f54
	if (ctx.r10.s32 == -1) {
		// stw r30,552(r31)
		PPC_STORE_U32(var_r31 + 552, var_r30);
		// b 0x82257f60
	} else {
	loc_82257F54:
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x822580f0
		phSimulator_80F0(ctx, base);
	}
loc_82257F60:
	// stw r30,548(r31)
	PPC_STORE_U32(var_r31 + 548, var_r30);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwzx r10,r29,r31
	ctx.r10.u64 = PPC_LOAD_U32(var_r29 + var_r31);
	// addi r8,r30,105
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 105;
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v11
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v8
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lvx128 v4,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v3,v4,v5
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v5.f32)));
	// stvx v3,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r3,r7,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r31);
	// ori r11,r3,3
	ctx.r11.u64 = ctx.r3.u64 | 3;
	// stwx r11,r7,r31
	PPC_STORE_U32(ctx.r7.u32 + var_r31, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phSimulator_7FE8_2h"))) PPC_WEAK_FUNC(phSimulator_7FE8_2h);
PPC_FUNC_IMPL(__imp__phSimulator_7FE8_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=208, savegprlr_28
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r30,73
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 73;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r29,r10,r31
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r31));
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x822580f0
	phSimulator_80F0(ctx, base);
	// addi r9,r29,105
	ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 105;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r8,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
	// rlwinm r6,r7,0,30,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r6,0
	// beq cr6,0x8225803c
	if (ctx.r6.s32 != 0) {
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// addi r4,r30,1
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 1;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82258340
		phSimulator_8340_w(ctx, base);
	}
loc_8225803C:
	// addi r4,r30,42
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 42;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// rlwinm r3,r4,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lwzx r3,r3,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r31);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phSimulator::vtable@+0x0 */;
	// lwz r31,28(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 28));
	// lwz r30,24(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// addi r10,r30,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v13,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// lvx128 v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v0,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v9,v0,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// vmsum3fp128 v11,v13,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// stvx v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,128(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fadds f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	return;
}

__attribute__((alias("__imp__phSimulator_80F0"))) PPC_WEAK_FUNC(phSimulator_80F0);
PPC_FUNC_IMPL(__imp__phSimulator_80F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 548);
	// cmpwi cr6,r11,-1
	// beq cr6,0x822581c4
	if (ctx.r11.s32 != -1) {
		// li r10,0
		ctx.r10.s64 = 0;
		// mr r30,r4
		var_r30 = ctx.r4.u32;
		// cmpw cr6,r11,r4
		ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
		// stw r10,556(r31)
		PPC_STORE_U32(var_r31 + 556, ctx.r10.u32);
		// beq cr6,0x82258184
	while (ctx.r11.s32 != (int32_t)var_r30) {
		loc_82258128:
			// lwz r4,548(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 548);
			// cmpw cr6,r4,r30
			// ble cr6,0x8225814c
			if (ctx.r4.s32 > (int32_t)var_r30) {
				// addi r9,r4,72
				ctx.r9.s64 = ctx.r4.s64 + 72;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// rlwinm r8,r9,2,0,29
				ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
				// lwzx r5,r8,r31
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r31);
				// bl 0x822581e0
				phSimulator_81E0_2h(ctx, base);
				// b 0x82258178
			} else {
			loc_8225814C:
				// lwz r11,556(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 556);
				// addi r7,r30,72
				ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 72;
				// addi r5,r11,140
				ctx.r5.s64 = ctx.r11.s64 + 140;
				// rlwinm r6,r7,2,0,29
				ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
				// rlwinm r4,r5,2,0,29
				ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
				// lwzx r11,r6,r31
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r31);
				// stwx r30,r4,r31
				PPC_STORE_U32(ctx.r4.u32 + var_r31, var_r30);
				// lwz r10,556(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 556);
				// mr r30,r11
				var_r30 = ctx.r11.u32;
				// addi r3,r10,1
				ctx.r3.s64 = ctx.r10.s64 + 1;
				// stw r3,556(r31)
				PPC_STORE_U32(var_r31 + 556, ctx.r3.u32);
			}
		loc_82258178:
			// lwz r11,548(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 548);
			// cmpw cr6,r11,r30
			// bne cr6,0x82258128
	}
	loc_82258184:
		// lwz r10,556(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 556);
		// cmpwi cr6,r10,0
		// ble cr6,0x822581c4
	while (ctx.r7.s32 > 0) {
		loc_82258190:
			// lwz r11,556(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 556);
			// mr r5,r30
			ctx.r5.u64 = var_r30;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// addi r9,r11,140
			ctx.r9.s64 = ctx.r11.s64 + 140;
			// rlwinm r8,r9,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
			// stw r11,556(r31)
			PPC_STORE_U32(var_r31 + 556, ctx.r11.u32);
			// lwzx r30,r8,r31
			var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + var_r31));
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// bl 0x82258340
			phSimulator_8340_w(ctx, base);
			// lwz r7,556(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 556);
			// cmpwi cr6,r7,0
			// bgt cr6,0x82258190
	}
	}
loc_822581C4:
	// blr
	return;
}

__attribute__((alias("__imp__phSimulator_81E0_2h"))) PPC_WEAK_FUNC(phSimulator_81E0_2h);
PPC_FUNC_IMPL(__imp__phSimulator_81E0_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t ea{};
	// FRAME: size=176, savegprlr_25
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r28,105
	ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 105;
	// mr r26,r5
	var_r26 = ctx.r5.u32;
	// rlwinm r25,r11,2,0,29
	var_r25 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
	// lwzx r10,r25,r31
	ctx.r10.u64 = PPC_LOAD_U32(var_r25 + var_r31);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r9,0
	// beq cr6,0x82258334
	if (ctx.r9.s32 != 0) {
		// addi r7,r28,41
		ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 41;
		// addi r8,r28,10
		ctx.r8.s64 = (int64_t)(int32_t)var_r28 + 10;
		// rlwinm r4,r7,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r6,r8,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r3,r26,10
		ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 10;
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// rlwinm r10,r3,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r29,r4,r31
		var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + var_r31));
		// lwzx r11,r6,r31
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r31);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// addi r30,r11,48
		var_r30 = (uint32_t)(ctx.r11.s64 + 48);  // addr:0x82600030
		// lwzx r27,r10,r31
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r31));
		// lwz r9,0(r29)
		ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 0)/* phSimulator::vtable@+0x0 */;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// lwz r8,108(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 108);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// addi r11,r29,464
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 464;
		// addi r10,r27,16
		ctx.r10.s64 = (int64_t)(int32_t)var_r27 + 16;
		// addi r8,r11,16
		ctx.r8.s64 = ctx.r11.s64 + 16;
		// addi r9,r27,48
		ctx.r9.s64 = (int64_t)(int32_t)var_r27 + 48;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r10,16
		ctx.r7.s64 = ctx.r10.s64 + 16;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r9,16
		ctx.r6.s64 = ctx.r9.s64 + 16;
		// vaddfp v12,v13,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r5,r30,16
		ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 16;
		// stvx v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// lvx128 v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v11,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v9,v10,v11
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvx v9,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// lvx128 v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v6,v7,v8
		simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v8.f32)));
		// stvx v6,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,96
		ctx.r11.s64 = ctx.r1.s64 + 96;
		// lvx128 v4,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v5,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v3,v4,v5
		simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v5.f32)));
		// stvx v3,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// lvx128 v1,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v2,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v31,v1,v2
		simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v2.f32)));
		// stvx v31,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v29,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v30,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v28,v29,v30
		simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v30.f32)));
		// stvx v28,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v27,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_setzero_si128());
		// stvx v27,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v26,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_setzero_si128());
		// stvx v26,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r8,552(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 552);
		// cmpw cr6,r8,r28
		// bne cr6,0x82258310
		if (ctx.r8.s32 == (int32_t)var_r28) {
			// stw r26,552(r31)
			PPC_STORE_U32(var_r31 + 552, var_r26);
		}
	loc_82258310:
		// stw r26,548(r31)
		PPC_STORE_U32(var_r31 + 548, var_r26);
		// addi r7,r26,105
		ctx.r7.s64 = (int64_t)(int32_t)var_r26 + 105;
		// lwzx r6,r25,r31
		ctx.r6.u64 = PPC_LOAD_U32(var_r25 + var_r31);
		// rlwinm r11,r7,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r5,r6,0,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
		// stwx r5,r25,r31
		PPC_STORE_U32(var_r25 + var_r31, ctx.r5.u32);
		// lwzx r4,r11,r31
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
		// ori r3,r4,3
		ctx.r3.u64 = ctx.r4.u64 | 3;
		// stwx r3,r11,r31
		PPC_STORE_U32(ctx.r11.u32 + var_r31, ctx.r3.u32);
	}
loc_82258334:
	return;
}

__attribute__((alias("__imp__phSimulator_8340_w"))) PPC_WEAK_FUNC(phSimulator_8340_w);
PPC_FUNC_IMPL(__imp__phSimulator_8340_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// addi r11,r5,105
	ctx.r11.s64 = ctx.r5.s64 + 105;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// lwzx r9,r10,r30
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r30);
	// rlwinm r8,r9,0,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r8,0
	// beq cr6,0x82258448
	if (ctx.r8.s32 != 0) {
		// addi r7,r5,10
		ctx.r7.s64 = ctx.r5.s64 + 10;
		// addi r11,r28,10
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 10;
		// rlwinm r5,r7,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r6,r28,41
		ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 41;
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// rlwinm r3,r6,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r5,r30
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r30);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lwzx r27,r10,r30
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r30));
		// addi r29,r11,16
		var_r29 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82600010
		// lwzx r3,r3,r30
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r30);
		// addi r9,r29,16
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 16;
		// addi r31,r3,464
		var_r31 = (uint32_t)(ctx.r3.s64 + 464);
		// lvx128 v0,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r31,16
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 16;
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lvx128 v12,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v0,v0,v12
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v13,v13,v0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r8,112(r9)
		// bctrl
		VCALL(ctx.r3.u32, 28, ctx, base);  // vtable slot 28 (byte +112)
		// addi r11,r27,16
		ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 16;
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r31,16
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 16;
		// addi r10,r11,16
		ctx.r10.s64 = ctx.r11.s64 + 16;
		// addi r7,r28,105
		ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 105;
		// addi r5,r29,16
		ctx.r5.s64 = (int64_t)(int32_t)var_r29 + 16;
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r31,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
		// vaddfp v11,v12,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r9,r7,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// stvx v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v10,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v9,v10,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v9,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v7,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v7,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwzx r3,r9,r30
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r30);
		// ori r11,r3,2
		ctx.r11.u64 = ctx.r3.u64 | 2;
		// stwx r11,r9,r30
		PPC_STORE_U32(ctx.r9.u32 + var_r30, ctx.r11.u32);
	}
loc_82258448:
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8450"))) PPC_WEAK_FUNC(phArticulatedCollider_8450);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8450) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_28
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r30,r3,44
	var_r30 = (uint32_t)(ctx.r3.s64 + 44);
	// addi r10,r3,168
	ctx.r10.s64 = ctx.r3.s64 + 168;
	// addi r29,r11,-1
	var_r29 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x825fffff
	// cmpwi cr6,r29,0
	// ble cr6,0x82258530
	if ((int32_t)var_r29 > 0) {
		// subf r28,r30,r10
		var_r28 = (uint32_t)(ctx.r10.s64 - (int64_t)(int32_t)var_r30);
	loc_82258478:
		// lwzx r3,r28,r30
		ctx.r3.u64 = PPC_LOAD_U32(var_r28 + var_r30);
		// lwz r31,0(r30)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// lwz r10,0(r11)
		// bctrl
		DTOR(ctx.r3.u32, ctx, base);  // vtable slot 0 (destructor)
		// addi r10,r31,288
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 288;
		// addi r9,r31,304
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 304;
		// addi r8,r31,272
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 272;
		// addi r11,r31,320
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 320;
		// addi r7,r31,192
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 192;
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r29,r29,-1
		var_r29 = (uint32_t)(var_r29 + -1);
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// cmpwi cr6,r29,0
		ctx.cr6.compare<int32_t>((int32_t)var_r29, 0, ctx.xer);
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f8,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// lfs f7,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f7.f64 = double(temp.f32);
		// lfs f6,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f6.f64 = double(temp.f32);
		// lfs f12,84(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		ctx.f12.f64 = double(temp.f32);
		// lfs f11,104(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
		ctx.f11.f64 = double(temp.f32);
		// lfs f0,88(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f5,f12,f11
		ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// lfs f13,100(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
		ctx.f13.f64 = double(temp.f32);
		// lfs f10,80(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f10.f64 = double(temp.f32);
		// lfs f9,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f3,f10,f13
		ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f4,f9,f0
		ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fmsubs f2,f0,f13,f5
		ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f5.f64));
		// fmsubs f0,f9,f12,f3
		ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
		// fmsubs f1,f10,f11,f4
		ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f4.f64));
		// fadds f13,f2,f8
		ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
		// stfs f13,0(r11)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// fadds f11,f0,f6
		ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
		// stfs f11,8(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
		// fadds f12,f1,f7
		ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
		// stfs f12,4(r11)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// bgt cr6,0x82258478
		if (ctx.cr6.gt) goto loc_82258478;
	}
loc_82258530:
	return;
}

__attribute__((alias("__imp__phJoint3Dof_8538_w"))) PPC_WEAK_FUNC(phJoint3Dof_8538_w);
PPC_FUNC_IMPL(__imp__phJoint3Dof_8538_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=256, savegprlr_27
	// addi r9,r4,10
	ctx.r9.s64 = ctx.r4.s64 + 10;
	// lfs f13,8(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lfs f12,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32253
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fneg f12,f12
	ctx.f12.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// addi r30,r31,16
	var_r30 = (uint32_t)(var_r31 + 16);
	// stfs f12,112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lwzx r11,r8,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	// addi r29,r31,32
	var_r29 = (uint32_t)(var_r31 + 32);
	// lfs f0,-12016(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r27,r31,48
	var_r27 = (uint32_t)(var_r31 + 48);
	// addi r28,r11,784
	var_r28 = (uint32_t)(ctx.r11.s64 + 784);  // addr:0x82600310
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stfs f0,120(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// addi r7,r28,16
	ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 16;
	// fneg f0,f13
	ctx.f0.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// addi r6,r28,32
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 32;
	// fneg f13,f11
	ctx.f13.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f13,104(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r5,r28,48
	ctx.r5.s64 = (int64_t)(int32_t)var_r28 + 48;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stvx v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// lvx128 v7,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lvx128 v6,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v5,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// lvx128 v8,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,176
	ctx.r8.s64 = ctx.r1.s64 + 176;
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v12,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v9,v10
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// stvx v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,192
	ctx.r7.s64 = ctx.r1.s64 + 192;
	// vmrglw v11,v9,v10
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vmrghw v4,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrglw v3,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v2,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// stvx v5,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// stvx v4,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// lvx128 v1,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r28,64
	ctx.r4.s64 = (int64_t)(int32_t)var_r28 + 64;
	// stvx v1,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stvx v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,176
	ctx.r8.s64 = ctx.r1.s64 + 176;
	// lvx128 v30,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v30,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820d4f88
	util_4F88(ctx, base);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r28,128
	ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 128;
	// vaddfp v29,v9,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v29.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// lvx128 v8,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v11,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v10,v12,v8
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// addi r10,r11,32
	ctx.r10.s64 = ctx.r11.s64 + 32;
	// vmrglw v7,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v8,v12,v8
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v10
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v11,v11,v10
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// stvx v29,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v28,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v27,v28,v12
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmrghw v12,v7,v8
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v27,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v26,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v25,v26,v0
	simde_mm_store_ps(ctx.v25.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v25,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v24,v0,v13
	simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v24,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v23,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v22,v23,v11
	simde_mm_store_ps(ctx.v22.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v23.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx v22,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v21,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v20,v21,v12
	simde_mm_store_ps(ctx.v20.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v21.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v20,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v19,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v18,v19,v0
	simde_mm_store_ps(ctx.v18.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v19.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v18,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v17,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v16,v17,v0
	simde_mm_store_ps(ctx.v16.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v17.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v16,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v15,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v14,v15,v0
	simde_mm_store_ps(ctx.v14.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v15.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v14,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8788_w"))) PPC_WEAK_FUNC(phArticulatedCollider_8788_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8788_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	// FRAME: size=192, savegprlr_26
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// mr r26,r7
	var_r26 = ctx.r7.u32;
	// lis r11,-32163
	// mr r31,r8
	var_r31 = ctx.r8.u32;
	// addi r6,r11,-18544
	ctx.r6.s64 = ctx.r11.s64 + -18544;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// mr r8,r26
	ctx.r8.u64 = var_r26;
	// mr r7,r27
	ctx.r7.u64 = var_r27;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// bl 0x82258870
	phArticulatedCollider_8870_w(ctx, base);
	// lis r11,-32163
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r6,r11,-18528
	ctx.r6.s64 = ctx.r11.s64 + -18528;
	// mr r8,r26
	ctx.r8.u64 = var_r26;
	// mr r7,r27
	ctx.r7.u64 = var_r27;
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82258870
	phArticulatedCollider_8870_w(ctx, base);
	// lis r11,-32163
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r6,r11,-18512
	ctx.r6.s64 = ctx.r11.s64 + -18512;
	// mr r8,r26
	ctx.r8.u64 = var_r26;
	// mr r7,r27
	ctx.r7.u64 = var_r27;
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82258870
	phArticulatedCollider_8870_w(ctx, base);
	// lis r11,-32253
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,0(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 0, temp.u32);
	// stfs f13,4(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 4, temp.u32);
	// stfs f12,8(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// lfs f11,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// stfs f11,16(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// stfs f10,20(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// stfs f9,24(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// lfs f8,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// stfs f8,32(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 32, temp.u32);
	// stfs f7,36(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f6,40(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f0,48(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	// stfs f0,52(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 52, temp.u32);
	// stfs f0,56(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 56, temp.u32);
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8870_w"))) PPC_WEAK_FUNC(phArticulatedCollider_8870_w);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8870_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t ea{};
	// FRAME: size=224, savegprlr_27
	// mr r27,r9
	var_r27 = ctx.r9.u32;
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// vpermwi128 v12,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// vpermwi128 v10,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// addi r11,r30,10
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 10;
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// mr r28,r8
	var_r28 = ctx.r8.u32;
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vmulfp128 v0,v11,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// addi r8,r1,160
	ctx.r8.s64 = ctx.r1.s64 + 160;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// lwzx r11,r10,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
	// addi r3,r11,784
	ctx.r3.s64 = ctx.r11.s64 + 784;
	// vnmsubfp v0,v10,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822556e0
	util_56E0(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpw cr6,r30,r29
	// stw r7,556(r31)
	PPC_STORE_U32(var_r31 + 556, ctx.r7.u32);
	// beq cr6,0x82258978
	if ((int32_t)var_r30 != (int32_t)var_r29) {
		// cmpw cr6,r30,r29
		ctx.cr6.compare<int32_t>((int32_t)var_r30, (int32_t)var_r29, ctx.xer);
	loc_822588F0:
		// bge cr6,0x82258920
		if (ctx.cr6.lt) {
			// lwz r11,556(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 556);
			// addi r5,r29,72
			ctx.r5.s64 = (int64_t)(int32_t)var_r29 + 72;
			// addi r6,r11,140
			ctx.r6.s64 = ctx.r11.s64 + 140;
			// rlwinm r3,r5,2,0,29
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r4,r6,2,0,29
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// stwx r29,r4,r31
			PPC_STORE_U32(ctx.r4.u32 + var_r31, var_r29);
			// lwz r11,556(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 556);
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// stw r11,556(r31)
			PPC_STORE_U32(var_r31 + 556, ctx.r11.u32);
			// lwzx r29,r3,r31
			var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + var_r31));
			// b 0x82258970
		} else {
		loc_82258920:
			// addi r10,r30,41
			ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 41;
			// addi r5,r1,112
			ctx.r5.s64 = ctx.r1.s64 + 112;
			// rlwinm r9,r10,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwzx r3,r9,r31
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + var_r31);
			// lwz r7,108(r8)
			// bctrl
			VCALL(ctx.r3.u32, 27, ctx, base);  // vtable slot 27 (byte +108)
			// addi r5,r1,112
			ctx.r5.s64 = ctx.r1.s64 + 112;
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// addi r6,r30,72
			ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 72;
			// addi r11,r1,128
			ctx.r11.s64 = ctx.r1.s64 + 128;
			// rlwinm r3,r6,2,0,29
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// lvx128 v0,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v0,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// lvx128 v13,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwzx r30,r3,r31
			var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + var_r31));
			// stvx v13,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		}
	loc_82258970:
		// cmpw cr6,r30,r29
		// bne cr6,0x822588f0
		if ((int32_t)var_r30 != (int32_t)var_r29) goto loc_822588F0;
	}
loc_82258978:
	// lwz r9,556(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 556);
	// cmpwi cr6,r9,0
	// ble cr6,0x822589f0
	if (ctx.r9.s32 > 0) {
	loc_82258984:
		// lwz r11,556(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 556);
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r8,r11,140
		ctx.r8.s64 = ctx.r11.s64 + 140;
		// rlwinm r7,r8,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// stw r11,556(r31)
		PPC_STORE_U32(var_r31 + 556, ctx.r11.u32);
		// lwzx r11,r7,r31
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r31);
		// addi r6,r11,41
		ctx.r6.s64 = ctx.r11.s64 + 41;
		// rlwinm r3,r6,2,0,29
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r3,r3,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + var_r31);
		// lwz r10,112(r11)
		// bctrl
		VCALL(ctx.r3.u32, 28, ctx, base);  // vtable slot 28 (byte +112)
		// addi r8,r1,112
		ctx.r8.s64 = ctx.r1.s64 + 112;
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// lwz r9,556(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 556);
		// addi r6,r1,128
		ctx.r6.s64 = ctx.r1.s64 + 128;
		// cmpwi cr6,r9,0
		// lvx128 v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v10,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// lvx128 v12,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bgt cr6,0x82258984
		if (ctx.r9.s32 > 0) goto loc_82258984;
		// b 0x822589f8
	} else {
	loc_822589F0:
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lvx128 v10,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_822589F8:
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v13,v10,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x9C));
	// vpermwi128 v12,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// vpermwi128 v11,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v10,v10,135
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x78));
	// vmulfp128 v0,v12,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vnmsubfp v0,v11,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v0,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// stvx v10,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8A30"))) PPC_WEAK_FUNC(phArticulatedCollider_8A30);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8A30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r7,r3,40
	ctx.r7.s64 = ctx.r3.s64 + 40;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r10,0
	// lwz r11,0(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r9,r11,272
	ctx.r9.s64 = ctx.r11.s64 + 272;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v13,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// blelr cr6
	if (ctx.r10.s32 <= 0) return;
loc_82258A58:
	// lwz r10,0(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// addi r9,r10,272
	ctx.r9.s64 = ctx.r10.s64 + 272;
	// addi r6,r10,288
	ctx.r6.s64 = ctx.r10.s64 + 288;
	// addi r5,r10,304
	ctx.r5.s64 = ctx.r10.s64 + 304;
	// addi r11,r10,320
	ctx.r11.s64 = ctx.r10.s64 + 320;
	// addi r4,r10,192
	ctx.r4.s64 = ctx.r10.s64 + 192;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f12,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f12,f11
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f13,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f10,f13
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f13,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f10,f11,f4
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r6,4(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r8,r6
	// blt cr6,0x82258a58
	if (ctx.r8.s32 < ctx.r6.s32) goto loc_82258A58;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8B10"))) PPC_WEAK_FUNC(phArticulatedCollider_8B10);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8B10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r5,r3,40
	ctx.r5.s64 = ctx.r3.s64 + 40;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r10,0
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r9,r11,288
	ctx.r9.s64 = ctx.r11.s64 + 288;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v10,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// blelr cr6
	if (ctx.r10.s32 <= 0) return;
	// vpermwi128 v9,v10,99
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x9C));
	// vpermwi128 v8,v10,135
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x78));
loc_82258B40:
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r4,r1,-64
	ctx.r4.s64 = ctx.r1.s64 + -64;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r7,r10,192
	ctx.r7.s64 = ctx.r10.s64 + 192;
	// addi r8,r10,272
	ctx.r8.s64 = ctx.r10.s64 + 272;
	// addi r9,r10,288
	ctx.r9.s64 = ctx.r10.s64 + 288;
	// addi r11,r10,320
	ctx.r11.s64 = ctx.r10.s64 + 320;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vmulfp128 v0,v12,v9
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vnmsubfp v0,v11,v8,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vaddfp v12,v0,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r12,304
	ctx.r12.s64 = 304;
	// stvx v0,r10,r12
	ea = (ctx.r10.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,-48
	ctx.r4.s64 = ctx.r1.s64 + -48;
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f12,-60(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,-56(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-64(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,-40(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,-44(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f12,f10
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f9,-48(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f0,f11,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 - ctx.f5.f64));
	// fmsubs f0,f9,f12,f3
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f3.f64));
	// fmsubs f1,f13,f10,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fadds f13,f2,f8
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f11,f0,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f12,f1,f7
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v7,v10,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r12,304
	ctx.r12.s64 = 304;
	// stvx v0,r10,r12
	ea = (ctx.r10.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// lvx128 v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// stvx v4,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,-12(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,-8(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f10,f13
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f7,f12,f11
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmsubs f2,f9,f12,f5
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f5.f64));
	// fmsubs f4,f0,f13,f7
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f7.f64));
	// lfs f0,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f3,f10,f11,f6
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 - ctx.f6.f64));
	// lfs f13,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f11,f2,f13
	ctx.f11.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f1,f4,f8
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f12,f3,f0
	ctx.f12.f64 = double(float(ctx.f3.f64 + ctx.f0.f64));
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpw cr6,r6,r8
	// blt cr6,0x82258b40
	if (ctx.r6.s32 < ctx.r8.s32) goto loc_82258B40;
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8C98_wrh"))) PPC_WEAK_FUNC(phArticulatedCollider_8C98_wrh);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8C98_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r22 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=256, savegprlr_16
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r20,r4
	var_r20 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// addi r16,r11,-1
	var_r16 = (uint32_t)(ctx.r11.s64 + -1);  // addr:0x8202ffff
	// mr r19,r6
	var_r19 = ctx.r6.u32;
	// mr r18,r7
	var_r18 = ctx.r7.u32;
	// mr r17,r8
	var_r17 = ctx.r8.u32;
	// li r24,0
	var_r24 = 0;
	// li r21,0
	var_r21 = 0;
	// cmpwi cr6,r16,0
	// ble cr6,0x82258dc8
	if ((int32_t)var_r16 > 0) {
		// lis r11,-32253
		// addi r23,r3,168
		var_r23 = (uint32_t)(ctx.r3.s64 + 168);
		// lfs f31,-12016(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
		var_f31 = double(temp.f32);
	loc_82258CE0:
		// lwz r3,0(r23)
		ctx.r3.u64 = PPC_LOAD_U32(var_r23 + 0);
		// lwz r10,12(r11)
		// bctrl
		VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
		// mr r25,r3
		var_r25 = ctx.r3.u32;
		// li r29,0
		var_r29 = 0;
		// cmpwi cr6,r25,0
		// ble cr6,0x82258db8
		if ((int32_t)var_r25 > 0) {
			// rlwinm r11,r24,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 2) & 0xFFFFFFFC;
			// subf r27,r30,r20
			var_r27 = var_r20 - var_r30;
			// add r31,r11,r30
			var_r31 = (uint32_t)(ctx.r11.u64 + var_r30);
			// subf r26,r30,r17
			var_r26 = var_r17 - var_r30;
			// subf r28,r30,r19
			var_r28 = var_r19 - var_r30;
			// subf r22,r30,r18
			var_r22 = var_r18 - var_r30;
			// add r24,r25,r24
			var_r24 = (uint32_t)(var_r25 + var_r24);
		loc_82258D20:
			// stwx r21,r27,r31
			PPC_STORE_U32(var_r27 + var_r31, var_r21);
			// addi r9,r1,104
			ctx.r9.s64 = ctx.r1.s64 + 104;
			// lwz r3,0(r23)
			ctx.r3.u64 = PPC_LOAD_U32(var_r23 + 0);
			// addi r8,r1,84
			ctx.r8.s64 = ctx.r1.s64 + 84;
			// addi r7,r1,92
			ctx.r7.s64 = ctx.r1.s64 + 92;
			// addi r6,r1,80
			ctx.r6.s64 = ctx.r1.s64 + 80;
			// addi r5,r1,88
			ctx.r5.s64 = ctx.r1.s64 + 88;
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// lwz r10,16(r11)
			// bctrl
			VCALL(ctx.r3.u32, 4, ctx, base);  // vtable slot 4 (byte +16)
			// lfs f13,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f13.f64 = double(temp.f32);
			// lfs f0,84(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f0.f64 = double(temp.f32);
			// lwz r9,88(r1)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			// fsubs f0,f13,f0
			ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
			// stfs f13,96(r1)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
			// stfs f0,100(r1)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
			// lfs f12,92(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
			ctx.f12.f64 = double(temp.f32);
			// stw r9,0(r31)
			PPC_STORE_U32(var_r31 + 0, ctx.r9.u32);
			// stfsx f12,r26,r31
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(var_r26 + var_r31, temp.u32);
			// stfsx f0,r28,r31
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r28 + var_r31, temp.u32);
			// lwz r8,96(r1)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			// lwz r6,100(r1)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
			// rlwinm r7,r8,1,31,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
			// rlwinm r5,r6,1,31,31
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0x1;
			// eqv r4,r7,r5
			ctx.r4.u64 = ~(ctx.r7.u64 ^ ctx.r5.u64);
			// clrlwi r3,r4,31
			ctx.r3.u64 = ctx.r4.u32 & 0x1;
			// cmplwi cr6,r3,0
			// bne cr6,0x82258d9c
			if (ctx.r3.u32 == 0) {
				// stfsx f31,r28,r31
				temp.f32 = float(var_f31);
				PPC_STORE_U32(var_r28 + var_r31, temp.u32);
			}
		loc_82258D9C:
			// lfs f11,104(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
			ctx.f11.f64 = double(temp.f32);
			// addi r29,r29,1
			var_r29 = (uint32_t)(var_r29 + 1);
			// fsubs f10,f13,f11
			ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
			// stfsx f10,r22,r31
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(var_r22 + var_r31, temp.u32);
			// addi r31,r31,4
			var_r31 = (uint32_t)(var_r31 + 4);
			// cmpw cr6,r29,r25
			// blt cr6,0x82258d20
			if ((int32_t)var_r29 < (int32_t)var_r25) goto loc_82258D20;
		}
	loc_82258DB8:
		// addi r21,r21,1
		var_r21 = (uint32_t)(var_r21 + 1);
		// addi r23,r23,4
		var_r23 = (uint32_t)(var_r23 + 4);
		// cmpw cr6,r21,r16
		// blt cr6,0x82258ce0
		if ((int32_t)var_r21 < (int32_t)var_r16) goto loc_82258CE0;
	}
loc_82258DC8:
	// mr r3,r24
	ctx.r3.u64 = var_r24;
	return;
}

__attribute__((alias("__imp__phJoint1Dof_8DD8_p44"))) PPC_WEAK_FUNC(phJoint1Dof_8DD8_p44);
PPC_FUNC_IMPL(__imp__phJoint1Dof_8DD8_p44) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r31,256
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 256;
	// addi r10,r31,192
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 192;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8211d048
	util_D048(ctx, base);
	// addi r11,r31,208
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 208;
	// addi r10,r31,144
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 144;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r6,r10,32
	ctx.r6.s64 = ctx.r10.s64 + 32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_8E48_fw"))) PPC_WEAK_FUNC(phArticulatedCollider_8E48_fw);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_8E48_fw) {
	PPC_FUNC_PROLOGUE();
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stfd f29,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f29.u64);
	// stfd f30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f30.u64);
	// stfd f31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r9,r10,464
	ctx.r9.s64 = ctx.r10.s64 + 464;
	// addi r5,r10,528
	ctx.r5.s64 = ctx.r10.s64 + 528;
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r10,400
	ctx.r11.s64 = ctx.r10.s64 + 400;
	// lfs f13,112(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r10,128
	ctx.r4.s64 = ctx.r10.s64 + 128;
	// addi r3,r10,144
	ctx.r3.s64 = ctx.r10.s64 + 144;
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,32(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stfs f0,36(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 36, temp.u32);
	// stfs f13,40(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 40, temp.u32);
	// stfs f0,48(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 48, temp.u32);
	// stfs f0,52(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 52, temp.u32);
	// stfs f0,56(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 56, temp.u32);
	// lfs f13,112(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,192(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 192);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f29,f11,f13
	var_f29 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f12,196(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 196);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f31,f10,f13
	var_f31 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f30,f12,f13
	var_f30 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f0,0(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f30,8(r9)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// fneg f9,f29
	ctx.f9.u64 = ctx.f29.u64 ^ 0x8000000000000000;
	// stfs f9,4(r9)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// fneg f8,f31
	ctx.f8.u64 = ctx.f31.u64 ^ 0x8000000000000000;
	// stfs f0,20(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// stfs f29,16(r9)
	temp.f32 = float(var_f29);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// fneg f7,f30
	ctx.f7.u64 = ctx.f30.u64 ^ 0x8000000000000000;
	// stfs f8,24(r9)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + 24, temp.u32);
	// stfs f7,32(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 32, temp.u32);
	// stfs f31,36(r9)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f0,48(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 48, temp.u32);
	// stfs f0,52(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 52, temp.u32);
	// stfs f0,56(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 56, temp.u32);
	// bl 0x82259108
	phArticulatedCollider_9108_2hr(ctx, base);
	// lfs f6,192(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 192);
	ctx.f6.f64 = double(temp.f32);
	// addi r11,r5,48
	ctx.r11.s64 = ctx.r5.s64 + 48;
	// lfs f5,196(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 196);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f0,f6,f31
	ctx.f0.f64 = double(float(ctx.f6.f64 * var_f31));
	// lfs f4,200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f13,f5,f30
	ctx.f13.f64 = double(float(ctx.f5.f64 * var_f30));
	// fmuls f12,f4,f29
	ctx.f12.f64 = double(float(ctx.f4.f64 * var_f29));
	// lfs f3,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// lfs f11,40(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,20(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,24(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 24);
	ctx.f5.f64 = double(temp.f32);
	// fadds f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// fadds f1,f12,f0
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fadds f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 + ctx.f12.f64));
	// fadds f4,f10,f0
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// stfs f4,40(r5)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r5.u32 + 40, temp.u32);
	// fadds f8,f1,f9
	ctx.f8.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
	// stfs f8,20(r5)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// fadds f12,f2,f13
	ctx.f12.f64 = double(float(ctx.f2.f64 + ctx.f13.f64));
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f3,196(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 196);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f0,f3,f31,f7
	ctx.f0.f64 = double(float(-(ctx.f3.f64 * var_f31 - ctx.f7.f64)));
	// stfs f0,4(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// stfs f0,16(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// lfs f2,200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f0,f2,f31,f6
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * var_f31 - ctx.f6.f64)));
	// stfs f0,8(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// stfs f0,32(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 32, temp.u32);
	// lfs f1,200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 200);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f0,f1,f30,f5
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * var_f30 - ctx.f5.f64)));
	// stfs f0,24(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 24, temp.u32);
	// stfs f0,36(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 36, temp.u32);
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f29,-32(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f30,-24(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// lfd f31,-16(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_8FC0_g"))) PPC_WEAK_FUNC(phBoundCapsule_8FC0_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_8FC0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// li r12,-32
	ctx.r12.s64 = -32;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + ctx.r12.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lvx128 v127,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// vmsum3fp128 v0,v127,v127
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v127.f32), 0xEF));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// stvx128 v127,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v127.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f0.f64 = double(temp.f32);
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// ble cr6,0x822590e8
	if (ctx.f1.f64 > ctx.f0.f64) {
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// frsp f1,f1
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = double(float(ctx.f1.f64));
		// stfs f1,96(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// lis r11,-32163
		// addi r31,r31,144
		var_r31 = (uint32_t)(var_r31 + 144);
		// addi r11,r11,-18496
		ctx.r11.s64 = ctx.r11.s64 + -18496;
		// addi r10,r31,16
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
		// addi r8,r31,48
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 48;
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// lvx128 v12,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r31,32
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 32;
		// lvx128 v8,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r3,r1,112
		ctx.r3.s64 = ctx.r1.s64 + 112;
		// lvx128 v10,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v6,v8,v10
		simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// vmrglw v10,v8,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// lvx128 v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v7,v12,v9
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
		// vmrglw v12,v12,v9
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
		// vmrglw v9,v7,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
		// vmrghw v8,v12,v10
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v11,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// vrefp v13,v11
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v11.f32)));
		// vnmsubfp v0,v13,v11,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmrghw v11,v7,v6
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
		// stvx v11,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v9,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v8,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmaddfp v0,v13,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v7,v127,v0
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v127.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v7,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bl 0x820c40a8
		phBoundCapsule_40A8_g(ctx, base);
		// addi r4,r1,112
		ctx.r4.s64 = ctx.r1.s64 + 112;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820c3bc8
		phJoint_3BC8_g(ctx, base);
		// addi r11,r31,32
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 32;
		// addi r10,r31,16
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 16;
		// lvx128 v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r31,48
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v11,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vmrglw v10,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// lvx128 v12,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmrghw v13,v0,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vmrglw v0,v0,v12
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
		// vmrghw v6,v11,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// vmrglw v5,v11,v13
		simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
		// vmrghw v4,v10,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// stvx v6,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v5,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v4,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_822590E8:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// li r0,-32
	ctx.r0.s64 = -32;
	// lvx128 v127,r1,r0
	ea = (ctx.r1.u32 + ctx.r0.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v127.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phArticulatedCollider_9108_2hr"))) PPC_WEAK_FUNC(phArticulatedCollider_9108_2hr);
PPC_FUNC_IMPL(__imp__phArticulatedCollider_9108_2hr) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f13,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f10,f13
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f0,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f9,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f7,f9,f12
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fmuls f6,f8,f13
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmadds f5,f7,f12,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f6.f64));
	// fmadds f4,f11,f0,f5
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f5.f64));
	// stfs f4,0(r5)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f3,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f2
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// lfs f0,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f10,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f9,f8
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f6,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f1,f12
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmadds f4,f11,f10,f5
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fmadds f0,f7,f6,f4
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f4.f64));
	// stfs f0,16(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// stfs f0,4(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lfs f3,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f2
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// lfs f0,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f10,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f9,f8
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f6,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f1,f12
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmadds f4,f11,f10,f5
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fmadds f0,f7,f6,f4
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f4.f64));
	// stfs f0,32(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 32, temp.u32);
	// stfs f0,8(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lfs f0,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// lfs f3,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f11,f3,f0
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f13,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f2,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f10,f2,f13
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// lfs f12,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f1,f12
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmadds f7,f10,f13,f8
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f8.f64));
	// fmadds f6,f9,f12,f7
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f6,20(r5)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// lfs f2,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f2,f1
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f5,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f4,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f12,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f8,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f0,f13
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fmadds f6,f3,f12,f7
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fmadds f0,f9,f8,f6
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f6.f64));
	// stfs f0,36(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 36, temp.u32);
	// stfs f0,24(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 24, temp.u32);
	// lfs f0,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f5,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f2,f5,f0
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f4,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f1,f4,f13
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// lfs f3,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f11,f3,f12
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f10,f2,f0
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmadds f9,f1,f13,f10
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f13.f64 + ctx.f10.f64));
	// fmadds f8,f11,f12,f9
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f8,40(r5)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r5.u32 + 40, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_9278_p42"))) PPC_WEAK_FUNC(phJoint1Dof_9278_p42);
PPC_FUNC_IMPL(__imp__phJoint1Dof_9278_p42) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r10,-32158
	// addi r11,r3,144
	ctx.r11.s64 = ctx.r3.s64 + 144;
	// addi r10,r10,-25632
	ctx.r10.s64 = ctx.r10.s64 + -25632;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// vand v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8)));
	// vcmpeqfp. v0,v12,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	ctx.cr6.setFromMask(simde_mm_load_ps(ctx.v0.f32), 0xF);
	// mfocrf r11,2
	ctx.r11.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// not r11,r11
	ctx.r11.u64 = ~ctx.r11.u64;
	// rlwinm r10,r11,25,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 25) & 0x1;
	// cmplwi cr6,r10,0
	// beq cr6,0x822593ec
	if (ctx.r10.u32 != 0) {
		// addi r11,r3,160
		ctx.r11.s64 = ctx.r3.s64 + 160;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// vand v0,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8)));
		// vcmpeqfp. v12,v12,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		ctx.cr6.setFromMask(simde_mm_load_ps(ctx.v12.f32), 0xF);
		// mfocrf r11,2
		ctx.r11.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
		// not r9,r11
		ctx.r9.u64 = ~ctx.r11.u64;
		// rlwinm r8,r9,25,31,31
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 25) & 0x1;
		// cmplwi cr6,r8,0
		// beq cr6,0x822593ec
		if (ctx.r8.u32 == 0) goto loc_822593EC;
		// addi r11,r3,176
		ctx.r11.s64 = ctx.r3.s64 + 176;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vxor v12,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
		// vand v0,v0,v13
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8)));
		// vcmpeqfp. v11,v12,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		ctx.cr6.setFromMask(simde_mm_load_ps(ctx.v11.f32), 0xF);
		// mfocrf r11,2
		ctx.r11.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
		// not r7,r11
		ctx.r7.u64 = ~ctx.r11.u64;
		// rlwinm r6,r7,25,31,31
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1;
		// cmplwi cr6,r6,0
		// beq cr6,0x822593ec
		if (ctx.r6.u32 == 0) goto loc_822593EC;
		// addi r11,r3,160
		ctx.r11.s64 = ctx.r3.s64 + 160;
		// lis r10,-32158
		// lis r6,-32163
		// addi r9,r10,-25616
		ctx.r9.s64 = ctx.r10.s64 + -25616;
		// lis r10,-32160
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r5,-32163
		// vmsum3fp128 v11,v12,v12
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
		// addi r7,r10,26448
		ctx.r7.s64 = ctx.r10.s64 + 26448;
		// addi r6,r6,-18480
		ctx.r6.s64 = ctx.r6.s64 + -18480;
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r5,-18496
		ctx.r5.s64 = ctx.r5.s64 + -18496;
		// addi r8,r3,144
		ctx.r8.s64 = ctx.r3.s64 + 144;
		// addi r10,r3,176
		ctx.r10.s64 = ctx.r3.s64 + 176;
		// lvx128 v9,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vrsqrtefp v10,v11
		simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
		// vcmpeqfp v0,v10,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsel v0,v10,v9,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
		// vmulfp128 v10,v0,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v9,v8,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vnmsubfp v13,v11,v10,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v13,v9,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v10,v12,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v11,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vpermwi128 v12,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// vpermwi128 v10,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vpermwi128 v13,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// vmulfp128 v0,v11,v12
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vnmsubfp v0,v10,v13,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v11,v13,v13
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// lvx128 v0,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v9,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v8,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vrsqrtefp v10,v11
		simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
		// vcmpeqfp v0,v10,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsel v0,v10,v9,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
		// vmulfp128 v10,v0,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v9,v8,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vnmsubfp v12,v11,v10,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v12.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// vmaddfp v0,v12,v9,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v9,v13,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v9,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vpermwi128 v12,v0,135
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
		// vpermwi128 v11,v13,99
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
		// vpermwi128 v10,v0,99
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
		// vpermwi128 v13,v13,135
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
		// vmulfp128 v0,v12,v11
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vnmsubfp v0,v10,v13,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// blr
		return;
	}
loc_822593EC:
	// lis r10,-32253
	// addi r11,r3,144
	ctx.r11.s64 = ctx.r3.s64 + 144;
	// addi r10,r10,-12016
	ctx.r10.s64 = ctx.r10.s64 + -12016;
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stfs f0,32(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stfs f0,36(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 36, temp.u32);
	// stfs f13,40(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 40, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_13"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_13);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_13) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=256, savegprlr_26
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r7
	var_r30 = ctx.r7.u32;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stw r6,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r6.u32);
	// stw r30,28(r31)
	PPC_STORE_U32(var_r31 + 28, var_r30);
	// bl 0x8225aff8
	phJoint1Dof_AFF8_p42(ctx, base);
	// addi r11,r6,144
	ctx.r11.s64 = ctx.r6.s64 + 144;
	// addi r10,r6,192
	ctx.r10.s64 = ctx.r6.s64 + 192;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r11,32
	ctx.r8.s64 = ctx.r11.s64 + 32;
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r11,48
	ctx.r7.s64 = ctx.r11.s64 + 48;
	// lfs f10,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// stfs f0,136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r31,560
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 560;
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r31,576
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 576;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v9,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v12,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// vmrghw v8,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r28,r3,32
	var_r28 = (uint32_t)(ctx.r3.s64 + 32);
	// lfs f13,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32253
	// lfs f11,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r6,r11,48
	ctx.r6.s64 = ctx.r11.s64 + 48;
	// vmrghw v11,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// vmrglw v10,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// lfs f9,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f9.f64 = double(temp.f32);
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r3,16
	ctx.r5.s64 = ctx.r3.s64 + 16;
	// vmsum3fp128 v13,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lfs f0,-12016(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// vmsum3fp128 v12,v0,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// lfs f8,116(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f8.f64 = double(temp.f32);
	// vmsum3fp128 v11,v0,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// lfs f7,104(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f6.f64 = double(temp.f32);
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// stfs f13,128(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r9,r3,32
	ctx.r9.s64 = ctx.r3.s64 + 32;
	// stfs f12,152(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// addi r26,r3,48
	var_r26 = (uint32_t)(ctx.r3.s64 + 48);
	// stfs f11,132(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f10,168(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// stfs f9,144(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f8,148(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f7,160(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// stfs f6,164(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// stfs f0,176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f0,180(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f0,184(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// vmrghw v0,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrglw v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v9,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v8,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v7,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v6,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v8,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// addi r11,r30,144
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 144;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r30,192
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 192;
	// addi r3,r11,32
	ctx.r3.s64 = ctx.r11.s64 + 32;
	// addi r10,r11,48
	ctx.r10.s64 = ctx.r11.s64 + 48;
	// addi r9,r11,16
	ctx.r9.s64 = ctx.r11.s64 + 16;
	// addi r8,r31,640
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 640;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// vsubfp v0,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r31,656
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 656;
	// lvx128 v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v8,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v9,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r30,r3,16
	var_r30 = (uint32_t)(ctx.r3.s64 + 16);
	// vmrglw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r29,r3,32
	var_r29 = (uint32_t)(ctx.r3.s64 + 32);
	// vmrglw v12,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r5,r11,48
	ctx.r5.s64 = ctx.r11.s64 + 48;
	// addi r28,r3,48
	var_r28 = (uint32_t)(ctx.r3.s64 + 48);
	// vmrghw v11,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r10,r3,32
	ctx.r10.s64 = ctx.r3.s64 + 32;
	// vmrglw v10,v9,v8
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// vmsum3fp128 v12,v0,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
	// vmsum3fp128 v11,v0,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v9,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrglw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrglw v13,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v1,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v1.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v31,v10,v9
	simde_mm_store_si128((simde__m128i*)ctx.v31.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v30,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v30.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v1,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v31,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v30,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25896(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25896);  /* glob:lbl_82079AD8 @ 0x82079ad8 */
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,892(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 892, temp.u32);
	return;
}

__attribute__((alias("__imp__phJoint1Dof_96C0"))) PPC_WEAK_FUNC(phJoint1Dof_96C0);
PPC_FUNC_IMPL(__imp__phJoint1Dof_96C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=128, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r7,r31,752
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 752;
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lfs f0,656(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 656);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lfs f13,672(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 672);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// lfs f12,688(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 688);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,144
	ctx.r10.s64 = ctx.r10.s64 + 144;
	// lfs f11,576(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 576);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,592(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 592);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// lfs f9,608(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 608);
	ctx.f9.f64 = double(temp.f32);
	// addi r5,r10,32
	ctx.r5.s64 = ctx.r10.s64 + 32;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r4,r11,16
	ctx.r4.s64 = ctx.r11.s64 + 16;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v6,v11,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v9,v9,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v10,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v8,v8,v13
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v7,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v7,v0
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v11,v11,v13
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v6,v9
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v6.u32)));
	// vmrghw v13,v10,v8
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v10,v7,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v7.u32)));
	// vmrghw v9,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v11,v0,v10
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vpermwi128 v10,v12,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x9C));
	// vpermwi128 v12,v12,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x78));
	// vmrghw v0,v13,v9
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmsum3fp128 v13,v11,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vpermwi128 v9,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v13,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lfs f2,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// vmulfp128 v0,v13,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vnmsubfp v0,v9,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmsum3fp128 v12,v11,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82430fe0
	phBoundCapsule_0FE0_g(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// stfs f1,736(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 736, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_14"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_14);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// fneg f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// lis r11,-32248
	// fneg f12,f2
	ctx.f12.u64 = ctx.f2.u64 ^ 0x8000000000000000;
	// lfd f0,-25856(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25856);
	// fsel f13,f13,f1,f0
	ctx.f13.f64 = ctx.f13.f64 >= 0.0 ? ctx.f1.f64 : ctx.f0.f64;
	// stfs f13,720(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 720, temp.u32);
	// fsel f0,f12,f0,f2
	ctx.f0.f64 = ctx.f12.f64 >= 0.0 ? ctx.f0.f64 : ctx.f2.f64;
	// stfs f13,728(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 728, temp.u32);
	// stfs f0,724(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 724, temp.u32);
	// stfs f0,732(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 732, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_9810"))) PPC_WEAK_FUNC(phJoint1Dof_9810);
PPC_FUNC_IMPL(__imp__phJoint1Dof_9810) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// mr r27,r7
	var_r27 = ctx.r7.u32;
	// bl 0x822596c0
	phJoint1Dof_96C0(ctx, base);
	// lfs f0,736(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 736);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32253
	// lfs f13,732(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 732);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f12,724(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 724);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f0,f12
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// lfs f12,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	// blt cr6,0x82259868
	if (ctx.f13.f64 >= ctx.f12.f64) {
		// bso cr6,0x82259868
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82259858, "bso");
		// li r11,1
		ctx.r11.s64 = 1;
		// stw r11,744(r31)
		PPC_STORE_U32(var_r31 + 744, ctx.r11.u32);
		// b 0x8225988c
	} else {
	loc_82259868:
		// lfs f11,728(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 728);
		ctx.f11.f64 = double(temp.f32);
		// fsubs f13,f0,f11
		ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
		// lfs f10,720(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 720);
		ctx.f10.f64 = double(temp.f32);
		// fsubs f11,f0,f10
		ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f10.f64));
		// fcmpu cr6,f13,f12
		// bgt cr6,0x822598fc
		if (ctx.f13.f64 > ctx.f12.f64) {
			// li r5,0
			ctx.r5.s64 = 0;
			// li r3,0
			ctx.r3.s64 = 0;
			// stw r5,744(r31)
			PPC_STORE_U32(var_r31 + 744, ctx.r5.u32);
			return;
		}
		// bso cr6,0x822598fc
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82259880, "bso");
		// li r10,-1
		ctx.r10.s64 = -1;
		// stw r10,744(r31)
		PPC_STORE_U32(var_r31 + 744, ctx.r10.u32);
	}
loc_8225988C:
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r9,r31,752
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 752;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,288
	ctx.r11.s64 = ctx.r11.s64 + 288;
	// addi r10,r10,288
	ctx.r10.s64 = ctx.r10.s64 + 288;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(var_r30, 0, ctx.xer);
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v0,v12
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f9,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f0,f9,f8
	ctx.f0.f64 = double(float(ctx.f9.f64 - ctx.f8.f64));
	// stfs f0,740(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 740, temp.u32);
	// beq cr6,0x822598f0
	if (!(ctx.cr6.eq)) {
		// lwz r6,744(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 744);
		// stw r6,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r6.u32);
		// stfs f11,0(r29)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r29 + 0, temp.u32);
		// stfs f13,0(r28)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r28 + 0, temp.u32);
		// stfs f0,0(r27)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r27 + 0, temp.u32);
	}
loc_822598F0:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_3"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_3);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_3) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82259810
	phJoint1Dof_9810(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,0
	// bne cr6,0x82259944
	if (ctx.r11.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82259944:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_4"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_4);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_4) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// lwz r10,744(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 744);
	// cmpwi cr6,r10,-1
	// bne cr6,0x822599a0
	if (ctx.r10.s32 == -1) {
		// lfs f0,736(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 736);
		ctx.f0.f64 = double(temp.f32);
		// fneg f13,f0
		ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// stfs f13,0(r6)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// lfs f12,720(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 720);
		ctx.f12.f64 = double(temp.f32);
		// fneg f11,f12
		ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
		// stfs f11,0(r8)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// lfs f10,728(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 728);
		ctx.f10.f64 = double(temp.f32);
		// fneg f9,f10
		ctx.f9.u64 = ctx.f10.u64 ^ 0x8000000000000000;
		// stfs f9,0(r9)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// lfs f8,740(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 740);
		ctx.f8.f64 = double(temp.f32);
		// fneg f7,f8
		ctx.f7.u64 = ctx.f8.u64 ^ 0x8000000000000000;
		// stfs f7,0(r7)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// blr
		return;
	}
loc_822599A0:
	// lfs f6,736(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 736);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,0(r6)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// lfs f5,724(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 724);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,0(r8)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// lfs f4,732(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 732);
	ctx.f4.f64 = double(temp.f32);
	// stfs f4,0(r9)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lfs f3,740(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 740);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,0(r7)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_5"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_5);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lwz r10,744(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 744);
	// cmpwi cr6,r10,1
	// bne cr6,0x82259a04
	if (ctx.r10.s32 == 1) {
		// addi r5,r11,752
		ctx.r5.s64 = ctx.r11.s64 + 752;
		// bl 0x822574f0
		phJoint1Dof_74F0_v12(ctx, base);
		// blr
		return;
	}
loc_82259A04:
	// addi r9,r11,752
	ctx.r9.s64 = ctx.r11.s64 + 752;
	// lis r11,-32160
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822574f0
	phJoint1Dof_74F0_v12(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_6"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_6);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r11,r5,42
	ctx.r11.s64 = ctx.r5.s64 + 42;
	// lwz r10,744(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 744);
	// addi r9,r3,752
	ctx.r9.s64 = ctx.r3.s64 + 752;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r10,-1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, -1, ctx.xer);
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwzx r11,r8,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r4.u32);
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r11,28(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r7,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r9,r6,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// addi r5,r9,16
	ctx.r5.s64 = ctx.r9.s64 + 16;
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v12,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// fneg f1,f1
	ctx.f1.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_1"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_1);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// addi r4,r3,752
	ctx.r4.s64 = ctx.r3.s64 + 752;
	// b 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_9"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_9);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// addi r11,r31,752
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 752;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,8(r10)
	// bctrl
	phJoint1Dof_vfn_2(ctx, base);  // vtable slot 2 (byte +8)  // phJoint1Dof::vfn_2
	// lwz r8,744(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 744);
	// cmpwi cr6,r8,-1
	// bne cr6,0x82259b18
	if (ctx.r8.s32 == -1) {
		// lis r11,-32160
		// lvx128 v13,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v12,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v12,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_82259B18:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_10"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_10);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lvx128 v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,752
	ctx.r10.s64 = ctx.r3.s64 + 752;
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,744(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 744);
	// cmpwi cr6,r9,-1
	// bnelr cr6
	if (ctx.r9.s32 != -1) return;
	// lis r10,-32160
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,26448
	ctx.r10.s64 = ctx.r10.s64 + 26448;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v11,v0,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_7"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_7);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_7) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r11,744(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 744);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// cmpwi cr6,r11,1
	// beq cr6,0x82259b8c
	if (ctx.r11.s32 != 1) {
		// fneg f0,f0
		ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	}
loc_82259B8C:
	// stfs f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r10,r3,752
	ctx.r10.s64 = ctx.r3.s64 + 752;
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v11,v12,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// vsubfp v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r11,r11,384
	ctx.r11.s64 = ctx.r11.s64 + 384;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v10,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_8"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_8);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r9,r3,752
	ctx.r9.s64 = ctx.r3.s64 + 752;
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// addi r11,r11,288
	ctx.r11.s64 = ctx.r11.s64 + 288;
	// addi r10,r10,288
	ctx.r10.s64 = ctx.r10.s64 + 288;
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v0,v12
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// stfs f0,740(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 740, temp.u32);
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lwz r6,744(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 744);
	// cmpwi cr6,r6,-1
	// bnelr cr6
	if (ctx.r6.s32 != -1) return;
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_11"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_11);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_29
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82259810
	phJoint1Dof_9810(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// clrlwi r11,r29,24
	ctx.r11.u64 = var_r29 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x82259de4
	if (ctx.r11.u32 != 0) {
		// addi r30,r31,752
		var_r30 = (uint32_t)(var_r31 + 752);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x8225ae38
		phJoint1Dof_AE38(ctx, base);
		// lis r11,-32253
		// lwz r10,92(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
		// lfs f13,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// addi r11,r11,-12024
		ctx.r11.s64 = ctx.r11.s64 + -12024;
		// cmpwi cr6,r10,1
		// lfs f10,8(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		ctx.f10.f64 = double(temp.f32);
		// bne cr6,0x82259d28
		if (ctx.r10.s32 == 1) {
			// fcmpu cr6,f13,f10
			// ble cr6,0x82259cd8
			if (ctx.f13.f64 > ctx.f10.f64) {
				// fdivs f12,f13,f1
				ctx.f12.f64 = double(float(ctx.f13.f64 / ctx.f1.f64));
				// lis r10,-32248
				ctx.r10.s64 = -2113404928;
				// lfs f0,-25320(r10)
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25320);  /* glob:lbl_82079D18 @ 0x82079d18 */
				ctx.f0.f64 = double(temp.f32);
				// fmuls f13,f13,f0
				ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
				// fneg f0,f12
				ctx.f0.u64 = ctx.f12.u64 ^ 0x8000000000000000;
				// b 0x82259cdc
			} else {
			loc_82259CD8:
				// fmr f0,f10
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = ctx.f10.f64;
			}
		loc_82259CDC:
			// fmuls f13,f13,f31
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = double(float(ctx.f13.f64 * var_f31));
			// lfs f11,84(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f11.f64 = double(temp.f32);
			// lfs f9,88(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f9.f64 = double(temp.f32);
			// fadds f12,f13,f11
			ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
			// fadds f11,f13,f9
			ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
			// fcmpu cr6,f12,f10
			// ble cr6,0x82259d94
			if (ctx.f12.f64 <= ctx.f10.f64) goto loc_82259D94;
			// lfs f13,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f11,f10
			// fdivs f13,f13,f1
			ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f1.f64));
			// lfs f9,128(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 128);
			ctx.f9.f64 = double(temp.f32);
			// fmuls f8,f13,f12
			ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
			// fnmsubs f0,f8,f9,f0
			ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f9.f64 - ctx.f0.f64)));
			// ble cr6,0x82259d94
			if (ctx.f11.f64 <= ctx.f10.f64) goto loc_82259D94;
			// fmuls f7,f13,f11
			ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
			// lis r11,-32164
			ctx.r11.s64 = -2107899904;
			// lfs f13,22580(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22580);  /* glob:lbl_825C5834 @ 0x825c5834 */
			ctx.f13.f64 = double(temp.f32);
			// fnmsubs f0,f7,f13,f0
			ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f13.f64 - ctx.f0.f64)));
			// b 0x82259d94
		} else {
		loc_82259D28:
			// fcmpu cr6,f13,f10
			ctx.fpscr.disableFlushMode();
			// bge cr6,0x82259d48
			if (ctx.f13.f64 < ctx.f10.f64) {
				// fdivs f6,f13,f1
				ctx.f6.f64 = double(float(ctx.f13.f64 / ctx.f1.f64));
				// lis r10,-32248
				ctx.r10.s64 = -2113404928;
				// lfs f0,-25320(r10)
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25320);  /* glob:lbl_82079D18 @ 0x82079d18 */
				ctx.f0.f64 = double(temp.f32);
				// fmuls f13,f13,f0
				ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
				// fneg f0,f6
				ctx.f0.u64 = ctx.f6.u64 ^ 0x8000000000000000;
				// b 0x82259d4c
			} else {
			loc_82259D48:
				// fmr f0,f10
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = ctx.f10.f64;
			}
		loc_82259D4C:
			// fmuls f13,f13,f31
			ctx.fpscr.disableFlushMode();
			ctx.f13.f64 = double(float(ctx.f13.f64 * var_f31));
			// lfs f5,84(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
			ctx.f5.f64 = double(temp.f32);
			// lfs f4,88(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f4.f64 = double(temp.f32);
			// fadds f12,f13,f5
			ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f5.f64));
			// fadds f11,f13,f4
			ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f4.f64));
			// fcmpu cr6,f12,f10
			// bge cr6,0x82259d94
			if (ctx.f12.f64 >= ctx.f10.f64) goto loc_82259D94;
			// lfs f13,0(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);  /* glob:0x825c0000 */
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f11,f10
			// fdivs f13,f13,f1
			ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f1.f64));
			// lfs f9,128(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 128);  /* glob:0x825c0080 */
			ctx.f9.f64 = double(temp.f32);
			// fmuls f3,f13,f12
			ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
			// fnmsubs f0,f3,f9,f0
			ctx.f0.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f0.f64)));
			// bge cr6,0x82259d94
			if (ctx.f11.f64 >= ctx.f10.f64) goto loc_82259D94;
			// fmuls f2,f13,f11
			ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
			// lis r11,-32164
			ctx.r11.s64 = -2107899904;
			// lfs f13,22580(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22580);  /* glob:lbl_825C5834 @ 0x825c5834 */
			ctx.f13.f64 = double(temp.f32);
			// fnmsubs f0,f2,f13,f0
			ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f13.f64 - ctx.f0.f64)));
		}
	loc_82259D94:
		// stfs f0,96(r1)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// lwz r11,28(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,384
		ctx.r11.s64 = ctx.r11.s64 + 384;
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r1,96
		ctx.r9.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// vmulfp128 v0,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vaddfp v11,v12,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32160
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
		// vsubfp v0,v13,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r11,r11,384
		ctx.r11.s64 = ctx.r11.s64 + 384;
		// lvx128 v10,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v9,v10,v0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_82259DE4:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_12"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_12);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t ea{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// addi r5,r31,752
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 752;
	// bl 0x82257bd0
	phJoint1Dof_7BD0_v12(ctx, base);
	// lwz r11,744(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 744);
	// cmpwi cr6,r11,-1
	// bne cr6,0x82259e44
	if (ctx.r11.s32 == -1) {
		// lis r11,-32160
		// lvx128 v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vsubfp v12,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v12,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_82259E44:
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_18"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_18);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32253
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lfs f31,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	var_f31 = double(temp.f32);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
	// fmr f13,f31
	ctx.f13.f64 = var_f31;
	// cmpwi cr6,r11,2
	// beq cr6,0x82259e98
	if (ctx.r11.s32 != 2) {
		// cmpwi cr6,r11,3
		// li r11,0
		ctx.r11.s64 = 0;
		// bne cr6,0x82259e9c
		if (ctx.r11.s32 != 3) goto loc_82259E9C;
	}
loc_82259E98:
	// li r11,1
	ctx.r11.s64 = 1;
loc_82259E9C:
	// clrlwi r10,r11,24
	ctx.r10.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r10,0
	// beq cr6,0x82259ec4
	if (ctx.r10.u32 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x822596c0
		phJoint1Dof_96C0(ctx, base);
		// lfs f13,736(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 736);
		ctx.f13.f64 = double(temp.f32);
		// lfs f0,792(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 792);
		ctx.f0.f64 = double(temp.f32);
		// fsubs f12,f0,f13
		ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
		// lfs f11,784(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 784);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f13,f12,f11
		ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	}
loc_82259EC4:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
	// fmr f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = var_f31;
	// cmpwi cr6,r11,1
	// beq cr6,0x82259ee0
	if (ctx.r11.s32 != 1) {
		// cmpwi cr6,r11,3
		// li r11,0
		ctx.r11.s64 = 0;
		// bne cr6,0x82259ee4
		if (ctx.r11.s32 != 3) goto loc_82259EE4;
	}
loc_82259EE0:
	// li r11,1
	ctx.r11.s64 = 1;
loc_82259EE4:
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82259f48
	if (ctx.r8.u32 != 0) {
		// lwz r11,28(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
		// addi r7,r31,752
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 752;
		// lwz r10,24(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 24);
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// addi r11,r11,288
		ctx.r11.s64 = ctx.r11.s64 + 288;
		// lfs f10,796(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 796);
		ctx.f10.f64 = double(temp.f32);
		// addi r10,r10,288
		ctx.r10.s64 = ctx.r10.s64 + 288;
		// lfs f9,788(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 788);
		ctx.f9.f64 = double(temp.f32);
		// addi r5,r1,96
		ctx.r5.s64 = ctx.r1.s64 + 96;
		// lvx128 v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmsum3fp128 v11,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// vmsum3fp128 v10,v0,v12
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
		// stvx v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lfs f8,80(r1)
		ctx.fpscr.disableFlushModeUnconditional();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f8.f64 = double(temp.f32);
		// lfs f7,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f7.f64 = double(temp.f32);
		// fsubs f0,f8,f7
		ctx.f0.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
		// stfs f0,740(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 740, temp.u32);
		// fsubs f6,f10,f0
		ctx.f6.f64 = double(float(ctx.f10.f64 - ctx.f0.f64));
		// fmuls f0,f6,f9
		ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	}
loc_82259F48:
	// addi r30,r31,752
	var_r30 = (uint32_t)(var_r31 + 752);
	// fadds f31,f0,f13
	ctx.fpscr.disableFlushMode();
	var_f31 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f0,-25512(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);  /* glob:lbl_82079C58 @ 0x82079c58 */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	// ble cr6,0x82259fdc
	if (ctx.f1.f64 > ctx.f0.f64) {
		// fdivs f13,f31,f1
		ctx.f13.f64 = double(float(var_f31 / ctx.f1.f64));
		// lfs f0,804(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 804);
		ctx.f0.f64 = double(temp.f32);
		// lfs f12,800(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 800);
		ctx.f12.f64 = double(temp.f32);
		// lwz r11,28(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
		// lvx128 v0,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,352
		ctx.r11.s64 = ctx.r11.s64 + 352;
		// lvx128 v9,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// fsubs f11,f13,f0
		ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// fsel f10,f11,f13,f0
		ctx.f10.f64 = ctx.f11.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
		// fsel f0,f11,f13,f0
		ctx.f0.f64 = ctx.f11.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
		// fsub f5,f10,f12
		ctx.f5.f64 = ctx.f10.f64 - ctx.f12.f64;
		// fsel f0,f5,f12,f0
		ctx.f0.f64 = ctx.f5.f64 >= 0.0 ? ctx.f12.f64 : ctx.f0.f64;
		// stfs f0,96(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// lvx128 v8,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vspltw v13,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vmulfp128 v0,v0,v13
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vaddfp v7,v9,v0
		simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v7,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r11,-32160
		// addi r11,r11,26448
		ctx.r11.s64 = ctx.r11.s64 + 26448;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
		// vsubfp v0,v13,v0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r11,r11,352
		ctx.r11.s64 = ctx.r11.s64 + 352;
		// lvx128 v6,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v5,v6,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvx v5,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	}
loc_82259FDC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_0"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_0);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=288, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lfs f0,584(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 584);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lfs f13,600(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 600);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,616(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 616);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,144
	ctx.r10.s64 = ctx.r10.s64 + 144;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r7,r10,32
	ctx.r7.s64 = ctx.r10.s64 + 32;
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f11,664(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 664);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lfs f10,680(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 680);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,696(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 696);
	ctx.f9.f64 = double(temp.f32);
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r9,r11,32
	ctx.r9.s64 = ctx.r11.s64 + 32;
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stfs f11,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v0,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v12,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v12,v12,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v11,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x8225b1f8
	phBoundCapsule_B1F8(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// addi r7,r11,16
	ctx.r7.s64 = ctx.r11.s64 + 16;
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,48
	ctx.r11.s64 = ctx.r11.s64 + 48;
	// stvx v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lvx128 v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r7,r1,208
	ctx.r7.s64 = ctx.r1.s64 + 208;
	// addi r10,r31,560
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 560;
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r8,r31,640
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 640;
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// lvx128 v8,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// lvx128 v7,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// addi r4,r11,32
	ctx.r4.s64 = ctx.r11.s64 + 32;
	// lvx128 v6,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r10,r9,144
	ctx.r10.s64 = ctx.r9.s64 + 144;
	// lvx128 v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// addi r8,r10,32
	ctx.r8.s64 = ctx.r10.s64 + 32;
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// addi r9,r9,192
	ctx.r9.s64 = ctx.r9.s64 + 192;
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// vmsum3fp128 v8,v10,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r5,r11,16
	ctx.r5.s64 = ctx.r11.s64 + 16;
	// lvx128 v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vmsum3fp128 v9,v9,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v10,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,192
	ctx.r4.s64 = ctx.r11.s64 + 192;
	// vmrghw v0,v8,v9
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// vmrghw v10,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v0,v0,v10
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmsum3fp128 v10,v11,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// lvx128 v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v13,v11,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmrghw v0,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v0,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vsubfp v13,v12,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,288
	ctx.r11.s64 = ctx.r11.s64 + 288;
	// lwz r9,28(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r10,r10,288
	ctx.r10.s64 = ctx.r10.s64 + 288;
	// addi r9,r9,288
	ctx.r9.s64 = ctx.r9.s64 + 288;
	// lvx128 v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v3,v13,v5
	simde_mm_store_ps(ctx.v3.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v5.f32), 0xEF));
	// vmsum3fp128 v2,v13,v4
	simde_mm_store_ps(ctx.v2.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v4.f32), 0xEF));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r11,r11,288
	ctx.r11.s64 = ctx.r11.s64 + 288;
	// lvx128 v1,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f8,128(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// stfs f6,112(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v31,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v31,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v31.u32), 0xFF));
	// vmulfp128 v30,v1,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r9,r11,288
	ctx.r9.s64 = ctx.r11.s64 + 288;
	// addi r11,r10,288
	ctx.r11.s64 = ctx.r10.s64 + 288;
	// lvx128 v29,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v28,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v27,v28,v29
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v29.f32)));
	// stvx v27,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r5,r11,192
	ctx.r5.s64 = ctx.r11.s64 + 192;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v13,v12,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// addi r4,r11,288
	ctx.r4.s64 = ctx.r11.s64 + 288;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// vpermwi128 v9,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// addi r11,r11,272
	ctx.r11.s64 = ctx.r11.s64 + 272;
	// addi r3,r10,288
	ctx.r3.s64 = ctx.r10.s64 + 288;
	// vpermwi128 v8,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// addi r10,r10,272
	ctx.r10.s64 = ctx.r10.s64 + 272;
	// vpermwi128 v10,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// lvx128 v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v13,v12,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x9C));
	// lvx128 v26,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v12,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0x78));
	// vpermwi128 v7,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v6,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vmulfp128 v0,v9,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v11,v7
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vnmsubfp v0,v8,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vnmsubfp v13,v10,v6,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vaddfp v0,v0,v26
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v26.f32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 28);
	// addi r11,r11,272
	ctx.r11.s64 = ctx.r11.s64 + 272;
	// lvx128 v25,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v24,v25,v13
	simde_mm_store_ps(ctx.v24.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v24,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_31"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_31);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_31) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r4,r31,752
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 752;
	// bl 0x8225ae38
	phJoint1Dof_AE38(ctx, base);
	// stfs f1,892(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 892, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_20"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_20);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r30,r31,752
	var_r30 = (uint32_t)(var_r31 + 752);
	// addi r10,r31,640
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 640;
	// addi r11,r31,768
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 768;
	// lfs f0,696(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 696);
	ctx.f0.f64 = double(temp.f32);
	// addi r29,r31,816
	var_r29 = (uint32_t)(var_r31 + 816);
	// lfs f13,680(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 680);
	ctx.f13.f64 = double(temp.f32);
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lfs f12,664(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 664);
	ctx.f12.f64 = double(temp.f32);
	// addi r3,r31,240
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 240;
	// stfs f12,0(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r30 + 0, temp.u32);
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// stfs f13,4(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r30 + 4, temp.u32);
	// stfs f0,8(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,144
	ctx.r10.s64 = ctx.r10.s64 + 144;
	// addi r9,r10,32
	ctx.r9.s64 = ctx.r10.s64 + 32;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v13,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,144
	ctx.r10.s64 = ctx.r10.s64 + 144;
	// addi r7,r10,32
	ctx.r7.s64 = ctx.r10.s64 + 32;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,192
	ctx.r10.s64 = ctx.r10.s64 + 192;
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v11,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vpermwi128 v10,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// vpermwi128 v13,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vmulfp128 v0,v12,v11
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vnmsubfp v0,v10,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x822556e0
	util_56E0(ctx, base);
	// addi r9,r29,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 16;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r30,16
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v9,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lfs f11,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lis r11,-32253
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v6,v9,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v7,v8,v0
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lfs f0,-12024(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f10,f0,f11
	ctx.f10.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
	// stvx v6,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f9,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// fadds f13,f8,f9
	ctx.f13.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// stfs f13,880(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 880, temp.u32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f7,884(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + 884, temp.u32);
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_19"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_19);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_19) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r30,r31,848
	var_r30 = (uint32_t)(var_r31 + 848);
	// addi r29,r31,752
	var_r29 = (uint32_t)(var_r31 + 752);
	// addi r3,r31,48
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 48;
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x822556e0
	util_56E0(ctx, base);
	// addi r10,r30,16
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r9,r29,16
	ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 16;
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lfs f0,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 16);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32253
	// lfs f11,880(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 880);
	ctx.f11.f64 = double(temp.f32);
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v9,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lfs f13,-12024(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stvx v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fadds f12,f9,f10
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f10.f64));
	// fmuls f8,f13,f12
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fmadds f7,f11,f0,f8
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f8.f64));
	// fdivs f6,f13,f7
	ctx.f6.f64 = double(float(ctx.f13.f64 / ctx.f7.f64));
	// stfs f6,888(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 888, temp.u32);
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_21"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_21);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_21) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r31,816
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 816;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f1,884(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 884);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8225b710
	phJoint1Dof_B710(ctx, base);
	// addi r4,r31,240
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 240;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82255558
	phJoint1Dof_5558(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_22"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_22);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_22) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r31,848
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 848;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f1,888(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 888);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8225b710
	phJoint1Dof_B710(ctx, base);
	// addi r4,r31,48
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 48;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82255558
	phJoint1Dof_5558(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_24"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_24);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_24) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,752
	ctx.r9.s64 = ctx.r3.s64 + 752;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f13,884(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 884);
	ctx.f13.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// addi r8,r3,816
	ctx.r8.s64 = ctx.r3.s64 + 816;
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32253
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f12,16(r3)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f0,-12024(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f11,f0,f12
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// stvx v7,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f10,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 + ctx.f10.f64));
	// fmuls f0,f8,f13
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f7,f11,f0
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f7
	ctx.f0.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// lvx128 v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v5,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
	// vmaddfp v4,v0,v13,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v2,v0,v12,v3
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_25"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_25);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_25) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,752
	ctx.r9.s64 = ctx.r3.s64 + 752;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,884(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 884);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,816
	ctx.r8.s64 = ctx.r3.s64 + 816;
	// vaddfp v8,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v5,v11,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v9,v10,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v3,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
	// lvx128 v2,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v2,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
	// vmaddfp v1,v0,v13,v4
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v30,v0,v12,v31
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_23"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_23);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_23) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,752
	ctx.r9.s64 = ctx.r3.s64 + 752;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,888(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 888);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// addi r8,r3,848
	ctx.r8.s64 = ctx.r3.s64 + 848;
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// lvx128 v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v5,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
	// vmaddfp v4,v0,v13,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v2,v0,v12,v3
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_26"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_26);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_26) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,752
	ctx.r9.s64 = ctx.r3.s64 + 752;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,888(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 888);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,848
	ctx.r8.s64 = ctx.r3.s64 + 848;
	// vaddfp v8,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v5,v11,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v9,v10,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v3,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
	// lvx128 v2,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v2,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
	// vmaddfp v1,v0,v13,v4
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v30,v0,v12,v31
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_28"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_28);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,816
	ctx.r9.s64 = ctx.r3.s64 + 816;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,884(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 884);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// addi r8,r3,752
	ctx.r8.s64 = ctx.r3.s64 + 752;
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// lvx128 v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v5,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
	// vmaddfp v4,v0,v13,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v2,v0,v12,v3
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_29"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_29);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_29) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,816
	ctx.r9.s64 = ctx.r3.s64 + 816;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,884(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 884);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,752
	ctx.r8.s64 = ctx.r3.s64 + 752;
	// vaddfp v8,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v5,v11,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v9,v10,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v3,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
	// lvx128 v2,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v2,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
	// vmaddfp v1,v0,v13,v4
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v30,v0,v12,v31
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_27"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_27);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_27) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,848
	ctx.r9.s64 = ctx.r3.s64 + 848;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,888(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 888);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// addi r8,r3,752
	ctx.r8.s64 = ctx.r3.s64 + 752;
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v11,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v7,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// vmsum3fp128 v10,v11,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// lvx128 v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v5,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), 0xFF));
	// vmaddfp v4,v0,v13,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v2,v0,v12,v3
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_30"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_30);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r9,r3,848
	ctx.r9.s64 = ctx.r3.s64 + 848;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r4,16
	ctx.r10.s64 = ctx.r4.s64 + 16;
	// lfs f0,888(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 888);
	ctx.f0.f64 = double(temp.f32);
	// addi r7,r9,16
	ctx.r7.s64 = ctx.r9.s64 + 16;
	// lvx128 v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r3,752
	ctx.r8.s64 = ctx.r3.s64 + 752;
	// vaddfp v8,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,-32
	ctx.r3.s64 = ctx.r1.s64 + -32;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v5,v11,v12
	simde_mm_store_ps(ctx.v5.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
	// vmsum3fp128 v9,v10,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r8,16
	ctx.r6.s64 = ctx.r8.s64 + 16;
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v6,v7,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f13,-32(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,-16(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lvx128 v3,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v3,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), 0xFF));
	// lvx128 v2,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v2,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), 0xFF));
	// vmaddfp v1,v0,v13,v4
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v30,v0,v12,v31
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// stvx v30,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_vfn_17"))) PPC_WEAK_FUNC(phJoint1Dof_vfn_17);
PPC_FUNC_IMPL(__imp__phJoint1Dof_vfn_17) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// addi r11,r3,432
	ctx.r11.s64 = ctx.r3.s64 + 432;
	// lis r10,-32253
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// addi r9,r3,528
	ctx.r9.s64 = ctx.r3.s64 + 528;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lfs f0,-12016(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r3,464
	ctx.r10.s64 = ctx.r3.s64 + 464;
	// stfs f0,740(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 740, temp.u32);
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v10,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
	// stvx v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v9,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
	// stvx v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__ph_ctor_ABE8"))) PPC_WEAK_FUNC(ph_ctor_ABE8);
PPC_FUNC_IMPL(__imp__ph_ctor_ABE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32251
	// addi r10,r3,48
	ctx.r10.s64 = ctx.r3.s64 + 48;
	// addi r9,r11,-356
	ctx.r9.s64 = ctx.r11.s64 + -356;
	// lis r11,-32253
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// li r9,24
	ctx.r9.s64 = 24;
	// lfs f0,-12016(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// li r11,0
	ctx.r11.s64 = 0;
	// stfs f0,16(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r11.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8225AC1C:
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r11.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x8225ac1c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225AC1C;
	// addi r10,r3,240
	ctx.r10.s64 = ctx.r3.s64 + 240;
	// li r9,24
	ctx.r9.s64 = 24;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8225AC34:
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r11.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x8225ac34
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225AC34;
	// std r11,432(r3)
	PPC_STORE_U64(ctx.r3.u32 + 432, ctx.r11.u64);
	// std r11,440(r3)
	PPC_STORE_U64(ctx.r3.u32 + 440, ctx.r11.u64);
	// std r11,448(r3)
	PPC_STORE_U64(ctx.r3.u32 + 448, ctx.r11.u64);
	// std r11,456(r3)
	PPC_STORE_U64(ctx.r3.u32 + 456, ctx.r11.u64);
	// std r11,464(r3)
	PPC_STORE_U64(ctx.r3.u32 + 464, ctx.r11.u64);
	// std r11,472(r3)
	PPC_STORE_U64(ctx.r3.u32 + 472, ctx.r11.u64);
	// std r11,480(r3)
	PPC_STORE_U64(ctx.r3.u32 + 480, ctx.r11.u64);
	// std r11,488(r3)
	PPC_STORE_U64(ctx.r3.u32 + 488, ctx.r11.u64);
	// std r11,496(r3)
	PPC_STORE_U64(ctx.r3.u32 + 496, ctx.r11.u64);
	// std r11,504(r3)
	PPC_STORE_U64(ctx.r3.u32 + 504, ctx.r11.u64);
	// std r11,512(r3)
	PPC_STORE_U64(ctx.r3.u32 + 512, ctx.r11.u64);
	// std r11,520(r3)
	PPC_STORE_U64(ctx.r3.u32 + 520, ctx.r11.u64);
	// std r11,528(r3)
	PPC_STORE_U64(ctx.r3.u32 + 528, ctx.r11.u64);
	// std r11,536(r3)
	PPC_STORE_U64(ctx.r3.u32 + 536, ctx.r11.u64);
	// std r11,544(r3)
	PPC_STORE_U64(ctx.r3.u32 + 544, ctx.r11.u64);
	// std r11,552(r3)
	PPC_STORE_U64(ctx.r3.u32 + 552, ctx.r11.u64);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_AC88_p39"))) PPC_WEAK_FUNC(phJoint1Dof_AC88_p39);
PPC_FUNC_IMPL(__imp__phJoint1Dof_AC88_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=208, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r11,r10,784
	ctx.r11.s64 = ctx.r10.s64 + 784;
	// addi r10,r10,848
	ctx.r10.s64 = ctx.r10.s64 + 848;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// addi r5,r10,32
	ctx.r5.s64 = ctx.r10.s64 + 32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// vmsum3fp128 v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r9,r8,784
	ctx.r9.s64 = ctx.r8.s64 + 784;
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r9,32
	ctx.r10.s64 = ctx.r9.s64 + 32;
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v12,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r8,848
	ctx.r11.s64 = ctx.r8.s64 + 848;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v13,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lwz r7,108(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 108);
	// vmrghw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v11,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
	// lis r11,-32160
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// vsubfp v7,v0,v9
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// lvx128 v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v6,v0,v8
	simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v7,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwz r9,116(r11)
  // [ph4a] slot load collapsed
	// stvx v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bctrl
	VCALL(var_r31, 29, ctx, base);  // pattern-B slot 29 (byte +116)
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v5,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v4,v5
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v5.f32)));
	// lvx128 v3,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v2,v3,v0
	simde_mm_store_ps(ctx.v2.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v2,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_AE38"))) PPC_WEAK_FUNC(phJoint1Dof_AE38);
PPC_FUNC_IMPL(__imp__phJoint1Dof_AE38) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=208, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lvx128 v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 24);
	// addi r11,r10,784
	ctx.r11.s64 = ctx.r10.s64 + 784;
	// addi r10,r10,848
	ctx.r10.s64 = ctx.r10.s64 + 848;
	// addi r7,r11,32
	ctx.r7.s64 = ctx.r11.s64 + 32;
	// addi r6,r11,16
	ctx.r6.s64 = ctx.r11.s64 + 16;
	// addi r5,r10,32
	ctx.r5.s64 = ctx.r10.s64 + 32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r10,16
	ctx.r4.s64 = ctx.r10.s64 + 16;
	// vmsum3fp128 v11,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r9,r8,784
	ctx.r9.s64 = ctx.r8.s64 + 784;
	// lvx128 v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v13,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// lvx128 v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r9,32
	ctx.r10.s64 = ctx.r9.s64 + 32;
	// lvx128 v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// vmrghw v13,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v12,v11,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r8,848
	ctx.r11.s64 = ctx.r8.s64 + 848;
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v13,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r6,r11,32
	ctx.r6.s64 = ctx.r11.s64 + 32;
	// addi r10,r11,16
	ctx.r10.s64 = ctx.r11.s64 + 16;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v10,v13,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lwz r7,108(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 108);
	// vmrghw v13,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v12,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v11,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v13,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmsum3fp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// vmrghw v0,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrghw v13,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vmrghw v10,v0,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
	// lis r11,-32160
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v9,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// vsubfp v7,v0,v9
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// lvx128 v8,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v6,v0,v8
	simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v7,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwz r9,116(r11)
  // [ph4a] slot load collapsed
	// stvx v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bctrl
	VCALL(var_r31, 29, ctx, base);  // pattern-B slot 29 (byte +116)
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// lvx128 v3,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lvx128 v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v0,v4,v5
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v5.f32)));
	// lvx128 v2,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v0,v2,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v1,v3,v0
	simde_mm_store_ps(ctx.v1.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f1,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__phJoint1Dof_AFF8_p42"))) PPC_WEAK_FUNC(phJoint1Dof_AFF8_p42);
PPC_FUNC_IMPL(__imp__phJoint1Dof_AFF8_p42) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32253
	// lfs f13,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// lfs f11,-4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f13,f11
	// bgt cr6,0x8225b04c
	if (ctx.f13.f64 <= ctx.f11.f64) {
		// lis r10,-32248
		ctx.r10.s64 = -2113404928;
		// lfs f12,-25608(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25608);  /* glob:lbl_82079BF8 @ 0x82079bf8 */
		ctx.f12.f64 = double(temp.f32);
		// fcmpu cr6,f13,f12
		// blt cr6,0x8225b04c
		if (ctx.f13.f64 >= ctx.f12.f64) {
			// lfs f0,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// fcmpu cr6,f0,f11
		} else {
			if (ctx.f0.f64 <= ctx.f11.f64) {
				// fcmpu cr6,f0,f12
			} else {
				if (ctx.f0.f64 >= ctx.f12.f64) {
					// lfs f13,8(r3)
					temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
					ctx.f13.f64 = double(temp.f32);
					// fneg f0,f0
					ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
					// lfs f12,0(r11)
					temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
					ctx.f12.f64 = double(temp.f32);
					// stfs f13,4(r4)
					temp.f32 = float(ctx.f13.f64);
					PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
					// stfs f0,8(r4)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
					// b 0x8225b060
					} else {
				}
			}
		}
	loc_8225B04C:
		// fneg f0,f13
		ctx.fpscr.disableFlushMode();
		ctx.f0.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// lfs f12,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// lfs f13,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,8(r4)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
		// stfs f0,4(r4)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	}
loc_8225B060:
	// stfs f12,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lis r11,-32158
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32160
	// vmsum3fp128 v11,v13,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r11,r11,-25616
	ctx.r11.s64 = ctx.r11.s64 + -25616;
	// addi r10,r10,26448
	ctx.r10.s64 = ctx.r10.s64 + 26448;
	// lis r9,-32163
	// lis r8,-32163
	// addi r9,r9,-18480
	ctx.r9.s64 = ctx.r9.s64 + -18480;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r8,-18496
	ctx.r8.s64 = ctx.r8.s64 + -18496;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrsqrtefp v10,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
	// vcmpeqfp v0,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vsel v0,v10,v9,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
	// vmulfp128 v10,v0,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v9,v8,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v12,v11,v10,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v12.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v12,v9,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// vpermwi128 v0,v13,135
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x78));
	// vpermwi128 v13,v13,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x9C));
	// vmulfp128 v0,v0,v12
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vnmsubfp v0,v13,v11,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)), simde_mm_load_ps(ctx.v0.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmsum3fp128 v11,v0,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrsqrtefp v10,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
	// vcmpeqfp v13,v10,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vsel v13,v10,v9,v13
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
	// vmulfp128 v10,v13,v13
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v9,v8,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vnmsubfp v12,v11,v10,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v12.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v13,v12,v9,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__game_B128"))) PPC_WEAK_FUNC(game_B128);
PPC_FUNC_IMPL(__imp__game_B128) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32253
	// lfs f13,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// lfs f11,-4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f13,f11
	// bgt cr6,0x8225b17c
	if (ctx.f13.f64 <= ctx.f11.f64) {
		// lis r10,-32248
		ctx.r10.s64 = -2113404928;
		// lfs f12,-25608(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25608);  /* glob:lbl_82079BF8 @ 0x82079bf8 */
		ctx.f12.f64 = double(temp.f32);
		// fcmpu cr6,f13,f12
		// blt cr6,0x8225b17c
		if (ctx.f13.f64 >= ctx.f12.f64) {
			// lfs f0,4(r3)
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// fcmpu cr6,f0,f11
		} else {
			if (ctx.f0.f64 <= ctx.f11.f64) {
				// fcmpu cr6,f0,f12
			} else {
				if (ctx.f0.f64 >= ctx.f12.f64) {
					// lfs f13,8(r3)
					temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
					ctx.f13.f64 = double(temp.f32);
					// fneg f0,f0
					ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
					// lfs f12,0(r11)
					temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
					ctx.f12.f64 = double(temp.f32);
					// stfs f13,4(r4)
					temp.f32 = float(ctx.f13.f64);
					PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
					// stfs f0,8(r4)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
					// b 0x8225b190
					} else {
				}
			}
		}
	loc_8225B17C:
		// fneg f0,f13
		ctx.fpscr.disableFlushMode();
		ctx.f0.u64 = ctx.f13.u64 ^ 0x8000000000000000;
		// lfs f12,4(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// lfs f13,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,8(r4)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
		// stfs f0,4(r4)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	}
loc_8225B190:
	// stfs f12,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lis r11,-32158
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v13,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r11,r11,-25616
	ctx.r11.s64 = ctx.r11.s64 + -25616;
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// vrsqrtefp v10,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v11.f32))));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18496
	ctx.r11.s64 = ctx.r11.s64 + -18496;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcmpeqfp v0,v10,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vsel v0,v10,v9,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8))));
	// vmulfp128 v10,v0,v0
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v9,v8,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v12,v11,v10,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v12.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v12,v9,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v13,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_B1F8"))) PPC_WEAK_FUNC(phBoundCapsule_B1F8);
PPC_FUNC_IMPL(__imp__phBoundCapsule_B1F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=192, savegprlr_29
	// lis r11,-32158
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r11,r11,-25616
	ctx.r11.s64 = ctx.r11.s64 + -25616;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lvx128 v7,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32160
	// vor v0,v7,v7
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_load_si128((simde__m128i*)ctx.v7.u8));
	// lvx128 v10,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r11,26448
	ctx.r11.s64 = ctx.r11.s64 + 26448;
	// vpermwi128 v12,v10,135
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x78));
	// vpermwi128 v8,v10,99
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0x9C));
	// vpermwi128 v13,v0,99
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x9C));
	// lvx128 v5,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18480
	ctx.r11.s64 = ctx.r11.s64 + -18480;
	// vmulfp128 v13,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvx128 v4,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32163
	// addi r11,r11,-18496
	ctx.r11.s64 = ctx.r11.s64 + -18496;
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vor v11,v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_load_si128((simde__m128i*)ctx.v9.u8));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vpermwi128 v0,v0,135
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x78));
	// lis r11,-32248
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vnmsubfp v13,v8,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v13.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// lfs f0,-25512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25512);
	ctx.f0.f64 = double(temp.f32);
	// vmsum3fp128 v12,v13,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrsqrtefp v8,v12
	simde_mm_store_ps(ctx.v8.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_sqrt_ps(simde_mm_load_ps(ctx.v12.f32))));
	// vcmpeqfp v0,v8,v6
	simde_mm_store_ps(ctx.v0.f32, simde_mm_cmpeq_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vsel v0,v8,v5,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_or_si128(simde_mm_andnot_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)), simde_mm_and_si128(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8))));
	// vmulfp128 v8,v0,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v6,v4,v0
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vnmsubfp v11,v12,v8,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v11.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v11,v6,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v6.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v0,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lfs f31,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	var_f31 = double(temp.f32);
	// fcmpu cr6,f31,f0
	// bge cr6,0x8225b308
	if (var_f31 < ctx.f0.f64) {
		// lis r11,-32163
		// li r5,16
		ctx.r5.s64 = 16;
		// addi r11,r11,-18432
		ctx.r11.s64 = ctx.r11.s64 + -18432;
		// addi r8,r31,16
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 16;
		// li r4,32
		ctx.r4.s64 = 32;
		// addi r7,r31,32
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 32;
		// li r10,48
		ctx.r10.s64 = 48;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r31,48
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
		// stvx v0,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r5,r11
		ea = (ctx.r5.u32 + ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r4,r11
		ea = (ctx.r4.u32 + ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v0,r10,r11
		ea = (ctx.r10.u32 + ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v0,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		return;
	}
loc_8225B308:
	// stfs f31,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// vmsum3fp128 v12,v7,v7
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
	// vmsum3fp128 v11,v10,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vrefp v0,v12
	simde_mm_store_ps(ctx.v0.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v12.f32)));
	// lfs f0,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// vnmsubfp v9,v0,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_xor_ps(simde_mm_sub_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v9.f32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x80000000)))));
	// vmaddfp v0,v0,v9,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v9,v13,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v9,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// lvx128 v8,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = double(float(ctx.f1.f64));
	// vmsum3fp128 v7,v8,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32), 0xEF));
	// lis r11,-32253
	// lfs f0,84(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,-12016
	ctx.r11.s64 = ctx.r11.s64 + -12016;
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f3,f11,f11
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// lfs f12,-8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fdivs f8,f12,f2
	ctx.f8.f64 = double(float(ctx.f12.f64 / ctx.f2.f64));
	// stvx v7,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fmuls f13,f31,f8
	ctx.f13.f64 = double(float(var_f31 * ctx.f8.f64));
	// fmuls f7,f0,f13
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fmuls f5,f11,f13
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f6,f10,f13
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f1,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f13,f1,f8
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f8.f64));
	// fsubs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// fmuls f11,f11,f12
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmuls f2,f12,f0
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fmadds f3,f3,f12,f13
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 + ctx.f13.f64));
	// stfs f3,0(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 0,/* phBoundCapsule::vtable@+0x0 */ temp.u32);
	// fmuls f4,f11,f0
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// fmadds f2,f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f13.f64));
	// fmuls f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmadds f13,f8,f10,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 + ctx.f13.f64));
	// fsubs f1,f4,f6
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f6.f64));
	// stfs f1,4(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// fadds f1,f7,f11
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f11.f64));
	// stfs f1,8(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// fadds f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// stfs f2,20(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// fsubs f1,f0,f5
	ctx.f1.f64 = double(float(ctx.f0.f64 - ctx.f5.f64));
	// stfs f6,16(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// stfs f1,24(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// fsubs f4,f11,f7
	ctx.f4.f64 = double(float(ctx.f11.f64 - ctx.f7.f64));
	// fadds f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 + ctx.f0.f64));
	// stfs f4,32(r31)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r31 + 32, temp.u32);
	// stfs f0,36(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f13,40(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f9,48(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	// stfs f9,52(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 52, temp.u32);
	// stfs f9,56(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 56, temp.u32);
	return;
}

__attribute__((alias("__imp__phJoint_vfn_17"))) PPC_WEAK_FUNC(phJoint_vfn_17);
PPC_FUNC_IMPL(__imp__phJoint_vfn_17) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// addi r11,r3,432
	ctx.r11.s64 = ctx.r3.s64 + 432;
	// addi r10,r3,464
	ctx.r10.s64 = ctx.r3.s64 + 464;
	// addi r8,r11,16
	ctx.r8.s64 = ctx.r11.s64 + 16;
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// addi r9,r3,528
	ctx.r9.s64 = ctx.r3.s64 + 528;
	// addi r6,r9,16
	ctx.r6.s64 = ctx.r9.s64 + 16;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v0,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_setzero_si128());
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v13,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_setzero_si128());
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_setzero_si128());
	// stvx v12,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_setzero_si128());
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v10,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_setzero_si128());
	// stvx v10,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vxor v9,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_setzero_si128());
	// stvx v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_B498_h"))) PPC_WEAK_FUNC(phJoint3Dof_B498_h);
PPC_FUNC_IMPL(__imp__phJoint3Dof_B498_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=336, savegprlr_26
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// addi r3,r27,128
	ctx.r3.s64 = (int64_t)(int32_t)var_r27 + 128;
	// bl 0x8225b7e0
	phJoint3Dof_B7E0(ctx, base);
	// addi r29,r27,64
	var_r29 = (uint32_t)(var_r27 + 64);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r11,r29,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 16;
	// addi r9,r29,32
	ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 32;
	// addi r8,r29,48
	ctx.r8.s64 = (int64_t)(int32_t)var_r29 + 48;
	// lvx128 v0,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lvx128 v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmrghw v10,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// vmrglw v12,v0,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vmrghw v0,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vmrglw v13,v13,v11
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vmrghw v11,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrglw v10,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vmrghw v9,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// stvx v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820d4f88
	util_4F88(ctx, base);
	// addi r30,r31,128
	var_r30 = (uint32_t)(var_r31 + 128);
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// addi r5,r1,176
	ctx.r5.s64 = ctx.r1.s64 + 176;
	// addi r11,r30,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r10,r30,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 32;
	// lvx128 v8,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r30,48
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 48;
	// lvx128 v5,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v7,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// stvx v8,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lvx128 v6,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lis r11,-32160
	// lvx128 v4,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r30,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 32;
	// addi r28,r11,26448
	var_r28 = (uint32_t)(ctx.r11.s64 + 26448);  // lbl_82606750 @ 0x82606750
	// addi r11,r30,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
	// addi r8,r27,16
	ctx.r8.s64 = (int64_t)(int32_t)var_r27 + 16;
	// addi r7,r27,32
	ctx.r7.s64 = (int64_t)(int32_t)var_r27 + 32;
	// li r4,4
	ctx.r4.s64 = 4;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v3,v0,v4
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v3,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v1,v0,v2
	simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
	// stvx v1,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r30,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 16;
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v31,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v30,v0,v31
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v31.f32)));
	// stvx v30,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r30,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 32;
	// lvx128 v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v29,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v28,v29,v0
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v28,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v27,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v26,v27,v0
	simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v26,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v25,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v24,v25,v0
	simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v24,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x82350808
	phJoint3Dof_0808_2hr(ctx, base);
	// addi r27,r31,64
	var_r27 = (uint32_t)(var_r31 + 64);
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x820d4f88
	util_4F88(ctx, base);
	// addi r5,r29,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r29 + 16;
	// addi r30,r31,16
	var_r30 = (uint32_t)(var_r31 + 16);
	// lvx128 v23,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r29,32
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 32;
	// stvx v23,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r26,r31,32
	var_r26 = (uint32_t)(var_r31 + 32);
	// addi r10,r29,48
	ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 48;
	// addi r6,r31,48
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 48;
	// lvx128 v22,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v22,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// lvx128 v21,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stvx v21,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v20,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v20,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820c3bc8
	phJoint_3BC8_g(ctx, base);
	// lis r11,-32253
	// lfs f13,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phJoint3Dof::vtable@+0x0 */;
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f11,20(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 20);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f10.f64 = double(temp.f32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f12,0(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 0,/* phJoint3Dof::vtable@+0x0 */ temp.u32);
	// lvx128 v19,r0,r31
	ea = (var_r31) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// fadds f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// stvx v19,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stfs f9,20(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// fadds f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lvx128 v18,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stfs f8,40(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// lvx128 v17,r0,r26
	ea = (var_r26) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v18,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stvx v17,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bl 0x820d4f88
	util_4F88(ctx, base);
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r27,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 16;
	// lvx128 v16,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r27,32
	ctx.r10.s64 = (int64_t)(int32_t)var_r27 + 32;
	// vsubfp v15,v0,v16
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v15.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v16.f32)));
	// stvx v15,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v14,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v14.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v63,v0,v14
	simde_mm_store_ps(ctx.v63.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v14.f32)));
	// stvx128 v63,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v62,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	ea = (var_r28) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp128 v61,v0,v62
	simde_mm_store_ps(ctx.v61.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v62.f32)));
	// stvx128 v61,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	return;
}

__attribute__((alias("__imp__phJoint1Dof_B710"))) PPC_WEAK_FUNC(phJoint1Dof_B710);
PPC_FUNC_IMPL(__imp__phJoint1Dof_B710) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// stfs f1,-32(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// stfs f1,-16(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// addi r8,r3,32
	ctx.r8.s64 = ctx.r3.s64 + 32;
	// lvx128 v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r4,16
	ctx.r11.s64 = ctx.r4.s64 + 16;
	// vspltw v12,v0,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xAA));
	// addi r10,r3,64
	ctx.r10.s64 = ctx.r3.s64 + 64;
	// vspltw v11,v0,2
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x55));
	// vspltw v10,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// addi r5,r10,32
	ctx.r5.s64 = ctx.r10.s64 + 32;
	// addi r7,r1,-32
	ctx.r7.s64 = ctx.r1.s64 + -32;
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lvx128 v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvx128 v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v9,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vmulfp128 v0,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v12,v0
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v12,v11,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v8,v10,v0
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v8,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v13,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xAA));
	// vspltw v11,v13,2
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0x55));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v12,v12,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v11,v11,v0
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v7,v13,v0
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// stvx v12,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r3,128
	ctx.r10.s64 = ctx.r3.s64 + 128;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vor v0,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vmulfp128 v13,v13,v9
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// addi r11,r10,32
	ctx.r11.s64 = ctx.r10.s64 + 32;
	// vspltw v12,v0,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xAA));
	// vspltw v11,v0,2
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0x55));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vmulfp128 v12,v12,v13
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v11,v11,v13
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v6,v0,v13
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// blr
	return;
}

__attribute__((alias("__imp__phJoint3Dof_B7E0"))) PPC_WEAK_FUNC(phJoint3Dof_B7E0);
PPC_FUNC_IMPL(__imp__phJoint3Dof_B7E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f13,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32253
	// lfs f10,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f2,f13,f13
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// fmuls f8,f13,f10
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f0,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f0,f0
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// lfs f12,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f0,f9
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// fmuls f5,f12,f12
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// lfs f11,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* phJoint3Dof::vtable@+0x0 */;
	ctx.f11.f64 = double(temp.f32);
	// fmuls f6,f11,f12
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmsubs f8,f0,f12,f8
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 - ctx.f8.f64));
	// fmsubs f7,f13,f12,f7
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 - ctx.f7.f64));
	// fmsubs f12,f9,f10,f5
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f10.f64 - ctx.f5.f64));
	// fmsubs f6,f13,f0,f6
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 - ctx.f6.f64));
	// fmsubs f5,f11,f10,f4
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 - ctx.f4.f64));
	// fmsubs f9,f11,f9,f2
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f9.f64 - ctx.f2.f64));
	// fmuls f3,f13,f8
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// fmadds f1,f11,f12,f3
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f3.f64));
	// fmadds f13,f0,f7,f1
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 + ctx.f1.f64));
	// lfs f0,-12024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12024);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fmuls f13,f0,f8
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fmuls f11,f0,f7
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// stfs f13,16(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmuls f10,f0,f6
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfs f11,32(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 32, temp.u32);
	// fmuls f8,f0,f5
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fmuls f7,f0,f9
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// stfs f8,20(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// stfs f7,40(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 40, temp.u32);
	// stfs f10,24(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 24, temp.u32);
	// stfs f10,36(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 36, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_1"))) PPC_WEAK_FUNC(fragDrawable_vfn_1);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_1) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// beqlr cr6
	if (ctx.r3.u32 == 0) return;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__util_B8A0"))) PPC_WEAK_FUNC(util_B8A0);
PPC_FUNC_IMPL(__imp__util_B8A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_27
	// lis r11,-32253
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r29,0
	var_r29 = 0;
	// addi r31,r30,16
	var_r31 = (uint32_t)(var_r30 + 16);
	// li r28,3
	var_r28 = 3;
	// lfs f31,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	var_f31 = double(temp.f32);
	// lis r11,-32251
	// addi r27,r31,48
	var_r27 = (uint32_t)(var_r31 + 48);
	// addi r11,r11,-76
	ctx.r11.s64 = ctx.r11.s64 + -76;
	// stw r29,4(r30)
	PPC_STORE_U32(var_r30 + 4, var_r29);
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// stw r29,8(r30)
	PPC_STORE_U32(var_r30 + 8, var_r29);
	// stw r29,12(r30)
	PPC_STORE_U32(var_r30 + 12, var_r29);
	// stfs f31,0(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 0, temp.u32);
	// stfs f31,4(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 4, temp.u32);
	// stfs f31,8(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 8, temp.u32);
	// stfs f31,16(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// stfs f31,20(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// stfs f31,24(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// stfs f31,32(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 32, temp.u32);
	// stfs f31,36(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f31,40(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
loc_8225B908:
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x8225db10
	ke_DB10(ctx, base);
	// addi r28,r28,-1
	var_r28 = (uint32_t)(var_r28 + -1);
	// addi r27,r27,4
	var_r27 = (uint32_t)(var_r27 + 4);
	// cmpwi cr6,r28,0
	// bge cr6,0x8225b908
	if ((int32_t)var_r28 >= 0) goto loc_8225B908;
	// stfs f31,96(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 96, temp.u32);
	// addi r11,r31,64
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 64;
	// li r9,4
	ctx.r9.s64 = 4;
	// stw r29,100(r31)
	PPC_STORE_U32(var_r31 + 100, var_r29);
	// stw r29,104(r31)
	PPC_STORE_U32(var_r31 + 104, var_r29);
	// li r10,-1
	// stw r29,108(r31)
	PPC_STORE_U32(var_r31 + 108, var_r29);
loc_8225B93C:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stfs f31,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stw r29,-16(r11)
	PPC_STORE_U32(ctx.r11.u32 + -16, var_r29);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// cmplwi cr6,r9,0
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bne cr6,0x8225b93c
	if (ctx.r9.u32 != 0) goto loc_8225B93C;
	// stw r29,128(r30)
	PPC_STORE_U32(var_r30 + 128, var_r29);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// sth r29,132(r30)
	PPC_STORE_U16(var_r30 + 132, (uint16_t)var_r29);
	// sth r29,134(r30)
	PPC_STORE_U16(var_r30 + 134, (uint16_t)var_r29);
	// stw r29,136(r30)
	PPC_STORE_U32(var_r30 + 136, var_r29);
	// stw r10,172(r30)
	PPC_STORE_U32(var_r30 + 172, ctx.r10.u32);
	// stw r29,176(r30)
	PPC_STORE_U32(var_r30 + 176, var_r29);
	// stw r10,180(r30)
	PPC_STORE_U32(var_r30 + 180, ctx.r10.u32);
	// stw r29,184(r30)
	PPC_STORE_U32(var_r30 + 184, var_r29);
	// stb r29,188(r30)
	PPC_STORE_U8(var_r30 + 188, (uint8_t)var_r29);
	// stb r29,189(r30)
	PPC_STORE_U8(var_r30 + 189, (uint8_t)var_r29);
	// stb r29,190(r30)
	PPC_STORE_U8(var_r30 + 190, (uint8_t)var_r29);
	// stb r29,191(r30)
	PPC_STORE_U8(var_r30 + 191, (uint8_t)var_r29);
	// stw r29,152(r30)
	PPC_STORE_U32(var_r30 + 152, var_r29);
	// stw r29,148(r30)
	PPC_STORE_U32(var_r30 + 148, var_r29);
	// stw r29,144(r30)
	PPC_STORE_U32(var_r30 + 144, var_r29);
	// stw r29,140(r30)
	PPC_STORE_U32(var_r30 + 140, var_r29);
	// stw r29,168(r30)
	PPC_STORE_U32(var_r30 + 168, var_r29);
	// stw r29,164(r30)
	PPC_STORE_U32(var_r30 + 164, var_r29);
	// stw r29,160(r30)
	PPC_STORE_U32(var_r30 + 160, var_r29);
	// stw r29,156(r30)
	PPC_STORE_U32(var_r30 + 156, var_r29);
	return;
}

__attribute__((alias("__imp__rmcDrawable_vfn_0"))) PPC_WEAK_FUNC(rmcDrawable_vfn_0);
PPC_FUNC_IMPL(__imp__rmcDrawable_vfn_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8225ba08
	rage_BA08_1(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x8225b9f0
	if (ctx.r11.u32 != 0) {
		// bl 0x820c00c0
		rage_free_00C0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8225B9F0:
	// blr
	return;
}

__attribute__((alias("__imp__rage_BA08_1"))) PPC_WEAK_FUNC(rage_BA08_1);
PPC_FUNC_IMPL(__imp__rage_BA08_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=160, savegprlr_24
	// lis r11,-32251
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r11,r11,-76
	ctx.r11.s64 = ctx.r11.s64 + -76;
	// li r24,0
	var_r24 = 0;
	// mr r25,r24
	var_r25 = (uint32_t)(var_r24);
	// stw r11,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
	// lhz r11,132(r27)
	ctx.r11.u64 = PPC_LOAD_U16(var_r27 + 132);
	// cmpwi cr6,r11,0
	// ble cr6,0x8225bb30
	if (ctx.r11.s32 > 0) {
		// mr r26,r24
		var_r26 = (uint32_t)(var_r24);
	loc_8225BA3C:
		// lwz r11,128(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 128);
		// lwzx r29,r26,r11
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r26 + ctx.r11.u32));
		// cmplwi cr6,r29,0
		// beq cr6,0x8225bb14
		if (var_r29 != 0) {
			// lhz r11,140(r29)
			ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 140);
			// mr r28,r24
			var_r28 = (uint32_t)(var_r24);
			// cmpwi cr6,r11,0
			// ble cr6,0x8225bac0
			if (ctx.r11.s32 > 0) {
				// mr r30,r24
				var_r30 = (uint32_t)(var_r24);
			loc_8225BA60:
				// lwz r10,136(r29)
				ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 136);
				// lwzx r31,r10,r30
				var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + var_r30));
				// cmplwi cr6,r31,0
				// beq cr6,0x8225baac
				if (var_r31 != 0) {
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// bl 0x8241eec0
					rage_EEC0_1(ctx, base);
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// bl 0x820f90d0
					atSingleton_Find_90D0(ctx, base);
					// clrlwi r9,r3,24
					ctx.r9.u64 = ctx.r3.u32 & 0xFF;
					// cmplwi cr6,r9,0
					// bne cr6,0x8225baac
					if (ctx.r9.u32 != 0) goto loc_8225BAAC;
					// lwz r8,0(r13)
					ctx.r8.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
					// li r7,4
					ctx.r7.s64 = 4;
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// lwzx r3,r7,r8
					ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
					// lwz r5,8(r6)
					// bctrl
					VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
				}
			loc_8225BAAC:
				// lhz r11,140(r29)
				ctx.r11.u64 = PPC_LOAD_U16(var_r29 + 140);
				// addi r28,r28,1
				var_r28 = (uint32_t)(var_r28 + 1);
				// addi r30,r30,4
				var_r30 = (uint32_t)(var_r30 + 4);
				// cmpw cr6,r28,r11
				// blt cr6,0x8225ba60
				if ((int32_t)var_r28 < ctx.r11.s32) goto loc_8225BA60;
			}
		loc_8225BAC0:
			// lhz r4,142(r29)
			ctx.r4.u64 = PPC_LOAD_U16(var_r29 + 142);
			// cmplwi cr6,r4,0
			// beq cr6,0x8225bb0c
			if (ctx.r4.u32 != 0) {
				// lwz r31,136(r29)
				var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 136));
				// cmplwi cr6,r31,0
				// beq cr6,0x8225bb0c
				if (var_r31 == 0) goto loc_8225BB0C;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x820f90d0
				atSingleton_Find_90D0(ctx, base);
				// clrlwi r3,r3,24
				ctx.r3.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r3,0
				// bne cr6,0x8225bb0c
				if (ctx.r3.u32 != 0) goto loc_8225BB0C;
				// lwz r11,0(r13)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
				// li r10,4
				ctx.r10.s64 = 4;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// lwzx r3,r10,r11
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
				// lwz r8,8(r9)
				// bctrl
				VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			}
		loc_8225BB0C:
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x820c00c0
			rage_free_00C0(ctx, base);
		}
	loc_8225BB14:
		// lwz r7,128(r27)
		ctx.r7.u64 = PPC_LOAD_U32(var_r27 + 128);
		// addi r25,r25,1
		var_r25 = (uint32_t)(var_r25 + 1);
		// stwx r24,r26,r7
		PPC_STORE_U32(var_r26 + ctx.r7.u32, var_r24);
		// addi r26,r26,4
		var_r26 = (uint32_t)(var_r26 + 4);
		// lhz r11,132(r27)
		ctx.r11.u64 = PPC_LOAD_U16(var_r27 + 132);
		// cmpw cr6,r25,r11
		// blt cr6,0x8225ba3c
		if ((int32_t)var_r25 < ctx.r11.s32) goto loc_8225BA3C;
	}
loc_8225BB30:
	// lwz r3,12(r27)
	ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 12);
	// cmplwi cr6,r3,0
	// beq cr6,0x8225bb50
	if (ctx.r3.u32 != 0) {
		// lwz r6,0(r3)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r5,0(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r5.u32);
	}
loc_8225BB50:
	// stw r24,12(r27)
	PPC_STORE_U32(var_r27 + 12, var_r24);
	// lwz r3,8(r27)
	ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 8);
	// cmplwi cr6,r3,0
	// beq cr6,0x8225bb68
	if (ctx.r3.u32 != 0) {
		// bl 0x82126a90
		rage_6A90(ctx, base);
		// stw r24,8(r27)
		PPC_STORE_U32(var_r27 + 8, var_r24);
	}
loc_8225BB68:
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 4);
	// cmplwi cr6,r3,0
	// beq cr6,0x8225bb88
	if (ctx.r3.u32 != 0) {
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	}
loc_8225BB88:
	// addi r31,r27,140
	var_r31 = (uint32_t)(var_r27 + 140);
	// stw r24,4(r27)
	PPC_STORE_U32(var_r27 + 4, var_r24);
	// li r29,4
	var_r29 = 4;
loc_8225BB94:
	// lwz r30,0(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */);
	// cmplwi cr6,r30,0
	// beq cr6,0x8225bbe4
	if (var_r30 != 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r9,r3,24
		ctx.r9.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r9,0
		// bne cr6,0x8225bbd4
		if (ctx.r9.u32 == 0) {
			// lwz r8,0(r13)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
			// li r7,4
			ctx.r7.s64 = 4;
			// mr r4,r30
			ctx.r4.u64 = var_r30;
			// lwzx r3,r7,r8
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
			// lwz r5,8(r6)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		}
	loc_8225BBD4:
		// lwz r3,16(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 16);
		// stw r24,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* rage_GameObject::vtable@+0x0 */ var_r24);
		// bl 0x82566b58
		atSingleton_6B58_p39(ctx, base);
		// stw r24,16(r31)
		PPC_STORE_U32(var_r31 + 16, var_r24);
	}
loc_8225BBE4:
	// addi r29,r29,-1
	var_r29 = (uint32_t)(var_r29 + -1);
	// addi r31,r31,4
	var_r31 = (uint32_t)(var_r31 + 4);
	// cmplwi cr6,r29,0
	// bne cr6,0x8225bb94
	if (var_r29 != 0) goto loc_8225BB94;
	// lhz r4,134(r27)
	ctx.r4.u64 = PPC_LOAD_U16(var_r27 + 134);
	// cmplwi cr6,r4,0
	// beq cr6,0x8225bc40
	if (ctx.r4.u32 != 0) {
		// lwz r31,128(r27)
		var_r31 = (uint32_t)(PPC_LOAD_U32(var_r27 + 128));
		// cmplwi cr6,r31,0
		// beq cr6,0x8225bc40
		if (var_r31 == 0) {
			// addi r3,r27,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r27 + 16;
			// bl 0x8225d240
			atSingleton_dtor_D240(ctx, base);
			// lis r11,-32254
			// addi r11,r11,30404
			ctx.r11.s64 = ctx.r11.s64 + 30404;
			// stw r11,0(r27)
			PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f90d0
		atSingleton_Find_90D0(ctx, base);
		// clrlwi r3,r3,24
		ctx.r3.u64 = ctx.r3.u32 & 0xFF;
		// cmplwi cr6,r3,0
		// bne cr6,0x8225bc40
		if (ctx.r3.u32 != 0) {
			// addi r3,r27,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r27 + 16;
			// bl 0x8225d240
			atSingleton_dtor_D240(ctx, base);
			// lis r11,-32254
			// addi r11,r11,30404
			ctx.r11.s64 = ctx.r11.s64 + 30404;
			// stw r11,0(r27)
			PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
			return;
		}
		// lwz r11,0(r13)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
		// li r10,4
		ctx.r10.s64 = 4;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwzx r3,r10,r11
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
		// lwz r8,8(r9)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_8225BC40:
	// addi r3,r27,16
	ctx.r3.s64 = (int64_t)(int32_t)var_r27 + 16;
	// bl 0x8225d240
	atSingleton_dtor_D240(ctx, base);
	// lis r11,-32254
	// addi r11,r11,30404
	ctx.r11.s64 = ctx.r11.s64 + 30404;
	// stw r11,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__pongDrawable_vfn_3"))) PPC_WEAK_FUNC(pongDrawable_vfn_3);
PPC_FUNC_IMPL(__imp__pongDrawable_vfn_3) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=304, savegprlr_27
	// lis r11,-32254
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// addi r5,r11,30520
	ctx.r5.s64 = ctx.r11.s64 + 30520;
	// lis r11,-32163
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// addi r3,r11,128
	ctx.r3.s64 = ctx.r11.s64 + 128;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// bl 0x822e3040
	rage_obj_factory_create_3040(ctx, base);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x8225bd1c
	if (var_r31 != 0) {
		// lis r11,-32250
		// lwz r9,0(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 0);
		// li r6,32
		ctx.r6.s64 = 32;
		// stw r29,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, var_r29);
		// addi r10,r11,-23148
		ctx.r10.s64 = ctx.r11.s64 + -23148;
		// stw r31,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r31);
		// li r5,2
		ctx.r5.s64 = 2;
		// li r11,0
		ctx.r11.s64 = 0;
		// li r8,1
		ctx.r8.s64 = 1;
		// lwz r7,8(r9)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
		// stw r6,96(r1)
		PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// stw r10,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
		// mr r6,r27
		ctx.r6.u64 = var_r27;
		// stw r5,100(r1)
		PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r5.u32);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stw r11,240(r1)
		PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r11.u32);
		// mr r5,r28
		ctx.r5.u64 = var_r28;
		// stw r8,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
		// stw r11,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r11.u32);
		// stb r11,108(r1)
		PPC_STORE_U8(ctx.r1.u32 + 108, ctx.r11.u8);
		// stw r11,236(r1)
		PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r11.u32);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x822e3b38
		rage_obj_finalize_3B38(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		return;
	}
loc_8225BD1C:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__pongDrawable_vfn_2"))) PPC_WEAK_FUNC(pongDrawable_vfn_2);
PPC_FUNC_IMPL(__imp__pongDrawable_vfn_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=256, savegprlr_28
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lis r11,-32251
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// addi r4,r11,-216
	ctx.r4.s64 = ctx.r11.s64 + -216;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// bl 0x822e5908
	ph_5908(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x8225bdd8
	if (ctx.r11.u32 != 0) {
		// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
		// li r5,128
		ctx.r5.s64 = 128;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r9,4(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r7,16(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 4, ctx, base);  // pattern-B slot 4 (byte +16)
		// mr r5,r3
		ctx.r5.u64 = ctx.r3.u64;
		// cmpwi cr6,r5,103
		// bne cr6,0x8225bdc8
		if (ctx.r5.s32 == 103) {
			// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
			// mr r6,r28
			ctx.r6.u64 = var_r28;
			// mr r5,r29
			ctx.r5.u64 = var_r29;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r10,16(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 4, ctx, base);  // pattern-B slot 4 (byte +16)
			return;
		}
	loc_8225BDC8:
		// lis r11,-32251
		// lwz r4,4(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r3,r11,-204
		ctx.r3.s64 = ctx.r11.s64 + -204;
		// bl 0x8240e6d0
		nop_8240E6D0(ctx, base);
	}
loc_8225BDD8:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_4"))) PPC_WEAK_FUNC(fragDrawable_vfn_4);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_4) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f894
	ctx.lr = 0x8225BDF0;
	__savegprlr_27(ctx, base);
	// lis r12,-1
	// ori r12,r12,17120
	ctx.r12.u64 = ctx.r12.u64 | 17120;
	// bl 0x82569394
	_RtlCheckStack12(ctx, base);
	// stwux r1,r1,r12
	ea = ctx.r1.u32 + ctx.r12.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// bl 0x8225dd70
	ph_DD70(ctx, base);
	// cmplwi cr6,r31,0
	// bne cr6,0x8225be24
	if (var_r31 == 0) {
		// addi r31,r1,96
		var_r31 = (uint32_t)(ctx.r1.s64 + 96);
	}
loc_8225BE24:
	// clrlwi r11,r29,24
	ctx.r11.u64 = var_r29 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x8225bf6c
	if (ctx.r11.u32 != 0) {
		// lis r11,-32253
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
		// li r28,4
		var_r28 = 4;
		// addi r5,r11,-2072
		ctx.r5.s64 = ctx.r11.s64 + -2072;
		// lis r11,-32238
		// li r29,0
		var_r29 = 0;
		// addi r9,r11,-13208
		ctx.r9.s64 = ctx.r11.s64 + -13208;
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// stw r28,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r28);
		// li r8,0
		ctx.r8.s64 = 0;
		// mr r4,r5
		ctx.r4.u64 = ctx.r5.u64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r29,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r29);
		// stw r9,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
		// ld r6,0(r10)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
		// ld r7,8(r10)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
		// bl 0x8225dde8
		ph_DDE8(ctx, base);
		// lis r11,-32253
		// stw r28,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r28);
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
		// addi r5,r11,-3268
		ctx.r5.s64 = ctx.r11.s64 + -3268;
		// stw r29,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r29);
		// lis r11,-32253
		// li r8,0
		ctx.r8.s64 = 0;
		// addi r4,r11,10828
		ctx.r4.s64 = ctx.r11.s64 + 10828;
		// lis r11,-32238
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// addi r6,r11,-13240
		ctx.r6.s64 = ctx.r11.s64 + -13240;
		// stw r6,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
		// ld r6,0(r7)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
		// ld r7,8(r7)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
		// bl 0x8225dde8
		ph_DDE8(ctx, base);
		// lis r11,-32253
		// stw r28,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r28);
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
		// addi r5,r11,10796
		ctx.r5.s64 = ctx.r11.s64 + 10796;
		// stw r29,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r29);
		// lis r11,-32238
		// li r8,0
		ctx.r8.s64 = 0;
		// addi r9,r11,-13224
		ctx.r9.s64 = ctx.r11.s64 + -13224;
		// mr r4,r5
		ctx.r4.u64 = ctx.r5.u64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r9,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
		// ld r6,0(r10)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
		// ld r7,8(r10)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
		// bl 0x8225dde8
		ph_DDE8(ctx, base);
		// lis r11,-32253
		// stw r28,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r28);
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
		// addi r5,r11,10840
		ctx.r5.s64 = ctx.r11.s64 + 10840;
		// stw r29,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r29);
		// lis r11,-32238
		// li r8,0
		ctx.r8.s64 = 0;
		// addi r6,r11,-13192
		ctx.r6.s64 = ctx.r11.s64 + -13192;
		// mr r4,r5
		ctx.r4.u64 = ctx.r5.u64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r6,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
		// ld r6,0(r7)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
		// ld r7,8(r7)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
		// bl 0x8225dde8
		ph_DDE8(ctx, base);
		// lis r11,-32251
		// stw r28,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r28);
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
		// addi r5,r11,-160
		ctx.r5.s64 = ctx.r11.s64 + -160;
		// stw r29,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r29);
		// lis r11,-32251
		// li r8,0
		ctx.r8.s64 = 0;
		// addi r4,r11,-148
		ctx.r4.s64 = ctx.r11.s64 + -148;
		// lis r11,-32218
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// addi r9,r11,-11728
		ctx.r9.s64 = ctx.r11.s64 + -11728;
		// stw r9,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
		// ld r6,0(r10)
		ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
		// ld r7,8(r10)
		ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
		// bl 0x8225dde8
		ph_DDE8(ctx, base);
	}
loc_8225BF6C:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8225df50
	ph_DF50(ctx, base);
	// lwz r1,0(r1)
	ctx.r1.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// b 0x8242f8e4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_13"))) PPC_WEAK_FUNC(fragDrawable_vfn_13);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_13) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=176, manual
	// lwz r31,4(r4)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 4));
	// lis r11,-32254
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r4,r11,30304
	ctx.r4.s64 = ctx.r11.s64 + 30304;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822e5908
	ph_5908(ctx, base);
	// lis r11,-32160
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r3,25460(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 25460);
	// lwz r10,16(r11)
	// bctrl
	VCALL(ctx.r3.u32, 4, ctx, base);  // vtable slot 4 (byte +16)
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,4(r30)
	PPC_STORE_U32(var_r30 + 4, ctx.r11.u32);
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// stb r9,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r9.u8);
	// lwz r7,4(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	// blr
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_11"))) PPC_WEAK_FUNC(fragDrawable_vfn_11);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=192, savegprlr_28
	// lwz r31,4(r4)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 4));
	// li r28,0
	var_r28 = 0;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// stb r28,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r28);
	// lwz r10,4(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	// addi r29,r30,16
	var_r29 = (uint32_t)(var_r30 + 16);
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r6,136(r30)
	ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 136);
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lbz r5,188(r30)
	ctx.r5.u64 = PPC_LOAD_U8(var_r30 + 188);
	// bl 0x8225d570
	atSingleton_D570_w(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 4);
	// bl 0x8225d6c8
	fragDrawable_D6C8_wrh(ctx, base);
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// stb r28,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r28);
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,4(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_12"))) PPC_WEAK_FUNC(fragDrawable_vfn_12);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=240, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lwz r3,4(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r5,128
	ctx.r5.s64 = 128;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,8(r30)
	PPC_STORE_U32(var_r30 + 8, ctx.r11.u32);
	// lwz r9,4(r10)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// lis r11,-32253
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,-2396
	ctx.r4.s64 = ctx.r11.s64 + -2396;
	// bl 0x82433a40
	_stricmp(ctx, base);
	// cmpwi cr6,r3,0
	// beq cr6,0x8225c13c
	if (ctx.r3.s32 != 0) {
		// lis r11,-32253
		// li r7,1
		ctx.r7.s64 = 1;
		// addi r5,r11,10796
		ctx.r5.s64 = ctx.r11.s64 + 10796;
		// lis r11,-32163
		// li r6,0
		ctx.r6.s64 = 0;
		// addi r3,r11,128
		ctx.r3.s64 = ctx.r11.s64 + 128;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// bl 0x822e3040
		rage_obj_factory_create_3040(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmplwi cr6,r31,0
		// beq cr6,0x8225c13c
		if (var_r31 == 0) {
			// blr
			return;
		}
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x8214b4e8
		crSkeketonData_AllocateAndLoad(ctx, base);
		// mr r8,r3
		ctx.r8.u64 = ctx.r3.u64;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r8,8(r30)
		PPC_STORE_U32(var_r30 + 8, ctx.r8.u32);
		// bl 0x822e3b38
		rage_obj_finalize_3B38(ctx, base);
	}
loc_8225C13C:
	// blr
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_14"))) PPC_WEAK_FUNC(fragDrawable_vfn_14);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=320, savegprlr_27
	// lwz r31,4(r4)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 4));
	// lis r11,-32254
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r4,r11,30304
	ctx.r4.s64 = ctx.r11.s64 + 30304;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x822e5908
	ph_5908(ctx, base);
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// li r5,128
	ctx.r5.s64 = 128;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,4(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	// li r27,0
	var_r27 = 0;
	// lis r11,-32253
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r11,-2396
	ctx.r4.s64 = ctx.r11.s64 + -2396;
	// stw r27,12(r30)
	PPC_STORE_U32(var_r30 + 12, var_r27);
	// bl 0x82433a40
	_stricmp(ctx, base);
	// cmpwi cr6,r3,0
	// beq cr6,0x8225c204
	if (ctx.r3.s32 != 0) {
		// lis r11,-32251
		// li r7,1
		ctx.r7.s64 = 1;
		// addi r5,r11,-136
		ctx.r5.s64 = ctx.r11.s64 + -136;
		// lis r11,-32163
		// li r6,0
		ctx.r6.s64 = 0;
		// addi r3,r11,128
		ctx.r3.s64 = ctx.r11.s64 + 128;
		// addi r4,r1,144
		ctx.r4.s64 = ctx.r1.s64 + 144;
		// bl 0x822e3040
		rage_obj_factory_create_3040(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmplwi cr6,r29,0
		// beq cr6,0x8225c204
		if (var_r29 == 0) goto loc_8225C204;
		// addi r4,r1,144
		ctx.r4.s64 = ctx.r1.s64 + 144;
		// bl 0x82375f40
		grmEdgeModel_Create(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// stw r11,12(r30)
		PPC_STORE_U32(var_r30 + 12, ctx.r11.u32);
		// bl 0x822e3b38
		rage_obj_finalize_3B38(ctx, base);
	}
loc_8225C204:
	// clrlwi r9,r28,24
	ctx.r9.u64 = var_r28 & 0xFF;
	// cmplwi cr6,r9,0
	// beq cr6,0x8225c230
	if (ctx.r9.u32 != 0) {
		// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
		// li r5,64
		ctx.r5.s64 = 64;
		// stb r27,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r27);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r7,4(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	}
loc_8225C230:
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_15"))) PPC_WEAK_FUNC(fragDrawable_vfn_15);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_15) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=208, savegprlr_26
	// lwz r27,4(r4)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 4));
	// li r26,0
	var_r26 = 0;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// lwz r11,0(r27)
  // [ph4a] vtable load collapsed
	// stb r26,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r26);
	// lwz r10,4(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	// bl 0x820c0038
	xe_main_thread_init_0038(ctx, base);
	// lwz r9,0(r13)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0)  /* SDA:0x82600000 */;
	// li r8,4
	ctx.r8.s64 = 4;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,144
	ctx.r4.s64 = 144;
	// lwzx r3,r8,r9
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// lwz r6,4(r7)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// cmplwi cr6,r3,0
	// beq cr6,0x8225c2b0
	if (ctx.r3.u32 != 0) {
		// mr r4,r27
		ctx.r4.u64 = var_r27;
		// bl 0x8241f578
		cmOperatorCtor_F578_w(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// b 0x8225c2b4
	} else {
	loc_8225C2B0:
		// mr r31,r26
		var_r31 = (uint32_t)(var_r26);
	}
loc_8225C2B4:
	// lhz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 140);
	// mr r28,r26
	var_r28 = (uint32_t)(var_r26);
	// cmpwi cr6,r11,0
	// ble cr6,0x8225c32c
	if (ctx.r11.s32 > 0) {
		// mr r30,r26
		var_r30 = (uint32_t)(var_r26);
	loc_8225C2C8:
		// lwz r5,136(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 136);
		// lwzx r3,r5,r30
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r30);
		// cmplwi cr6,r3,0
		// beq cr6,0x8225c318
		if (ctx.r3.u32 != 0) {
			// lwz r10,128(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 128);
			// lwz r4,132(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 132);
			// addi r10,r10,16
			ctx.r10.s64 = ctx.r10.s64 + 16;
			// lwz r11,64(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 64);
			// rlwinm r9,r4,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r8,r10,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// lwzx r7,r8,r29
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + var_r29);
			// lwz r6,0(r7)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			// lwzx r5,r6,r9
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
			// rlwinm r9,r11,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// add r4,r11,r9
			ctx.r4.u64 = ctx.r11.u64 + ctx.r9.u64;
			// rlwinm r11,r4,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
			// lwz r10,4(r5)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
			// add r11,r10,r11
			ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
			// addi r4,r11,12
			ctx.r4.s64 = ctx.r11.s64 + 12;
			// bl 0x8241f0c0
			fragDrawable_F0C0(ctx, base);
		}
	loc_8225C318:
		// lhz r11,140(r31)
		ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 140);
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmpw cr6,r28,r11
		// blt cr6,0x8225c2c8
		if ((int32_t)var_r28 < ctx.r11.s32) goto loc_8225C2C8;
	}
loc_8225C32C:
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r29,128
	ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 128;
	// bl 0x8225cbf0
	atSingleton_CBF0_h(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// stb r26,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r26);
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// stw r31,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r31);
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	return;
}

__attribute__((alias("__imp__fragDrawable_vfn_9"))) PPC_WEAK_FUNC(fragDrawable_vfn_9);
PPC_FUNC_IMPL(__imp__fragDrawable_vfn_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_28
	// lis r11,-32253
	// lfs f31,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	var_f31 = double(temp.f32);
	// addi r31,r3,16
	var_r31 = (uint32_t)(ctx.r3.s64 + 16);
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lfs f0,-12016(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12016);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// mr r28,r6
	var_r28 = ctx.r6.u32;
	// mr r29,r7
	var_r29 = ctx.r7.u32;
	// bl 0x820e86a8
	cmSampleCamActions_86A8_g(ctx, base);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f3,104(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f3.f64 = double(temp.f32);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f2,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// fmr f4,f31
	ctx.f4.f64 = var_f31;
	// bl 0x82153bf8
	LocomotionState_3BF8_g(ctx, base);
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// cmpwi cr6,r3,0
	// beq cr6,0x8225c3fc
	if (ctx.r3.s32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// fsubs f0,f13,f31
		ctx.f0.f64 = double(float(ctx.f13.f64 - var_f31));
		// addi r10,r31,64
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 64;
	loc_8225C3DC:
		// lfs f12,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f12.f64 = double(temp.f32);
		// fcmpu cr6,f0,f12
		// ble cr6,0x8225c400
		if (ctx.f0.f64 <= ctx.f12.f64) goto loc_8225C400;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmpwi cr6,r11,3
		// blt cr6,0x8225c3dc
		if (ctx.r11.s32 < 3) goto loc_8225C3DC;
		// b 0x8225c400
	} else {
	loc_8225C3FC:
		// li r11,3
		ctx.r11.s64 = 3;
	}
loc_8225C400:
	// cmplwi cr6,r29,0
	// stb r11,0(r28)
	PPC_STORE_U8(var_r28 + 0, ctx.r11.u8);
	// beq cr6,0x8225c424
	if (var_r29 != 0) {
		// fsubs f0,f13,f31
		ctx.fpscr.disableFlushMode();
		ctx.f0.f64 = double(float(ctx.f13.f64 - var_f31));
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfd f13,-25856(r11)
		ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25856);  /* glob:lbl_82079B00 @ 0x82079b00 */
		// fneg f11,f0
		ctx.f11.u64 = ctx.f0.u64 ^ 0x8000000000000000;
		// fsel f10,f11,f13,f0
		ctx.f10.f64 = ctx.f11.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
		// stfs f10,0(r29)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r29 + 0, temp.u32);
	}
loc_8225C424:
	return;
}

