#include "tt-decomp_init.h"

__attribute__((alias("__imp__phInst_23E0_p39"))) PPC_WEAK_FUNC(phInst_23E0_p39);
PPC_FUNC_IMPL(__imp__phInst_23E0_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x824623E8;
	__savegprlr_29(ctx, base);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r11,r3,24
	ctx.r11.s64 = ctx.r3.s64 + 24;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r10,2
	ctx.r9.s64 = ctx.r10.s64 + 2;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// lfs f11,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,16
	ctx.r8.s64 = 16;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
loc_82462418:
	// addi r7,r10,-2
	ctx.r7.s64 = ctx.r10.s64 + -2;
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// rlwinm r31,r10,2,23,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x1FC);
	// rlwinm r6,r7,2,23,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x1FC;
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r30,r10,2
	var_r30 = (uint32_t)(ctx.r10.s64 + 2);
	// rlwinm r7,r7,2,23,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x1FC;
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r30,r30,2,23,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x1FC);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f12,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// fmadds f11,f0,f12,f10
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f11,-8(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + -8, temp.u32);
	// rlwinm r6,r6,2,23,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x1FC;
	// addi r29,r10,3
	var_r29 = (uint32_t)(ctx.r10.s64 + 3);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r29,r29,2,23,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0x1FC);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,4(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fmadds f11,f0,f12,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f11,-4(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
	// addi r7,r4,24
	ctx.r7.s64 = ctx.r4.s64 + 24;
	// lfs f4,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmadds f11,f0,f12,f8
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f8.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmadds f11,f0,f12,f7
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// addi r6,r10,4
	ctx.r6.s64 = ctx.r10.s64 + 4;
	// rlwinm r31,r6,2,23,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x1FC);
	// addi r6,r4,28
	ctx.r6.s64 = ctx.r4.s64 + 28;
	// lfs f3,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r11
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,16(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmadds f11,f0,f12,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f6.f64));
	// stfs f11,8(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// addi r30,r10,5
	var_r30 = (uint32_t)(ctx.r10.s64 + 5);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// rlwinm r30,r30,2,23,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x1FC);
	// fmadds f10,f13,f11,f12
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r29,r11
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f0,f12,f5
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f5.f64));
	// stfs f11,12(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// stfs f10,20(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// addi r4,r4,32
	ctx.r4.s64 = ctx.r4.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f4
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f4.f64));
	// stfs f11,16(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r11
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f3
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f3.f64));
	// stfs f11,20(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// fmadds f12,f13,f11,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// fmr f11,f12
	ctx.f11.f64 = ctx.f12.f64;
	// bne cr6,0x82462418
	if (!ctx.cr6.eq) goto loc_82462418;
	// neg r7,r5
	ctx.r7.s64 = static_cast<int64_t>(-ctx.r5.u64);
	// subfic r9,r5,2
	ctx.xer.ca = ctx.r5.u32 <= 2;
	ctx.r9.s64 = 2 - ctx.r5.s64;
	// addi r10,r11,8
	ctx.r10.s64 = ctx.r11.s64 + 8;
	// li r8,16
	ctx.r8.s64 = 16;
loc_82462548:
	// rlwinm r5,r7,2,23,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x1FC;
	// lfs f2,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// addi r6,r9,-1
	ctx.r6.s64 = ctx.r9.s64 + -1;
	// lfs f1,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// addi r31,r9,1
	var_r31 = (uint32_t)(ctx.r9.s64 + 1);
	// rlwinm r6,r6,2,23,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x1FC;
	// lfs f10,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r31,r31,2,23,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0x1FC);
	// lfs f9,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f11,r5,r11
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r11.u32);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r5,r9,2,23,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0x1FC;
	// fmadds f12,f0,f11,f2
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f2.f64));
	// stfs f12,-8(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// addi r30,r9,2
	var_r30 = (uint32_t)(ctx.r9.s64 + 2);
	// lfs f8,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f8.f64 = double(temp.f32);
	// addi r29,r9,3
	var_r29 = (uint32_t)(ctx.r9.s64 + 3);
	// lfs f7,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// rlwinm r30,r30,2,23,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x1FC);
	// rlwinm r29,r29,2,23,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0x1FC);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmadds f11,f13,f12,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f11.f64));
	// lfsx f12,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,4(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fmadds f11,f0,f12,f1
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f1.f64));
	// stfs f11,-4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// addi r6,r9,4
	ctx.r6.s64 = ctx.r9.s64 + 4;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r5,r11
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmadds f11,f0,f12,f10
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r5,r4,28
	ctx.r5.s64 = ctx.r4.s64 + 28;
	// lfs f5,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmadds f11,f0,f12,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// rlwinm r31,r6,2,23,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x1FC);
	// addi r6,r4,24
	ctx.r6.s64 = ctx.r4.s64 + 24;
	// lfs f6,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r11
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,16(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmadds f11,f0,f12,f8
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f8.f64));
	// stfs f11,8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r30,r9,5
	var_r30 = (uint32_t)(ctx.r9.s64 + 5);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// rlwinm r30,r30,2,23,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x1FC);
	// fmadds f10,f13,f11,f12
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r29,r11
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f0,f12,f7
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f11,12(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f10,20(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// addi r4,r4,32
	ctx.r4.s64 = ctx.r4.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f6.f64));
	// stfs f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r11
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f5
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f5.f64));
	// stfs f11,20(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// fmadds f12,f13,f11,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bne cr6,0x82462548
	if (!ctx.cr6.eq) goto loc_82462548;
	// li r5,0
	ctx.r5.s64 = 0;
	// stfs f12,12(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// stw r5,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r5.u32);
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_2678_p39"))) PPC_WEAK_FUNC(phInst_2678_p39);
PPC_FUNC_IMPL(__imp__phInst_2678_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f894
	ctx.lr = 0x82462680;
	__savegprlr_27(ctx, base);
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r10,r3,44
	ctx.r10.s64 = ctx.r3.s64 + 44;
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f11,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lwz r7,36(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lfs f10,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// lwz r30,24(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// subf r31,r8,r11
	var_r31 = (uint32_t)(ctx.r11.s64 - ctx.r8.s64);
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// subf r30,r30,r11
	var_r30 = (uint32_t)(ctx.r11.s64 - (int64_t)(int32_t)var_r30);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// subf r11,r11,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
	// lfs f12,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// li r9,256
	ctx.r9.s64 = 256;
	// stfs f10,0(r6)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// addi r7,r7,256
	ctx.r7.s64 = ctx.r7.s64 + 256;
loc_824626CC:
	// rlwinm r29,r11,2,19,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x1FFC);
	// lfs f9,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// add r28,r31,r11
	var_r28 = (uint32_t)(var_r31 + ctx.r11.u64);
	// add r27,r30,r11
	var_r27 = (uint32_t)(var_r30 + ctx.r11.u64);
	// rlwinm r28,r28,2,19,29
	var_r28 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0x1FFC);
	// rlwinm r27,r27,2,19,29
	var_r27 = (uint32_t)(__builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0x1FFC);
	// lfsx f8,r29,r10
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r10.u32);
	ctx.f8.f64 = double(temp.f32);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lfsx f6,r28,r10
	temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r10.u32);
	ctx.f6.f64 = double(temp.f32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// lfsx f5,r27,r10
	temp.u32 = PPC_LOAD_U32(var_r27 + ctx.r10.u32);
	ctx.f5.f64 = double(temp.f32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stfs f9,0(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f4,0(r6)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r9,0
	// fmadds f3,f6,f13,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f13.f64 + ctx.f7.f64));
	// stfs f3,0(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// bne cr6,0x824626cc
	if (ctx.r9.u32 != 0) goto loc_824626CC;
	// clrlwi r7,r7,21
	ctx.r7.u64 = ctx.r7.u32 & 0x7FF;
	// fmr f2,f3
	ctx.f2.f64 = ctx.f3.f64;
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// stfs f2,8(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f1,32(r3)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// stw r7,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r7.u32);
	// b 0x8242f8e4
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_2748_p39"))) PPC_WEAK_FUNC(phInst_2748_p39);
PPC_FUNC_IMPL(__imp__phInst_2748_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x82462750;
	__savegprlr_29(ctx, base);
	// lwz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r9,r3,24
	ctx.r9.s64 = ctx.r3.s64 + 24;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r7,2
	ctx.r10.s64 = ctx.r7.s64 + 2;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// subf r11,r11,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
	// lfs f11,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,32
	ctx.r8.s64 = 32;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r5,r7,256
	ctx.r5.s64 = ctx.r7.s64 + 256;
loc_82462784:
	// addi r7,r11,-2
	ctx.r7.s64 = ctx.r11.s64 + -2;
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// rlwinm r31,r11,2,22,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x3FC);
	// rlwinm r6,r7,2,22,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x3FC;
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r30,r11,2
	var_r30 = (uint32_t)(ctx.r11.s64 + 2);
	// rlwinm r7,r7,2,22,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x3FC;
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r30,r30,2,22,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x3FC);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r11,1
	ctx.r6.s64 = ctx.r11.s64 + 1;
	// fmadds f11,f0,f12,f10
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f11,-8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// rlwinm r6,r6,2,22,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x3FC;
	// addi r29,r11,3
	var_r29 = (uint32_t)(ctx.r11.s64 + 3);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r29,r29,2,22,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0x3FC);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r7,r9
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,4(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fmadds f11,f0,f12,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f11,-4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// addi r7,r4,24
	ctx.r7.s64 = ctx.r4.s64 + 24;
	// lfs f4,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r9
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmadds f11,f0,f12,f8
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f8.f64));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmadds f11,f0,f12,f7
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r6,r11,4
	ctx.r6.s64 = ctx.r11.s64 + 4;
	// rlwinm r31,r6,2,22,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x3FC);
	// addi r6,r4,28
	ctx.r6.s64 = ctx.r4.s64 + 28;
	// lfs f3,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r9
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,16(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmadds f11,f0,f12,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f6.f64));
	// stfs f11,8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r30,r11,5
	var_r30 = (uint32_t)(ctx.r11.s64 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r30,r30,2,22,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x3FC);
	// fmadds f10,f13,f11,f12
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r29,r9
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f0,f12,f5
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f5.f64));
	// stfs f11,12(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f10,20(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// addi r4,r4,32
	ctx.r4.s64 = ctx.r4.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r9
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f4
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f4.f64));
	// stfs f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r9
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f3
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f3.f64));
	// stfs f11,20(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bne cr6,0x82462784
	if (!ctx.cr6.eq) goto loc_82462784;
	// clrlwi r5,r5,24
	ctx.r5.u64 = ctx.r5.u32 & 0xFF;
	// stfs f11,12(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// stw r5,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r5.u32);
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_28B8_p39"))) PPC_WEAK_FUNC(phInst_28B8_p39);
PPC_FUNC_IMPL(__imp__phInst_28B8_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x824628C0;
	__savegprlr_29(ctx, base);
	// lwz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r9,r3,24
	ctx.r9.s64 = ctx.r3.s64 + 24;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r7,2
	ctx.r10.s64 = ctx.r7.s64 + 2;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// subf r11,r11,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
	// lfs f11,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,32
	ctx.r8.s64 = 32;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r5,r7,256
	ctx.r5.s64 = ctx.r7.s64 + 256;
loc_824628F4:
	// addi r7,r11,-2
	ctx.r7.s64 = ctx.r11.s64 + -2;
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// rlwinm r31,r11,2,21,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x7FC);
	// rlwinm r6,r7,2,21,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x7FC;
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r30,r11,2
	var_r30 = (uint32_t)(ctx.r11.s64 + 2);
	// rlwinm r7,r7,2,21,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x7FC;
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r30,r30,2,21,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x7FC);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r11,1
	ctx.r6.s64 = ctx.r11.s64 + 1;
	// fmadds f11,f0,f12,f10
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f11,-8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// rlwinm r6,r6,2,21,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x7FC;
	// addi r29,r11,3
	var_r29 = (uint32_t)(ctx.r11.s64 + 3);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r29,r29,2,21,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0x7FC);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r7,r9
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,4(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fmadds f11,f0,f12,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f11,-4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// addi r7,r4,24
	ctx.r7.s64 = ctx.r4.s64 + 24;
	// lfs f4,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r9
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmadds f11,f0,f12,f8
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f8.f64));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmadds f11,f0,f12,f7
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r6,r11,4
	ctx.r6.s64 = ctx.r11.s64 + 4;
	// rlwinm r31,r6,2,21,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x7FC);
	// addi r6,r4,28
	ctx.r6.s64 = ctx.r4.s64 + 28;
	// lfs f3,0(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r9
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,16(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmadds f11,f0,f12,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f6.f64));
	// stfs f11,8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r30,r11,5
	var_r30 = (uint32_t)(ctx.r11.s64 + 5);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r30,r30,2,21,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x7FC);
	// fmadds f10,f13,f11,f12
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r29,r9
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f0,f12,f5
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f5.f64));
	// stfs f11,12(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f10,20(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// addi r4,r4,32
	ctx.r4.s64 = ctx.r4.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r9
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f4
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f4.f64));
	// stfs f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r9
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,0(r6)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmadds f11,f0,f12,f3
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f3.f64));
	// stfs f11,20(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bne cr6,0x824628f4
	if (!ctx.cr6.eq) goto loc_824628F4;
	// clrlwi r5,r5,23
	ctx.r5.u64 = ctx.r5.u32 & 0x1FF;
	// stfs f11,12(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// stw r5,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r5.u32);
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_2A28_p39"))) PPC_WEAK_FUNC(phInst_2A28_p39);
PPC_FUNC_IMPL(__imp__phInst_2A28_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f898
	ctx.lr = 0x82462A30;
	__savegprlr_28(ctx, base);
	// lwz r7,16(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// addi r9,r3,24
	ctx.r9.s64 = ctx.r3.s64 + 24;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r7,2
	ctx.r10.s64 = ctx.r7.s64 + 2;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// subf r11,r11,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
	// lfs f11,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,32
	ctx.r8.s64 = 32;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r7,r7,256
	ctx.r7.s64 = ctx.r7.s64 + 256;
loc_82462A64:
	// addi r6,r11,-2
	ctx.r6.s64 = ctx.r11.s64 + -2;
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,0(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// addi r31,r11,-1
	var_r31 = (uint32_t)(ctx.r11.s64 + -1);
	// rlwinm r6,r6,2,21,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x7FC;
	// lfs f9,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r31,r31,2,21,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0x7FC);
	// lfs f8,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r30,r11,2,21,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x7FC);
	// lfs f7,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r29,r11,2
	var_r29 = (uint32_t)(ctx.r11.s64 + 2);
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// addi r28,r11,3
	var_r28 = (uint32_t)(ctx.r11.s64 + 3);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r11,1
	ctx.r6.s64 = ctx.r11.s64 + 1;
	// fmadds f11,f0,f12,f10
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f11,-8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
	// rlwinm r6,r6,2,21,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x7FC;
	// lfs f4,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r29,r29,2,21,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0x7FC);
	// lfs f3,28(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 28);
	ctx.f3.f64 = double(temp.f32);
	// rlwinm r28,r28,2,21,29
	var_r28 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0x7FC);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r4,r4,32
	ctx.r4.s64 = ctx.r4.s64 + 32;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r9
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,4(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fmadds f11,f0,f12,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f11,-4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
	// addi r31,r11,5
	var_r31 = (uint32_t)(ctx.r11.s64 + 5);
	// rlwinm r31,r31,2,21,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0x7FC);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r30,r9
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,8(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// fmadds f11,f0,f12,f8
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f8.f64));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,12(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// fmadds f11,f0,f12,f7
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f7.f64));
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r6,r11,4
	ctx.r6.s64 = ctx.r11.s64 + 4;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r6,r6,2,21,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x7FC;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r29,r9
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,16(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// fmadds f11,f0,f12,f6
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f6.f64));
	// stfs f11,8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r28,r9
	temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,20(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// fmadds f11,f0,f12,f5
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f5.f64));
	// stfs f11,12(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r6,r9
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,24(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 24, temp.u32);
	// fmadds f11,f0,f12,f4
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f4.f64));
	// stfs f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfsx f12,r31,r9
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,28(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 28, temp.u32);
	// fmadds f11,f0,f12,f3
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f3.f64));
	// stfs f11,20(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// fmadds f11,f13,f11,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bne cr6,0x82462a64
	if (!ctx.cr6.eq) goto loc_82462A64;
	// clrlwi r5,r7,23
	ctx.r5.u64 = ctx.r7.u32 & 0x1FF;
	// stfs f11,12(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// stw r5,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r5.u32);
	// b 0x8242f8e8
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_2B90_p39"))) PPC_WEAK_FUNC(phInst_2B90_p39);
PPC_FUNC_IMPL(__imp__phInst_2B90_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f89c
	ctx.lr = 0x82462B98;
	__savegprlr_29(ctx, base);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r31,r3,32
	var_r31 = (uint32_t)(ctx.r3.s64 + 32);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lwz r30,0(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r10,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
	// lfs f13,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// subf r10,r30,r7
	ctx.r10.s64 = ctx.r7.s64 - (int64_t)(int32_t)var_r30;
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// li r11,256
	ctx.r11.s64 = 256;
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + var_r31;
	// stfs f13,0(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r7,r7,256
	ctx.r7.s64 = ctx.r7.s64 + 256;
loc_82462BD8:
	// rlwinm r30,r10,2,20,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFC);
	// lfs f10,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r29,r9,2,20,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFC);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// lfsx f9,r30,r31
	temp.u32 = PPC_LOAD_U32(var_r30 + var_r31);
	ctx.f9.f64 = double(temp.f32);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// lfsx f8,r29,r31
	temp.u32 = PPC_LOAD_U32(var_r29 + var_r31);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f0,f9,f12
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// stfs f10,0(r8)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// fmuls f13,f8,f11
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stfs f13,0(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// cmplwi cr6,r11,0
	// bne cr6,0x82462bd8
	if (ctx.r11.u32 != 0) goto loc_82462BD8;
	// clrlwi r9,r7,22
	ctx.r9.u64 = ctx.r7.u32 & 0x3FF;
	// stfs f0,8(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f13,20(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stw r9,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r9.u32);
	// b 0x8242f8ec
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_2C38_p39"))) PPC_WEAK_FUNC(phInst_2C38_p39);
PPC_FUNC_IMPL(__imp__phInst_2C38_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r19 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f870
	ctx.lr = 0x82462C40;
	__savegprlr_18(ctx, base);
	// lis r11,-32256
	// lfs f0,20(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// addi r10,r1,-128
	ctx.r10.s64 = ctx.r1.s64 + -128;
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,22696(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22696);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// fmuls f13,f12,f0
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fctidz f11,f13
	ctx.f11.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f11,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f11.u32);
	// lwz r9,-128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	// rlwinm r28,r9,31,1,31
	var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF);
	// cmplwi cr6,r28,256
	// blt cr6,0x82462c7c
	if (var_r28 >= 256) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82462C7C:
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmplwi cr6,r11,0
	// beq cr6,0x824630e8
	if (ctx.r11.u32 != 0) {
		// lis r11,-32256
		// lis r9,-32248
		// li r23,1
		var_r23 = 1;
		// cmpwi cr6,r28,4
		// lfs f11,15788(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);
		ctx.f11.f64 = double(temp.f32);
		// addi r11,r3,32
		ctx.r11.s64 = ctx.r3.s64 + 32;
		// lfs f0,-25744(r9)
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25744);
		ctx.f0.f64 = double(temp.f32);
		// fsubs f13,f11,f12
		ctx.f13.f64 = double(float(ctx.f11.f64 - ctx.f12.f64));
		// blt cr6,0x82462dfc
		if ((int32_t)var_r28 >= 4) {
			// lwz r6,8(r3)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			// addi r8,r10,2
			ctx.r8.s64 = ctx.r10.s64 + 2;
			// lwz r31,12(r3)
			var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 12));
			// rlwinm r29,r28,30,2,31
			var_r29 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 30) & 0x3FFFFFFF);
			// subf r30,r6,r10
			var_r30 = (uint32_t)(ctx.r10.s64 - ctx.r6.s64);
			// rlwinm r27,r29,2,0,29
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC);
			// subf r26,r31,r10
			var_r26 = (uint32_t)(ctx.r10.s64 - (int64_t)(int32_t)var_r31);
			// rlwinm r8,r8,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// subf r24,r31,r6
			var_r24 = (uint32_t)(ctx.r6.s64 - (int64_t)(int32_t)var_r31);
			// addi r7,r4,8
			ctx.r7.s64 = ctx.r4.s64 + 8;
			// addi r9,r5,12
			ctx.r9.s64 = ctx.r5.s64 + 12;
			// add r8,r8,r11
			ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
			// subf r25,r5,r4
			var_r25 = (uint32_t)(ctx.r4.s64 - ctx.r5.s64);
			// addi r31,r26,2
			var_r31 = (uint32_t)(var_r26 + 2);
			// addi r6,r30,2
			ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 2;
			// add r10,r27,r10
			ctx.r10.u64 = var_r27 + ctx.r10.u64;
			// addi r23,r27,1
			var_r23 = (uint32_t)(var_r27 + 1);
		loc_82462CF0:
			// mr r27,r30
			var_r27 = (uint32_t)(var_r30);
			// fmr f10,f12
			ctx.fpscr.disableFlushMode();
			ctx.f10.f64 = ctx.f12.f64;
			// addi r22,r6,-1
			var_r22 = (uint32_t)(ctx.r6.s64 + -1);
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// rlwinm r18,r27,2,22,29
			var_r18 = (uint32_t)(__builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0x3FC);
			// lfs f9,-8(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -8);
			ctx.f9.f64 = double(temp.f32);
			// add r26,r24,r30
			var_r26 = (uint32_t)(var_r24 + var_r30);
			// lfs f8,-4(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -4);
			ctx.f8.f64 = double(temp.f32);
			// mr r27,r22
			var_r27 = (uint32_t)(var_r22);
			// lfs f7,0(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f7.f64 = double(temp.f32);
			// rlwinm r22,r26,2,22,29
			var_r22 = (uint32_t)(__builtin_rotateleft64(var_r26 | (var_r26 << 32), 2) & 0x3FC);
			// lfsx f6,r25,r9
			temp.u32 = PPC_LOAD_U32(var_r25 + ctx.r9.u32);
			ctx.f6.f64 = double(temp.f32);
			// addi r21,r31,-1
			var_r21 = (uint32_t)(var_r31 + -1);
			// lfsx f4,r18,r11
			temp.u32 = PPC_LOAD_U32(var_r18 + ctx.r11.u32);
			ctx.f4.f64 = double(temp.f32);
			// mr r26,r31
			var_r26 = (uint32_t)(var_r31);
			// fmuls f3,f4,f13
			ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
			// rlwinm r21,r21,2,22,29
			var_r21 = (uint32_t)(__builtin_rotateleft64(var_r21 | (var_r21 << 32), 2) & 0x3FC);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// addi r20,r6,1
			var_r20 = (uint32_t)(ctx.r6.s64 + 1);
			// lfsx f2,r22,r11
			temp.u32 = PPC_LOAD_U32(var_r22 + ctx.r11.u32);
			ctx.f2.f64 = double(temp.f32);
			// rlwinm r22,r27,2,22,29
			var_r22 = (uint32_t)(__builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0x3FC);
			// stfs f9,-8(r8)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r8.u32 + -8, temp.u32);
			// fmr f5,f12
			ctx.f5.f64 = ctx.f12.f64;
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// mr r27,r6
			var_r27 = ctx.r6.u32;
			// addi r19,r31,1
			var_r19 = (uint32_t)(var_r31 + 1);
			// addi r29,r29,-1
			var_r29 = (uint32_t)(var_r29 + -1);
			// lfsx f9,r22,r11
			temp.u32 = PPC_LOAD_U32(var_r22 + ctx.r11.u32);
			ctx.f9.f64 = double(temp.f32);
			// rlwinm r22,r27,2,22,29
			var_r22 = (uint32_t)(__builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0x3FC);
			// rlwinm r27,r20,2,22,29
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r20 | (var_r20 << 32), 2) & 0x3FC);
			// addi r30,r30,4
			var_r30 = (uint32_t)(var_r30 + 4);
			// fmadds f1,f2,f10,f3
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 + ctx.f3.f64));
			// lfsx f3,r21,r11
			temp.u32 = PPC_LOAD_U32(var_r21 + ctx.r11.u32);
			ctx.f3.f64 = double(temp.f32);
			// fmuls f4,f9,f13
			ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// stfs f8,-4(r8)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r8.u32 + -4, temp.u32);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// lfsx f9,r22,r11
			temp.u32 = PPC_LOAD_U32(var_r22 + ctx.r11.u32);
			ctx.f9.f64 = double(temp.f32);
			// rlwinm r21,r26,2,22,29
			var_r21 = (uint32_t)(__builtin_rotateleft64(var_r26 | (var_r26 << 32), 2) & 0x3FC);
			// stfs f1,-8(r9)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r9.u32 + -8, temp.u32);
			// fmr f10,f12
			ctx.f10.f64 = ctx.f12.f64;
			// rlwinm r26,r19,2,22,29
			var_r26 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0x3FC);
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// addi r7,r7,16
			ctx.r7.s64 = ctx.r7.s64 + 16;
			// cmplwi cr6,r29,0
			ctx.cr6.compare<uint32_t>(var_r29, 0, ctx.xer);
			// addi r6,r6,4
			ctx.r6.s64 = ctx.r6.s64 + 4;
			// addi r31,r31,4
			var_r31 = (uint32_t)(var_r31 + 4);
			// fmadds f2,f3,f5,f4
			ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f4.f64));
			// lfsx f5,r21,r11
			temp.u32 = PPC_LOAD_U32(var_r21 + ctx.r11.u32);
			ctx.f5.f64 = double(temp.f32);
			// fmuls f8,f9,f13
			ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
			// stfs f7,0(r8)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// lfsx f3,r27,r11
			temp.u32 = PPC_LOAD_U32(var_r27 + ctx.r11.u32);
			ctx.f3.f64 = double(temp.f32);
			// stfs f2,-4(r9)
			temp.f32 = float(ctx.f2.f64);
			PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
			// fmr f1,f12
			ctx.f1.f64 = ctx.f12.f64;
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// fmadds f4,f5,f10,f8
			ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f10.f64 + ctx.f8.f64));
			// lfsx f10,r26,r11
			temp.u32 = PPC_LOAD_U32(var_r26 + ctx.r11.u32);
			ctx.f10.f64 = double(temp.f32);
			// fmuls f2,f3,f13
			ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
			// stfs f4,0(r9)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
			// stfs f6,4(r8)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// addi r8,r8,16
			ctx.r8.s64 = ctx.r8.s64 + 16;
			// fmadds f9,f10,f1,f2
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f2.f64));
			// stfs f9,4(r9)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
			// addi r9,r9,16
			ctx.r9.s64 = ctx.r9.s64 + 16;
			// bne cr6,0x82462cf0
			if (!ctx.cr6.eq) goto loc_82462CF0;
			// stfs f12,4(r3)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		}
	loc_82462DFC:
		// cmplw cr6,r23,r28
		// bgt cr6,0x82462e94
		if (var_r23 <= var_r28) {
			// rlwinm r7,r23,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(var_r23 | (var_r23 << 32), 2) & 0xFFFFFFFC;
			// lwz r30,8(r3)
			var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 8));
			// subf r9,r23,r28
			ctx.r9.s64 = (int64_t)(int32_t)var_r28 - (int64_t)(int32_t)var_r23;
			// lwz r27,12(r3)
			var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 12));
			// rlwinm r8,r10,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// lfs f12,4(r3)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// add r29,r7,r4
			var_r29 = (uint32_t)(ctx.r7.u64 + ctx.r4.u64);
			// add r6,r8,r11
			ctx.r6.u64 = ctx.r8.u64 + ctx.r11.u64;
			// subf r8,r30,r10
			ctx.r8.s64 = ctx.r10.s64 - (int64_t)(int32_t)var_r30;
			// add r31,r7,r5
			var_r31 = (uint32_t)(ctx.r7.u64 + ctx.r5.u64);
			// addi r7,r29,-4
			ctx.r7.s64 = (int64_t)(int32_t)var_r29 + -4;
			// subf r27,r27,r30
			var_r27 = var_r30 - var_r27;
			// add r10,r9,r10
			ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
		loc_82462E3C:
			// mr r30,r8
			var_r30 = ctx.r8.u32;
			// fmr f8,f12
			ctx.fpscr.disableFlushMode();
			ctx.f8.f64 = ctx.f12.f64;
			// add r29,r27,r8
			var_r29 = (uint32_t)(var_r27 + ctx.r8.u64);
			// lfs f7,0(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f7.f64 = double(temp.f32);
			// rlwinm r30,r30,2,22,29
			var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x3FC);
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// rlwinm r29,r29,2,22,29
			var_r29 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0x3FC);
			// addi r9,r9,-1
			ctx.r9.s64 = ctx.r9.s64 + -1;
			// addi r8,r8,1
			ctx.r8.s64 = ctx.r8.s64 + 1;
			// addi r7,r7,4
			ctx.r7.s64 = ctx.r7.s64 + 4;
			// lfsx f6,r30,r11
			temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
			ctx.f6.f64 = double(temp.f32);
			// cmplwi cr6,r9,0
			ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// lfsx f4,r29,r11
			temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
			ctx.f4.f64 = double(temp.f32);
			// stfs f7,0(r6)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// addi r6,r6,4
			ctx.r6.s64 = ctx.r6.s64 + 4;
			// fmadds f3,f4,f8,f5
			ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f5.f64));
			// stfs f3,0(r31)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// addi r31,r31,4
			var_r31 = (uint32_t)(var_r31 + 4);
			// bne cr6,0x82462e3c
			if (!ctx.cr6.eq) goto loc_82462E3C;
			// stfs f12,4(r3)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		}
	loc_82462E94:
		// lwz r24,16(r3)
		var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 16));
		// lwz r9,8(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// cmplw cr6,r24,r9
		// beq cr6,0x82462eac
		if (var_r24 != ctx.r9.u32) {
			// fmr f12,f11
			ctx.fpscr.disableFlushMode();
			ctx.f12.f64 = ctx.f11.f64;
			// b 0x82462eb4
		} else {
		loc_82462EAC:
			// lis r8,-32256
			ctx.r8.s64 = -2113929216;
			// lfs f12,15784(r8)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
			ctx.f12.f64 = double(temp.f32);
		}
	loc_82462EB4:
		// subfic r7,r28,256
		ctx.xer.ca = var_r28 <= 256;
		ctx.r7.s64 = 256 - (int64_t)(int32_t)var_r28;
		// stfs f12,4(r3)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		// fsubs f13,f11,f12
		ctx.f13.f64 = double(float(ctx.f11.f64 - ctx.f12.f64));
		// mr r26,r28
		var_r26 = (uint32_t)(var_r28);
		// cmpwi cr6,r7,4
		// stw r24,8(r3)
		PPC_STORE_U32(ctx.r3.u32 + 8, var_r24);
		// stw r9,12(r3)
		PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
		// blt cr6,0x82463038
		if (ctx.r7.s32 >= 4) {
			// subfic r6,r28,252
			ctx.xer.ca = var_r28 <= 252;
			ctx.r6.s64 = 252 - (int64_t)(int32_t)var_r28;
			// rotlwi r27,r9,0
			var_r27 = (uint32_t)(ctx.r9.u32);
			// rlwinm r9,r6,30,2,31
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
			// addi r8,r10,2
			ctx.r8.s64 = ctx.r10.s64 + 2;
			// addi r29,r9,1
			var_r29 = (uint32_t)(ctx.r9.s64 + 1);  // addr:0x82080001
			// addi r6,r28,2
			ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 2;
			// addi r7,r28,3
			ctx.r7.s64 = (int64_t)(int32_t)var_r28 + 3;
			// rlwinm r26,r29,2,0,29
			var_r26 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 2) & 0xFFFFFFFC);
			// subf r30,r24,r10
			var_r30 = (uint32_t)(ctx.r10.s64 - (int64_t)(int32_t)var_r24);
			// rlwinm r9,r6,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// subf r31,r27,r10
			var_r31 = (uint32_t)(ctx.r10.s64 - (int64_t)(int32_t)var_r27);
			// rlwinm r7,r7,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r8,r8,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// add r10,r26,r10
			ctx.r10.u64 = var_r26 + ctx.r10.u64;
			// add r7,r7,r4
			ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
			// add r8,r8,r11
			ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
			// add r9,r9,r5
			ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
			// subf r25,r5,r4
			var_r25 = (uint32_t)(ctx.r4.s64 - ctx.r5.s64);
			// addi r6,r30,2
			ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 2;
			// addi r31,r31,2
			var_r31 = (uint32_t)(var_r31 + 2);
			// subf r24,r27,r24
			var_r24 = var_r24 - var_r27;
			// add r26,r26,r28
			var_r26 = (uint32_t)(var_r26 + var_r28);
		loc_82462F2C:
			// mr r28,r30
			var_r28 = (uint32_t)(var_r30);
			// fmr f2,f12
			ctx.fpscr.disableFlushMode();
			ctx.f2.f64 = ctx.f12.f64;
			// addi r23,r6,-1
			var_r23 = (uint32_t)(ctx.r6.s64 + -1);
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// rlwinm r19,r28,2,22,29
			var_r19 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0x3FC);
			// lfs f1,-12(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -12);
			ctx.f1.f64 = double(temp.f32);
			// add r27,r24,r30
			var_r27 = (uint32_t)(var_r24 + var_r30);
			// lfs f11,-8(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -8);
			ctx.f11.f64 = double(temp.f32);
			// mr r28,r23
			var_r28 = (uint32_t)(var_r23);
			// lfsx f10,r25,r9
			temp.u32 = PPC_LOAD_U32(var_r25 + ctx.r9.u32);
			ctx.f10.f64 = double(temp.f32);
			// rlwinm r23,r27,2,22,29
			var_r23 = (uint32_t)(__builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0x3FC);
			// lfs f9,0(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f9.f64 = double(temp.f32);
			// addi r22,r31,-1
			var_r22 = (uint32_t)(var_r31 + -1);
			// lfsx f7,r19,r11
			temp.u32 = PPC_LOAD_U32(var_r19 + ctx.r11.u32);
			ctx.f7.f64 = double(temp.f32);
			// mr r27,r31
			var_r27 = (uint32_t)(var_r31);
			// fmuls f6,f7,f13
			ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
			// rlwinm r22,r22,2,22,29
			var_r22 = (uint32_t)(__builtin_rotateleft64(var_r22 | (var_r22 << 32), 2) & 0x3FC);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// addi r21,r6,1
			var_r21 = (uint32_t)(ctx.r6.s64 + 1);
			// lfsx f5,r23,r11
			temp.u32 = PPC_LOAD_U32(var_r23 + ctx.r11.u32);
			ctx.f5.f64 = double(temp.f32);
			// rlwinm r23,r28,2,22,29
			var_r23 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0x3FC);
			// stfs f1,-8(r8)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r8.u32 + -8, temp.u32);
			// fmr f8,f12
			ctx.f8.f64 = ctx.f12.f64;
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// mr r28,r6
			var_r28 = ctx.r6.u32;
			// lfsx f7,r22,r11
			temp.u32 = PPC_LOAD_U32(var_r22 + ctx.r11.u32);
			ctx.f7.f64 = double(temp.f32);
			// rlwinm r22,r27,2,22,29
			var_r22 = (uint32_t)(__builtin_rotateleft64(var_r27 | (var_r27 << 32), 2) & 0x3FC);
			// addi r20,r31,1
			var_r20 = (uint32_t)(var_r31 + 1);
			// addi r29,r29,-1
			var_r29 = (uint32_t)(var_r29 + -1);
			// rlwinm r27,r20,2,22,29
			var_r27 = (uint32_t)(__builtin_rotateleft64(var_r20 | (var_r20 << 32), 2) & 0x3FC);
			// addi r30,r30,4
			var_r30 = (uint32_t)(var_r30 + 4);
			// fmadds f4,f5,f2,f6
			ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f6.f64));
			// lfsx f2,r23,r11
			temp.u32 = PPC_LOAD_U32(var_r23 + ctx.r11.u32);
			ctx.f2.f64 = double(temp.f32);
			// fmuls f1,f2,f13
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
			// rlwinm r23,r28,2,22,29
			var_r23 = (uint32_t)(__builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0x3FC);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// stfs f11,-4(r8)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r8.u32 + -4, temp.u32);
			// stfs f4,-4(r9)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
			// rlwinm r28,r21,2,22,29
			var_r28 = (uint32_t)(__builtin_rotateleft64(var_r21 | (var_r21 << 32), 2) & 0x3FC);
			// fmr f3,f12
			ctx.f3.f64 = ctx.f12.f64;
			// addi r7,r7,16
			ctx.r7.s64 = ctx.r7.s64 + 16;
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// cmplwi cr6,r29,0
			ctx.cr6.compare<uint32_t>(var_r29, 0, ctx.xer);
			// lfsx f4,r23,r11
			temp.u32 = PPC_LOAD_U32(var_r23 + ctx.r11.u32);
			ctx.f4.f64 = double(temp.f32);
			// addi r6,r6,4
			ctx.r6.s64 = ctx.r6.s64 + 4;
			// addi r31,r31,4
			var_r31 = (uint32_t)(var_r31 + 4);
			// fmadds f6,f7,f8,f1
			ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f8.f64 + ctx.f1.f64));
			// lfsx f1,r22,r11
			temp.u32 = PPC_LOAD_U32(var_r22 + ctx.r11.u32);
			ctx.f1.f64 = double(temp.f32);
			// fmuls f2,f4,f13
			ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
			// stfs f10,0(r8)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// lfsx f10,r28,r11
			temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r11.u32);
			ctx.f10.f64 = double(temp.f32);
			// lfsx f7,r27,r11
			temp.u32 = PPC_LOAD_U32(var_r27 + ctx.r11.u32);
			ctx.f7.f64 = double(temp.f32);
			// fmr f5,f12
			ctx.f5.f64 = ctx.f12.f64;
			// stfs f6,0(r9)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
			// stfs f9,4(r8)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// addi r8,r8,16
			ctx.r8.s64 = ctx.r8.s64 + 16;
			// fmadds f11,f1,f3,f2
			ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 + ctx.f2.f64));
			// stfs f11,4(r9)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
			// fmuls f8,f10,f13
			ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// fmadds f6,f7,f5,f8
			ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f5.f64 + ctx.f8.f64));
			// stfs f6,8(r9)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
			// addi r9,r9,16
			ctx.r9.s64 = ctx.r9.s64 + 16;
			// bne cr6,0x82462f2c
			if (!ctx.cr6.eq) goto loc_82462F2C;
			// stfs f12,4(r3)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		}
	loc_82463038:
		// cmplwi cr6,r26,256
		// bge cr6,0x824630cc
		if (var_r26 < 256) {
			// rlwinm r7,r26,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(var_r26 | (var_r26 << 32), 2) & 0xFFFFFFFC;
			// lwz r31,8(r3)
			var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 8));
			// rlwinm r9,r10,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// lwz r29,12(r3)
			var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 12));
			// add r30,r7,r5
			var_r30 = (uint32_t)(ctx.r7.u64 + ctx.r5.u64);
			// lfs f12,4(r3)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
			ctx.f12.f64 = double(temp.f32);
			// subfic r8,r26,256
			ctx.xer.ca = var_r26 <= 256;
			ctx.r8.s64 = 256 - (int64_t)(int32_t)var_r26;
			// add r6,r9,r11
			ctx.r6.u64 = ctx.r9.u64 + ctx.r11.u64;
			// subf r9,r31,r10
			ctx.r9.s64 = ctx.r10.s64 - (int64_t)(int32_t)var_r31;
			// add r7,r7,r4
			ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
			// addi r4,r30,4
			ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 4;
			// subf r29,r29,r31
			var_r29 = var_r31 - var_r29;
			// add r10,r8,r10
			ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
		loc_82463074:
			// mr r31,r9
			var_r31 = ctx.r9.u32;
			// fmr f5,f12
			ctx.fpscr.disableFlushMode();
			ctx.f5.f64 = ctx.f12.f64;
			// add r30,r29,r9
			var_r30 = (uint32_t)(var_r29 + ctx.r9.u64);
			// lfs f4,0(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f4.f64 = double(temp.f32);
			// rlwinm r31,r31,2,22,29
			var_r31 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0x3FC);
			// fsubs f12,f12,f0
			ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
			// rlwinm r30,r30,2,22,29
			var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x3FC);
			// addi r8,r8,-1
			ctx.r8.s64 = ctx.r8.s64 + -1;
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// addi r7,r7,4
			ctx.r7.s64 = ctx.r7.s64 + 4;
			// lfsx f3,r31,r11
			temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
			ctx.f3.f64 = double(temp.f32);
			// cmplwi cr6,r8,0
			ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
			// fmuls f2,f3,f13
			ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
			// lfsx f1,r30,r11
			temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
			ctx.f1.f64 = double(temp.f32);
			// stfs f4,0(r6)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
			// fadds f13,f13,f0
			ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
			// addi r6,r6,4
			ctx.r6.s64 = ctx.r6.s64 + 4;
			// fmadds f11,f1,f5,f2
			ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f2.f64));
			// stfs f11,0(r4)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
			// addi r4,r4,4
			ctx.r4.s64 = ctx.r4.s64 + 4;
			// bne cr6,0x82463074
			if (!ctx.cr6.eq) goto loc_82463074;
			// stfs f12,4(r3)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
		}
	loc_824630CC:
		// clrlwi r4,r10,24
		ctx.r4.u64 = ctx.r10.u32 & 0xFF;
		// lfs f5,1024(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1024);
		ctx.f5.f64 = double(temp.f32);
		// clrlwi r10,r10,24
		ctx.r10.u64 = ctx.r10.u32 & 0xFF;
		// stfs f5,20(r3)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
		// stw r4,24(r3)
		PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r4.u32);
		// stw r10,24(r3)
		PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r10.u32);
		// b 0x8242f8c0
		__restgprlr_18(ctx, base);
		return;
	}
loc_824630E8:
	// subf r11,r10,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r10.s64;
	// lwz r31,8(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 8));
	// lwz r30,12(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 12));
	// lis r9,-32256
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r3,32
	ctx.r6.s64 = ctx.r3.s64 + 32;
	// add r8,r11,r5
	ctx.r8.u64 = ctx.r11.u64 + ctx.r5.u64;
	// lfs f0,15788(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 15788);
	ctx.f0.f64 = double(temp.f32);
	// subf r11,r31,r10
	ctx.r11.s64 = ctx.r10.s64 - (int64_t)(int32_t)var_r31;
	// fsubs f13,f0,f12
	ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// subf r31,r30,r31
	var_r31 = var_r31 - var_r30;
	// fmr f0,f12
	ctx.f0.f64 = ctx.f12.f64;
	// lis r30,-32248
	var_r30 = (uint32_t)(-2113404928);
	// li r9,256
	ctx.r9.s64 = 256;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// addi r10,r10,256
	ctx.r10.s64 = ctx.r10.s64 + 256;
	// lfs f12,-25744(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + -25744);
	ctx.f12.f64 = double(temp.f32);
loc_82463134:
	// add r30,r31,r11
	var_r30 = (uint32_t)(var_r31 + ctx.r11.u64);
	// lfs f9,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// rlwinm r29,r11,2,22,29
	var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0x3FC);
	// rlwinm r30,r30,2,22,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0x3FC);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// lfsx f8,r29,r6
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r6.u32);
	ctx.f8.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lfsx f7,r30,r6
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r6.u32);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f11,f8,f13
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f10,f7,f0
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f9,0(r7)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// fsubs f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fadds f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// fadds f6,f11,f10
	ctx.f6.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f6,0(r8)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne cr6,0x82463134
	if (!ctx.cr6.eq) goto loc_82463134;
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// lfs f5,1024(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1024);
	ctx.f5.f64 = double(temp.f32);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// stfs f5,20(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// stw r10,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r10.u32);
	// b 0x8242f8c0
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_31A0_p39"))) PPC_WEAK_FUNC(phInst_31A0_p39);
PPC_FUNC_IMPL(__imp__phInst_31A0_p39) {
	PPC_FUNC_PROLOGUE();
	// lis r11,4
	ctx.r11.s64 = 262144;
	// li r3,0
	ctx.r3.s64 = 0;
	// ori r10,r11,47652
	ctx.r10.u64 = ctx.r11.u64 | 47652;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_31B8_p39"))) PPC_WEAK_FUNC(phInst_31B8_p39);
PPC_FUNC_IMPL(__imp__phInst_31B8_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r15 = 0;
	uint32_t var_r14 = 0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	double var_f31 = 0.0;
	double var_f28 = 0.0;
	double var_f27 = 0.0;
	double var_f26 = 0.0;
	double var_f25 = 0.0;
	double var_f23 = 0.0;
	double var_f24 = 0.0;
	double var_f18 = 0.0;
	double var_f15 = 0.0;
	double var_f22 = 0.0;
	double var_f21 = 0.0;
	double var_f20 = 0.0;
	double var_f19 = 0.0;
	double var_f17 = 0.0;
	double var_f16 = 0.0;
	double var_f14 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f860
	ctx.lr = 0x824631C0;
	__savegprlr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x824365e0
	__savefpr_14(ctx, base);
	// ld r12,-4096(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// ld r12,-8192(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8192);
	// ld r12,-12288(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -12288);
	// ld r12,-16384(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16384);
	// stwu r1,-17888(r1)
	ea = -17888 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lwz r11,100(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 100);
	// stw r30,17908(r1)
	PPC_STORE_U32(ctx.r1.u32 + 17908, var_r30);
	// cmpwi cr6,r11,1
	// beq cr6,0x8246320c
	if (ctx.r11.s32 != 1) {
		// cmpwi cr6,r11,2
		// beq cr6,0x82463204
		if (ctx.r11.s32 != 2) {
			// lwz r11,0(r4)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
			// lwz r29,4(r4)
			var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 4));
			// b 0x82463214
			goto loc_82463214;
		}
	loc_82463204:
		// lwz r11,4(r4)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// b 0x82463210
	} else {
	loc_8246320C:
		// lwz r11,0(r4)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	}
loc_82463210:
	// mr r29,r11
	var_r29 = ctx.r11.u32;
loc_82463214:
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// lwz r8,12(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// lwz r7,16(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r11.u32);
	// stw r29,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, var_r29);
	// stw r10,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, ctx.r10.u32);
	// stw r9,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, ctx.r9.u32);
	// stw r8,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r8.u32);
	// stw r7,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r7.u32);
	// dcbt r0,r11
	// dcbt r0,r29
	// li r6,128
	ctx.r6.s64 = 128;
	// dcbt r6,r11
	// li r5,128
	ctx.r5.s64 = 128;
	// dcbt r5,r29
	// addi r5,r1,7664
	ctx.r5.s64 = ctx.r1.s64 + 7664;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// addi r3,r30,104
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 104;
	// bl 0x82461b00
	phInst_1B00_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// li r6,256
	ctx.r6.s64 = 256;
	// addi r5,r1,9840
	ctx.r5.s64 = ctx.r1.s64 + 9840;
	// addi r4,r1,7664
	ctx.r4.s64 = ctx.r1.s64 + 7664;
	// addi r3,r3,168
	ctx.r3.s64 = ctx.r3.s64 + 168;
	// bl 0x824621e0
	phInst_21E0_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// li r7,256
	ctx.r7.s64 = 256;
	// addi r6,r1,5600
	ctx.r6.s64 = ctx.r1.s64 + 5600;
	// addi r5,r1,1440
	ctx.r5.s64 = ctx.r1.s64 + 1440;
	// addi r4,r1,9840
	ctx.r4.s64 = ctx.r1.s64 + 9840;
	// addi r3,r3,240
	ctx.r3.s64 = ctx.r3.s64 + 240;
	// bl 0x82462310
	phInst_2310_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r4,r1,1440
	ctx.r4.s64 = ctx.r1.s64 + 1440;
	// addi r3,r3,2332
	ctx.r3.s64 = ctx.r3.s64 + 2332;
	// bl 0x824623e0
	phInst_23E0_p39(ctx, base);
	// addi r4,r1,16559
	ctx.r4.s64 = ctx.r1.s64 + 16559;
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// rlwinm r31,r4,0,0,24
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80);
	// li r7,256
	ctx.r7.s64 = 256;
	// addi r6,r1,12016
	ctx.r6.s64 = ctx.r1.s64 + 12016;
	// addi r4,r1,1440
	ctx.r4.s64 = ctx.r1.s64 + 1440;
	// addi r3,r3,2868
	ctx.r3.s64 = ctx.r3.s64 + 2868;
	// mr r5,r31
	ctx.r5.u64 = var_r31;
	// bl 0x82462678
	phInst_2678_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r3,11104
	ctx.r3.s64 = ctx.r3.s64 + 11104;
	// bl 0x82462748
	phInst_2748_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r5,r1,400
	ctx.r5.s64 = ctx.r1.s64 + 400;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// addi r3,r3,12152
	ctx.r3.s64 = ctx.r3.s64 + 12152;
	// bl 0x82462a28
	phInst_2A28_p39(ctx, base);
	// addi r3,r1,14223
	ctx.r3.s64 = ctx.r1.s64 + 14223;
	// addi r10,r1,15391
	ctx.r10.s64 = ctx.r1.s64 + 15391;
	// rlwinm r11,r3,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r5,r10,0,0,24
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// addi r6,r11,8
	ctx.r6.s64 = ctx.r11.s64 + 8;
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r4,r1,400
	ctx.r4.s64 = ctx.r1.s64 + 400;
	// addi r3,r3,14224
	ctx.r3.s64 = ctx.r3.s64 + 14224;
	// stw r5,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r5.u32);
	// stw r6,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r6.u32);
	// bl 0x82462b90
	phInst_2B90_p39(ctx, base);
	// addis r3,r30,1
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 65536;
	// addi r5,r1,7664
	ctx.r5.s64 = ctx.r1.s64 + 7664;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// addi r3,r3,136
	ctx.r3.s64 = ctx.r3.s64 + 136;
	// bl 0x82461b00
	phInst_1B00_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// li r6,256
	ctx.r6.s64 = 256;
	// addi r5,r1,9840
	ctx.r5.s64 = ctx.r1.s64 + 9840;
	// addi r4,r1,7664
	ctx.r4.s64 = ctx.r1.s64 + 7664;
	// addi r3,r3,200
	ctx.r3.s64 = ctx.r3.s64 + 200;
	// bl 0x824621e0
	phInst_21E0_p39(ctx, base);
	// li r7,256
	ctx.r7.s64 = 256;
	// addi r6,r1,400
	ctx.r6.s64 = ctx.r1.s64 + 400;
	// addi r5,r1,2480
	ctx.r5.s64 = ctx.r1.s64 + 2480;
	// addi r4,r1,9840
	ctx.r4.s64 = ctx.r1.s64 + 9840;
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r3,r3,18360
	ctx.r3.s64 = ctx.r3.s64 + 18360;
	// bl 0x82462310
	phInst_2310_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r4,r1,2480
	ctx.r4.s64 = ctx.r1.s64 + 2480;
	// addi r3,r3,20452
	ctx.r3.s64 = ctx.r3.s64 + 20452;
	// bl 0x824623e0
	phInst_23E0_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// li r7,256
	ctx.r7.s64 = 256;
	// addi r6,r1,13056
	ctx.r6.s64 = ctx.r1.s64 + 13056;
	// addi r5,r1,4560
	ctx.r5.s64 = ctx.r1.s64 + 4560;
	// addi r4,r1,2480
	ctx.r4.s64 = ctx.r1.s64 + 2480;
	// addi r3,r3,20988
	ctx.r3.s64 = ctx.r3.s64 + 20988;
	// bl 0x82462678
	phInst_2678_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r4,r1,4560
	ctx.r4.s64 = ctx.r1.s64 + 4560;
	// addi r3,r3,29224
	ctx.r3.s64 = ctx.r3.s64 + 29224;
	// bl 0x82462748
	phInst_2748_p39(ctx, base);
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r5,r1,3520
	ctx.r5.s64 = ctx.r1.s64 + 3520;
	// addi r4,r1,4560
	ctx.r4.s64 = ctx.r1.s64 + 4560;
	// addi r3,r3,30272
	ctx.r3.s64 = ctx.r3.s64 + 30272;
	// bl 0x82462a28
	phInst_2A28_p39(ctx, base);
	// addi r9,r1,7791
	ctx.r9.s64 = ctx.r1.s64 + 7791;
	// addi r8,r1,9967
	ctx.r8.s64 = ctx.r1.s64 + 9967;
	// rlwinm r11,r9,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r5,r8,0,0,24
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// addi r6,r11,8
	ctx.r6.s64 = ctx.r11.s64 + 8;
	// addis r3,r30,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 131072;
	// addi r4,r1,3520
	ctx.r4.s64 = ctx.r1.s64 + 3520;
	// addi r3,r3,32344
	ctx.r3.s64 = ctx.r3.s64 + 32344;
	// stw r5,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r5.u32);
	// stw r6,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r6.u32);
	// stw r5,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r5.u32);
	// bl 0x82462b90
	phInst_2B90_p39(ctx, base);
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lfs f0,96(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lis r3,2
	ctx.r3.s64 = 131072;
	// ori r6,r7,18352
	ctx.r6.u64 = ctx.r7.u64 | 18352;
	// ori r4,r5,236
	ctx.r4.u64 = ctx.r5.u64 | 236;
	// ori r11,r3,18356
	ctx.r11.u64 = ctx.r3.u64 | 18356;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// addi r8,r1,2480
	ctx.r8.s64 = ctx.r1.s64 + 2480;
	// lfsx f13,r30,r6
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r6.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r7,r1,5600
	ctx.r7.s64 = ctx.r1.s64 + 5600;
	// lfsx f12,r30,r4
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r4.u32);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r1,4560
	ctx.r6.s64 = ctx.r1.s64 + 4560;
	// lfsx f11,r30,r11
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
	ctx.f11.f64 = double(temp.f32);
	// addi r5,r1,1440
	ctx.r5.s64 = ctx.r1.s64 + 1440;
	// addi r4,r1,400
	ctx.r4.s64 = ctx.r1.s64 + 400;
	// addi r3,r1,2484
	ctx.r3.s64 = ctx.r1.s64 + 2484;
	// addi r11,r1,5604
	ctx.r11.s64 = ctx.r1.s64 + 5604;
	// ori r9,r10,232
	ctx.r9.u64 = ctx.r10.u64 | 232;
	// addi r22,r31,4
	var_r22 = (uint32_t)(var_r31 + 4);
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - (int64_t)(int32_t)var_r31;
	// subf r7,r31,r7
	ctx.r7.s64 = ctx.r7.s64 - (int64_t)(int32_t)var_r31;
	// subf r6,r31,r6
	ctx.r6.s64 = ctx.r6.s64 - (int64_t)(int32_t)var_r31;
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - (int64_t)(int32_t)var_r31;
	// lfsx f10,r30,r9
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r9.u32);
	ctx.f10.f64 = double(temp.f32);
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - (int64_t)(int32_t)var_r31;
	// stw r22,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, var_r22);
	// subf r3,r31,r3
	ctx.r3.s64 = ctx.r3.s64 - (int64_t)(int32_t)var_r31;
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// subf r11,r31,r11
	ctx.r11.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r31;
	// stw r7,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
	// addi r10,r1,4564
	ctx.r10.s64 = ctx.r1.s64 + 4564;
	// stw r6,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r6.u32);
	// addi r9,r1,1444
	ctx.r9.s64 = ctx.r1.s64 + 1444;
	// stw r5,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r5.u32);
	// addi r30,r1,404
	var_r30 = (uint32_t)(ctx.r1.s64 + 404);
	// stw r4,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r4.u32);
	// addi r29,r1,2488
	var_r29 = (uint32_t)(ctx.r1.s64 + 2488);
	// stw r3,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r3.u32);
	// addi r28,r1,5608
	var_r28 = (uint32_t)(ctx.r1.s64 + 5608);
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r11.u32);
	// addi r27,r1,4568
	var_r27 = (uint32_t)(ctx.r1.s64 + 4568);
	// addi r26,r1,1448
	var_r26 = (uint32_t)(ctx.r1.s64 + 1448);
	// addi r25,r1,408
	var_r25 = (uint32_t)(ctx.r1.s64 + 408);
	// addi r24,r1,2492
	var_r24 = (uint32_t)(ctx.r1.s64 + 2492);
	// addi r23,r1,5612
	var_r23 = (uint32_t)(ctx.r1.s64 + 5612);
	// subf r8,r31,r30
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 - (int64_t)(int32_t)var_r31;
	// subf r7,r31,r29
	ctx.r7.s64 = (int64_t)(int32_t)var_r29 - (int64_t)(int32_t)var_r31;
	// subf r6,r31,r28
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 - (int64_t)(int32_t)var_r31;
	// subf r5,r31,r27
	ctx.r5.s64 = (int64_t)(int32_t)var_r27 - (int64_t)(int32_t)var_r31;
	// subf r4,r31,r26
	ctx.r4.s64 = (int64_t)(int32_t)var_r26 - (int64_t)(int32_t)var_r31;
	// stw r8,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r8.u32);
	// addi r8,r1,1452
	ctx.r8.s64 = ctx.r1.s64 + 1452;
	// stw r7,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r7.u32);
	// addi r7,r1,412
	ctx.r7.s64 = ctx.r1.s64 + 412;
	// stw r6,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r6.u32);
	// addi r6,r1,2496
	ctx.r6.s64 = ctx.r1.s64 + 2496;
	// stw r5,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r5.u32);
	// addi r5,r1,5616
	ctx.r5.s64 = ctx.r1.s64 + 5616;
	// stw r4,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r4.u32);
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// addi r4,r1,4576
	ctx.r4.s64 = ctx.r1.s64 + 4576;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// subf r3,r31,r25
	ctx.r3.s64 = (int64_t)(int32_t)var_r25 - (int64_t)(int32_t)var_r31;
	// stw r6,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r6.u32);
	// subf r10,r31,r10
	ctx.r10.s64 = ctx.r10.s64 - (int64_t)(int32_t)var_r31;
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// subf r11,r31,r24
	ctx.r11.s64 = (int64_t)(int32_t)var_r24 - (int64_t)(int32_t)var_r31;
	// stw r9,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r9.u32);
	// addi r9,r1,4572
	ctx.r9.s64 = ctx.r1.s64 + 4572;
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// addi r8,r1,6664
	ctx.r8.s64 = ctx.r1.s64 + 6664;
	// stw r3,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r3.u32);
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// addi r3,r1,1456
	ctx.r3.s64 = ctx.r1.s64 + 1456;
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// subf r10,r31,r23
	ctx.r10.s64 = (int64_t)(int32_t)var_r23 - (int64_t)(int32_t)var_r31;
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r11.u32);
	// addi r7,r1,3544
	ctx.r7.s64 = ctx.r1.s64 + 3544;
	// addi r11,r1,416
	ctx.r11.s64 = ctx.r1.s64 + 416;
	// stw r9,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r9.u32);
	// addi r6,r1,5620
	ctx.r6.s64 = ctx.r1.s64 + 5620;
	// stw r3,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r3.u32);
	// addi r5,r1,4580
	ctx.r5.s64 = ctx.r1.s64 + 4580;
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// addi r10,r1,2500
	ctx.r10.s64 = ctx.r1.s64 + 2500;
	// addi r4,r1,1460
	ctx.r4.s64 = ctx.r1.s64 + 1460;
	// addi r3,r1,420
	ctx.r3.s64 = ctx.r1.s64 + 420;
	// addi r30,r1,2504
	var_r30 = (uint32_t)(ctx.r1.s64 + 2504);
	// addi r29,r1,5624
	var_r29 = (uint32_t)(ctx.r1.s64 + 5624);
	// addi r28,r1,4584
	var_r28 = (uint32_t)(ctx.r1.s64 + 4584);
	// addi r27,r1,1464
	var_r27 = (uint32_t)(ctx.r1.s64 + 1464);
	// addi r26,r1,424
	var_r26 = (uint32_t)(ctx.r1.s64 + 424);
	// addi r25,r1,6640
	var_r25 = (uint32_t)(ctx.r1.s64 + 6640);
	// addi r24,r1,3520
	var_r24 = (uint32_t)(ctx.r1.s64 + 3520);
	// addi r23,r1,6644
	var_r23 = (uint32_t)(ctx.r1.s64 + 6644);
	// addi r22,r1,3524
	var_r22 = (uint32_t)(ctx.r1.s64 + 3524);
	// addi r21,r1,6648
	var_r21 = (uint32_t)(ctx.r1.s64 + 6648);
	// addi r20,r1,3528
	var_r20 = (uint32_t)(ctx.r1.s64 + 3528);
	// addi r19,r1,6652
	var_r19 = (uint32_t)(ctx.r1.s64 + 6652);
	// addi r18,r1,3532
	var_r18 = (uint32_t)(ctx.r1.s64 + 3532);
	// addi r17,r1,6656
	var_r17 = (uint32_t)(ctx.r1.s64 + 6656);
	// addi r16,r1,3536
	var_r16 = (uint32_t)(ctx.r1.s64 + 3536);
	// addi r15,r1,6660
	var_r15 = (uint32_t)(ctx.r1.s64 + 6660);
	// addi r14,r1,3540
	var_r14 = (uint32_t)(ctx.r1.s64 + 3540);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// subf r8,r31,r11
	ctx.r8.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r31;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// stw r9,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r9.u32);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// subf r7,r31,r10
	ctx.r7.s64 = ctx.r10.s64 - (int64_t)(int32_t)var_r31;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// lwz r9,92(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r31;
	// subf r11,r31,r15
	ctx.r11.s64 = (int64_t)(int32_t)var_r15 - (int64_t)(int32_t)var_r31;
	// subf r10,r31,r14
	ctx.r10.s64 = (int64_t)(int32_t)var_r14 - (int64_t)(int32_t)var_r31;
	// subf r6,r31,r6
	ctx.r6.s64 = ctx.r6.s64 - (int64_t)(int32_t)var_r31;
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - (int64_t)(int32_t)var_r31;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - (int64_t)(int32_t)var_r31;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r11.u32);
	// subf r3,r31,r3
	ctx.r3.s64 = ctx.r3.s64 - (int64_t)(int32_t)var_r31;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// subf r30,r31,r30
	var_r30 = var_r30 - var_r31;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// subf r29,r31,r29
	var_r29 = var_r29 - var_r31;
	// subf r10,r31,r11
	ctx.r10.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r31;
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// subf r28,r31,r28
	var_r28 = var_r28 - var_r31;
	// subf r27,r31,r27
	var_r27 = var_r27 - var_r31;
	// subf r26,r31,r26
	var_r26 = var_r26 - var_r31;
	// subf r25,r31,r25
	var_r25 = var_r25 - var_r31;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// subf r10,r31,r11
	ctx.r10.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r31;
	// lwz r11,160(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// subf r24,r31,r24
	var_r24 = var_r24 - var_r31;
	// subf r23,r31,r23
	var_r23 = var_r23 - var_r31;
	// subf r22,r31,r22
	var_r22 = var_r22 - var_r31;
	// subf r21,r31,r21
	var_r21 = var_r21 - var_r31;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// subf r20,r31,r20
	var_r20 = var_r20 - var_r31;
	// subf r19,r31,r19
	var_r19 = var_r19 - var_r31;
	// subf r18,r31,r18
	var_r18 = var_r18 - var_r31;
	// subf r17,r31,r17
	var_r17 = var_r17 - var_r31;
	// subf r16,r31,r16
	var_r16 = var_r16 - var_r31;
	// li r10,0
	ctx.r10.s64 = 0;
loc_8246365C:
	// addi r31,r1,4560
	var_r31 = (uint32_t)(ctx.r1.s64 + 4560);
	// lwz r15,140(r1)
	var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 140));
	// lwz r14,152(r1)
	var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 152));
	// lfs f9,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f9,f12
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfs f4,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f4,f4,f12
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// lfsx f3,r10,r31
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
	ctx.f3.f64 = double(temp.f32);
	// lwz r31,136(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 136));
	// lfsx f8,r15,r11
	temp.u32 = PPC_LOAD_U32(var_r15 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// lwz r15,156(r1)
	var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 156));
	// fmuls f9,f8,f11
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// fmuls f8,f7,f12
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfsx f7,r14,r11
	temp.u32 = PPC_LOAD_U32(var_r14 + ctx.r11.u32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r14,112(r1)
	var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 112));
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// lfsx f2,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f2.f64 = double(temp.f32);
	// addi r31,r1,2480
	var_r31 = (uint32_t)(ctx.r1.s64 + 2480);
	// lfsx f5,r15,r11
	temp.u32 = PPC_LOAD_U32(var_r15 + ctx.r11.u32);
	ctx.f5.f64 = double(temp.f32);
	// addi r15,r1,1440
	var_r15 = (uint32_t)(ctx.r1.s64 + 1440);
	// fmuls f5,f5,f11
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfsx f3,r14,r11
	temp.u32 = PPC_LOAD_U32(var_r14 + ctx.r11.u32);
	ctx.f3.f64 = double(temp.f32);
	// lwz r14,120(r1)
	var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 120));
	// lfsx f30,r10,r31
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
	var_f30 = double(temp.f32);
	// lwz r31,100(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 100));
	// lfsx f29,r10,r15
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r15);
	var_f29 = double(temp.f32);
	// addi r15,r1,400
	var_r15 = (uint32_t)(ctx.r1.s64 + 400);
	// fmadds f7,f7,f13,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f3.f64));
	// fmadds f6,f30,f13,f6
	ctx.f6.f64 = double(float(var_f30 * ctx.f13.f64 + ctx.f6.f64));
	// lfsx f31,r14,r11
	temp.u32 = PPC_LOAD_U32(var_r14 + ctx.r11.u32);
	var_f31 = double(temp.f32);
	// lwz r14,188(r1)
	var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 188));
	// lfsx f28,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f28 = double(temp.f32);
	// lwz r31,180(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 180));
	// fmadds f3,f31,f10,f9
	ctx.f3.f64 = double(float(var_f31 * ctx.f10.f64 + ctx.f9.f64));
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f28,f13,f8
	ctx.f8.f64 = double(float(var_f28 * ctx.f13.f64 + ctx.f8.f64));
	// fmadds f1,f29,f10,f1
	ctx.f1.f64 = double(float(var_f29 * ctx.f10.f64 + ctx.f1.f64));
	// lfsx f31,r14,r11
	temp.u32 = PPC_LOAD_U32(var_r14 + ctx.r11.u32);
	var_f31 = double(temp.f32);
	// lwz r14,196(r1)
	var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 196));
	// lfsx f27,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f27 = double(temp.f32);
	// lwz r31,172(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 172));
	// fmadds f5,f27,f10,f5
	ctx.f5.f64 = double(float(var_f27 * ctx.f10.f64 + ctx.f5.f64));
	// lfsx f27,r10,r15
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r15);
	var_f27 = double(temp.f32);
	// lwz r15,200(r1)
	var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 200));
	// fmadds f9,f9,f12,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 + ctx.f7.f64));
	// lfsx f29,r14,r11
	temp.u32 = PPC_LOAD_U32(var_r14 + ctx.r11.u32);
	var_f29 = double(temp.f32);
	// lwz r14,184(r1)
	var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 184));
	// lfsx f26,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f26 = double(temp.f32);
	// lwz r31,164(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 164));
	// fmadds f4,f26,f13,f4
	ctx.f4.f64 = double(float(var_f26 * ctx.f13.f64 + ctx.f4.f64));
	// fadds f7,f3,f31
	ctx.f7.f64 = double(float(ctx.f3.f64 + var_f31));
	// fadds f1,f1,f27
	ctx.f1.f64 = double(float(ctx.f1.f64 + var_f27));
	// fadds f3,f8,f29
	ctx.f3.f64 = double(float(ctx.f8.f64 + var_f29));
	// lfsx f27,r14,r11
	temp.u32 = PPC_LOAD_U32(var_r14 + ctx.r11.u32);
	var_f27 = double(temp.f32);
	// lfsx f30,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f30 = double(temp.f32);
	// addi r31,r1,5600
	var_r31 = (uint32_t)(ctx.r1.s64 + 5600);
	// fmadds f2,f30,f10,f2
	ctx.f2.f64 = double(float(var_f30 * ctx.f10.f64 + ctx.f2.f64));
	// lfsx f30,r15,r11
	temp.u32 = PPC_LOAD_U32(var_r15 + ctx.r11.u32);
	var_f30 = double(temp.f32);
	// lwz r15,176(r1)
	var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 176));
	// fadds f8,f5,f30
	ctx.f8.f64 = double(float(ctx.f5.f64 + var_f30));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfsx f9,r25,r11
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r25 + ctx.r11.u32, temp.u32);
	// lfsx f28,r10,r31
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
	var_f28 = double(temp.f32);
	// addi r31,r1,6640
	var_r31 = (uint32_t)(ctx.r1.s64 + 6640);
	// fadds f6,f6,f28
	ctx.f6.f64 = double(float(ctx.f6.f64 + var_f28));
	// lfsx f26,r15,r11
	temp.u32 = PPC_LOAD_U32(var_r15 + ctx.r11.u32);
	var_f26 = double(temp.f32);
	// lwz r15,192(r1)
	var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 192));
	// fadds f5,f4,f26
	ctx.f5.f64 = double(float(ctx.f4.f64 + var_f26));
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfsx f7,r24,r11
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r24 + ctx.r11.u32, temp.u32);
	// lfsx f28,r15,r11
	temp.u32 = PPC_LOAD_U32(var_r15 + ctx.r11.u32);
	var_f28 = double(temp.f32);
	// addi r15,r1,3520
	var_r15 = (uint32_t)(ctx.r1.s64 + 3520);
	// fadds f4,f2,f28
	ctx.f4.f64 = double(float(ctx.f2.f64 + var_f28));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfsx f2,r10,r31
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + var_r31, temp.u32);
	// fmuls f6,f3,f0
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfsx f1,r10,r15
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + var_r15, temp.u32);
	// stfsx f6,r23,r11
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r23 + ctx.r11.u32, temp.u32);
	// lwz r31,168(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 168));
	// fmuls f3,f8,f0
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f2,f5,f0
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f6,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f1,f4,f0
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfsx f1,r20,r11
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r20 + ctx.r11.u32, temp.u32);
	// fmuls f1,f6,f12
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f9,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f7,f9,f12
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// stfsx f3,r22,r11
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r22 + ctx.r11.u32, temp.u32);
	// lfsx f8,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// lwz r31,96(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 96));
	// fmuls f4,f8,f11
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// lfs f3,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f3,f12
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// stfsx f2,r21,r11
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r21 + ctx.r11.u32, temp.u32);
	// lfsx f2,r5,r11
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r11.u32);
	ctx.f2.f64 = double(temp.f32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lfs f9,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfsx f5,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f5.f64 = double(temp.f32);
	// lwz r31,124(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 124));
	// fmuls f6,f5,f11
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfsx f8,r28,r11
	temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r11.u32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f9,f12
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfsx f30,r9,r11
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	var_f30 = double(temp.f32);
	// fmuls f8,f8,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// lfsx f29,r7,r11
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
	var_f29 = double(temp.f32);
	// fmadds f7,f27,f13,f7
	ctx.f7.f64 = double(float(var_f27 * ctx.f13.f64 + ctx.f7.f64));
	// lfsx f28,r4,r11
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r11.u32);
	var_f28 = double(temp.f32);
	// lfsx f5,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f5.f64 = double(temp.f32);
	// lwz r31,132(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 132));
	// fmadds f5,f5,f10,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f10.f64 + ctx.f4.f64));
	// lfsx f26,r30,r11
	temp.u32 = PPC_LOAD_U32(var_r30 + ctx.r11.u32);
	var_f26 = double(temp.f32);
	// fmadds f3,f29,f13,f3
	ctx.f3.f64 = double(float(var_f29 * ctx.f13.f64 + ctx.f3.f64));
	// lfsx f25,r27,r11
	temp.u32 = PPC_LOAD_U32(var_r27 + ctx.r11.u32);
	var_f25 = double(temp.f32);
	// lfsx f29,r3,r11
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r11.u32);
	var_f29 = double(temp.f32);
	// fmadds f2,f28,f10,f2
	ctx.f2.f64 = double(float(var_f28 * ctx.f10.f64 + ctx.f2.f64));
	// lfsx f28,r29,r11
	temp.u32 = PPC_LOAD_U32(var_r29 + ctx.r11.u32);
	var_f28 = double(temp.f32);
	// lfsx f31,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f31 = double(temp.f32);
	// lwz r31,128(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 128));
	// fmadds f1,f31,f13,f1
	ctx.f1.f64 = double(float(var_f31 * ctx.f13.f64 + ctx.f1.f64));
	// lfsx f31,r8,r11
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	var_f31 = double(temp.f32);
	// fmadds f6,f30,f10,f6
	ctx.f6.f64 = double(float(var_f30 * ctx.f10.f64 + ctx.f6.f64));
	// lfsx f30,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	var_f30 = double(temp.f32);
	// fmadds f9,f26,f13,f9
	ctx.f9.f64 = double(float(var_f26 * ctx.f13.f64 + ctx.f9.f64));
	// lfsx f23,r26,r11
	temp.u32 = PPC_LOAD_U32(var_r26 + ctx.r11.u32);
	var_f23 = double(temp.f32);
	// fmadds f8,f25,f10,f8
	ctx.f8.f64 = double(float(var_f25 * ctx.f10.f64 + ctx.f8.f64));
	// lfsx f24,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f24 = double(temp.f32);
	// lwz r31,116(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 116));
	// fadds f7,f7,f24
	ctx.f7.f64 = double(float(ctx.f7.f64 + var_f24));
	// lfsx f27,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	var_f27 = double(temp.f32);
	// lwz r31,92(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 92));
	// fadds f5,f5,f27
	ctx.f5.f64 = double(float(ctx.f5.f64 + var_f27));
	// lfsx f4,r31,r11
	temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
	ctx.f4.f64 = double(temp.f32);
	// lwz r31,88(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
	// fadds f4,f1,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f4.f64));
	// fadds f1,f6,f31
	ctx.f1.f64 = double(float(ctx.f6.f64 + var_f31));
	// fadds f6,f3,f30
	ctx.f6.f64 = double(float(ctx.f3.f64 + var_f30));
	// fadds f3,f2,f29
	ctx.f3.f64 = double(float(ctx.f2.f64 + var_f29));
	// fadds f2,f9,f28
	ctx.f2.f64 = double(float(ctx.f9.f64 + var_f28));
	// fadds f9,f8,f23
	ctx.f9.f64 = double(float(ctx.f8.f64 + var_f23));
	// fmuls f8,f7,f0
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfsx f8,r19,r11
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r19 + ctx.r11.u32, temp.u32);
	// fmuls f7,f5,f0
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfsx f7,r18,r11
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r18 + ctx.r11.u32, temp.u32);
	// fmuls f5,f4,f0
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfsx f5,r17,r11
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r17 + ctx.r11.u32, temp.u32);
	// fmuls f4,f1,f0
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfsx f4,r16,r11
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r16 + ctx.r11.u32, temp.u32);
	// fmuls f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfsx f1,r31,r11
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + ctx.r11.u32, temp.u32);
	// lwz r31,104(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 104));
	// fmuls f8,f3,f0
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f7,f2,f0
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfsx f8,r31,r11
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + ctx.r11.u32, temp.u32);
	// lwz r31,80(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
	// stfsx f7,r31,r11
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + ctx.r11.u32, temp.u32);
	// lwz r31,84(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 84));
	// stfsx f6,r31,r11
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + ctx.r11.u32, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// cmpwi cr6,r10,1024
	// blt cr6,0x8246365c
	if (ctx.r10.s32 < 1024) goto loc_8246365C;
	// lwz r11,17908(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 17908);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lis r4,2
	ctx.r4.s64 = 131072;
	// lwz r31,17908(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 17908));
	// addis r9,r11,3
	ctx.r9.s64 = ctx.r11.s64 + 196608;
	// addis r8,r11,3
	ctx.r8.s64 = ctx.r11.s64 + 196608;
	// addi r9,r9,-29064
	ctx.r9.s64 = ctx.r9.s64 + -29064;
	// addi r8,r8,-26992
	ctx.r8.s64 = ctx.r8.s64 + -26992;
	// ori r5,r6,38556
	ctx.r5.u64 = ctx.r6.u64 | 38556;
	// ori r3,r4,40624
	ctx.r3.u64 = ctx.r4.u64 | 40624;
	// addis r7,r11,3
	ctx.r7.s64 = ctx.r11.s64 + 196608;
	// stw r9,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// stw r8,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r8.u32);
	// addi r7,r7,-24920
	ctx.r7.s64 = ctx.r7.s64 + -24920;
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// rotlwi r5,r5,0
	ctx.r5.u64 = ctx.r5.u32;
	// stw r3,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r3.u32);
	// rotlwi r3,r3,0
	ctx.r3.u64 = ctx.r3.u32;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// addis r6,r31,3
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// ori r10,r11,40656
	ctx.r10.u64 = ctx.r11.u64 | 40656;
	// stw r7,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, ctx.r7.u32);
	// ori r8,r9,40676
	ctx.r8.u64 = ctx.r9.u64 | 40676;
	// addi r6,r6,-29052
	ctx.r6.s64 = ctx.r6.s64 + -29052;
	// add r4,r31,r5
	ctx.r4.u64 = var_r31 + ctx.r5.u64;
	// add r11,r31,r3
	ctx.r11.u64 = var_r31 + ctx.r3.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// stw r8,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r8.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = ctx.r10.u32;
	// stw r6,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r6.u32);
	// rotlwi r8,r8,0
	ctx.r8.u64 = ctx.r8.u32;
	// stw r4,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r4.u32);
	// addis r6,r31,3
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r11.u32);
	// addis r4,r31,3
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// add r9,r31,r10
	ctx.r9.u64 = var_r31 + ctx.r10.u64;
	// add r7,r31,r8
	ctx.r7.u64 = var_r31 + ctx.r8.u64;
	// addi r6,r6,-24828
	ctx.r6.s64 = ctx.r6.s64 + -24828;
	// addi r4,r4,-24816
	ctx.r4.s64 = ctx.r4.s64 + -24816;
	// addi r11,r11,-6312
	ctx.r11.s64 = ctx.r11.s64 + -6312;
	// addis r5,r31,3
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r9,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r9.u32);
	// stw r7,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r7.u32);
	// addis r3,r31,3
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r6,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r6.u32);
	// addi r5,r5,-8412
	ctx.r5.s64 = ctx.r5.s64 + -8412;
	// stw r4,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r4.u32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r11,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, ctx.r11.u32);
	// addis r9,r31,3
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r8,r31,3
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r7,r31,3
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r6,r31,3
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r5,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r5.u32);
	// addis r4,r31,3
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addi r3,r3,-8384
	ctx.r3.s64 = ctx.r3.s64 + -8384;
	// addi r10,r10,-4244
	ctx.r10.s64 = ctx.r10.s64 + -4244;
	// addi r9,r9,-4212
	ctx.r9.s64 = ctx.r9.s64 + -4212;
	// addi r8,r8,-4192
	ctx.r8.s64 = ctx.r8.s64 + -4192;
	// addi r7,r7,-4160
	ctx.r7.s64 = ctx.r7.s64 + -4160;
	// addi r6,r6,12256
	ctx.r6.s64 = ctx.r6.s64 + 12256;
	// stw r3,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r3.u32);
	// addi r4,r4,-4148
	ctx.r4.s64 = ctx.r4.s64 + -4148;
	// stw r10,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r10.u32);
	// addi r11,r11,12284
	ctx.r11.s64 = ctx.r11.s64 + 12284;
	// stw r9,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r9.u32);
	// stw r8,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r8.u32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r7,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r7.u32);
	// stw r6,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r6.u32);
	// stw r4,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r4.u32);
	// stw r11,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r11.u32);
	// lwz r17,152(r1)
	var_r17 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 152));
	// lwz r5,136(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// addi r3,r17,16
	ctx.r3.s64 = (int64_t)(int32_t)var_r17 + 16;
	// addi r30,r5,16
	var_r30 = (uint32_t)(ctx.r5.s64 + 16);
	// addi r10,r10,20504
	ctx.r10.s64 = ctx.r10.s64 + 20504;
	// addis r9,r31,3
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r8,r31,4
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r9,r9,28720
	ctx.r9.s64 = ctx.r9.s64 + 28720;
	// addi r8,r8,-28600
	ctx.r8.s64 = ctx.r8.s64 + -28600;
	// stw r10,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r10.u32);
	// addis r10,r31,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addis r7,r31,4
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r10,r10,16616
	ctx.r10.s64 = ctx.r10.s64 + 16616;
	// addis r6,r31,4
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// stw r9,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r9.u32);
	// addis r5,r31,4
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// stw r8,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r8.u32);
	// addis r4,r31,4
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r7,r7,-24484
	ctx.r7.s64 = ctx.r7.s64 + -24484;
	// addi r6,r6,-8072
	ctx.r6.s64 = ctx.r6.s64 + -8072;
	// stw r10,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, ctx.r10.u32);
	// addi r5,r5,-8036
	ctx.r5.s64 = ctx.r5.s64 + -8036;
	// addi r4,r4,184
	ctx.r4.s64 = ctx.r4.s64 + 184;
	// addis r9,r31,4
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r7,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, ctx.r7.u32);
	// addi r9,r9,20732
	ctx.r9.s64 = ctx.r9.s64 + 20732;
	// stw r6,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r6.u32);
	// addi r10,r10,-8396
	ctx.r10.s64 = ctx.r10.s64 + -8396;
	// stw r5,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r5.u32);
	// addis r8,r31,5
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// stw r4,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r4.u32);
	// addis r7,r31,4
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r8,r8,-28392
	ctx.r8.s64 = ctx.r8.s64 + -28392;
	// addis r6,r31,4
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// stw r9,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, ctx.r9.u32);
	// addis r5,r31,3
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r10,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, ctx.r10.u32);
	// addis r4,r31,3
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addi r7,r7,20744
	ctx.r7.s64 = ctx.r7.s64 + 20744;
	// addi r6,r6,-24472
	ctx.r6.s64 = ctx.r6.s64 + -24472;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// addi r5,r5,-8420
	ctx.r5.s64 = ctx.r5.s64 + -8420;
	// addi r4,r4,-4252
	ctx.r4.s64 = ctx.r4.s64 + -4252;
	// addi r29,r10,16
	var_r29 = (uint32_t)(ctx.r10.s64 + 16);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// stw r7,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r7.u32);
	// addis r9,r31,3
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r6,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r6.u32);
	// addis r10,r31,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// stw r5,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, ctx.r5.u32);
	// addis r26,r31,3
	var_r26 = (uint32_t)(var_r31 + 196608);
	// stw r4,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, ctx.r4.u32);
	// addis r25,r31,3
	var_r25 = (uint32_t)(var_r31 + 196608);
	// addis r24,r31,4
	var_r24 = (uint32_t)(var_r31 + 262144);
	// addi r11,r11,8400
	ctx.r11.s64 = ctx.r11.s64 + 8400;
	// addi r9,r9,-6324
	ctx.r9.s64 = ctx.r9.s64 + -6324;
	// addis r8,r31,3
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addi r10,r10,168
	ctx.r10.s64 = ctx.r10.s64 + 168;
	// addi r26,r26,12272
	var_r26 = (uint32_t)(var_r26 + 12272);
	// addi r25,r25,28708
	var_r25 = (uint32_t)(var_r25 + 28708);
	// stw r11,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, ctx.r11.u32);
	// addi r24,r24,-28612
	var_r24 = (uint32_t)(var_r24 + -28612);
	// stw r9,360(r1)
	PPC_STORE_U32(ctx.r1.u32 + 360, ctx.r9.u32);
	// addi r8,r8,-24836
	ctx.r8.s64 = ctx.r8.s64 + -24836;
	// addis r7,r31,3
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r10,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r10.u32);
	// addis r6,r31,3
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// stw r26,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, var_r26);
	// addis r22,r31,3
	var_r22 = (uint32_t)(var_r31 + 196608);
	// stw r25,376(r1)
	PPC_STORE_U32(ctx.r1.u32 + 376, var_r25);
	// addis r21,r31,3
	var_r21 = (uint32_t)(var_r31 + 196608);
	// stw r24,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, var_r24);
	// addis r5,r31,3
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addis r4,r31,4
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addis r20,r31,3
	var_r20 = (uint32_t)(var_r31 + 196608);
	// addi r7,r7,-24896
	ctx.r7.s64 = ctx.r7.s64 + -24896;
	// addi r6,r6,-4228
	ctx.r6.s64 = ctx.r6.s64 + -4228;
	// addi r22,r22,-24864
	var_r22 = (uint32_t)(var_r22 + -24864);
	// addi r21,r21,-4196
	var_r21 = (uint32_t)(var_r21 + -4196);
	// addi r5,r5,-4168
	ctx.r5.s64 = ctx.r5.s64 + -4168;
	// addi r11,r8,24
	ctx.r11.s64 = ctx.r8.s64 + 24;
	// addi r28,r9,16
	var_r28 = (uint32_t)(ctx.r9.s64 + 16);  // addr:0x82000010
	// addi r4,r4,-24492
	ctx.r4.s64 = ctx.r4.s64 + -24492;
	// addi r20,r20,20488
	var_r20 = (uint32_t)(var_r20 + 20488);
	// addis r23,r31,4
	var_r23 = (uint32_t)(var_r31 + 262144);
	// lwz r19,208(r1)
	var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 208));
	// addis r9,r31,4
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r23,r23,20724
	var_r23 = (uint32_t)(var_r23 + 20724);
	// addi r9,r9,-8076
	ctx.r9.s64 = ctx.r9.s64 + -8076;
	// addis r10,r31,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// li r18,256
	var_r18 = 256;
	// addi r10,r10,-8048
	ctx.r10.s64 = ctx.r10.s64 + -8048;
	// stw r23,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r23);
	// addis r23,r31,5
	var_r23 = (uint32_t)(var_r31 + 327680);
	// stw r9,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r9.u32);
	// addis r9,r31,4
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r23,r23,-28396
	var_r23 = (uint32_t)(var_r23 + -28396);
	// addi r9,r9,8388
	ctx.r9.s64 = ctx.r9.s64 + 8388;
	// stw r18,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, var_r18);
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addi r27,r20,20
	var_r27 = (uint32_t)(var_r20 + 20);
	// addi r10,r10,12248
	ctx.r10.s64 = ctx.r10.s64 + 12248;
	// stw r23,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, var_r23);
	// li r23,0
	var_r23 = 0;
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// addis r9,r31,4
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r26,r26,16
	var_r26 = (uint32_t)(var_r26 + 16);
	// addi r9,r9,16604
	ctx.r9.s64 = ctx.r9.s64 + 16604;
	// stw r10,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, ctx.r10.u32);
	// addi r25,r25,16
	var_r25 = (uint32_t)(var_r25 + 16);
	// stw r23,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, var_r23);
	// addi r10,r5,24
	ctx.r10.s64 = ctx.r5.s64 + 24;
	// lwz r23,204(r1)
	var_r23 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 204));
	// addi r24,r24,16
	var_r24 = (uint32_t)(var_r24 + 16);
	// subf r16,r19,r23
	var_r16 = var_r23 - var_r19;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r9,r4,24
	ctx.r9.s64 = ctx.r4.s64 + 24;
	// stw r16,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, var_r16);
	// lwz r16,220(r1)
	var_r16 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 220));
	// subf r16,r19,r16
	var_r16 = var_r16 - var_r19;
	// stw r16,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, var_r16);
	// lwz r16,216(r1)
	var_r16 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 216));
	// subf r16,r19,r16
	var_r16 = var_r16 - var_r19;
	// stw r16,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, var_r16);
	// lwz r16,212(r1)
	var_r16 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 212));
	// subf r19,r19,r16
	var_r19 = var_r16 - var_r19;
	// lis r16,-32256
	var_r16 = (uint32_t)(-2113929216);
	// stw r19,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, var_r19);
	// lis r19,-32256
	var_r19 = (uint32_t)(-2113929216);
	// lfs f5,22700(r16)
	temp.u32 = PPC_LOAD_U32(var_r16 + 22700);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,296(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
	// lfs f18,16056(r19)
	temp.u32 = PPC_LOAD_U32(var_r19 + 16056);
	var_f18 = double(temp.f32);
	// b 0x82463c44
	goto loc_82463C44;
	do {
		// lwz r18,120(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 120));
	loc_82463C44:
		// lwz r19,100(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 100));
		// clrlwi r19,r19,27
		var_r19 = (uint32_t)(var_r19 & 0x1F);
		// cmpwi cr6,r19,0
		// bne cr6,0x82463c64
		if ((int32_t)var_r19 == 0) {
			// lwz r19,148(r1)
			var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 148));
			// dcbt r18,r19
			// lwz r19,144(r1)
			var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 144));
			// dcbt r18,r19
		}
	loc_82463C64:
		// lwz r19,116(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 116));
		// lfs f8,8(r17)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r17 + 8);
		ctx.f8.f64 = double(temp.f32);
		// lwz r15,168(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 168));
		// lfs f15,4(r17)
		temp.u32 = PPC_LOAD_U32(var_r17 + 4);
		var_f15 = double(temp.f32);
		// lwz r18,0(r17)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r17 + 0));
		// addi r17,r1,6640
		var_r17 = (uint32_t)(ctx.r1.s64 + 6640);
		// addi r16,r1,3520
		var_r16 = (uint32_t)(ctx.r1.s64 + 3520);
		// addi r14,r1,12016
		var_r14 = (uint32_t)(ctx.r1.s64 + 12016);
		// lfs f13,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f13.f64 = double(temp.f32);
		// lwz r19,128(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 128));
		// lfs f6,0(r15)
		temp.u32 = PPC_LOAD_U32(var_r15 + 0);
		ctx.f6.f64 = double(temp.f32);
		// lwz r15,184(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 184));
		// lfs f12,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f12.f64 = double(temp.f32);
		// lwz r19,132(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 132));
		// lfs f3,0(r15)
		temp.u32 = PPC_LOAD_U32(var_r15 + 0);
		ctx.f3.f64 = double(temp.f32);
		// lwz r15,192(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 192));
		// lfs f5,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f5.f64 = double(temp.f32);
		// lwz r19,204(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 204));
		// lfs f7,0(r15)
		temp.u32 = PPC_LOAD_U32(var_r15 + 0);
		ctx.f7.f64 = double(temp.f32);
		// lwz r15,176(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 176));
		// add r19,r19,r23
		var_r19 = (uint32_t)(var_r19 + var_r23);
		// lfs f31,0(r15)
		temp.u32 = PPC_LOAD_U32(var_r15 + 0);
		var_f31 = double(temp.f32);
		// lwz r15,112(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 112));
		// stw r19,92(r1)
		PPC_STORE_U32(ctx.r1.u32 + 92, var_r19);
		// lwz r19,124(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 124));
		// lfs f4,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f4.f64 = double(temp.f32);
		// lwz r19,0(r3)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// stw r19,288(r1)
		PPC_STORE_U32(ctx.r1.u32 + 288, var_r19);
		// mr r19,r15
		var_r19 = (uint32_t)(var_r15);
		// lfsx f2,r19,r17
		temp.u32 = PPC_LOAD_U32(var_r19 + var_r17);
		ctx.f2.f64 = double(temp.f32);
		// addi r17,r1,13056
		var_r17 = (uint32_t)(ctx.r1.s64 + 13056);
		// lfsx f1,r19,r16
		temp.u32 = PPC_LOAD_U32(var_r19 + var_r16);
		ctx.f1.f64 = double(temp.f32);
		// lwz r19,200(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 200));
		// fadds f9,f2,f6
		ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f6.f64));
		// fadds f6,f1,f3
		ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f3.f64));
		// lfs f30,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f30 = double(temp.f32);
		// lwz r19,196(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 196));
		// lfs f29,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f29 = double(temp.f32);
		// lwz r19,188(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 188));
		// lfs f28,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f28 = double(temp.f32);
		// lwz r19,164(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 164));
		// lfs f27,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f27 = double(temp.f32);
		// lwz r19,172(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 172));
		// lfs f3,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f3.f64 = double(temp.f32);
		// lwz r19,180(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 180));
		// lfs f26,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f26 = double(temp.f32);
		// lwz r19,348(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 348));
		// lfs f25,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f25 = double(temp.f32);
		// lwz r19,244(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 244));
		// lfs f24,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f24 = double(temp.f32);
		// lwz r19,232(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 232));
		// lfs f23,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f23 = double(temp.f32);
		// lwz r19,356(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 356));
		// lfs f22,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f22 = double(temp.f32);
		// lwz r19,240(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 240));
		// lfs f21,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f21 = double(temp.f32);
		// lwz r19,340(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 340));
		// lfs f2,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f2.f64 = double(temp.f32);
		// lwz r19,248(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 248));
		// lfs f20,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f20 = double(temp.f32);
		// lwz r19,372(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 372));
		// lfs f19,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		var_f19 = double(temp.f32);
		// lwz r19,256(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 256));
		// lfs f1,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f1.f64 = double(temp.f32);
		// lwz r19,224(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 224));
		// stfs f1,228(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
		// lfs f0,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f0.f64 = double(temp.f32);
		// lwz r19,264(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 264));
		// stfs f0,364(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
		// lfs f11,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f11.f64 = double(temp.f32);
		// lwz r19,324(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 324));
		// lfs f10,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f10.f64 = double(temp.f32);
		// lwz r19,272(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 272));
		// stfs f10,108(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
		// lfs f1,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f1.f64 = double(temp.f32);
		// lwz r19,328(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 328));
		// stfs f1,252(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
		// lfs f0,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,268(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 268, temp.u32);
		// lwz r19,280(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 280));
		// fadds f17,f9,f5
		var_f17 = double(float(ctx.f9.f64 + ctx.f5.f64));
		// lwz r15,96(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 96));
		// fadds f16,f6,f4
		var_f16 = double(float(ctx.f6.f64 + ctx.f4.f64));
		// lfs f10,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f10.f64 = double(temp.f32);
		// lwz r19,336(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 336));
		// stfs f10,260(r1)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
		// lfsx f10,r15,r14
		temp.u32 = PPC_LOAD_U32(var_r15 + var_r14);
		ctx.f10.f64 = double(temp.f32);
		// lfs f1,0(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 0);
		ctx.f1.f64 = double(temp.f32);
		// lwz r19,0(r3)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
		// stfs f1,284(r1)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r1.u32 + 284, temp.u32);
		// fmuls f17,f17,f18
		var_f17 = double(float(var_f17 * var_f18));
		// subf r19,r18,r19
		var_r19 = var_r19 - var_r18;
		// lwz r18,88(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
		// lfsx f1,r15,r17
		temp.u32 = PPC_LOAD_U32(var_r15 + var_r17);
		ctx.f1.f64 = double(temp.f32);
		// lwz r17,104(r1)
		var_r17 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 104));
		// clrlwi r19,r19,23
		var_r19 = (uint32_t)(var_r19 & 0x1FF);
		// fmuls f16,f16,f18
		var_f16 = double(float(var_f16 * var_f18));
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// lfsx f0,r18,r23
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r23);
		ctx.f0.f64 = double(temp.f32);
		// lwz r18,92(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 92));
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// lfsx f14,r17,r23
		temp.u32 = PPC_LOAD_U32(var_r17 + var_r23);
		var_f14 = double(temp.f32);
		// lwz r17,152(r1)
		var_r17 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 152));
		// lfs f6,0(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 0);
		ctx.f6.f64 = double(temp.f32);
		// lwz r18,288(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 288));
		// lfsx f9,r19,r3
		temp.u32 = PPC_LOAD_U32(var_r19 + ctx.r3.u32);
		ctx.f9.f64 = double(temp.f32);
		// fmadds f8,f8,f9,f13
		ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f9.f64 + ctx.f13.f64));
		// fadds f8,f8,f12
		ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
		// fadds f8,f8,f14
		ctx.f8.f64 = double(float(ctx.f8.f64 + var_f14));
		// stfsx f8,r18,r3
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r18 + ctx.r3.u32, temp.u32);
		// lwz r19,0(r3)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
		// addi r19,r19,1
		var_r19 = (uint32_t)(var_r19 + 1);
		// clrlwi r19,r19,23
		var_r19 = (uint32_t)(var_r19 & 0x1FF);
		// stw r19,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, var_r19);
		// fmadds f9,f15,f8,f9
		ctx.f9.f64 = double(float(var_f15 * ctx.f8.f64 + ctx.f9.f64));
		// lwz r19,136(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 136));
		// stfs f9,12(r17)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r17 + 12, temp.u32);
		// lwz r18,0(r30)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// fadds f9,f10,f31
		ctx.f9.f64 = double(float(ctx.f10.f64 + var_f31));
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f15,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		var_f15 = double(temp.f32);
		// lfs f14,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		var_f14 = double(temp.f32);
		// subf r18,r16,r18
		var_r18 = var_r18 - var_r16;
		// clrlwi r18,r18,23
		var_r18 = (uint32_t)(var_r18 & 0x1FF);
		// addi r16,r18,2
		var_r16 = (uint32_t)(var_r18 + 2);
		// lwz r18,0(r30)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f8,r16,r30
		temp.u32 = PPC_LOAD_U32(var_r16 + var_r30);
		ctx.f8.f64 = double(temp.f32);
		// fmadds f7,f15,f8,f7
		ctx.f7.f64 = double(float(var_f15 * ctx.f8.f64 + ctx.f7.f64));
		// stfsx f7,r18,r30
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r18 + var_r30, temp.u32);
		// lwz r18,0(r30)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r30 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,23
		var_r18 = (uint32_t)(var_r18 & 0x1FF);
		// stw r18,0(r30)
		PPC_STORE_U32(var_r30 + 0, var_r18);
		// fmadds f8,f7,f14,f8
		ctx.f8.f64 = double(float(ctx.f7.f64 * var_f14 + ctx.f8.f64));
		// stfs f8,12(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// lwz r19,344(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 344));
		// addi r18,r19,12
		var_r18 = (uint32_t)(var_r19 + 12);
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f7,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f7.f64 = double(temp.f32);
		// clrlwi r16,r16,31
		var_r16 = (uint32_t)(var_r16 & 0x1);
		// lfs f31,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		var_f31 = double(temp.f32);
		// addi r16,r16,4
		var_r16 = (uint32_t)(var_r16 + 4);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// lfsx f8,r16,r19
		temp.u32 = PPC_LOAD_U32(var_r16 + var_r19);
		ctx.f8.f64 = double(temp.f32);
		// stfs f31,8(r18)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r18 + 8, temp.u32);
		// fmadds f8,f8,f7,f9
		ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f9.f64));
		// stfs f9,4(r18)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r18 + 4, temp.u32);
		// stfs f8,8(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// lwz r19,0(r7)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 0));
		// lfs f7,8(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
		ctx.f7.f64 = double(temp.f32);
		// clrlwi r18,r19,31
		var_r18 = (uint32_t)(var_r19 & 0x1);
		// lfs f31,4(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
		var_f31 = double(temp.f32);
		// addi r19,r7,20
		var_r19 = (uint32_t)(ctx.r7.s64 + 20);
		// lfs f15,12(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 12);
		var_f15 = double(temp.f32);
		// addi r18,r18,6
		var_r18 = (uint32_t)(var_r18 + 6);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfs f8,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f8.f64 = double(temp.f32);
		// lfsx f9,r18,r7
		temp.u32 = PPC_LOAD_U32(var_r18 + ctx.r7.u32);
		ctx.f9.f64 = double(temp.f32);
		// fsubs f12,f13,f12
		ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
		// stfs f8,8(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// fmadds f8,f7,f9,f30
		ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + var_f30));
		// stfs f8,4(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// addi r19,r22,16
		var_r19 = (uint32_t)(var_r22 + 16);
		// lfs f30,0(r23)
		temp.u32 = PPC_LOAD_U32(var_r23 + 0);
		var_f30 = double(temp.f32);
		// fmuls f7,f8,f31
		ctx.f7.f64 = double(float(ctx.f8.f64 * var_f31));
		// fmadds f9,f9,f15,f7
		ctx.f9.f64 = double(float(ctx.f9.f64 * var_f15 + ctx.f7.f64));
		// stfs f9,16(r7)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r7.u32 + 16, temp.u32);
		// lwz r18,0(r22)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r22 + 0));
		// lfs f9,12(r22)
		temp.u32 = PPC_LOAD_U32(var_r22 + 12);
		ctx.f9.f64 = double(temp.f32);
		// lfs f8,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f8.f64 = double(temp.f32);
		// clrlwi r18,r18,31
		var_r18 = (uint32_t)(var_r18 & 0x1);
		// addi r18,r18,5
		var_r18 = (uint32_t)(var_r18 + 5);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f7,r18,r22
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r22);
		ctx.f7.f64 = double(temp.f32);
		// fmadds f9,f7,f9,f29
		ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + var_f29));
		// stfs f8,8(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// stfs f9,4(r19)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// lfs f8,8(r22)
		temp.u32 = PPC_LOAD_U32(var_r22 + 8);
		ctx.f8.f64 = double(temp.f32);
		// fmuls f7,f9,f8
		ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
		// stfs f7,4(r22)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r22 + 4, temp.u32);
		// lwz r19,0(r11)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 0));
		// lfs f8,4(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
		ctx.f8.f64 = double(temp.f32);
		// lwz r18,0(r8)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + 0));
		// lfs f7,16(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16);
		ctx.f7.f64 = double(temp.f32);
		// mr r16,r19
		var_r16 = (uint32_t)(var_r19);
		// subf r19,r18,r19
		var_r19 = var_r19 - var_r18;
		// lwz r18,12(r8)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r8.u32 + 12));
		// clrlwi r19,r19,20
		var_r19 = (uint32_t)(var_r19 & 0xFFF);
		// subf r18,r18,r16
		var_r18 = var_r16 - var_r18;
		// addi r16,r19,2
		var_r16 = (uint32_t)(var_r19 + 2);
		// clrlwi r19,r18,20
		var_r19 = (uint32_t)(var_r18 & 0xFFF);
		// rlwinm r18,r16,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// lfsx f9,r18,r11
		temp.u32 = PPC_LOAD_U32(var_r18 + ctx.r11.u32);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f8,f9,f8
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
		// lfs f9,296(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f31,f28,f9
		var_f31 = double(float(var_f28 * ctx.f9.f64));
		// lfsx f13,r19,r11
		temp.u32 = PPC_LOAD_U32(var_r19 + ctx.r11.u32);
		ctx.f13.f64 = double(temp.f32);
		// stfs f8,8(r8)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
		// fmuls f8,f13,f7
		ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
		// stfs f8,20(r8)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r8.u32 + 20, temp.u32);
		// lwz r19,0(r11)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 0));
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// stfsx f31,r19,r11
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r19 + ctx.r11.u32, temp.u32);
		// lwz r19,0(r11)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 0));
		// addi r19,r19,1
		var_r19 = (uint32_t)(var_r19 + 1);
		// clrlwi r19,r19,20
		var_r19 = (uint32_t)(var_r19 & 0xFFF);
		// stw r19,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, var_r19);
		// lwz r19,352(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 352));
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f7,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f7.f64 = double(temp.f32);
		// clrlwi r18,r18,31
		var_r18 = (uint32_t)(var_r18 & 0x1);
		// addi r18,r18,4
		var_r18 = (uint32_t)(var_r18 + 4);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f13,f13,f7
		ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
		// stfs f13,8(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// addi r19,r19,12
		var_r19 = (uint32_t)(var_r19 + 12);
		// lfs f8,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f8.f64 = double(temp.f32);
		// stfs f8,8(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// stfs f27,4(r19)
		temp.f32 = float(var_f27);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// lwz r19,304(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 304));
		// lwz r16,0(r29)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0));
		// mr r18,r16
		var_r18 = (uint32_t)(var_r16);
		// lwz r14,0(r19)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f7,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f7.f64 = double(temp.f32);
		// lfs f8,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f8.f64 = double(temp.f32);
		// subf r16,r14,r16
		var_r16 = var_r16 - var_r14;
		// addi r14,r18,2
		var_r14 = (uint32_t)(var_r18 + 2);
		// clrlwi r18,r16,23
		var_r18 = (uint32_t)(var_r16 & 0x1FF);
		// rlwinm r16,r14,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r14 | (var_r14 << 32), 2) & 0xFFFFFFFC);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r29
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r29);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f7,f7,f13,f12
		ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f12.f64));
		// fadds f12,f7,f30
		ctx.f12.f64 = double(float(ctx.f7.f64 + var_f30));
		// stfsx f12,r16,r29
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r16 + var_r29, temp.u32);
		// lwz r18,0(r29)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r29 + 0));
		// fmadds f13,f12,f8,f13
		ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f8.f64 + ctx.f13.f64));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// fmuls f9,f22,f9
		ctx.f9.f64 = double(float(var_f22 * ctx.f9.f64));
		// clrlwi r18,r18,23
		var_r18 = (uint32_t)(var_r18 & 0x1FF);
		// stw r18,0(r29)
		PPC_STORE_U32(var_r29 + 0, var_r18);
		// stfs f13,12(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// lwz r19,360(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 360));
		// lwz r16,0(r28)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
		// mr r18,r16
		var_r18 = (uint32_t)(var_r16);
		// lwz r14,0(r19)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f12,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f12.f64 = double(temp.f32);
		// lfs f8,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f8.f64 = double(temp.f32);
		// subf r16,r14,r16
		var_r16 = var_r16 - var_r14;
		// addi r14,r18,2
		var_r14 = (uint32_t)(var_r18 + 2);
		// clrlwi r18,r16,23
		var_r18 = (uint32_t)(var_r16 & 0x1FF);
		// rlwinm r16,r14,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r14 | (var_r14 << 32), 2) & 0xFFFFFFFC);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r28
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r28);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f12,f12,f13,f26
		ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + var_f26));
		// stfsx f12,r16,r28
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r16 + var_r28, temp.u32);
		// lwz r18,0(r28)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,23
		var_r18 = (uint32_t)(var_r18 & 0x1FF);
		// stw r18,0(r28)
		PPC_STORE_U32(var_r28 + 0, var_r18);
		// fmadds f12,f12,f8,f13
		ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f8.f64 + ctx.f13.f64));
		// stfs f12,12(r19)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// lwz r19,312(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 312));
		// fadds f13,f1,f25
		ctx.f13.f64 = double(float(ctx.f1.f64 + var_f25));
		// addi r18,r19,12
		var_r18 = (uint32_t)(var_r19 + 12);
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f7,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f7.f64 = double(temp.f32);
		// clrlwi r16,r16,31
		var_r16 = (uint32_t)(var_r16 & 0x1);
		// lfs f12,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f12.f64 = double(temp.f32);
		// addi r16,r16,4
		var_r16 = (uint32_t)(var_r16 + 4);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// lfsx f8,r16,r19
		temp.u32 = PPC_LOAD_U32(var_r16 + var_r19);
		ctx.f8.f64 = double(temp.f32);
		// stfs f12,8(r18)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r18 + 8, temp.u32);
		// fmadds f12,f8,f7,f13
		ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f13.f64));
		// stfs f13,4(r18)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r18 + 4, temp.u32);
		// stfs f12,8(r19)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// lwz r19,0(r6)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r6.u32 + 0));
		// lfs f12,12(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 12);
		ctx.f12.f64 = double(temp.f32);
		// clrlwi r18,r19,31
		var_r18 = (uint32_t)(var_r19 & 0x1);
		// lfs f7,8(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
		ctx.f7.f64 = double(temp.f32);
		// addi r19,r6,20
		var_r19 = (uint32_t)(ctx.r6.s64 + 20);
		// lfs f8,4(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
		ctx.f8.f64 = double(temp.f32);
		// addi r18,r18,6
		var_r18 = (uint32_t)(var_r18 + 6);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfs f31,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		var_f31 = double(temp.f32);
		// lfsx f13,r18,r6
		temp.u32 = PPC_LOAD_U32(var_r18 + ctx.r6.u32);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f12,f13,f12
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
		// stfs f31,8(r19)
		temp.f32 = float(var_f31);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// fmadds f13,f13,f7,f24
		ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + var_f24));
		// stfs f13,4(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// addi r19,r21,16
		var_r19 = (uint32_t)(var_r21 + 16);
		// fmadds f13,f8,f13,f12
		ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f12.f64));
		// stfs f13,16(r6)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
		// lwz r18,0(r21)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r21 + 0));
		// lfs f8,12(r21)
		temp.u32 = PPC_LOAD_U32(var_r21 + 12);
		ctx.f8.f64 = double(temp.f32);
		// lfs f7,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f7.f64 = double(temp.f32);
		// clrlwi r18,r18,31
		var_r18 = (uint32_t)(var_r18 & 0x1);
		// addi r18,r18,5
		var_r18 = (uint32_t)(var_r18 + 5);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r21
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r21);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f13,f13,f8,f23
		ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f8.f64 + var_f23));
		// stfs f7,8(r19)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// stfs f13,4(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// lfs f12,8(r21)
		temp.u32 = PPC_LOAD_U32(var_r21 + 8);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f8,f12,f13
		ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
		// stfs f8,4(r21)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r21 + 4, temp.u32);
		// lwz r19,0(r10)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 0));
		// lfs f7,4(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
		ctx.f7.f64 = double(temp.f32);
		// lwz r16,0(r5)
		var_r16 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + 0));
		// lfs f8,16(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16);
		ctx.f8.f64 = double(temp.f32);
		// lwz r14,12(r5)
		var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r5.u32 + 12));
		// mr r18,r19
		var_r18 = (uint32_t)(var_r19);
		// subf r19,r16,r19
		var_r19 = var_r19 - var_r16;
		// subf r16,r14,r18
		var_r16 = var_r18 - var_r14;
		// clrlwi r18,r19,20
		var_r18 = (uint32_t)(var_r19 & 0xFFF);
		// clrlwi r19,r16,20
		var_r19 = (uint32_t)(var_r16 & 0xFFF);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r10
		temp.u32 = PPC_LOAD_U32(var_r18 + ctx.r10.u32);
		ctx.f13.f64 = double(temp.f32);
		// lfsx f12,r19,r10
		temp.u32 = PPC_LOAD_U32(var_r19 + ctx.r10.u32);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f7,f7,f13
		ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
		// fmuls f13,f12,f8
		ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
		// stfs f7,8(r5)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
		// stfs f13,20(r5)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
		// lwz r19,0(r10)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 0));
		// fadds f13,f0,f20
		ctx.f13.f64 = double(float(ctx.f0.f64 + var_f20));
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// stfsx f9,r19,r10
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r19 + ctx.r10.u32, temp.u32);
		// lwz r19,0(r10)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r10.u32 + 0));
		// addi r19,r19,1
		var_r19 = (uint32_t)(var_r19 + 1);
		// clrlwi r19,r19,20
		var_r19 = (uint32_t)(var_r19 & 0xFFF);
		// stw r19,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, var_r19);
		// lwz r19,368(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 368));
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f9,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f9.f64 = double(temp.f32);
		// clrlwi r18,r18,31
		var_r18 = (uint32_t)(var_r18 & 0x1);
		// addi r18,r18,4
		var_r18 = (uint32_t)(var_r18 + 4);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f12,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f8,f12,f9
		ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
		// stfs f8,8(r19)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// addi r19,r19,12
		var_r19 = (uint32_t)(var_r19 + 12);
		// lfs f7,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f7.f64 = double(temp.f32);
		// stfs f7,8(r19)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r19 + 8, temp.u32);
		// stfs f21,4(r19)
		temp.f32 = float(var_f21);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// lwz r19,320(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 320));
		// lwz r16,0(r26)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r26 + 0));
		// mr r18,r16
		var_r18 = (uint32_t)(var_r16);
		// lwz r14,0(r19)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f9,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f9.f64 = double(temp.f32);
		// lfs f8,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f8.f64 = double(temp.f32);
		// subf r16,r14,r16
		var_r16 = var_r16 - var_r14;
		// addi r14,r18,2
		var_r14 = (uint32_t)(var_r18 + 2);
		// clrlwi r18,r16,21
		var_r18 = (uint32_t)(var_r16 & 0x7FF);
		// rlwinm r16,r14,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r14 | (var_r14 << 32), 2) & 0xFFFFFFFC);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f12,r18,r26
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r26);
		ctx.f12.f64 = double(temp.f32);
		// fmadds f7,f12,f9,f6
		ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f6.f64));
		// fadds f9,f7,f2
		ctx.f9.f64 = double(float(ctx.f7.f64 + ctx.f2.f64));
		// fadds f9,f9,f3
		ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f3.f64));
		// stfsx f9,r16,r26
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r16 + var_r26, temp.u32);
		// lwz r18,0(r26)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r26 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,21
		var_r18 = (uint32_t)(var_r18 & 0x7FF);
		// stw r18,0(r26)
		PPC_STORE_U32(var_r26 + 0, var_r18);
		// fmadds f12,f9,f8,f12
		ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f12.f64));
		// stfs f12,12(r19)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// lwz r18,0(r27)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r27 + 0));
		// lwz r16,0(r20)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r20 + 0));
		// lfs f8,12(r20)
		temp.u32 = PPC_LOAD_U32(var_r20 + 12);
		ctx.f8.f64 = double(temp.f32);
		// mr r19,r18
		var_r19 = (uint32_t)(var_r18);
		// lfs f31,8(r20)
		temp.u32 = PPC_LOAD_U32(var_r20 + 8);
		var_f31 = double(temp.f32);
		// subf r18,r16,r18
		var_r18 = var_r18 - var_r16;
		// lfs f7,4(r20)
		temp.u32 = PPC_LOAD_U32(var_r20 + 4);
		ctx.f7.f64 = double(temp.f32);
		// addi r16,r19,2
		var_r16 = (uint32_t)(var_r19 + 2);
		// clrlwi r19,r18,21
		var_r19 = (uint32_t)(var_r18 & 0x7FF);
		// rlwinm r18,r16,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// lfsx f12,r19,r27
		temp.u32 = PPC_LOAD_U32(var_r19 + var_r27);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f9,f12,f8
		ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
		// fmadds f8,f12,f31,f13
		ctx.f8.f64 = double(float(ctx.f12.f64 * var_f31 + ctx.f13.f64));
		// stfsx f8,r18,r27
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r18 + var_r27, temp.u32);
		// lwz r19,0(r27)
		var_r19 = (uint32_t)(PPC_LOAD_U32(var_r27 + 0));
		// addi r19,r19,1
		var_r19 = (uint32_t)(var_r19 + 1);
		// clrlwi r19,r19,21
		var_r19 = (uint32_t)(var_r19 & 0x7FF);
		// fmadds f9,f7,f13,f9
		ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f9.f64));
		// stw r19,0(r27)
		PPC_STORE_U32(var_r27 + 0, var_r19);
		// stfs f9,16(r20)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r20 + 16, temp.u32);
		// lwz r16,0(r25)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r25 + 0));
		// lwz r19,376(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 376));
		// lwz r14,0(r19)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// mr r18,r16
		var_r18 = (uint32_t)(var_r16);
		// lfs f7,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f7.f64 = double(temp.f32);
		// subf r16,r14,r16
		var_r16 = var_r16 - var_r14;
		// lfs f9,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f9.f64 = double(temp.f32);
		// addi r14,r18,2
		var_r14 = (uint32_t)(var_r18 + 2);
		// lfs f8,364(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
		ctx.f8.f64 = double(temp.f32);
		// clrlwi r18,r16,21
		var_r18 = (uint32_t)(var_r16 & 0x7FF);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r25
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r25);
		ctx.f13.f64 = double(temp.f32);
		// rlwinm r18,r14,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r14 | (var_r14 << 32), 2) & 0xFFFFFFFC);
		// fmadds f12,f13,f7,f19
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + var_f19));
		// fadds f7,f10,f8
		ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
		// lfs f8,228(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
		ctx.f8.f64 = double(temp.f32);
		// stfsx f12,r18,r25
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r18 + var_r25, temp.u32);
		// lwz r18,0(r25)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r25 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,21
		var_r18 = (uint32_t)(var_r18 & 0x7FF);
		// fmadds f13,f12,f9,f13
		ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f13.f64));
		// stw r18,0(r25)
		PPC_STORE_U32(var_r25 + 0, var_r18);
		// stfs f13,12(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// lwz r18,0(r24)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r24 + 0));
		// lwz r19,332(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 332));
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r14,r18,2,0,29
		var_r14 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lwz r18,0(r24)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r24 + 0));
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f12,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f12.f64 = double(temp.f32);
		// lfs f10,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f10.f64 = double(temp.f32);
		// subf r18,r16,r18
		var_r18 = var_r18 - var_r16;
		// clrlwi r18,r18,22
		var_r18 = (uint32_t)(var_r18 & 0x3FF);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r24
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r24);
		ctx.f13.f64 = double(temp.f32);
		// fmadds f9,f13,f12,f0
		ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f0.f64));
		// fadds f12,f9,f8
		ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
		// stfsx f12,r14,r24
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r14 + var_r24, temp.u32);
		// lwz r18,0(r24)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r24 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,22
		var_r18 = (uint32_t)(var_r18 & 0x3FF);
		// stw r18,0(r24)
		PPC_STORE_U32(var_r24 + 0, var_r18);
		// fmadds f13,f12,f10,f13
		ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 + ctx.f13.f64));
		// stfs f13,12(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// lwz r19,0(r9)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + 0));
		// lwz r16,0(r4)
		var_r16 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 0));
		// lfs f10,4(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		ctx.f10.f64 = double(temp.f32);
		// mr r18,r19
		var_r18 = (uint32_t)(var_r19);
		// lwz r14,12(r4)
		var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r4.u32 + 12));
		// subf r19,r16,r19
		var_r19 = var_r19 - var_r16;
		// lfs f9,16(r4)
		temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
		ctx.f9.f64 = double(temp.f32);
		// subf r16,r14,r18
		var_r16 = var_r18 - var_r14;
		// clrlwi r18,r19,20
		var_r18 = (uint32_t)(var_r19 & 0xFFF);
		// clrlwi r19,r16,20
		var_r19 = (uint32_t)(var_r16 & 0xFFF);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// lfsx f13,r18,r9
		temp.u32 = PPC_LOAD_U32(var_r18 + ctx.r9.u32);
		ctx.f13.f64 = double(temp.f32);
		// lfsx f12,r19,r9
		temp.u32 = PPC_LOAD_U32(var_r19 + ctx.r9.u32);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f8,f10,f13
		ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fmuls f13,f9,f12
		ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
		// stfs f8,8(r4)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
		// stfs f13,20(r4)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
		// lwz r19,0(r9)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + 0));
		// addi r19,r19,2
		var_r19 = (uint32_t)(var_r19 + 2);
		// rlwinm r19,r19,2,0,29
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC);
		// stfsx f7,r19,r9
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r19 + ctx.r9.u32, temp.u32);
		// lwz r19,0(r9)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r9.u32 + 0));
		// addi r19,r19,1
		var_r19 = (uint32_t)(var_r19 + 1);
		// clrlwi r19,r19,20
		var_r19 = (uint32_t)(var_r19 & 0xFFF);
		// stw r19,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, var_r19);
		// lwz r19,236(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 236));
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f12,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f12.f64 = double(temp.f32);
		// lfs f9,12(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 12);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f10,f12,f11
		ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
		// clrlwi r16,r18,31
		var_r16 = (uint32_t)(var_r18 & 0x1);
		// addi r18,r19,16
		var_r18 = (uint32_t)(var_r19 + 16);
		// addi r16,r16,5
		var_r16 = (uint32_t)(var_r16 + 5);
		// lfs f8,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f8.f64 = double(temp.f32);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// lfsx f7,r16,r19
		temp.u32 = PPC_LOAD_U32(var_r16 + var_r19);
		ctx.f7.f64 = double(temp.f32);
		// stfs f8,8(r18)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r18 + 8, temp.u32);
		// fmadds f13,f7,f9,f10
		ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + ctx.f10.f64));
		// stfs f11,4(r18)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r18 + 4, temp.u32);
		// stfs f13,4(r19)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// lwz r19,100(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 100));
		// clrlwi r19,r19,27
		var_r19 = (uint32_t)(var_r19 & 0x1F);
		// cmpwi cr6,r19,0
		// bne cr6,0x82464430
		if ((int32_t)var_r19 != 0) goto loc_82464430;
		// lwz r19,120(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 120));
		// addi r19,r19,128
		var_r19 = (uint32_t)(var_r19 + 128);
		// stw r19,120(r1)
		PPC_STORE_U32(ctx.r1.u32 + 120, var_r19);
		// lwz r19,156(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 156));
		// fsubs f3,f2,f3
		ctx.fpscr.disableFlushMode();
		ctx.f3.f64 = double(float(ctx.f2.f64 - ctx.f3.f64));
		// lfs f10,108(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
		ctx.f10.f64 = double(temp.f32);
		// fadds f13,f6,f10
		ctx.f13.f64 = double(float(ctx.f6.f64 + ctx.f10.f64));
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f2,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f2.f64 = double(temp.f32);
		// lfs f11,4(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 4);
		ctx.f11.f64 = double(temp.f32);
		// addi r19,r19,16
		var_r19 = (uint32_t)(var_r19 + 16);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r14,r18,2,0,29
		var_r14 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// subf r18,r16,r18
		var_r18 = var_r18 - var_r16;
		// clrlwi r18,r18,21
		var_r18 = (uint32_t)(var_r18 & 0x7FF);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f12,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f12.f64 = double(temp.f32);
		// fmadds f9,f12,f2,f3
		ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f2.f64 + ctx.f3.f64));
		// fadds f0,f9,f0
		ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
		// stfsx f0,r14,r19
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r14 + var_r19, temp.u32);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,21
		var_r18 = (uint32_t)(var_r18 & 0x7FF);
		// stw r18,0(r19)
		PPC_STORE_U32(var_r19 + 0, var_r18);
		// fmadds f0,f0,f11,f12
		ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f12.f64));
		// lwz r19,156(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 156));
		// lfs f11,252(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
		ctx.f11.f64 = double(temp.f32);
		// lwz r18,140(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 140));
		// stfs f0,12(r19)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// addi r19,r18,20
		var_r19 = (uint32_t)(var_r18 + 20);
		// lwz r16,0(r18)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
		// lfs f8,12(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 12);
		ctx.f8.f64 = double(temp.f32);
		// lfs f7,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f7.f64 = double(temp.f32);
		// lwz r14,0(r19)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// stw r16,108(r1)
		PPC_STORE_U32(ctx.r1.u32 + 108, var_r16);
		// mr r16,r18
		var_r16 = (uint32_t)(var_r18);
		// lfs f3,8(r16)
		temp.u32 = PPC_LOAD_U32(var_r16 + 8);
		ctx.f3.f64 = double(temp.f32);
		// lwz r18,108(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 108));
		// subf r14,r18,r14
		var_r14 = var_r14 - var_r18;
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// stw r18,108(r1)
		PPC_STORE_U32(ctx.r1.u32 + 108, var_r18);
		// clrlwi r18,r14,21
		var_r18 = (uint32_t)(var_r14 & 0x7FF);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f0,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f2,f8,f0
		ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
		// fmadds f0,f0,f3,f13
		ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f3.f64 + ctx.f13.f64));
		// fmadds f12,f13,f7,f2
		ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + ctx.f2.f64));
		// lwz r14,108(r1)
		var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 108));
		// rlwinm r14,r14,2,0,29
		var_r14 = (uint32_t)(__builtin_rotateleft64(var_r14 | (var_r14 << 32), 2) & 0xFFFFFFFC);
		// stfsx f0,r14,r19
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r14 + var_r19, temp.u32);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,21
		var_r18 = (uint32_t)(var_r18 & 0x7FF);
		// stw r18,0(r19)
		PPC_STORE_U32(var_r19 + 0, var_r18);
		// stfs f12,16(r16)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r16 + 16, temp.u32);
		// lwz r18,160(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 160));
		// addi r19,r18,16
		var_r19 = (uint32_t)(var_r18 + 16);
		// lwz r14,0(r18)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
		// lfs f13,8(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 8);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f12.f64 = double(temp.f32);
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// mr r18,r16
		var_r18 = (uint32_t)(var_r16);
		// subf r16,r14,r16
		var_r16 = var_r16 - var_r14;
		// addi r14,r18,2
		var_r14 = (uint32_t)(var_r18 + 2);
		// clrlwi r18,r16,21
		var_r18 = (uint32_t)(var_r16 & 0x7FF);
		// rlwinm r16,r14,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r14 | (var_r14 << 32), 2) & 0xFFFFFFFC);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f0,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f0.f64 = double(temp.f32);
		// fmadds f13,f0,f13,f11
		ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f11.f64));
		// stfsx f13,r16,r19
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r16 + var_r19, temp.u32);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,21
		var_r18 = (uint32_t)(var_r18 & 0x7FF);
		// stw r18,0(r19)
		PPC_STORE_U32(var_r19 + 0, var_r18);
		// fmadds f0,f12,f13,f0
		ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f13.f64 + ctx.f0.f64));
		// lwz r19,160(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 160));
		// lwz r18,84(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 84));
		// stfs f0,12(r19)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// addi r19,r18,16
		var_r19 = (uint32_t)(var_r18 + 16);
		// lwz r14,0(r18)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
		// lfs f10,8(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 8);
		ctx.f10.f64 = double(temp.f32);
		// addi r15,r15,4
		var_r15 = (uint32_t)(var_r15 + 4);
		// lfs f9,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f9.f64 = double(temp.f32);
		// lfs f8,260(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
		ctx.f8.f64 = double(temp.f32);
		// fadds f7,f1,f8
		ctx.f7.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
		// stfs f16,0(r23)
		temp.f32 = float(var_f16);
		PPC_STORE_U32(var_r23 + 0, temp.u32);
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// stw r15,96(r1)
		PPC_STORE_U32(ctx.r1.u32 + 96, var_r15);
		// subf r18,r14,r16
		var_r18 = var_r16 - var_r14;
		// lwz r14,104(r1)
		var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 104));
		// addi r16,r16,2
		var_r16 = (uint32_t)(var_r16 + 2);
		// lwz r15,100(r1)
		var_r15 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 100));
		// clrlwi r18,r18,22
		var_r18 = (uint32_t)(var_r18 & 0x3FF);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// stfsx f17,r14,r23
		temp.f32 = float(var_f17);
		PPC_STORE_U32(var_r14 + var_r23, temp.u32);
		// lwz r14,88(r1)
		var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// addi r15,r15,1
		var_r15 = (uint32_t)(var_r15 + 1);
		// stfsx f5,r14,r23
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(var_r14 + var_r23, temp.u32);
		// lwz r14,92(r1)
		var_r14 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 92));
		// lfsx f0,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f0.f64 = double(temp.f32);
		// addi r23,r23,4
		var_r23 = (uint32_t)(var_r23 + 4);
		// fmadds f6,f0,f10,f6
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 + ctx.f6.f64));
		// lfs f5,268(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
		ctx.f5.f64 = double(temp.f32);
		// stw r15,100(r1)
		PPC_STORE_U32(ctx.r1.u32 + 100, var_r15);
		// stfs f4,0(r14)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(var_r14 + 0, temp.u32);
		// fadds f13,f6,f5
		ctx.f13.f64 = double(float(ctx.f6.f64 + ctx.f5.f64));
		// stfsx f13,r16,r19
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r16 + var_r19, temp.u32);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,22
		var_r18 = (uint32_t)(var_r18 & 0x3FF);
		// stw r18,0(r19)
		PPC_STORE_U32(var_r19 + 0, var_r18);
		// fmadds f0,f9,f13,f0
		ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f0.f64));
		// lwz r19,84(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 84));
		// lwz r18,80(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
		// stfs f0,12(r19)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r19 + 12, temp.u32);
		// addi r19,r18,24
		var_r19 = (uint32_t)(var_r18 + 24);
		// lfs f4,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f4.f64 = double(temp.f32);
		// lwz r14,0(r18)
		var_r14 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
		// lfs f3,16(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 16);
		ctx.f3.f64 = double(temp.f32);
		// lwz r18,12(r18)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r18 + 12));
		// lwz r16,0(r19)
		var_r16 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// mr r15,r16
		var_r15 = (uint32_t)(var_r16);
		// subf r16,r14,r16
		var_r16 = var_r16 - var_r14;
		// subf r18,r18,r15
		var_r18 = var_r15 - var_r18;
		// clrlwi r16,r16,20
		var_r16 = (uint32_t)(var_r16 & 0xFFF);
		// clrlwi r18,r18,20
		var_r18 = (uint32_t)(var_r18 & 0xFFF);
		// addi r16,r16,2
		var_r16 = (uint32_t)(var_r16 + 2);
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// lfsx f0,r16,r19
		temp.u32 = PPC_LOAD_U32(var_r16 + var_r19);
		ctx.f0.f64 = double(temp.f32);
		// lfsx f13,r18,r19
		temp.u32 = PPC_LOAD_U32(var_r18 + var_r19);
		ctx.f13.f64 = double(temp.f32);
		// lwz r18,80(r1)
		var_r18 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
		// fmuls f2,f0,f4
		ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
		// fmuls f1,f3,f13
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
		// lfs f13,284(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
		ctx.f13.f64 = double(temp.f32);
		// stfs f2,8(r18)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(var_r18 + 8, temp.u32);
		// stfs f1,20(r18)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r18 + 20, temp.u32);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,2
		var_r18 = (uint32_t)(var_r18 + 2);
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// stfsx f7,r18,r19
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r18 + var_r19, temp.u32);
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// addi r18,r18,1
		var_r18 = (uint32_t)(var_r18 + 1);
		// clrlwi r18,r18,20
		var_r18 = (uint32_t)(var_r18 & 0xFFF);
		// stw r18,0(r19)
		PPC_STORE_U32(var_r19 + 0, var_r18);
		// lwz r19,276(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 276));
		// lwz r18,0(r19)
		var_r18 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
		// lfs f0,8(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 8);
		ctx.f0.f64 = double(temp.f32);
		// lfs f11,12(r19)
		temp.u32 = PPC_LOAD_U32(var_r19 + 12);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f12,f0,f13
		ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
		// clrlwi r16,r18,31
		var_r16 = (uint32_t)(var_r18 & 0x1);
		// addi r18,r19,16
		var_r18 = (uint32_t)(var_r19 + 16);
		// addi r16,r16,5
		var_r16 = (uint32_t)(var_r16 + 5);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// lfs f10,4(r18)
		temp.u32 = PPC_LOAD_U32(var_r18 + 4);
		ctx.f10.f64 = double(temp.f32);
		// lfsx f9,r16,r19
		temp.u32 = PPC_LOAD_U32(var_r16 + var_r19);
		ctx.f9.f64 = double(temp.f32);
		// stfs f10,8(r18)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r18 + 8, temp.u32);
		// fmadds f0,f9,f11,f12
		ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f11.f64 + ctx.f12.f64));
		// stfs f13,4(r18)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r18 + 4, temp.u32);
		// stfs f0,4(r19)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r19 + 4, temp.u32);
		// lwz r19,112(r1)
		var_r19 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 112));
		// addi r19,r19,4
		var_r19 = (uint32_t)(var_r19 + 4);
		// cmpwi cr6,r19,1024
		// stw r19,112(r1)
		PPC_STORE_U32(ctx.r1.u32 + 112, var_r19);
		// blt cr6,0x82463c40
		} while ((int32_t)var_r19 < 1024);
	// lwz r27,220(r1)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 220));
	// addis r3,r31,5
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// addi r3,r3,-28368
	ctx.r3.s64 = ctx.r3.s64 + -28368;
	// bl 0x824628b8
	phInst_28B8_p39(ctx, base);
	// addis r3,r31,5
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// addi r3,r3,-26296
	ctx.r3.s64 = ctx.r3.s64 + -26296;
	// bl 0x824628b8
	phInst_28B8_p39(ctx, base);
	// lwz r29,208(r1)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 208));
	// addis r3,r31,5
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// addi r3,r3,-24224
	ctx.r3.s64 = ctx.r3.s64 + -24224;
	// bl 0x824628b8
	phInst_28B8_p39(ctx, base);
	// addis r3,r31,5
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// addi r3,r3,-22152
	ctx.r3.s64 = ctx.r3.s64 + -22152;
	// bl 0x824628b8
	phInst_28B8_p39(ctx, base);
	// lwz r4,216(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// addis r3,r31,5
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addi r30,r4,-8
	var_r30 = (uint32_t)(ctx.r4.s64 + -8);
	// addi r3,r3,-20080
	ctx.r3.s64 = ctx.r3.s64 + -20080;
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// bl 0x82462c38
	phInst_2C38_p39(ctx, base);
	// lwz r4,212(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// addis r3,r31,5
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addi r28,r4,-8
	var_r28 = (uint32_t)(ctx.r4.s64 + -8);
	// addi r3,r3,-19024
	ctx.r3.s64 = ctx.r3.s64 + -19024;
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// bl 0x82462c38
	phInst_2C38_p39(ctx, base);
	// addi r8,r31,92
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 92;
	// lis r11,-32165
	// lwz r9,148(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// subf r4,r29,r27
	ctx.r4.s64 = (int64_t)(int32_t)var_r27 - (int64_t)(int32_t)var_r29;
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// addi r11,r11,8944
	ctx.r11.s64 = ctx.r11.s64 + 8944;
	// lwz r7,300(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// subf r31,r29,r28
	var_r31 = var_r28 - var_r29;
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r8,144(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// subf r3,r29,r30
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 - (int64_t)(int32_t)var_r29;
	// subf r26,r29,r9
	var_r26 = (uint32_t)(ctx.r9.s64 - (int64_t)(int32_t)var_r29);
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// subf r25,r30,r9
	var_r25 = (uint32_t)(ctx.r9.s64 - (int64_t)(int32_t)var_r30);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// lwz r9,316(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// addi r11,r29,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 16;
	// vsubfp v13,v13,v0
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
	// subf r27,r30,r27
	var_r27 = var_r27 - var_r30;
	// vaddfp v12,v0,v0
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// subf r28,r30,r28
	var_r28 = var_r28 - var_r30;
	// subf r29,r29,r8
	var_r29 = (uint32_t)(ctx.r8.s64 - (int64_t)(int32_t)var_r29);
	// subf r30,r30,r8
	var_r30 = (uint32_t)(ctx.r8.s64 - (int64_t)(int32_t)var_r30);
	// lwz r8,308(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// li r5,32
	ctx.r5.s64 = 32;
loc_824647BC:
	// addi r24,r11,-16
	var_r24 = (uint32_t)(ctx.r11.s64 + -16);  // addr:0x825afff0
	// lvx128 v11,r27,r10
	ea = (var_r27 + ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v10,r4,r11
	ea = (ctx.r4.u32 + ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v11,v0,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v10,v0,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v9,v0,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// lvx128 v7,r25,r10
	ea = (var_r25 + ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r23,r8,16
	var_r23 = (uint32_t)(ctx.r8.s64 + 16);  // addr:0x82000010
	// lvx128 v8,r0,r24
	ea = (var_r24) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v5,v12,v5
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v8,v0,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
	// lvx128 v3,r28,r10
	ea = (var_r28 + ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v1,r30,r10
	ea = (var_r30 + ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v3,v12,v3
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v3.f32)));
	// lvx128 v6,r26,r11
	ea = (var_r26 + ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r24,r9,16
	var_r24 = (uint32_t)(ctx.r9.s64 + 16);  // addr:0x82000010
	// lvx128 v4,r3,r11
	ea = (ctx.r3.u32 + ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r22,r7,16
	var_r22 = (uint32_t)(ctx.r7.s64 + 16);
	// lvx128 v2,r31,r11
	ea = (var_r31 + ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v4,v12,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v4.f32)));
	// lvx128 v31,r29,r11
	ea = (var_r29 + ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v2,v12,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v2.f32)));
	// addi r21,r6,16
	var_r21 = (uint32_t)(ctx.r6.s64 + 16);
	// vmaddfp v11,v13,v7,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)), simde_mm_load_ps(ctx.v11.f32)));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vmaddfp v10,v13,v6,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v6.f32)), simde_mm_load_ps(ctx.v10.f32)));
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// vmaddfp v9,v13,v31,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v31.f32)), simde_mm_load_ps(ctx.v9.f32)));
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// stvx v5,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r7,32
	ctx.r7.s64 = ctx.r7.s64 + 32;
	// vmaddfp v8,v13,v1,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v1.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v3,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r6,r6,32
	ctx.r6.s64 = ctx.r6.s64 + 32;
	// stvx v4,r0,r22
	ea = (var_r22) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r21
	ea = (var_r21) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// stvx v10,r0,r24
	ea = (var_r24) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r23
	ea = (var_r23) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// bne cr6,0x824647bc
	if (!ctx.cr6.eq) goto loc_824647BC;
	// addi r1,r1,17888
	ctx.r1.s64 = ctx.r1.s64 + 17888;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8243662c
	__restfpr_14(ctx, base);
	// b 0x8242f8b0
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__game_vt58B0_0"))) PPC_WEAK_FUNC(game_vt58B0_0);
PPC_FUNC_IMPL(__imp__game_vt58B0_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,22704
	ctx.r11.s64 = ctx.r11.s64 + 22704;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// cmplwi cr6,r10,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x824648c8
	if (ctx.r10.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_824648C8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__game_vt58B0_2"))) PPC_WEAK_FUNC(game_vt58B0_2);
PPC_FUNC_IMPL(__imp__game_vt58B0_2) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82464b98
	phInst_4B98_p39(ctx, base);
	return;
}

__attribute__((alias("__imp__game_vt58B0_15"))) PPC_WEAK_FUNC(game_vt58B0_15);
PPC_FUNC_IMPL(__imp__game_vt58B0_15) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=192, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r1,120
	ctx.r4.s64 = ctx.r1.s64 + 120;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x82465da8
	util_5DA8(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x82464a5c
	if (ctx.r3.s32 >= 0) {
		// addi r4,r1,104
		ctx.r4.s64 = ctx.r1.s64 + 104;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82465da8
		util_5DA8(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x82464a5c
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// lis r9,0
		ctx.r9.s64 = 0;
		// li r11,0
		ctx.r11.s64 = 0;
		// li r10,6
		ctx.r10.s64 = 6;
		// ori r8,r9,48000
		ctx.r8.u64 = ctx.r9.u64 | 48000;
		// li r5,3
		ctx.r5.s64 = 3;
		// addi r4,r1,104
		ctx.r4.s64 = ctx.r1.s64 + 104;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stb r11,104(r1)
		PPC_STORE_U8(ctx.r1.u32 + 104, ctx.r11.u8);
		// stb r10,105(r1)
		PPC_STORE_U8(ctx.r1.u32 + 105, ctx.r10.u8);
		// stw r8,108(r1)
		PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
		// bl 0x82466018
		phDemoWorld_6018_g(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x82464a5c
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// lbz r11,121(r1)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 121);
		// cmplwi cr6,r11,1
		// blt cr6,0x82464a58
		if (ctx.r11.u32 >= 1) {
			// beq cr6,0x824649a4
			if (!(ctx.cr6.eq)) {
				// cmplwi cr6,r11,3
				// blt cr6,0x82464994
				if (ctx.r11.u32 >= 3) {
					// lis r3,-32761
					// ori r3,r3,87
					ctx.r3.u64 = ctx.r3.u64 | 87;
					// b 0x82464a5c
					// blr
					return;
				}
			loc_82464994:
				// lwz r6,16(r31)
				ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 16);
				// li r4,0
				ctx.r4.s64 = 0;
				// lwz r11,4(r6)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
				// b 0x824649b0
			} else {
			loc_824649A4:
				// lwz r5,16(r31)
				ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 16);
				// li r4,1
				ctx.r4.s64 = 1;
				// lwz r11,4(r5)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
			}
		loc_824649B0:
			// add r11,r11,r31
			ctx.r11.u64 = ctx.r11.u64 + var_r31;
			// addi r3,r11,16
			ctx.r3.s64 = ctx.r11.s64 + 16;
			// bl 0x8246dcf0
			game_DCF0_h(ctx, base);
			// lwz r11,128(r1)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
			// addi r3,r1,88
			ctx.r3.s64 = ctx.r1.s64 + 88;
			// addi r4,r11,1024
			ctx.r4.s64 = ctx.r11.s64 + 1024;
			// stw r11,80(r1)
			PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
			// lwz r11,112(r1)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
			// stw r4,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
			// addi r10,r11,1024
			ctx.r10.s64 = ctx.r11.s64 + 1024;
			// addi r9,r11,2048
			ctx.r9.s64 = ctx.r11.s64 + 2048;
			// addi r8,r11,4096
			ctx.r8.s64 = ctx.r11.s64 + 4096;
			// addi r7,r11,5120
			ctx.r7.s64 = ctx.r11.s64 + 5120;
			// stw r11,144(r1)
			PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r11.u32);
			// stw r10,148(r1)
			PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r10.u32);
			// stw r9,152(r1)
			PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
			// stw r8,156(r1)
			PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r8.u32);
			// stw r7,160(r1)
			PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
			// bl 0x825681b0
			util_81B0(ctx, base);
			// lwz r3,16(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 16);
			// li r6,256
			ctx.r6.s64 = 256;
			// addi r5,r1,144
			ctx.r5.s64 = ctx.r1.s64 + 144;
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwz r11,4(r3)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
			// add r11,r11,r31
			ctx.r11.u64 = ctx.r11.u64 + var_r31;
			// addi r3,r11,16
			ctx.r3.s64 = ctx.r11.s64 + 16;
			// bl 0x824631b8
			phInst_31B8_p39(ctx, base);
			// addi r3,r1,96
			ctx.r3.s64 = ctx.r1.s64 + 96;
			// bl 0x825681b0
			util_81B0(ctx, base);
			// addis r10,r31,5
			ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 327680;
			// addis r11,r31,5
			ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
			// ld r7,88(r1)
			ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
			// addi r10,r10,-17892
			ctx.r10.s64 = ctx.r10.s64 + -17892;
			// addi r11,r11,-17900
			ctx.r11.s64 = ctx.r11.s64 + -17900;
			// ld r8,0(r10)
			ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
			// ld r9,0(r11)
			ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
			// addi r6,r8,1
			ctx.r6.s64 = ctx.r8.s64 + 1;
			// ld r8,96(r1)
			ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
			// subf r9,r7,r9
			ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
			// add r5,r9,r8
			ctx.r5.u64 = ctx.r9.u64 + ctx.r8.u64;
			// std r6,0(r10)
			PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r6.u64);
			// std r5,0(r11)
			PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r5.u64);
		}
	loc_82464A58:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82464A5C:
	// blr
	return;
}

__attribute__((alias("__imp__game_vt58B0_1"))) PPC_WEAK_FUNC(game_vt58B0_1);
PPC_FUNC_IMPL(__imp__game_vt58B0_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32256
	// addi r31,r3,-52
	var_r31 = (uint32_t)(ctx.r3.s64 + -52);
	// addi r10,r11,22708
	ctx.r10.s64 = ctx.r11.s64 + 22708;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r9,r11,22704
	ctx.r9.s64 = ctx.r11.s64 + 22704;
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// cmplwi cr6,r11,0
	// addi r11,r31,52
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 52;
	// lwz r8,-52(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + -52);
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// add r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 + ctx.r11.u64;
	// stw r10,-52(r7)
	PPC_STORE_U32(ctx.r7.u32 + -52, ctx.r10.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82464ad4
	if (ctx.r11.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_82464AD4:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_4AF0_2hr"))) PPC_WEAK_FUNC(atSingleton_4AF0_2hr);
PPC_FUNC_IMPL(__imp__atSingleton_4AF0_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_29
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,22456
	ctx.r11.s64 = ctx.r11.s64 + 22456;
	// li r7,1
	ctx.r7.s64 = 1;
	// lis r10,-32256
	// lis r9,-32256
	// lis r8,-32256
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r11.u32);
	// addi r10,r10,22740
	ctx.r10.s64 = ctx.r10.s64 + 22740;
	// stw r7,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r7.u32);
	// addi r9,r9,22712
	ctx.r9.s64 = ctx.r9.s64 + 22712;
	// lwz r6,4(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// addi r30,r31,16
	var_r30 = (uint32_t)(var_r31 + 16);
	// addi r8,r8,27376
	ctx.r8.s64 = ctx.r8.s64 + 27376;
	// li r29,0
	var_r29 = 0;
	// addi r3,r30,52
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 52;
	// stw r6,12(r31)
	PPC_STORE_U32(var_r31 + 12, ctx.r6.u32);
	// stw r10,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r9.u32);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r29);
	// stw r8,0(r30)
	PPC_STORE_U32(var_r30 + 0,/* atSingleton::vtable@+0x0 */ ctx.r8.u32);
	// bl 0x8246eb88
	atSingleton_EB88(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8246fb90
	atSingleton_FB90_2h(ctx, base);
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 0)/* atSingleton::vtable@+0x0 */;
	// lis r11,-32256
	// lis r4,4
	ctx.r4.s64 = 262144;
	// lis r8,4
	ctx.r8.s64 = 262144;
	// addi r11,r11,22708
	ctx.r11.s64 = ctx.r11.s64 + 22708;
	// lwz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// ori r10,r4,47636
	ctx.r10.u64 = ctx.r4.u64 | 47636;
	// ori r7,r8,47644
	ctx.r7.u64 = ctx.r8.u64 | 47644;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stwx r11,r9,r30
	PPC_STORE_U32(ctx.r9.u32 + var_r30, ctx.r11.u32);
	// stdx r29,r31,r10
	PPC_STORE_U64(var_r31 + ctx.r10.u32, var_r29);
	// stdx r29,r31,r7
	PPC_STORE_U64(var_r31 + ctx.r7.u32, var_r29);
	return;
}

__attribute__((alias("__imp__phInst_4B98_p39"))) PPC_WEAK_FUNC(phInst_4B98_p39);
PPC_FUNC_IMPL(__imp__phInst_4B98_p39) {
	PPC_FUNC_PROLOGUE();
	// lis r10,-32256
	// lis r9,-32256
	// addi r11,r3,16
	ctx.r11.s64 = ctx.r3.s64 + 16;
	// addi r10,r10,22740
	ctx.r10.s64 = ctx.r10.s64 + 22740;
	// addi r9,r9,22712
	ctx.r9.s64 = ctx.r9.s64 + 22712;
	// addi r11,r11,52
	ctx.r11.s64 = ctx.r11.s64 + 52;
	// lis r8,-32256
	// lis r7,-32256
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// lis r6,-32256
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// addi r8,r8,22708
	ctx.r8.s64 = ctx.r8.s64 + 22708;
	// lwz r10,-52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -52);
	// addi r7,r7,22704
	ctx.r7.s64 = ctx.r7.s64 + 22704;
	// addi r6,r6,15792
	ctx.r6.s64 = ctx.r6.s64 + 15792;
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r8,-52(r9)
	PPC_STORE_U32(ctx.r9.u32 + -52, ctx.r8.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
	// stw r6,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_4BF0_fw"))) PPC_WEAK_FUNC(atSingleton_4BF0_fw);
PPC_FUNC_IMPL(__imp__atSingleton_4BF0_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lis r4,4
	ctx.r4.s64 = 262144;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// ori r4,r4,47652
	ctx.r4.u64 = ctx.r4.u64 | 47652;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0)/* atSingleton::vtable@+0x0 */;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// cmplwi cr6,r3,0
	// beq cr6,0x82464c44
	if (ctx.r3.u32 != 0) {
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// bl 0x82464af0
		atSingleton_4AF0_2hr(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmplwi cr6,r31,0
		// bne cr6,0x82464c54
		if (var_r31 != 0) goto loc_82464C54;
	}
loc_82464C44:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
loc_82464C54:
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,28(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82464c88
	if ((int32_t)var_r30 >= 0) {
		// stw r31,0(r28)
		PPC_STORE_U32(var_r28 + 0, var_r31);
		return;
	}
loc_82464C88:
	// addi r3,r31,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 4;
	// lwz r6,12(r7)
	// bctrl
	VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__ph_vt58FC_10_4CA8"))) PPC_WEAK_FUNC(ph_vt58FC_10_4CA8);
PPC_FUNC_IMPL(__imp__ph_vt58FC_10_4CA8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// mr r31,r6
	var_r31 = ctx.r6.u32;
	// cmplwi cr6,r11,1
	// bne cr6,0x82464d04
	if (ctx.r11.u32 == 1) {
		// clrlwi r10,r5,24
		ctx.r10.u64 = ctx.r5.u32 & 0xFF;
		// cmplwi cr6,r10,3
		// bne cr6,0x82464d04
		if (ctx.r10.u32 != 3) goto loc_82464D04;
		// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
		// addi r6,r31,4
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 4;
		// lwz r4,0(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lwz r8,36(r9)
  // [ph4a] slot load collapsed
		// lwz r5,0(r6)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// bctrl
		VCALL(ctx.r3.u32, 9, ctx, base);  // pattern-B slot 9 (byte +36)
		// blr
		return;
	}
loc_82464D04:
	// cmplwi cr6,r11,0
	// bne cr6,0x82464d50
	if (ctx.r11.u32 == 0) {
		// clrlwi r7,r5,24
		ctx.r7.u64 = ctx.r5.u32 & 0xFF;
		// cmplwi cr6,r7,2
		// bne cr6,0x82464d50
		if (ctx.r7.u32 != 2) goto loc_82464D50;
		// lwz r6,0(r3)
  // [ph4a] vtable load collapsed
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lwz r5,28(r6)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 7, ctx, base);  // pattern-B slot 7 (byte +28)
		// cmpwi cr6,r3,0
		// blt cr6,0x82464d94
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// lbz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// blr
		return;
	}
loc_82464D50:
	// cmplwi cr6,r11,2
	// bne cr6,0x82464d8c
	if (ctx.r11.u32 == 2) {
		// clrlwi r10,r5,24
		ctx.r10.u64 = ctx.r5.u32 & 0xFF;
		// cmplwi cr6,r10,3
		// bne cr6,0x82464d8c
		if (ctx.r10.u32 != 3) goto loc_82464D8C;
		// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
		// lwz r4,0(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lwz r8,44(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 11, ctx, base);  // pattern-B slot 11 (byte +44)
		// blr
		return;
	}
loc_82464D8C:
	// lis r3,-32761
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
loc_82464D94:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt58FC_11_4DA8"))) PPC_WEAK_FUNC(ph_vt58FC_11_4DA8);
PPC_FUNC_IMPL(__imp__ph_vt58FC_11_4DA8) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r11,1
	// bne cr6,0x82464dd4
	if (ctx.r11.u32 == 1) {
		// clrlwi r10,r5,24
		ctx.r10.u64 = ctx.r5.u32 & 0xFF;
		// cmplwi cr6,r10,3
		// bne cr6,0x82464dd4
		if (ctx.r10.u32 != 3) goto loc_82464DD4;
		// lwz r9,0(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r4,0(r6)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// lwz r8,40(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
		// mtctr r8
		ctx.ctr.u64 = ctx.r8.u64;
		// bctr
		PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
		return;
	}
loc_82464DD4:
	// cmplwi cr6,r11,0
	// bne cr6,0x82464e00
	if (ctx.r11.u32 == 0) {
		// clrlwi r7,r5,24
		ctx.r7.u64 = ctx.r5.u32 & 0xFF;
		// cmplwi cr6,r7,2
		// bne cr6,0x82464e00
		if (ctx.r7.u32 != 2) goto loc_82464E00;
		// lwz r5,0(r3)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r4,0(r6)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// clrlwi r4,r4,24
		ctx.r4.u64 = ctx.r4.u32 & 0xFF;
		// lwz r11,32(r5)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 32);
		// mtctr r11
		ctx.ctr.u64 = ctx.r11.u64;
		// bctr
		PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
		return;
	}
loc_82464E00:
	// cmplwi cr6,r11,2
	// bne cr6,0x82464e28
	if (ctx.r11.u32 == 2) {
		// clrlwi r10,r5,24
		ctx.r10.u64 = ctx.r5.u32 & 0xFF;
		// cmplwi cr6,r10,3
		// bne cr6,0x82464e28
		if (ctx.r10.u32 != 3) {
			// lis r3,-32761
			// ori r3,r3,87
			ctx.r3.u64 = ctx.r3.u64 | 87;
			// blr
			return;
		}
		// lwz r9,0(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r4,0(r6)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		// lwz r8,48(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
		// mtctr r8
		ctx.ctr.u64 = ctx.r8.u64;
		// bctr
		PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
		return;
	}
loc_82464E28:
	// lis r3,-32761
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt58FC_0_4E38"))) PPC_WEAK_FUNC(ph_vt58FC_0_4E38);
PPC_FUNC_IMPL(__imp__ph_vt58FC_0_4E38) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82465910
	phInst_5910_p39(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt58FC_14_4E60"))) PPC_WEAK_FUNC(ph_vt58FC_14_4E60);
PPC_FUNC_IMPL(__imp__ph_vt58FC_14_4E60) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,26(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 26);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt58FC_15_4E70"))) PPC_WEAK_FUNC(ph_vt58FC_15_4E70);
PPC_FUNC_IMPL(__imp__ph_vt58FC_15_4E70) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r4,26(r11)
	PPC_STORE_U8(ctx.r11.u32 + 26, ctx.r4.u8);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt58FC_20_4E80"))) PPC_WEAK_FUNC(ph_vt58FC_20_4E80);
PPC_FUNC_IMPL(__imp__ph_vt58FC_20_4E80) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 12);
	// lbz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 8);
	// cmplwi cr6,r11,0
	// beq cr6,0x82464ec4
	if (ctx.r11.u32 != 0) {
		// lbz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// clrlwi r9,r10,24
		ctx.r9.u64 = ctx.r10.u32 & 0xFF;
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
		// cmplw cr6,r9,r8
		// bgt cr6,0x82464ec4
		if (ctx.r9.u32 > ctx.r8.u32) goto loc_82464EC4;
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
	}
loc_82464EC4:
	// clrlwi r11,r10,24
	ctx.r11.u64 = ctx.r10.u32 & 0xFF;
	// stb r10,24(r31)
	PPC_STORE_U8(var_r31 + 24, ctx.r10.u8);
	// cmplwi cr6,r11,0
	// beq cr6,0x82464ef8
	if (ctx.r11.u32 != 0) {
		// lwz r7,0(r5)
  // [ph4a] vtable load collapsed
		// rlwinm r10,r11,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
		// mr r3,r5
		ctx.r3.u64 = ctx.r5.u64;
		// add r6,r11,r10
		ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
		// rlwinm r4,r6,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// lwz r5,20(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r5.u32, 5, ctx, base);  // pattern-B slot 5 (byte +20)
		// stw r3,20(r31)
		PPC_STORE_U32(var_r31 + 20, ctx.r3.u32);
	}
loc_82464EF8:
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r4,12(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 12);
	// lwz r10,40(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 10, ctx, base);  // pattern-B slot 10 (byte +40)
	// cmpwi cr6,r3,0
	// blt cr6,0x82464f30
	if (ctx.r3.s32 >= 0) {
		// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lbz r4,9(r30)
		ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 9);
		// lwz r8,32(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 8, ctx, base);  // pattern-B slot 8 (byte +32)
	}
loc_82464F30:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt58FC_16_4F48"))) PPC_WEAK_FUNC(ph_vt58FC_16_4F48);
PPC_FUNC_IMPL(__imp__ph_vt58FC_16_4F48) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_25
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r27,r5
	var_r27 = ctx.r5.u32;
	// mr r25,r6
	var_r25 = ctx.r6.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// addi r29,r11,30832
	var_r29 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r28,r13
	var_r28 = ctx.r13.u32;
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82464f90
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 8);
		// cmplw cr6,r28,r10
		// beq cr6,0x82464fa4
		if (var_r28 == ctx.r10.u32) goto loc_82464FA4;
	}
loc_82464F90:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r28,8(r29)
	PPC_STORE_U32(var_r29 + 8, var_r28);
	// stb r26,12(r29)
	PPC_STORE_U8(var_r29 + 12, (uint8_t)var_r26);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
loc_82464FA4:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmplwi cr6,r27,0
	// stw r11,4(r29)
	PPC_STORE_U32(var_r29 + 4, ctx.r11.u32);
	// lbz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 16);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// beq cr6,0x82465044
	if (var_r27 != 0) {
		// addi r8,r30,8
		ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 8;
		// stb r11,0(r30)
		PPC_STORE_U8(var_r30 + 0, ctx.r11.u8);
		// li r9,0
		ctx.r9.s64 = 0;
		// stw r8,4(r30)
		PPC_STORE_U32(var_r30 + 4, ctx.r8.u32);
		// lbz r7,16(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 16);
		// cmplwi cr6,r7,0
		// beq cr6,0x82465048
		if (ctx.r7.u32 == 0) goto loc_82465048;
		// li r10,0
		ctx.r10.s64 = 0;
		// li r11,0
		ctx.r11.s64 = 0;
	loc_82464FE8:
		// lwz r6,20(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 20);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// lwz r5,4(r30)
		ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lbzx r4,r11,r6
		ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r6.u32);
		// stbx r4,r10,r5
		PPC_STORE_U8(ctx.r10.u32 + ctx.r5.u32, ctx.r4.u8);
		// lwz r7,20(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lwz r8,4(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
		// add r3,r11,r7
		ctx.r3.u64 = ctx.r11.u64 + ctx.r7.u64;
		// add r8,r10,r8
		ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
		// lbz r7,1(r3)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
		// stb r7,1(r8)
		PPC_STORE_U8(ctx.r8.u32 + 1, ctx.r7.u8);
		// lwz r7,20(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lwz r8,4(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
		// add r6,r11,r7
		ctx.r6.u64 = ctx.r11.u64 + ctx.r7.u64;
		// add r5,r10,r8
		ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
		// addi r11,r11,12
		ctx.r11.s64 = ctx.r11.s64 + 12;
		// addi r10,r10,8
		ctx.r10.s64 = ctx.r10.s64 + 8;
		// lfs f0,8(r6)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,4(r5)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
		// lbz r4,16(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 16);
		// cmplw cr6,r9,r4
		// blt cr6,0x82464fe8
		if (ctx.r9.u32 < ctx.r4.u32) goto loc_82464FE8;
		// b 0x82465048
	} else {
	loc_82465044:
		// stw r10,0(r25)
		PPC_STORE_U32(var_r25 + 0, ctx.r10.u32);
	}
loc_82465048:
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x82465090
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x82465090
		if (ctx.r10.u32 != ctx.r11.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r29)
		PPC_STORE_U32(var_r29 + 4, ctx.r11.u32);
		// bne cr6,0x82465090
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r31,12(r29)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r29 + 12));
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// stb r11,12(r29)
		PPC_STORE_U8(var_r29 + 12, ctx.r11.u8);
		// stw r11,8(r29)
		PPC_STORE_U32(var_r29 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82465090:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt58FC_17_50A0"))) PPC_WEAK_FUNC(ph_vt58FC_17_50A0);
PPC_FUNC_IMPL(__imp__ph_vt58FC_17_50A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r29,r11,30832
	var_r29 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r28,r13
	var_r28 = ctx.r13.u32;
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x824650e0
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 8);
		// cmplw cr6,r28,r10
		// beq cr6,0x824650fc
		if (var_r28 == ctx.r10.u32) goto loc_824650FC;
	}
loc_824650E0:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r28
	ctx.r10.u64 = var_r28;
	// stw r10,8(r29)
	PPC_STORE_U32(var_r29 + 8, ctx.r10.u32);
	// stb r27,12(r29)
	PPC_STORE_U8(var_r29 + 12, (uint8_t)var_r27);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
	// b 0x82465100
	goto loc_82465100;
loc_824650FC:
	// lbz r27,12(r29)
	var_r27 = (uint32_t)(PPC_LOAD_U8(var_r29 + 12));
loc_82465100:
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// cmplwi cr6,r30,0
	// stw r9,4(r29)
	PPC_STORE_U32(var_r29 + 4, ctx.r9.u32);
	// beq cr6,0x82465248
	if (var_r30 != 0) {
		// lbz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 0);
		// lbz r8,24(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 24);
		// mr r7,r11
		ctx.r7.u64 = ctx.r11.u64;
		// cmplw cr6,r7,r8
		// ble cr6,0x82465174
		if (ctx.r7.u32 > ctx.r8.u32) {
			// mr r11,r13
			ctx.r11.u64 = ctx.r13.u64;
			// cmpwi cr6,r9,0
			// beq cr6,0x82465164
			if (ctx.r9.s32 != 0) {
				// cmplw cr6,r11,r10
				// bne cr6,0x82465164
				if (ctx.r11.u32 != ctx.r10.u32) {
					// lis r3,-32761
					// ori r3,r3,87
					ctx.r3.u64 = ctx.r3.u64 | 87;
					return;
				}
				// addi r11,r9,-1
				ctx.r11.s64 = ctx.r9.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r29)
				PPC_STORE_U32(var_r29 + 4, ctx.r11.u32);
				// bne cr6,0x82465164
				if (ctx.r11.s32 != 0) {
					// lis r3,-32761
					// ori r3,r3,87
					ctx.r3.u64 = ctx.r3.u64 | 87;
					return;
				}
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// stb r11,12(r29)
				PPC_STORE_U8(var_r29 + 12, ctx.r11.u8);
				// mr r31,r27
				var_r31 = (uint32_t)(var_r27);
				// stw r11,8(r29)
				PPC_STORE_U32(var_r29 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_82465164:
			// lis r3,-32761
			// ori r3,r3,87
			ctx.r3.u64 = ctx.r3.u64 | 87;
			return;
		}
	loc_82465174:
		// clrlwi r6,r11,24
		ctx.r6.u64 = ctx.r11.u32 & 0xFF;
		// stb r11,16(r31)
		PPC_STORE_U8(var_r31 + 16, ctx.r11.u8);
		// li r9,0
		ctx.r9.s64 = 0;
		// cmplwi cr6,r6,0
		// beq cr6,0x82465200
		if (ctx.r6.u32 != 0) {
			// li r10,0
			ctx.r10.s64 = 0;
			// li r11,0
			ctx.r11.s64 = 0;
		loc_82465190:
			// lwz r5,4(r30)
			ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// lwz r4,20(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 20);
			// lbzx r3,r5,r11
			ctx.r3.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r11.u32);
			// stbx r3,r10,r4
			PPC_STORE_U8(ctx.r10.u32 + ctx.r4.u32, ctx.r3.u8);
			// lwz r7,4(r30)
			ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
			// lwz r8,20(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 20);
			// add r7,r7,r11
			ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
			// add r6,r10,r8
			ctx.r6.u64 = ctx.r10.u64 + ctx.r8.u64;
			// lbz r5,1(r7)
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
			// stb r5,1(r6)
			PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r5.u8);
			// lwz r7,4(r30)
			ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
			// lwz r8,20(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 20);
			// add r4,r7,r11
			ctx.r4.u64 = ctx.r7.u64 + ctx.r11.u64;
			// add r3,r10,r8
			ctx.r3.u64 = ctx.r10.u64 + ctx.r8.u64;
			// lfs f0,4(r4)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
			ctx.f0.f64 = double(temp.f32);
			// stfs f0,4(r3)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
			// lwz r7,4(r30)
			ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
			// lwz r8,20(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 20);
			// add r7,r7,r11
			ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
			// add r6,r10,r8
			ctx.r6.u64 = ctx.r10.u64 + ctx.r8.u64;
			// addi r11,r11,8
			ctx.r11.s64 = ctx.r11.s64 + 8;
			// addi r10,r10,12
			ctx.r10.s64 = ctx.r10.s64 + 12;
			// lfs f13,4(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
			ctx.f13.f64 = double(temp.f32);
			// stfs f13,8(r6)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r6.u32 + 8, temp.u32);
			// lbz r5,16(r31)
			ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 16);
			// cmplw cr6,r9,r5
			// blt cr6,0x82465190
			if (ctx.r9.u32 < ctx.r5.u32) goto loc_82465190;
		}
	loc_82465200:
		// lbz r4,16(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 16);
		// li r9,0
		ctx.r9.s64 = 0;
		// cmplwi cr6,r4,0
		// beq cr6,0x82465240
		if (ctx.r4.u32 != 0) {
			// lwz r11,20(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
			// clrlwi r8,r4,24
			ctx.r8.u64 = ctx.r4.u32 & 0xFF;
			// addi r10,r11,1
			ctx.r10.s64 = ctx.r11.s64 + 1;
		loc_8246521C:
			// lbz r11,0(r10)
			ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// cmplw cr6,r11,r9
			// ble cr6,0x82465230
			if (ctx.r11.u32 > ctx.r9.u32) {
				// mr r9,r11
				ctx.r9.u64 = ctx.r11.u64;
			}
		loc_82465230:
			// addi r8,r8,-1
			ctx.r8.s64 = ctx.r8.s64 + -1;
			// addi r10,r10,12
			ctx.r10.s64 = ctx.r10.s64 + 12;
			// cmplwi cr6,r8,0
			// bne cr6,0x8246521c
			if (ctx.r8.u32 != 0) goto loc_8246521C;
		}
	loc_82465240:
		// stb r9,25(r31)
		PPC_STORE_U8(var_r31 + 25, ctx.r9.u8);
		// b 0x82465258
	} else {
	loc_82465248:
		// li r11,0
		ctx.r11.s64 = 0;
		// li r10,0
		ctx.r10.s64 = 0;
		// stb r11,16(r31)
		PPC_STORE_U8(var_r31 + 16, ctx.r11.u8);
		// stb r10,25(r31)
		PPC_STORE_U8(var_r31 + 25, ctx.r10.u8);
	}
loc_82465258:
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x824652a0
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x824652a0
		if (ctx.r10.u32 != ctx.r11.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r29)
		PPC_STORE_U32(var_r29 + 4, ctx.r11.u32);
		// bne cr6,0x824652a0
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r31,12(r29)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r29 + 12));
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// stb r11,12(r29)
		PPC_STORE_U8(var_r29 + 12, ctx.r11.u8);
		// stw r11,8(r29)
		PPC_STORE_U32(var_r29 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824652A0:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt58FC_18_52B0"))) PPC_WEAK_FUNC(ph_vt58FC_18_52B0);
PPC_FUNC_IMPL(__imp__ph_vt58FC_18_52B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_27
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r30,r11,30832
	var_r30 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x824652f0
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 8);
		// cmplw cr6,r29,r10
		// beq cr6,0x82465308
		if (var_r29 == ctx.r10.u32) goto loc_82465308;
	}
loc_824652F0:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r29
	ctx.r10.u64 = var_r29;
	// stw r10,8(r30)
	PPC_STORE_U32(var_r30 + 8, ctx.r10.u32);
	// stb r27,12(r30)
	PPC_STORE_U8(var_r30 + 12, (uint8_t)var_r27);
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
loc_82465308:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r11,4(r30)
	PPC_STORE_U32(var_r30 + 4, ctx.r11.u32);
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 0);
	// cmplwi cr6,r9,0
	// beq cr6,0x82465390
	if (ctx.r9.u32 != 0) {
		// lis r11,-32256
		// li r9,0
		ctx.r9.s64 = 0;
		// lfs f0,15788(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);
		ctx.f0.f64 = double(temp.f32);
	loc_8246532C:
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// lbz r8,16(r28)
		ctx.r8.u64 = PPC_LOAD_U8(var_r28 + 16);
		// add r11,r9,r11
		ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
		// lbz r7,0(r11)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// cmplw cr6,r7,r8
		// bge cr6,0x82465370
		if (ctx.r7.u32 < ctx.r8.u32) {
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// lwz r8,20(r28)
			ctx.r8.u64 = PPC_LOAD_U32(var_r28 + 20);
			// add r11,r9,r11
			ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
			// lbz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
			// rotlwi r7,r10,1
			ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
			// add r5,r10,r7
			ctx.r5.u64 = ctx.r10.u64 + ctx.r7.u64;
			// rlwinm r10,r5,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			// add r4,r10,r8
			ctx.r4.u64 = ctx.r10.u64 + ctx.r8.u64;
			// lfs f13,8(r4)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// stfs f13,4(r11)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
			// b 0x82465374
		} else {
		loc_82465370:
			// stfs f0,4(r11)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		}
	loc_82465374:
		// lbz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U8(var_r31 + 0);
		// addi r6,r6,1
		ctx.r6.s64 = ctx.r6.s64 + 1;
		// addi r9,r9,8
		ctx.r9.s64 = ctx.r9.s64 + 8;
		// cmplw cr6,r6,r3
		// blt cr6,0x8246532c
		if (ctx.r6.u32 < ctx.r3.u32) goto loc_8246532C;
		// lwz r10,8(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 8);
		// lwz r11,4(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	}
loc_82465390:
	// mr r9,r13
	ctx.r9.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x824653d0
	if (ctx.r11.s32 != 0) {
		// cmplw cr6,r9,r10
		// bne cr6,0x824653d0
		if (ctx.r9.u32 != ctx.r10.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r30)
		PPC_STORE_U32(var_r30 + 4, ctx.r11.u32);
		// bne cr6,0x824653d0
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r31,12(r30)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r30 + 12));
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stb r11,12(r30)
		PPC_STORE_U8(var_r30 + 12, ctx.r11.u8);
		// stw r11,8(r30)
		PPC_STORE_U32(var_r30 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824653D0:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt58FC_19_53E0"))) PPC_WEAK_FUNC(ph_vt58FC_19_53E0);
PPC_FUNC_IMPL(__imp__ph_vt58FC_19_53E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_27
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r28,r13
	var_r28 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82465420
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r28,r10
		// beq cr6,0x82465438
		if (var_r28 == ctx.r10.u32) goto loc_82465438;
	}
loc_82465420:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r28
	ctx.r10.u64 = var_r28;
	// stw r10,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
	// stb r27,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82465438:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U8(var_r30 + 0);
	// cmplwi cr6,r9,0
	// beq cr6,0x824654ac
	if (ctx.r9.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_82465450:
		// lwz r8,4(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
		// rlwinm r9,r11,3,0,28
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// lbz r7,16(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 16);
		// lbzx r6,r8,r9
		ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r9.u32);
		// cmplw cr6,r6,r7
		// bge cr6,0x824654f8
		if (ctx.r6.u32 >= ctx.r7.u32) goto loc_824654F8;
		// rotlwi r10,r8,0
		ctx.r10.u64 = ctx.r8.u32;
		// lwz r8,20(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 20);
		// addi r5,r11,1
		ctx.r5.s64 = ctx.r11.s64 + 1;
		// add r11,r10,r9
		ctx.r11.u64 = ctx.r10.u64 + ctx.r9.u64;
		// lbz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lfs f0,4(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f0.f64 = double(temp.f32);
		// clrlwi r11,r5,24
		ctx.r11.u64 = ctx.r5.u32 & 0xFF;
		// rotlwi r9,r10,1
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
		// add r4,r10,r9
		ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
		// rlwinm r10,r4,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// add r3,r10,r8
		ctx.r3.u64 = ctx.r10.u64 + ctx.r8.u64;
		// stfs f0,8(r3)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
		// lbz r10,0(r30)
		ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 0);
		// cmplw cr6,r11,r10
		// blt cr6,0x82465450
		if (ctx.r11.u32 < ctx.r10.u32) goto loc_82465450;
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_824654AC:
	// mr r9,r13
	ctx.r9.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x824654ec
	if (ctx.r11.s32 != 0) {
		// cmplw cr6,r9,r10
		// bne cr6,0x824654ec
		if (ctx.r9.u32 != ctx.r10.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x824654ec
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824654EC:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
loc_824654F8:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x82465540
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x82465540
		if (ctx.r10.u32 != ctx.r11.u32) {
			// lis r3,-32761
			// ori r3,r3,87
			ctx.r3.u64 = ctx.r3.u64 | 87;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82465540
		if (ctx.r11.s32 != 0) {
			// lis r3,-32761
			// ori r3,r3,87
			ctx.r3.u64 = ctx.r3.u64 | 87;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82465540:
	// lis r3,-32761
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	return;
}

__attribute__((alias("__imp__ph_vt58FC_13_5550"))) PPC_WEAK_FUNC(ph_vt58FC_13_5550);
PPC_FUNC_IMPL(__imp__ph_vt58FC_13_5550) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	double var_f26 = 0.0;
	double var_f27 = 0.0;
	double var_f28 = 0.0;
	double var_f29 = 0.0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x82465558;
	__savegprlr_26(ctx, base);
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82436610
	__savefpr_26(ctx, base);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// cmplwi cr6,r29,0
	// addi r3,r29,-8
	ctx.r3.s64 = (int64_t)(int32_t)var_r29 + -8;
	// bne cr6,0x82465580
	if (var_r29 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82465580:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// bl 0x82465d80
	ph_5D80(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x82465900
	if (ctx.r3.s32 >= 0) {
		// cmplwi cr6,r28,0
		// addi r31,r28,-8
		var_r31 = (uint32_t)(var_r28 + -8);
		// bne cr6,0x824655a0
		if (var_r28 == 0) {
			// li r31,0
			var_r31 = 0;
		}
	loc_824655A0:
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82465d80
		ph_5D80(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x82465900
		if (ctx.r3.s32 < 0) {
			// addi r1,r1,272
			ctx.r1.s64 = ctx.r1.s64 + 272;
			// addi r12,r1,-56
			ctx.r12.s64 = ctx.r1.s64 + -56;
			// bl 0x8243665c
			__restfpr_26(ctx, base);
			// b 0x8242f8e0
			__restgprlr_26(ctx, base);
			return;
		}
		// lbz r11,26(r27)
		ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 26);
		// cmplwi cr6,r11,1
		// bne cr6,0x824655d8
		if (ctx.r11.u32 == 1) {
			// lbz r10,25(r27)
			ctx.r10.u64 = PPC_LOAD_U8(var_r27 + 25);
			// li r5,1
			ctx.r5.s64 = 1;
			// addi r4,r1,96
			ctx.r4.s64 = ctx.r1.s64 + 96;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r10,97(r1)
			PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r10.u8);
			// bl 0x82465ef0
			ph_5EF0(ctx, base);
		}
	loc_824655D8:
		// cmpwi cr6,r3,0
		// blt cr6,0x82465900
		if (ctx.r3.s32 < 0) {
			// addi r1,r1,272
			ctx.r1.s64 = ctx.r1.s64 + 272;
			// addi r12,r1,-56
			ctx.r12.s64 = ctx.r1.s64 + -56;
			// bl 0x8243665c
			__restfpr_26(ctx, base);
			// b 0x8242f8e0
			__restgprlr_26(ctx, base);
			return;
		}
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// addi r26,r11,30832
		var_r26 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r31,r13
		var_r31 = ctx.r13.u32;
		// lwz r11,4(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8246560c
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r26)
			ctx.r9.u64 = PPC_LOAD_U32(var_r26 + 8);
			// cmplw cr6,r31,r9
			// beq cr6,0x82465620
			if (var_r31 == ctx.r9.u32) goto loc_82465620;
		}
	loc_8246560C:
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r31,8(r26)
		PPC_STORE_U32(var_r26 + 8, var_r31);
		// stb r30,12(r26)
		PPC_STORE_U8(var_r26 + 12, (uint8_t)var_r30);
		// lwz r11,4(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 4);
	loc_82465620:
		// addi r8,r11,1
		ctx.r8.s64 = ctx.r11.s64 + 1;
		// stw r8,4(r26)
		PPC_STORE_U32(var_r26 + 4, ctx.r8.u32);
		// lbz r11,16(r27)
		ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 16);
		// cmplwi cr6,r11,0
		// beq cr6,0x824657e8
		if (ctx.r11.u32 != 0) {
			// lis r7,-32256
			// lis r6,-32256
			// lis r8,-32256
			// lis r9,-32256
			// lis r10,-32256
			// lis r11,-32256
			// lfs f26,22772(r7)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 22772);
			var_f26 = double(temp.f32);
			// lfs f27,22868(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 22868);
			var_f27 = double(temp.f32);
			// li r30,0
			var_r30 = 0;
			// lfs f28,22776(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 22776);
			var_f28 = double(temp.f32);
			// li r31,0
			var_r31 = 0;
			// lfs f29,16056(r9)
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16056);
			var_f29 = double(temp.f32);
			// lfs f30,15788(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15788);
			var_f30 = double(temp.f32);
			// lfs f31,15784(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
			var_f31 = double(temp.f32);
		loc_8246566C:
			// lwz r11,20(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 20);
			// lwz r4,104(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
			// add r10,r31,r11
			ctx.r10.u64 = var_r31 + ctx.r11.u64;
			// lbz r8,97(r1)
			ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 97);
			// lbz r5,113(r1)
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + 113);
			// lbz r11,1(r10)
			ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
			// lfs f13,8(r10)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
			ctx.f13.f64 = double(temp.f32);
			// lbz r9,0(r10)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
			// rotlwi r6,r11,10
			ctx.r6.u64 = __builtin_rotateleft32(ctx.r11.u32, 10);
			// rotlwi r7,r9,10
			ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 10);
			// add r3,r6,r4
			ctx.r3.u64 = ctx.r6.u64 + ctx.r4.u64;
			// lwz r6,120(r1)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
			// cmplw cr6,r11,r8
			// add r7,r7,r6
			ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
			// bge cr6,0x824657d0
			if (ctx.r11.u32 < ctx.r8.u32) {
				// cmplw cr6,r9,r5
				// bge cr6,0x824657b8
				if (ctx.r9.u32 < ctx.r5.u32) {
					// lfs f0,4(r10)
					temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
					ctx.f0.f64 = double(temp.f32);
					// addi r5,r1,84
					ctx.r5.s64 = ctx.r1.s64 + 84;
					// stfs f0,80(r1)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
					// fsubs f0,f13,f0
					ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
					// addi r4,r1,80
					ctx.r4.s64 = ctx.r1.s64 + 80;
					// stfs f31,144(r1)
					temp.f32 = float(var_f31);
					PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
					// stfs f30,148(r1)
					temp.f32 = float(var_f30);
					PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
					// addi r11,r1,88
					ctx.r11.s64 = ctx.r1.s64 + 88;
					// stfs f29,152(r1)
					temp.f32 = float(var_f29);
					PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
					// addi r6,r1,128
					ctx.r6.s64 = ctx.r1.s64 + 128;
					// stfs f28,156(r1)
					temp.f32 = float(var_f28);
					PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
					// addi r8,r1,144
					ctx.r8.s64 = ctx.r1.s64 + 144;
					// lbz r9,26(r27)
					ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 26);
					// cmplwi cr6,r9,0
					ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
					// lvx128 v10,r0,r8
					ea = (ctx.r8.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// fmuls f0,f0,f27
					ctx.f0.f64 = double(float(ctx.f0.f64 * var_f27));
					// stfs f0,84(r1)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
					// fmuls f12,f0,f26
					ctx.f12.f64 = double(float(ctx.f0.f64 * var_f26));
					// stfs f12,88(r1)
					temp.f32 = float(ctx.f12.f64);
					PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
					// lvlx v0,0,r5
					temp.u32 = ctx.r5.u32;
					simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
					// lvlx v13,0,r4
					temp.u32 = ctx.r4.u32;
					simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
					// vspltw v11,v0,0
					simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
					// vspltw v0,v13,0
					simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
					// lvlx v12,0,r11
					temp.u32 = ctx.r11.u32;
					simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
					// vspltw v13,v12,0
					simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
					// stvx v0,r0,r6
					ea = (ctx.r6.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// vmaddfp v0,v11,v10,v0
					ctx.fpscr.enableFlushModeUnconditional();
					simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)), simde_mm_load_ps(ctx.v0.f32)));
					// bne cr6,0x82465778
					if (ctx.cr6.eq) {
						// mr r11,r3
						ctx.r11.u64 = ctx.r3.u64;
						// li r8,64
						ctx.r8.s64 = 64;
						// li r9,0
						ctx.r9.s64 = 0;
					loc_8246572C:
						// dcbt r9,r3
						// addi r9,r9,128
						ctx.r9.s64 = ctx.r9.s64 + 128;
						// cmplwi cr6,r9,1024
						// blt cr6,0x8246572c
						if (ctx.r9.u32 < 1024) goto loc_8246572C;
						// subf r9,r3,r7
						ctx.r9.s64 = ctx.r7.s64 - ctx.r3.s64;
					loc_82465740:
						// vor v9,v0,v0
						simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
						// lvx128 v12,r0,r11
						ea = (ctx.r11.u32) & ~0xF;
						simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
						// lvx128 v11,r9,r11
						ea = (ctx.r9.u32 + ctx.r11.u32) & ~0xF;
						simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
						// vaddfp v0,v0,v13
						ctx.fpscr.enableFlushMode();
						simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
						// addi r5,r1,128
						ctx.r5.s64 = ctx.r1.s64 + 128;
						// addi r8,r8,-1
						ctx.r8.s64 = ctx.r8.s64 + -1;
						// vmaddfp v12,v11,v9,v12
						simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v12.f32)));
						// cmplwi cr6,r8,0
						// stvx v0,r0,r5
						ea = (ctx.r5.u32) & ~0xF;
						simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
						// stvx v12,r0,r11
						ea = (ctx.r11.u32) & ~0xF;
						simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
						// addi r11,r11,16
						ctx.r11.s64 = ctx.r11.s64 + 16;
						// bne cr6,0x82465740
						if (ctx.r8.u32 != 0) goto loc_82465740;
						// stfs f13,4(r10)
						ctx.fpscr.disableFlushModeUnconditional();
						temp.f32 = float(ctx.f13.f64);
						PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
						// b 0x824657d0
						goto loc_824657D0;
					}
				loc_82465778:
					// mr r9,r7
					ctx.r9.u64 = ctx.r7.u64;
					// li r11,64
					ctx.r11.s64 = 64;
				loc_82465780:
					// vor v8,v0,v0
					simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
					// lvx128 v12,r0,r9
					ea = (ctx.r9.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// vaddfp v0,v0,v13
					ctx.fpscr.enableFlushMode();
					simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
					// addi r4,r1,128
					ctx.r4.s64 = ctx.r1.s64 + 128;
					// addi r11,r11,-1
					ctx.r11.s64 = ctx.r11.s64 + -1;
					// addi r9,r9,16
					ctx.r9.s64 = ctx.r9.s64 + 16;
					// vmulfp128 v12,v12,v8
					simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
					// cmplwi cr6,r11,0
					// stvx v0,r0,r4
					ea = (ctx.r4.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// stvx v12,r0,r3
					ea = (ctx.r3.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// addi r3,r3,16
					ctx.r3.s64 = ctx.r3.s64 + 16;
					// bne cr6,0x82465780
					if (ctx.r11.u32 != 0) goto loc_82465780;
					// stfs f13,4(r10)
					ctx.fpscr.disableFlushModeUnconditional();
					temp.f32 = float(ctx.f13.f64);
					PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
					// b 0x824657d0
				} else {
				loc_824657B8:
					// lbz r11,26(r27)
					ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 26);
					// cmplwi cr6,r11,0
					// beq cr6,0x824657d0
					if (ctx.r11.u32 == 0) goto loc_824657D0;
					// li r5,1024
					ctx.r5.s64 = 1024;
					// li r4,0
					ctx.r4.s64 = 0;
					// bl 0x82566898
					util_6898(ctx, base);
				}
			}
		loc_824657D0:
			// lbz r10,16(r27)
			ctx.r10.u64 = PPC_LOAD_U8(var_r27 + 16);
			// addi r30,r30,1
			var_r30 = (uint32_t)(var_r30 + 1);
			// addi r31,r31,12
			var_r31 = (uint32_t)(var_r31 + 12);
			// cmplw cr6,r30,r10
			// blt cr6,0x8246566c
			if (var_r30 < ctx.r10.u32) goto loc_8246566C;
			// b 0x824658b4
		} else {
		loc_824657E8:
			// cmplw cr6,r29,r28
			// beq cr6,0x824658b4
			if (var_r29 == var_r28) goto loc_824658B4;
			// lbz r11,97(r1)
			ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 97);
			// li r30,0
			var_r30 = 0;
			// lbz r10,113(r1)
			ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 113);
			// lwz r4,120(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
			// cmplwi cr6,r11,0
			// lwz r3,104(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
			// beq cr6,0x824658b4
			if (ctx.r11.u32 == 0) goto loc_824658B4;
			// li r31,0
			var_r31 = 0;
		loc_82465810:
			// cmplw cr6,r30,r10
			// bge cr6,0x82465874
			if (var_r30 < ctx.r10.u32) {
				// lbz r7,26(r27)
				ctx.r7.u64 = PPC_LOAD_U8(var_r27 + 26);
				// cmplwi cr6,r7,0
				// bne cr6,0x82465868
				if (ctx.r7.u32 == 0) {
					// mr r11,r3
					ctx.r11.u64 = ctx.r3.u64;
					// li r9,64
					ctx.r9.s64 = 64;
					// li r10,0
					ctx.r10.s64 = 0;
				loc_82465830:
					// dcbt r10,r3
					// addi r10,r10,128
					ctx.r10.s64 = ctx.r10.s64 + 128;
					// cmplwi cr6,r10,1024
					// blt cr6,0x82465830
					if (ctx.r10.u32 < 1024) goto loc_82465830;
					// subf r10,r3,r4
					ctx.r10.s64 = ctx.r4.s64 - ctx.r3.s64;
				loc_82465844:
					// lvx128 v0,r0,r11
					ea = (ctx.r11.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// addi r9,r9,-1
					ctx.r9.s64 = ctx.r9.s64 + -1;
					// lvx128 v13,r10,r11
					ea = (ctx.r10.u32 + ctx.r11.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// vaddfp v7,v13,v0
					ctx.fpscr.enableFlushMode();
					simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
					// cmplwi cr6,r9,0
					// stvx v7,r0,r11
					ea = (ctx.r11.u32) & ~0xF;
					simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
					// addi r11,r11,16
					ctx.r11.s64 = ctx.r11.s64 + 16;
					// bne cr6,0x82465844
					if (ctx.r9.u32 != 0) goto loc_82465844;
					// b 0x8246588c
					goto loc_8246588C;
				}
			loc_82465868:
				// li r5,1024
				ctx.r5.s64 = 1024;
				// bl 0x82568680
				util_8680(ctx, base);
				// b 0x8246588c
			} else {
			loc_82465874:
				// lbz r6,26(r27)
				ctx.r6.u64 = PPC_LOAD_U8(var_r27 + 26);
				// cmplwi cr6,r6,0
				// beq cr6,0x824658b4
				if (ctx.r6.u32 == 0) goto loc_824658B4;
				// li r5,1024
				ctx.r5.s64 = 1024;
				// li r4,0
				ctx.r4.s64 = 0;
				// bl 0x82566898
				util_6898(ctx, base);
			}
		loc_8246588C:
			// lbz r11,97(r1)
			ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 97);
			// addi r30,r30,1
			var_r30 = (uint32_t)(var_r30 + 1);
			// addi r31,r31,1024
			var_r31 = (uint32_t)(var_r31 + 1024);
			// lbz r10,113(r1)
			ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 113);
			// cmplw cr6,r30,r11
			// lwz r11,120(r1)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
			// add r4,r31,r11
			ctx.r4.u64 = var_r31 + ctx.r11.u64;
			// lwz r11,104(r1)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
			// add r3,r31,r11
			ctx.r3.u64 = var_r31 + ctx.r11.u64;
			// blt cr6,0x82465810
			if (var_r30 < ctx.r11.u32) goto loc_82465810;
		}
	loc_824658B4:
		// lwz r11,4(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x824658fc
		if (ctx.r11.s32 != 0) {
			// lwz r3,8(r26)
			ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 8);
			// cmplw cr6,r10,r3
			// bne cr6,0x824658fc
			if (ctx.r10.u32 != ctx.r3.u32) goto loc_824658FC;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r26)
			PPC_STORE_U32(var_r26 + 4, ctx.r11.u32);
			// bne cr6,0x824658fc
			if (ctx.r11.s32 != 0) goto loc_824658FC;
			// lbz r31,12(r26)
			var_r31 = (uint32_t)(PPC_LOAD_U8(var_r26 + 12));
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// stb r11,12(r26)
			PPC_STORE_U8(var_r26 + 12, ctx.r11.u8);
			// stw r11,8(r26)
			PPC_STORE_U32(var_r26 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_824658FC:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82465900:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x8243665c
	__restfpr_26(ctx, base);
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_5910_p39"))) PPC_WEAK_FUNC(phInst_5910_p39);
PPC_FUNC_IMPL(__imp__phInst_5910_p39) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32256
	// lis r10,-32256
	// lis r9,-32256
	// addi r11,r11,22808
	ctx.r11.s64 = ctx.r11.s64 + 22808;
	// addi r10,r10,22780
	ctx.r10.s64 = ctx.r10.s64 + 22780;
	// addi r9,r9,15792
	ctx.r9.s64 = ctx.r9.s64 + 15792;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_5938_p39"))) PPC_WEAK_FUNC(phInst_5938_p39);
PPC_FUNC_IMPL(__imp__phInst_5938_p39) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lbz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// cmplwi cr6,r10,0
	// beq cr6,0x82465960
	if (ctx.r10.u32 != 0) {
		// lbz r10,0(r10)
		ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// clrlwi r9,r11,24
		ctx.r9.u64 = ctx.r11.u32 & 0xFF;
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
		// cmplw cr6,r9,r8
		// bgt cr6,0x82465960
		if (ctx.r9.u32 > ctx.r8.u32) {
			// clrlwi r11,r11,24
			ctx.r11.u64 = ctx.r11.u32 & 0xFF;
			// li r3,0
			ctx.r3.s64 = 0;
			// rlwinm r10,r11,1,0,30
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// add r7,r11,r10
			ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
			// rlwinm r11,r7,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r6,r11,28
			ctx.r6.s64 = ctx.r11.s64 + 28;
			// stw r6,0(r4)
			PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r6.u32);
			// blr
			return;
		}
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82465960:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r11,28
	ctx.r6.s64 = ctx.r11.s64 + 28;
	// stw r6,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_5980_p39"))) PPC_WEAK_FUNC(phInst_5980_p39);
PPC_FUNC_IMPL(__imp__phInst_5980_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r27,r5
	var_r27 = ctx.r5.u32;
	// li r4,28
	ctx.r4.s64 = 28;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// lwz r10,20(r11)
	// bctrl
	VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x82465a24
	if (var_r30 != 0) {
		// lis r11,-32256
		// addi r31,r30,4
		var_r31 = (uint32_t)(var_r30 + 4);
		// addi r11,r11,22456
		ctx.r11.s64 = ctx.r11.s64 + 22456;
		// li r8,1
		ctx.r8.s64 = 1;
		// lis r10,-32256
		// lis r9,-32256
		// addi r10,r10,22808
		ctx.r10.s64 = ctx.r10.s64 + 22808;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// addi r9,r9,22780
		ctx.r9.s64 = ctx.r9.s64 + 22780;
		// stw r8,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r8.u32);
		// rotlwi r6,r10,0
		ctx.r6.u64 = ctx.r10.u32;
		// lwz r7,4(r29)
		ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 4);
		// mr r5,r28
		ctx.r5.u64 = var_r28;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// lwz r11,52(r6)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 52);
		// stw r7,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r7.u32);
		// stw r10,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r10.u32);
		// stw r9,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r9.u32);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmpwi cr6,r29,0
		// blt cr6,0x82465a34
		if ((int32_t)var_r29 < 0) {
			// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r9,12(r10)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 3, ctx, base);  // pattern-B slot 3 (byte +12)
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			return;
		}
		// stw r30,0(r27)
		PPC_STORE_U32(var_r27 + 0, var_r30);
		return;
	}
loc_82465A24:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
}

__attribute__((alias("__imp__phInst_5A58_p39"))) PPC_WEAK_FUNC(phInst_5A58_p39);
PPC_FUNC_IMPL(__imp__phInst_5A58_p39) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	// beq cr6,0x82465b6c
	if (ctx.r11.u32 != 0) {
		// clrlwi r10,r4,24
		ctx.r10.u64 = ctx.r4.u32 & 0xFF;
		// cmplwi cr6,r10,0
		// beq cr6,0x82465b6c
		if (ctx.r10.u32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// cmplw cr6,r11,r10
		// bne cr6,0x82465a9c
		if (ctx.r11.u32 == ctx.r10.u32) {
			// cmplwi cr6,r11,6
			// bgt cr6,0x82465b6c
			if (ctx.r11.u32 > 6) {
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
			// lis r11,-32256
			// stb r3,0(r5)
			PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r3.u8);
			// mr r3,r5
			ctx.r3.u64 = ctx.r5.u64;
			// addi r11,r11,22928
			ctx.r11.s64 = ctx.r11.s64 + 22928;
			// addi r11,r11,48
			ctx.r11.s64 = ctx.r11.s64 + 48;
			// stw r11,4(r5)
			PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
			// blr
			return;
		}
	loc_82465A9C:
		// cmplwi cr6,r10,1
		// bne cr6,0x82465ac8
		if (ctx.r10.u32 == 1) {
			// cmplwi cr6,r11,6
			// bgt cr6,0x82465b6c
			if (ctx.r11.u32 > 6) {
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
			// lis r11,-32256
			// stb r3,0(r5)
			PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r3.u8);
			// mr r3,r5
			ctx.r3.u64 = ctx.r5.u64;
			// addi r11,r11,22928
			ctx.r11.s64 = ctx.r11.s64 + 22928;
			// addi r10,r11,96
			ctx.r10.s64 = ctx.r11.s64 + 96;
			// stw r10,4(r5)
			PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
			// blr
			return;
		}
	loc_82465AC8:
		// cmpwi cr6,r11,1
		// beq cr6,0x82465b38
		if (ctx.r11.s32 != 1) {
			// cmpwi cr6,r11,2
			// beq cr6,0x82465b08
			if (ctx.r11.s32 != 2) {
				// cmpwi cr6,r11,4
				// bne cr6,0x82465b6c
				if (ctx.r11.s32 != 4) {
					// li r3,0
					ctx.r3.s64 = 0;
					// blr
					return;
				}
				// cmpwi cr6,r10,6
				// bne cr6,0x82465b6c
				if (ctx.r10.s32 != 6) {
					// li r3,0
					ctx.r3.s64 = 0;
					// blr
					return;
				}
				// lis r11,-32256
				// li r8,4
				ctx.r8.s64 = 4;
				// addi r11,r11,22928
				ctx.r11.s64 = ctx.r11.s64 + 22928;
				// mr r3,r5
				ctx.r3.u64 = ctx.r5.u64;
				// addi r9,r11,144
				ctx.r9.s64 = ctx.r11.s64 + 144;
				// stb r8,0(r5)
				PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r8.u8);
				// stw r9,4(r5)
				PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r9.u32);
				// blr
				return;
			}
		loc_82465B08:
			// cmpwi cr6,r10,4
			// beq cr6,0x82465b18
			if (ctx.r10.s32 != 4) {
				// cmpwi cr6,r10,6
				// bne cr6,0x82465b6c
				if (ctx.r10.s32 != 6) {
					// li r3,0
					ctx.r3.s64 = 0;
					// blr
					return;
				}
			}
		loc_82465B18:
			// lis r11,-32256
			// li r6,2
			ctx.r6.s64 = 2;
			// addi r11,r11,22928
			ctx.r11.s64 = ctx.r11.s64 + 22928;
			// mr r3,r5
			ctx.r3.u64 = ctx.r5.u64;
			// addi r7,r11,48
			ctx.r7.s64 = ctx.r11.s64 + 48;
			// stb r6,0(r5)
			PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r6.u8);
			// stw r7,4(r5)
			PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r7.u32);
			// blr
			return;
		}
	loc_82465B38:
		// cmpwi cr6,r10,2
		// beq cr6,0x82465b50
		if (ctx.r10.s32 != 2) {
			// cmpwi cr6,r10,4
			// beq cr6,0x82465b50
			if (ctx.r10.s32 == 4) {
				// lis r11,-32256
				// li r4,2
				ctx.r4.s64 = 2;
				// addi r11,r11,22928
				ctx.r11.s64 = ctx.r11.s64 + 22928;
				// mr r3,r5
				ctx.r3.u64 = ctx.r5.u64;
				// stb r4,0(r5)
				PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r4.u8);
				// stw r11,4(r5)
				PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
				// blr
				return;
			}
			// cmpwi cr6,r10,6
			// bne cr6,0x82465b6c
			if (ctx.r10.s32 != 6) {
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
		}
	loc_82465B50:
		// lis r11,-32256
		// li r4,2
		ctx.r4.s64 = 2;
		// addi r11,r11,22928
		ctx.r11.s64 = ctx.r11.s64 + 22928;
		// mr r3,r5
		ctx.r3.u64 = ctx.r5.u64;
		// stb r4,0(r5)
		PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r4.u8);
		// stw r11,4(r5)
		PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
		// blr
		return;
	}
loc_82465B6C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__ke_5B78"))) PPC_WEAK_FUNC(ke_5B78);
PPC_FUNC_IMPL(__imp__ke_5B78) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// lis r11,2
	ctx.r11.s64 = 131072;
	// ori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 | 1;
	// cmplw cr6,r3,r11
	// bgt cr6,0x82465cd4
	if (ctx.r3.u32 <= ctx.r11.u32) {
		// beq cr6,0x82465cb8
		if (!(ctx.cr6.eq)) {
			// addis r11,r3,-1
			ctx.r11.s64 = ctx.r3.s64 + -65536;
			// addic. r11,r11,-1
			ctx.xer.ca = ctx.r11.u32 > 0;
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// beq 0x82465c64
			if (ctx.r11.s32 != 0) {
				// cmplwi cr6,r11,1
				// beq cr6,0x82465c0c
				if (ctx.r11.u32 != 1) {
					// cmplwi cr6,r11,2
					// bne cr6,0x82465ce8
					if (ctx.r11.u32 != 2) {
						// lis r3,-32761
						// ori r3,r3,87
						ctx.r3.u64 = ctx.r3.u64 | 87;
						return;
					}
					// lis r11,-32162
					// mr r10,r13
					ctx.r10.u64 = ctx.r13.u64;
					// addi r3,r11,30832
					ctx.r3.s64 = ctx.r11.s64 + 30832;
					// lwz r11,4(r3)
					ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
					// cmpwi cr6,r11,0
					// beq cr6,0x82465c00
					if (ctx.r11.s32 != 0) {
						// lwz r9,8(r3)
						ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
						// cmplw cr6,r10,r9
						// bne cr6,0x82465c00
						if (ctx.r10.u32 != ctx.r9.u32) {
							// li r3,0
							ctx.r3.s64 = 0;
							return;
						}
						// addi r11,r11,-1
						ctx.r11.s64 = ctx.r11.s64 + -1;
						// cmpwi cr6,r11,0
						// stw r11,4(r3)
						PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
						// bne cr6,0x82465c00
						if (ctx.r11.s32 != 0) {
							// li r3,0
							ctx.r3.s64 = 0;
							return;
						}
						// lbz r31,12(r3)
						var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 12));
						// stb r11,12(r3)
						PPC_STORE_U8(ctx.r3.u32 + 12, ctx.r11.u8);
						// stw r11,8(r3)
						PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
						// bl 0x8258631c
						__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
						// mr r3,r31
						ctx.r3.u64 = var_r31;
						// bl 0x8258654c
						__imp__KfLowerIrql(ctx, base);
					}
				loc_82465C00:
					// li r3,0
					ctx.r3.s64 = 0;
					return;
				}
			loc_82465C0C:
				// bl 0x8258653c
				__imp__KeRaiseIrqlToDpcLevel(ctx, base);
				// lis r11,-32162
				// mr r29,r3
				var_r29 = ctx.r3.u32;
				// addi r31,r11,30832
				var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
				// mr r30,r13
				var_r30 = ctx.r13.u32;
				// lwz r11,4(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
				// cmpwi cr6,r11,0
				// beq cr6,0x82465c38
				if (ctx.r11.s32 != 0) {
					// lwz r10,8(r31)
					ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
					// cmplw cr6,r30,r10
					// beq cr6,0x82465ca4
					if (var_r30 == ctx.r10.u32) goto loc_82465CA4;
				}
			loc_82465C38:
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x8258658c
				__imp__KeTryToAcquireSpinLockAtRaisedIrql(ctx, base);
				// clrlwi r10,r3,24
				ctx.r10.u64 = ctx.r3.u32 & 0xFF;
				// cmplwi cr6,r10,0
				// bne cr6,0x82465c98
				if (ctx.r10.u32 != 0) goto loc_82465C98;
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
				// lis r3,-32768
				// ori r3,r3,16388
				ctx.r3.u64 = ctx.r3.u64 | 16388;
				return;
			}
		loc_82465C64:
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lis r11,-32162
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// addi r31,r11,30832
			var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// cmpwi cr6,r11,0
			// beq cr6,0x82465c90
			if (ctx.r11.s32 != 0) {
				// lwz r10,8(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r30,r10
				// beq cr6,0x82465ca4
				if (var_r30 == ctx.r10.u32) goto loc_82465CA4;
			}
		loc_82465C90:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		loc_82465C98:
			// stw r30,8(r31)
			PPC_STORE_U32(var_r31 + 8, var_r30);
			// stb r29,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		loc_82465CA4:
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		loc_82465CAC:
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
	loc_82465CB8:
		// lis r11,-32165
		// addi r11,r11,8992
		ctx.r11.s64 = ctx.r11.s64 + 8992;
		// addi r3,r11,4
		ctx.r3.s64 = ctx.r11.s64 + 4;
		// bl 0x82585e0c
		__imp__RtlEnterCriticalSection(ctx, base);
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_82465CD4:
	// addis r11,r3,-2
	ctx.r11.s64 = ctx.r3.s64 + -131072;
	// addic. r11,r11,-2
	ctx.xer.ca = ctx.r11.u32 > 1;
	ctx.r11.s64 = ctx.r11.s64 + -2;
	// beq 0x82465d14
	if (ctx.r11.s32 != 0) {
		// cmplwi cr6,r11,1
		// beq cr6,0x82465cf8
		if (ctx.r11.u32 != 1) {
		loc_82465CE8:
			// lis r3,-32761
			// ori r3,r3,87
			ctx.r3.u64 = ctx.r3.u64 | 87;
			return;
		}
	loc_82465CF8:
		// lis r11,-32165
		// addi r11,r11,8992
		ctx.r11.s64 = ctx.r11.s64 + 8992;
		// addi r3,r11,4
		ctx.r3.s64 = ctx.r11.s64 + 4;
		// bl 0x82585dfc
		__imp__RtlLeaveCriticalSection(ctx, base);
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_82465D14:
	// lis r11,-32165
	// addi r11,r11,8992
	ctx.r11.s64 = ctx.r11.s64 + 8992;
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// bl 0x8258625c
	__imp__RtlTryEnterCriticalSection(ctx, base);
	// cmpwi cr6,r3,0
	// bne cr6,0x82465cac
	if (ctx.r3.s32 != 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
	// lis r3,-32768
	// ori r3,r3,16388
	ctx.r3.u64 = ctx.r3.u64 | 16388;
	return;
}

__attribute__((alias("__imp__phInst_5D40_2hr"))) PPC_WEAK_FUNC(phInst_5D40_2hr);
PPC_FUNC_IMPL(__imp__phInst_5D40_2hr) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x82465d50
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82465D50:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInst_5D60_p39"))) PPC_WEAK_FUNC(phInst_5D60_p39);
PPC_FUNC_IMPL(__imp__phInst_5D60_p39) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x82465d70
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82465D70:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__ph_5D80"))) PPC_WEAK_FUNC(ph_5D80);
PPC_FUNC_IMPL(__imp__ph_5D80) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r10,r11,20
	ctx.r10.s64 = ctx.r11.s64 + 20;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r8,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r8.u32);
	// lwz r7,28(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// stw r7,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__util_5DA8"))) PPC_WEAK_FUNC(util_5DA8);
PPC_FUNC_IMPL(__imp__util_5DA8) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r11,r3,-8
	ctx.r11.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x82465db8
	if (ctx.r3.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_82465DB8:
	// addi r10,r11,20
	ctx.r10.s64 = ctx.r11.s64 + 20;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r8,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r8.u32);
	// lwz r7,28(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// stw r7,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_5DE0_2h"))) PPC_WEAK_FUNC(phInst_5DE0_2h);
PPC_FUNC_IMPL(__imp__phInst_5DE0_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r10,r11,23104
	ctx.r10.s64 = ctx.r11.s64 + 23104;
	// li r9,1
	ctx.r9.s64 = 1;
	// lis r8,0
	ctx.r8.s64 = 0;
	// li r29,0
	var_r29 = 0;
	// ori r11,r8,48000
	ctx.r11.u64 = ctx.r8.u64 | 48000;
	// stw r10,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r9.u32);
	// lbz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// clrlwi r3,r7,24
	ctx.r3.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r3,0
	// stb r7,8(r31)
	PPC_STORE_U8(var_r31 + 8, ctx.r7.u8);
	// lwz r6,16(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// stb r29,20(r31)
	PPC_STORE_U8(var_r31 + 20, (uint8_t)var_r29);
	// stw r11,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r11.u32);
	// stw r6,32(r31)
	PPC_STORE_U32(var_r31 + 32, ctx.r6.u32);
	// bne cr6,0x82465ecc
	if (ctx.r3.u32 == 0) {
		// addi r30,r4,4
		var_r30 = (uint32_t)(ctx.r4.s64 + 4);
		// lwz r10,0(r5)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
		// mr r3,r5
		ctx.r3.u64 = ctx.r5.u64;
		// li r9,256
		ctx.r9.s64 = 256;
		// lwz r7,4(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lwz r8,20(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
		// divwu r5,r11,r7
		ctx.r5.u32 = ctx.r7.u32 ? ctx.r11.u32 / ctx.r7.u32 : 0;
		// lbz r6,1(r30)
		ctx.r6.u64 = PPC_LOAD_U8(var_r30 + 1);
		// twllei r7,0
		if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
		// divwu r4,r9,r5
		ctx.r4.u32 = ctx.r5.u32 ? ctx.r9.u32 / ctx.r5.u32 : 0;
		// twllei r5,0
		if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
		// clrlwi r10,r4,16
		ctx.r10.u64 = ctx.r4.u32 & 0xFFFF;
		// mullw r9,r10,r6
		ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
		// rlwinm r11,r9,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r4,r11,128
		ctx.r4.s64 = ctx.r11.s64 + 128;
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
		// addi r8,r3,127
		ctx.r8.s64 = ctx.r3.s64 + 127;
		// cmplwi cr6,r30,0
		// rlwinm r11,r8,0,0,24
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
		// beq cr6,0x82465eb0
		if (var_r30 != 0) {
			// lwz r7,0(r30)
			ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 0);
			// addi r6,r31,12
			ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 12;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stw r7,0(r6)
			PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
			// lwz r5,4(r30)
			ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
			// stw r11,28(r31)
			PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
			// stb r29,21(r31)
			PPC_STORE_U8(var_r31 + 21, (uint8_t)var_r29);
			// stw r5,4(r6)
			PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r5.u32);
			return;
		}
	loc_82465EB0:
		// stw r29,12(r31)
		PPC_STORE_U32(var_r31 + 12, var_r29);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r29,16(r31)
		PPC_STORE_U32(var_r31 + 16, var_r29);
		// stw r11,28(r31)
		PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
		// stb r29,21(r31)
		PPC_STORE_U8(var_r31 + 21, (uint8_t)var_r29);
		return;
	}
loc_82465ECC:
	// lwz r11,12(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r29,12(r31)
	PPC_STORE_U32(var_r31 + 12, var_r29);
	// stw r29,16(r31)
	PPC_STORE_U32(var_r31 + 16, var_r29);
	// stb r29,21(r31)
	PPC_STORE_U8(var_r31 + 21, (uint8_t)var_r29);
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__ph_5EF0"))) PPC_WEAK_FUNC(ph_5EF0);
PPC_FUNC_IMPL(__imp__ph_5EF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// bne cr6,0x82465f18
	if (var_r30 == 0) {
		// addi r30,r31,12
		var_r30 = (uint32_t)(var_r31 + 12);
	}
loc_82465F18:
	// lis r9,0
	ctx.r9.s64 = 0;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 16);
	// ori r11,r9,48000
	ctx.r11.u64 = ctx.r9.u64 | 48000;
	// lbz r9,1(r30)
	ctx.r9.u64 = PPC_LOAD_U8(var_r30 + 1);
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// lbz r6,13(r31)
	ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 13);
	// divwu r8,r11,r10
	ctx.r8.u32 = ctx.r10.u32 ? ctx.r11.u32 / ctx.r10.u32 : 0;
	// li r10,256
	ctx.r10.s64 = 256;
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r7,r10,r8
	ctx.r7.u32 = ctx.r8.u32 ? ctx.r10.u32 / ctx.r8.u32 : 0;
	// divwu r8,r11,r3
	ctx.r8.u32 = ctx.r3.u32 ? ctx.r11.u32 / ctx.r3.u32 : 0;
	// clrlwi r4,r7,16
	ctx.r4.u64 = ctx.r7.u32 & 0xFFFF;
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 24);
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r11,r11,r7
	ctx.r11.u32 = ctx.r7.u32 ? ctx.r11.u32 / ctx.r7.u32 : 0;
	// divwu r8,r10,r8
	ctx.r8.u32 = ctx.r8.u32 ? ctx.r10.u32 / ctx.r8.u32 : 0;
	// divwu r10,r10,r11
	ctx.r10.u32 = ctx.r11.u32 ? ctx.r10.u32 / ctx.r11.u32 : 0;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// clrlwi r3,r10,16
	ctx.r3.u64 = ctx.r10.u32 & 0xFFFF;
	// clrlwi r10,r8,16
	ctx.r10.u64 = ctx.r8.u32 & 0xFFFF;
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lbz r4,21(r31)
	ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 21);
	// mullw r8,r6,r10
	ctx.r8.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// twllei r11,0
	if (ctx.r11.s32 == 0 || ctx.r11.u32 < 0u) __builtin_trap();
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// mullw r11,r3,r4
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r4.s32);
	// cmplw cr6,r9,r8
	// ble cr6,0x82465f98
	if (ctx.r9.u32 > ctx.r8.u32) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		// b 0x82465ffc
	} else {
	loc_82465F98:
		// clrlwi r10,r5,24
		ctx.r10.u64 = ctx.r5.u32 & 0xFF;
		// clrlwi r8,r10,31
		ctx.r8.u64 = ctx.r10.u32 & 0x1;
		// cmplwi cr6,r8,0
		// bne cr6,0x82465fbc
		if (ctx.r8.u32 == 0) {
			// cmplw cr6,r11,r9
			// ble cr6,0x82465fbc
			if (ctx.r11.u32 <= ctx.r9.u32) goto loc_82465FBC;
			// lis r3,-32768
			// ori r3,r3,65535
			ctx.r3.u64 = ctx.r3.u64 | 65535;
			// b 0x82465ffc
		} else {
		loc_82465FBC:
			// rlwinm r6,r10,0,30,30
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
			// cmplwi cr6,r6,0
			// beq cr6,0x82465ff0
			if (ctx.r6.u32 != 0) {
				// cmplwi cr6,r8,0
				// beq cr6,0x82465fd4
				if (ctx.r8.u32 != 0) {
					// li r11,0
					ctx.r11.s64 = 0;
				}
			loc_82465FD4:
				// subf r5,r11,r9
				ctx.r5.s64 = ctx.r9.s64 - ctx.r11.s64;
				// lwz r10,28(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
				// rlwinm r11,r11,2,0,29
				ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
				// rlwinm r5,r5,2,0,29
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
				// li r4,0
				ctx.r4.s64 = 0;
				// add r3,r11,r10
				ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
				// bl 0x8242fed0
				memset(ctx, base);
			}
		loc_82465FF0:
			// lbz r4,1(r30)
			ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 1);
			// li r3,0
			ctx.r3.s64 = 0;
			// stb r4,21(r31)
			PPC_STORE_U8(var_r31 + 21, ctx.r4.u8);
		}
	}
loc_82465FFC:
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_6018_g"))) PPC_WEAK_FUNC(phDemoWorld_6018_g);
PPC_FUNC_IMPL(__imp__phDemoWorld_6018_g) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x82466028
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_82466028:
	// b 0x82465ef0
	ph_5EF0(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_6030_2hr"))) PPC_WEAK_FUNC(phInst_6030_2hr);
PPC_FUNC_IMPL(__imp__phInst_6030_2hr) {
	PPC_FUNC_PROLOGUE();
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r10,0
	// bne cr6,0x82466074
	if (ctx.r10.u32 == 0) {
		// lis r9,0
		ctx.r9.s64 = 0;
		// lwz r6,8(r3)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// lbz r5,5(r3)
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
		// li r7,256
		ctx.r7.s64 = 256;
		// ori r8,r9,48000
		ctx.r8.u64 = ctx.r9.u64 | 48000;
		// twllei r6,0
		if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
		// divwu r3,r8,r6
		ctx.r3.u32 = ctx.r6.u32 ? ctx.r8.u32 / ctx.r6.u32 : 0;
		// divwu r11,r7,r3
		ctx.r11.u32 = ctx.r3.u32 ? ctx.r7.u32 / ctx.r3.u32 : 0;
		// twllei r3,0
		if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
		// clrlwi r9,r11,16
		ctx.r9.u64 = ctx.r11.u32 & 0xFFFF;
		// mullw r11,r9,r5
		ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
		// addi r8,r11,32
		ctx.r8.s64 = ctx.r11.s64 + 32;
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	}
loc_82466074:
	// addi r7,r11,36
	ctx.r7.s64 = ctx.r11.s64 + 36;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r7,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_6088_2h"))) PPC_WEAK_FUNC(phInst_6088_2h);
PPC_FUNC_IMPL(__imp__phInst_6088_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// li r4,36
	ctx.r4.s64 = 36;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,20(r11)
	// bctrl
	VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	// cmplwi cr6,r3,0
	// beq cr6,0x824660f8
	if (ctx.r3.u32 != 0) {
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x82465de0
		phInst_5DE0_2h(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x824660f8
		if (ctx.r3.u32 == 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
		// li r31,0
		var_r31 = 0;
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// lwz r8,20(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // pattern-B slot 5 (byte +20)
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		return;
	}
loc_824660F8:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
}

__attribute__((alias("__imp__ph_vt5A40_0_6108"))) PPC_WEAK_FUNC(ph_vt5A40_0_6108);
PPC_FUNC_IMPL(__imp__ph_vt5A40_0_6108) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32256
	// addi r11,r11,15792
	ctx.r11.s64 = ctx.r11.s64 + 15792;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_6118_fw"))) PPC_WEAK_FUNC(atSingleton_6118_fw);
PPC_FUNC_IMPL(__imp__atSingleton_6118_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,23136
	ctx.r11.s64 = ctx.r11.s64 + 23136;
	// addi r3,r31,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 4;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// bl 0x8246fd60
	atSingleton_FD60_2hr(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_6158_p39"))) PPC_WEAK_FUNC(phInst_6158_p39);
PPC_FUNC_IMPL(__imp__phInst_6158_p39) {
	PPC_FUNC_PROLOGUE();
	// lis r10,-32256
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r10,r10,23136
	ctx.r10.s64 = ctx.r10.s64 + 23136;
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// b 0x8246fc68
	ph_FC68_h(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt57D8_1_6170"))) PPC_WEAK_FUNC(ph_vt57D8_1_6170);
PPC_FUNC_IMPL(__imp__ph_vt57D8_1_6170) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// addi r9,r11,32
	ctx.r9.s64 = ctx.r11.s64 + 32;
	// li r8,14
	ctx.r8.s64 = 14;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_82466188:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x82466188
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82466188;
	// lis r10,-32256
	// li r9,1
	ctx.r9.s64 = 1;
	// li r6,32
	ctx.r6.s64 = 32;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// lfs f0,15788(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15788);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	// stfs f0,92(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 92, temp.u32);
	// sth r9,88(r11)
	PPC_STORE_U16(ctx.r11.u32 + 88, ctx.r9.u16);
	// stb r6,152(r11)
	PPC_STORE_U8(ctx.r11.u32 + 152, ctx.r6.u8);
	// sth r9,90(r11)
	PPC_STORE_U16(ctx.r11.u32 + 90, ctx.r9.u16);
	// lfs f13,15784(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15784);
	ctx.f13.f64 = double(temp.f32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stfs f13,96(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 96, temp.u32);
	// stw r10,148(r11)
	PPC_STORE_U32(ctx.r11.u32 + 148, ctx.r10.u32);
	// b 0x8246fca0
	phBoundBVH_FCA0_p45(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt57D8_3_61E0"))) PPC_WEAK_FUNC(ph_vt57D8_3_61E0);
PPC_FUNC_IMPL(__imp__ph_vt57D8_3_61E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,152(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 152);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	// beq cr6,0x82466220
	if (ctx.r10.u32 != 0) {
		// lis r3,-32768
		// ori r3,r3,65535
		ctx.r3.u64 = ctx.r3.u64 | 65535;
		// blr
		return;
	}
loc_82466220:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// lis r5,-32768
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,16388
	ctx.r5.u64 = ctx.r5.u64 | 16388;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,76(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 19, ctx, base);  // pattern-B slot 19 (byte +76)
	// cmplwi cr6,r3,0
	// bne cr6,0x82466220
	if (ctx.r3.u32 != 0) goto loc_82466220;
	// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r6,68(r7)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 17, ctx, base);  // pattern-B slot 17 (byte +68)
	// li r5,0
	ctx.r5.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r5,148(r31)
	PPC_STORE_U32(var_r31 + 148, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A60_0_6288"))) PPC_WEAK_FUNC(ph_vt5A60_0_6288);
PPC_FUNC_IMPL(__imp__ph_vt5A60_0_6288) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,23136
	ctx.r11.s64 = ctx.r11.s64 + 23136;
	// addi r3,r31,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 4;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x8246fc68
	ph_FC68_h(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// cmplwi cr6,r11,0
	// beq cr6,0x824662d8
	if (ctx.r11.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_824662D8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt57D8_12_62F8"))) PPC_WEAK_FUNC(ph_vt57D8_12_62F8);
PPC_FUNC_IMPL(__imp__ph_vt57D8_12_62F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lbz r10,152(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 152);
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82466368
	if (ctx.r9.u32 == 0) {
		// addi r10,r3,4
		ctx.r10.s64 = ctx.r3.s64 + 4;
		// lwz r9,0(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// cmplw cr6,r10,r9
		// beq cr6,0x8246632c
		if (ctx.r10.u32 != ctx.r9.u32) {
			// lwz r8,8(r10)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
			// subf r10,r8,r9
			ctx.r10.s64 = ctx.r9.s64 - ctx.r8.s64;
			// cmplwi cr6,r10,0
			// bne cr6,0x82466368
			if (ctx.r10.u32 != 0) {
				// lis r3,-32768
				// ori r3,r3,65535
				ctx.r3.u64 = ctx.r3.u64 | 65535;
				// blr
				return;
			}
		}
	loc_8246632C:
		// addi r10,r3,32
		ctx.r10.s64 = ctx.r3.s64 + 32;
		// li r9,14
		ctx.r9.s64 = 14;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_82466338:
		// lwz r7,0(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// stw r7,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bdnz 0x82466338
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_82466338;
		// lwz r6,0(r3)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lis r11,-32256
		// stb r5,153(r3)
		PPC_STORE_U8(ctx.r3.u32 + 153, ctx.r5.u8);
		// lfs f1,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
		ctx.f1.f64 = double(temp.f32);
		// lwz r4,64(r6)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 64);
		// mtctr r4
		ctx.ctr.u64 = ctx.r4.u64;
		// bctr
		PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
		return;
	}
loc_82466368:
	// lis r3,-32768
	// ori r3,r3,65535
	ctx.r3.u64 = ctx.r3.u64 | 65535;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt57D8_2_6378"))) PPC_WEAK_FUNC(ph_vt57D8_2_6378);
PPC_FUNC_IMPL(__imp__ph_vt57D8_2_6378) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// mr r26,r5
	var_r26 = ctx.r5.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x824663bc
	if (ctx.r11.s32 != 0) {
		// lwz r6,8(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r6
		// beq cr6,0x824663dc
		if (var_r30 == ctx.r6.u32) goto loc_824663DC;
	}
loc_824663BC:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r6,r30
	ctx.r6.u64 = var_r30;
	// mr r30,r29
	var_r30 = (uint32_t)(var_r29);
	// stw r6,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r6.u32);
	// stb r30,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// b 0x824663e0
	goto loc_824663E0;
loc_824663DC:
	// lbz r30,12(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
loc_824663E0:
	// addi r9,r28,4
	ctx.r9.s64 = (int64_t)(int32_t)var_r28 + 4;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// addi r11,r9,12
	ctx.r11.s64 = ctx.r9.s64 + 12;
	// stw r8,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r8.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r11,r10
	// beq cr6,0x824664f4
	if (ctx.r11.u32 != ctx.r10.u32) {
		// lwz r7,8(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// subf r11,r7,r10
		ctx.r11.s64 = ctx.r10.s64 - ctx.r7.s64;
		// cmplwi cr6,r11,0
		// beq cr6,0x824664f4
		if (ctx.r11.u32 == 0) goto loc_824664F4;
		// add r10,r7,r11
		ctx.r10.u64 = ctx.r7.u64 + ctx.r11.u64;
		// lwz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// cmplw cr6,r8,r10
		// beq cr6,0x82466438
		if (ctx.r8.u32 != ctx.r10.u32) {
			// lwz r7,4(r10)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
			// stw r7,4(r8)
			PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r7.u32);
			// lwz r6,4(r10)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
			// lwz r5,0(r10)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
			// stw r5,0(r6)
			PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r5.u32);
			// stw r10,4(r10)
			PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
			// stw r10,0(r10)
			PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
		}
	loc_82466438:
		// lwz r10,8(r9)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
		// li r8,22
		ctx.r8.s64 = 22;
		// add r10,r10,r11
		ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// lwz r4,4(r9)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
		// stw r4,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
		// stw r10,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
		// addi r9,r11,8
		ctx.r9.s64 = ctx.r11.s64 + 8;
		// lwz r3,4(r10)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// stw r10,0(r3)
		PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
		// mr r10,r27
		ctx.r10.u64 = var_r27;
		// mtctr r8
		ctx.ctr.u64 = ctx.r8.u64;
	loc_82466468:
		// lwz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// stw r8,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// bdnz 0x82466468
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_82466468;
		// lhz r7,88(r28)
		ctx.r7.u64 = PPC_LOAD_U16(var_r28 + 88);
		// lwz r6,12(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		// lwz r5,20(r11)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
		// twllei r7,0
		if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
		// divwu r4,r6,r7
		ctx.r4.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
		// lwz r3,24(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
		// stw r4,12(r11)
		PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r4.u32);
		// lhz r10,88(r28)
		ctx.r10.u64 = PPC_LOAD_U16(var_r28 + 88);
		// divwu r9,r5,r10
		ctx.r9.u32 = ctx.r10.u32 ? ctx.r5.u32 / ctx.r10.u32 : 0;
		// twllei r10,0
		if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
		// stw r9,20(r11)
		PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r9.u32);
		// lhz r8,88(r28)
		ctx.r8.u64 = PPC_LOAD_U16(var_r28 + 88);
		// divwu. r10,r3,r8
		ctx.r10.u32 = ctx.r8.u32 ? ctx.r3.u32 / ctx.r8.u32 : 0;
		// twllei r8,0
		if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
		// stw r10,24(r11)
		PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
		// bne 0x824664cc
		if (ctx.r10.s32 == 0) {
			// rotlwi r7,r4,0
			ctx.r7.u64 = ctx.r4.u32;
			// rotlwi r6,r9,0
			ctx.r6.u64 = ctx.r9.u32;
			// subf r5,r6,r7
			ctx.r5.s64 = ctx.r7.s64 - ctx.r6.s64;
			// stw r5,24(r11)
			PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r5.u32);
		}
	loc_824664CC:
		// clrlwi r4,r26,31
		ctx.r4.u64 = var_r26 & 0x1;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// cmplwi cr6,r4,0
		// li r4,24
		ctx.r4.s64 = 24;
		// beq cr6,0x82466540
		if (ctx.r4.u32 == 0) goto loc_82466540;
		// lwz r11,0(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
		// li r5,8
		ctx.r5.s64 = 8;
		// lwz r10,68(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
		// mtctr r10
		ctx.ctr.u64 = ctx.r10.u64;
		// b 0x82466550
	} else {
	loc_824664F4:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r8,0
		// beq cr6,0x82466530
		if (ctx.r8.s32 != 0) {
			// cmplw cr6,r11,r6
			// bne cr6,0x82466530
			if (ctx.r11.u32 != ctx.r6.u32) {
				// lis r3,-32761
				// ori r3,r3,14
				ctx.r3.u64 = ctx.r3.u64 | 14;
				return;
			}
			// addi r11,r8,-1
			ctx.r11.s64 = ctx.r8.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x82466530
			if (ctx.r11.s32 != 0) {
				// lis r3,-32761
				// ori r3,r3,14
				ctx.r3.u64 = ctx.r3.u64 | 14;
				return;
			}
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82466530:
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	loc_82466540:
		// lwz r9,0(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 0);
		// li r5,0
		ctx.r5.s64 = 0;
		// lwz r8,68(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 68);
		// mtctr r8
		ctx.ctr.u64 = ctx.r8.u64;
	}
loc_82466550:
	// bctrl
	ctx.lr = 0x82466554;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r11,r28,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 16;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r11,r10
	// beq cr6,0x82466574
	if (ctx.r11.u32 != ctx.r10.u32) {
		// lwz r7,8(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// subf r11,r7,r10
		ctx.r11.s64 = ctx.r10.s64 - ctx.r7.s64;
		// cmplwi cr6,r11,0
		// bne cr6,0x82466590
		if (ctx.r11.u32 != 0) goto loc_82466590;
	}
loc_82466574:
	// lwz r6,0(r28)
  // [ph4a] vtable load collapsed
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// lwz r11,68(r6)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r28, 17, ctx, base);  // pattern-B slot 17 (byte +68)
loc_82466590:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x824665d8
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x824665d8
		if (ctx.r10.u32 != ctx.r11.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x824665d8
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824665D8:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A60_19_65E8"))) PPC_WEAK_FUNC(ph_vt5A60_19_65E8);
PPC_FUNC_IMPL(__imp__ph_vt5A60_19_65E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82466628
	if (ctx.r11.s32 != 0) {
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r8
		// beq cr6,0x82466648
		if (var_r30 == ctx.r8.u32) goto loc_82466648;
	}
loc_82466628:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = var_r30;
	// mr r30,r29
	var_r30 = (uint32_t)(var_r29);
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// stb r30,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// b 0x8246664c
	goto loc_8246664C;
loc_82466648:
	// lbz r30,12(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
loc_8246664C:
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// cmplwi cr6,r27,0
	// stw r9,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r9.u32);
	// bne cr6,0x824666d0
	if (var_r27 == 0) {
		// addi r11,r28,4
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 4;
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r11,r10
		// beq cr6,0x82466678
		if (ctx.r11.u32 != ctx.r10.u32) {
			// lwz r11,8(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
			// subf r11,r11,r10
			ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
			// b 0x8246667c
		} else {
		loc_82466678:
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_8246667C:
		// mr r27,r11
		var_r27 = ctx.r11.u32;
		// cmplwi cr6,r11,0
		// bne cr6,0x824666d0
		if (ctx.r11.u32 != 0) goto loc_824666D0;
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r9,0
		// beq cr6,0x824666c4
		if (ctx.r9.s32 != 0) {
			// cmplw cr6,r11,r8
			// bne cr6,0x824666c4
			if (ctx.r11.u32 != ctx.r8.u32) {
				// li r3,0
				ctx.r3.s64 = 0;
				return;
			}
			// addi r11,r9,-1
			ctx.r11.s64 = ctx.r9.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x824666c4
			if (ctx.r11.s32 != 0) {
				// li r3,0
				ctx.r3.s64 = 0;
				return;
			}
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_824666C4:
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_824666D0:
	// addi r10,r28,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r28 + 4;
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + var_r27;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r9,r11
	// beq cr6,0x82466704
	if (ctx.r9.u32 != ctx.r11.u32) {
		// lwz r8,4(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r8,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
		// lwz r7,4(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r6,0(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// stw r6,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
		// stw r11,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
		// stw r11,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
	}
loc_82466704:
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// addi r9,r28,100
	ctx.r9.s64 = (int64_t)(int32_t)var_r28 + 100;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r8,12
	ctx.r8.s64 = 12;
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + var_r27;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r5,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r11.u32);
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_82466738:
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x82466738
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82466738;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// stb r11,96(r27)
	PPC_STORE_U8(var_r27 + 96, ctx.r11.u8);
	// lwz r9,68(r10)
	// bctrl
	VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x824667b0
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x824667b0
		if (ctx.r10.u32 != ctx.r9.u32) {
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x824667b0
		if (ctx.r11.s32 != 0) {
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824667B0:
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	return;
}

__attribute__((alias("__imp__util_67C0"))) PPC_WEAK_FUNC(util_67C0);
PPC_FUNC_IMPL(__imp__util_67C0) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// b 0x820c01b8
	rage_01B8(ctx, base);
	return;
}

__attribute__((alias("__imp__phDemoWorld_67D0_g"))) PPC_WEAK_FUNC(phDemoWorld_67D0_g);
PPC_FUNC_IMPL(__imp__phDemoWorld_67D0_g) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// b 0x820c02d0
	_locale_register(ctx, base);
	return;
}

__attribute__((alias("__imp__aud_67E0"))) PPC_WEAK_FUNC(aud_67E0);
PPC_FUNC_IMPL(__imp__aud_67E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32256
	// lis r10,-32256
	// lis r9,-32256
	// addi r30,r31,4
	var_r30 = (uint32_t)(var_r31 + 4);
	// addi r11,r11,23336
	ctx.r11.s64 = ctx.r11.s64 + 23336;
	// lwz r3,180(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
	// addi r10,r10,23296
	ctx.r10.s64 = ctx.r10.s64 + 23296;
	// addi r9,r9,23216
	ctx.r9.s64 = ctx.r9.s64 + 23216;
	// cmplwi cr6,r3,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r9.u32);
	// beq cr6,0x82466838
	if (ctx.r3.u32 != 0) {
		// bl 0x8246ae30
		aud_AE30(ctx, base);
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,180(r31)
		PPC_STORE_U32(var_r31 + 180, ctx.r11.u32);
	}
loc_82466838:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82460148
	atSingleton_0148_p39(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A60_57_6858"))) PPC_WEAK_FUNC(ph_vt5A60_57_6858);
PPC_FUNC_IMPL(__imp__ph_vt5A60_57_6858) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=176, manual
	// lwz r11,8(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplwi cr6,r11,255
	// ble cr6,0x824668a8
	if (ctx.r11.u32 > 255) {
		// cmpwi cr6,r11,-1
		// beq cr6,0x824668a8
		if (ctx.r11.s32 == -1) {
			// addi r3,r3,16
			ctx.r3.s64 = ctx.r3.s64 + 16;
			// bl 0x82466378
			ph_vt57D8_2_6378(ctx, base);
			// blr
			return;
		}
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// mr r11,r4
		ctx.r11.u64 = ctx.r4.u64;
		// li r9,22
		ctx.r9.s64 = 22;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_82466888:
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bdnz 0x82466888
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_82466888;
		// li r8,-1
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// stw r8,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	}
loc_824668A8:
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// bl 0x82466378
	ph_vt57D8_2_6378(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A60_40_68C0"))) PPC_WEAK_FUNC(ph_vt5A60_40_68C0);
PPC_FUNC_IMPL(__imp__ph_vt5A60_40_68C0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82466a20
	aud_6A20_wrap_6A20(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_20_68C8"))) PPC_WEAK_FUNC(ph_vt5A60_20_68C8);
PPC_FUNC_IMPL(__imp__ph_vt5A60_20_68C8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466a20
	aud_6A20_wrap_6A20(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_22_68D0"))) PPC_WEAK_FUNC(ph_vt5A60_22_68D0);
PPC_FUNC_IMPL(__imp__ph_vt5A60_22_68D0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466858
	ph_vt5A60_57_6858(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A68_63_68D8"))) PPC_WEAK_FUNC(ph_vt5A68_63_68D8);
PPC_FUNC_IMPL(__imp__ph_vt5A68_63_68D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,72(r11)
	// bctrl
	VCALL(ctx.r3.u32, 18, ctx, base);  // vtable slot 18 (byte +72)
	// cmpwi cr6,r3,0
	// blt cr6,0x82466918
	if (ctx.r3.s32 >= 0) {
		// lwz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lwz r8,16(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
		// stw r8,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r8.u32);
	}
loc_82466918:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A6C_63_6930"))) PPC_WEAK_FUNC(ph_vt5A6C_63_6930);
PPC_FUNC_IMPL(__imp__ph_vt5A6C_63_6930) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,72(r11)
	// bctrl
	VCALL(ctx.r3.u32, 18, ctx, base);  // vtable slot 18 (byte +72)
	// cmpwi cr6,r3,0
	// blt cr6,0x8246696c
	if (ctx.r3.s32 >= 0) {
		// lwz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// stw r31,16(r9)
		PPC_STORE_U32(ctx.r9.u32 + 16, var_r31);
	}
loc_8246696C:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A70_63_6980"))) PPC_WEAK_FUNC(ph_vt5A70_63_6980);
PPC_FUNC_IMPL(__imp__ph_vt5A70_63_6980) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,72(r11)
	// bctrl
	VCALL(ctx.r3.u32, 18, ctx, base);  // vtable slot 18 (byte +72)
	// cmpwi cr6,r3,0
	// blt cr6,0x824669c0
	if (ctx.r3.s32 >= 0) {
		// lwz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lwz r8,92(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 92);
		// stw r8,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r8.u32);
	}
loc_824669C0:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A88_63_69D8"))) PPC_WEAK_FUNC(ph_vt5A88_63_69D8);
PPC_FUNC_IMPL(__imp__ph_vt5A88_63_69D8) {
	PPC_FUNC_PROLOGUE();
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// addi r8,r3,16
	ctx.r8.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r10,0
	// beq cr6,0x82466a08
	if (ctx.r10.u32 != 0) {
		// addi r11,r8,32
		ctx.r11.s64 = ctx.r8.s64 + 32;
		// li r9,14
		ctx.r9.s64 = 14;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_824669F4:
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bdnz 0x824669f4
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_824669F4;
	}
loc_82466A08:
	// cmplwi cr6,r5,0
	// li r3,0
	ctx.r3.s64 = 0;
	// beqlr cr6
	if (ctx.r5.u32 == 0) return;
	// lbz r8,153(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 153);
	// stb r8,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r8.u8);
	// blr
	return;
}

__attribute__((alias("__imp__aud_6A20_wrap_6A20"))) PPC_WEAK_FUNC(aud_6A20_wrap_6A20);
PPC_FUNC_IMPL(__imp__aud_6A20_wrap_6A20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x824667e0
	aud_67E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A8C_63_6A50"))) PPC_WEAK_FUNC(ph_vt5A8C_63_6A50);
PPC_FUNC_IMPL(__imp__ph_vt5A8C_63_6A50) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r3,r31,16
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 16;
	// bl 0x824662f8
	ph_vt57D8_12_62F8(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x82466a80
	if (ctx.r3.s32 >= 0) {
		// li r11,1
		ctx.r11.s64 = 1;
		// sth r11,104(r31)
		PPC_STORE_U16(var_r31 + 104, ctx.r11.u16);
		// sth r11,106(r31)
		PPC_STORE_U16(var_r31 + 106, ctx.r11.u16);
	}
loc_82466A80:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A7C_63_6A98"))) PPC_WEAK_FUNC(ph_vt5A7C_63_6A98);
PPC_FUNC_IMPL(__imp__ph_vt5A7C_63_6A98) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_28
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82466adc
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r10
		// beq cr6,0x82466af0
		if (var_r29 == ctx.r10.u32) goto loc_82466AF0;
	}
loc_82466ADC:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82466AF0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// stfs f31,108(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r30 + 108, temp.u32);
	// lbz r11,52(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 52);
	// cmplwi cr6,r11,0
	// beq cr6,0x82466b34
	if (ctx.r11.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_82466B0C:
		// lwz r9,184(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 184);
		// mulli r10,r11,88
		ctx.r10.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(88));
		// add r7,r10,r9
		ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
		// lfs f0,108(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 108);
		ctx.f0.f64 = double(temp.f32);
		// addi r8,r11,1
		ctx.r8.s64 = ctx.r11.s64 + 1;
		// clrlwi r11,r8,24
		ctx.r11.u64 = ctx.r8.u32 & 0xFF;
		// stfs f0,40(r7)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r7.u32 + 40, temp.u32);
		// lbz r6,52(r30)
		ctx.r6.u64 = PPC_LOAD_U8(var_r30 + 52);
		// cmplw cr6,r11,r6
		// blt cr6,0x82466b0c
		if (ctx.r11.u32 < ctx.r6.u32) goto loc_82466B0C;
	}
loc_82466B34:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x82466b7c
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x82466b7c
		if (ctx.r10.u32 != ctx.r11.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82466b7c
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82466B7C:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A84_63_6B90"))) PPC_WEAK_FUNC(ph_vt5A84_63_6B90);
PPC_FUNC_IMPL(__imp__ph_vt5A84_63_6B90) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_26
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r28,r11,30832
	var_r28 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r31,r13
	var_r31 = ctx.r13.u32;
	// lwz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82466bd4
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 8);
		// cmplw cr6,r31,r10
		// beq cr6,0x82466be8
		if (var_r31 == ctx.r10.u32) goto loc_82466BE8;
	}
loc_82466BD4:
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r31,8(r28)
	PPC_STORE_U32(var_r28 + 8, var_r31);
	// stb r30,12(r28)
	PPC_STORE_U8(var_r28 + 12, (uint8_t)var_r30);
	// lwz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 4);
loc_82466BE8:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r28)
	PPC_STORE_U32(var_r28 + 4, ctx.r11.u32);
	// stfs f31,112(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r29 + 112, temp.u32);
	// lbz r11,52(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 52);
	// cmplwi cr6,r11,0
	// beq cr6,0x82466c88
	if (ctx.r11.u32 != 0) {
		// lis r11,-32248
		// li r30,0
		var_r30 = 0;
		// li r26,1
		var_r26 = 1;
		// lfd f31,-25408(r11)
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25408);
	loc_82466C10:
		// rlwinm r9,r30,3,0,28
		ctx.r9.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 3) & 0xFFFFFFF8;
		// lwz r10,184(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 184);
		// mulli r11,r30,88
		ctx.r11.s64 = static_cast<int64_t>(var_r30 * static_cast<uint64_t>(88));
		// lfs f2,112(r29)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r29 + 112);
		ctx.f2.f64 = double(temp.f32);
		// fmr f1,f31
		ctx.f1.f64 = var_f31;
		// add r9,r9,r29
		ctx.r9.u64 = ctx.r9.u64 + var_r29;
		// addi r8,r30,7
		ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 7;
		// add r31,r11,r10
		var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// rlwinm r7,r8,3,0,28
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
		// lbz r11,60(r9)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 60);
		// lwzx r27,r7,r29
		var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + var_r29));
		// stb r26,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r26);
		// stb r11,13(r31)
		PPC_STORE_U8(var_r31 + 13, ctx.r11.u8);
		// bl 0x82431308
		atSingleton_1308_g(ctx, base);
		// mr r5,r27
		ctx.r5.u64 = var_r27;
		// lwz r3,80(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 80);
		// addi r4,r31,16
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 16;
		// ori r11,r3,1
		ctx.r11.u64 = ctx.r3.u64 | 1;
		// addi r6,r30,1
		ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 1;
		// std r5,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
		// clrlwi r30,r6,24
		var_r30 = (uint32_t)(ctx.r6.u32 & 0xFF);
		// stw r11,80(r31)
		PPC_STORE_U32(var_r31 + 80, ctx.r11.u32);
		// lfd f0,80(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f13,f0
		ctx.f13.f64 = double(ctx.f0.s64);
		// fmul f12,f13,f1
		ctx.f12.f64 = ctx.f13.f64 * ctx.f1.f64;
		// fctidz f11,f12
		ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
		// stfiwx f11,0,r4
		PPC_STORE_U32(ctx.r4.u32, ctx.f11.u32);
		// lbz r10,52(r29)
		ctx.r10.u64 = PPC_LOAD_U8(var_r29 + 52);
		// cmplw cr6,r30,r10
		// blt cr6,0x82466c10
		if (var_r30 < ctx.r10.u32) goto loc_82466C10;
	}
loc_82466C88:
	// lwz r9,4(r28)
	ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x82466cd0
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x82466cd0
		if (ctx.r10.u32 != ctx.r11.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r28)
		PPC_STORE_U32(var_r28 + 4, ctx.r11.u32);
		// bne cr6,0x82466cd0
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r31,12(r28)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r28 + 12));
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// stb r11,12(r28)
		PPC_STORE_U8(var_r28 + 12, ctx.r11.u8);
		// stw r11,8(r28)
		PPC_STORE_U32(var_r28 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82466CD0:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A94_63_6CE0"))) PPC_WEAK_FUNC(ph_vt5A94_63_6CE0);
PPC_FUNC_IMPL(__imp__ph_vt5A94_63_6CE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=224, savegprlr_18
	// addi r31,r1,-224
	var_r31 = (uint32_t)(ctx.r1.s64 + -224);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// lbz r19,52(r28)
	var_r19 = (uint32_t)(PPC_LOAD_U8(var_r28 + 52));
	// rotlwi r11,r19,1
	ctx.r11.u64 = __builtin_rotateleft32(var_r19, 1);
	// add r11,r19,r11
	ctx.r11.u64 = var_r19 + ctx.r11.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r9,r10
	ctx.r9.s64 = static_cast<int64_t>(-ctx.r10.u64);
	// rlwinm r12,r9,0,0,27
	ctx.r12.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// bl 0x82569394
	_RtlCheckStack12(ctx, base);
	// lwz r8,0(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 0);
	// cmplwi cr6,r19,0
	// li r21,1
	var_r21 = 1;
	// stwux r8,r1,r12
	ea = ctx.r1.u32 + ctx.r12.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r1.u32 = ea;
	// addi r18,r1,80
	var_r18 = (uint32_t)(ctx.r1.s64 + 80);
	// beq cr6,0x82466eb4
	if (var_r19 != 0) {
		// lis r11,-32248
		// lis r7,1398
		ctx.r7.s64 = 91619328;
		// lis r6,0
		ctx.r6.s64 = 0;
		// li r26,0
		var_r26 = 0;
		// addi r29,r28,60
		var_r29 = (uint32_t)(var_r28 + 60);
		// lfd f31,-25408(r11)
		ctx.fpscr.disableFlushMode();
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25408);
		// addi r27,r18,4
		var_r27 = (uint32_t)(var_r18 + 4);
		// addi r23,r28,250
		var_r23 = (uint32_t)(var_r28 + 250);
		// mr r24,r19
		var_r24 = (uint32_t)(var_r19);
		// ori r20,r7,6641
		var_r20 = (uint32_t)(ctx.r7.u64 | 6641);
		// ori r22,r6,48000
		var_r22 = (uint32_t)(ctx.r6.u64 | 48000);
	loc_82466D58:
		// lwz r5,188(r28)
		ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 188);
		// lwz r4,-4(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + -4);
		// mullw r3,r4,r5
		ctx.r3.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
		// rlwinm r11,r3,8,0,23
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 8) & 0xFFFFFF00;
		// mulhwu r10,r11,r20
		ctx.r10.u64 = (uint64_t(ctx.r11.u32) * uint64_t(var_r20)) >> 32;
		// rlwinm r11,r10,22,10,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 22) & 0x3FFFFF;
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// sth r9,0(r23)
		PPC_STORE_U16(var_r23 + 0, ctx.r9.u16);
		// lbz r7,0(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 0);
		// cmplwi cr6,r7,1
		// li r7,3968
		ctx.r7.s64 = 3968;
		// beq cr6,0x82466d8c
		if (ctx.r7.u32 != 1) {
			// li r7,1920
			ctx.r7.s64 = 1920;
		}
	loc_82466D8C:
		// lbz r9,1(r29)
		ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 1);
		// cmplwi cr6,r9,0
		// bne cr6,0x82466da0
		if (ctx.r9.u32 == 0) {
			// li r11,8
			ctx.r11.s64 = 8;
			// b 0x82466e04
		} else {
		loc_82466DA0:
			// addi r6,r11,127
			ctx.r6.s64 = ctx.r11.s64 + 127;
			// rlwinm r11,r6,25,7,31
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
			// addi r5,r11,-1
			ctx.r5.s64 = ctx.r11.s64 + -1;
			// rlwinm r8,r11,7,0,24
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0xFFFFFF80;
			// and r4,r5,r11
			ctx.r4.u64 = ctx.r5.u64 & ctx.r11.u64;
			// cmplwi cr6,r4,0
			// beq cr6,0x82466dc8
			if (ctx.r4.u32 != 0) {
				// cntlzw r11,r11
				ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
				// subfic r3,r11,32
				ctx.xer.ca = ctx.r11.u32 <= 32;
				ctx.r3.s64 = 32 - ctx.r11.s64;
				// slw r11,r21,r3
				ctx.r11.u64 = ctx.r3.u8 & 0x20 ? 0 : (var_r21 << (ctx.r3.u8 & 0x3F));
			}
		loc_82466DC8:
			// cmplwi cr6,r11,2
			// bge cr6,0x82466dd8
			if (ctx.r11.u32 < 2) {
				// li r11,2
				ctx.r11.s64 = 2;
				// b 0x82466de4
			} else {
			loc_82466DD8:
				// cmplwi cr6,r11,8
				// ble cr6,0x82466de4
				if (ctx.r11.u32 <= 8) goto loc_82466DE4;
				// li r11,8
				ctx.r11.s64 = 8;
			}
		loc_82466DE4:
			// rlwinm r10,r11,7,0,24
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0xFFFFFF80;
			// cmplwi cr6,r9,255
			// add r10,r10,r8
			ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
			// beq cr6,0x82466dfc
			if (ctx.r9.u32 != 255) {
				// rlwinm r9,r9,7,0,24
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
				// add r10,r9,r10
				ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
			}
		loc_82466DFC:
			// cmplw cr6,r10,r7
			// ble cr6,0x82466e08
			if (ctx.r10.u32 <= ctx.r7.u32) goto loc_82466E08;
		}
	loc_82466E04:
		// mr r10,r7
		ctx.r10.u64 = ctx.r7.u64;
	loc_82466E08:
		// lwz r9,-4(r29)
		ctx.r9.u64 = PPC_LOAD_U32(var_r29 + -4);
		// fmr f1,f31
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = var_f31;
		// stw r9,-4(r27)
		PPC_STORE_U32(var_r27 + -4, ctx.r9.u32);
		// lbz r7,0(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 0);
		// stw r10,0(r27)
		PPC_STORE_U32(var_r27 + 0, ctx.r10.u32);
		// stb r11,5(r27)
		PPC_STORE_U8(var_r27 + 5, ctx.r11.u8);
		// stb r7,4(r27)
		PPC_STORE_U8(var_r27 + 4, ctx.r7.u8);
		// lwz r11,184(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 184);
		// lbz r6,248(r28)
		ctx.r6.u64 = PPC_LOAD_U8(var_r28 + 248);
		// add r5,r26,r11
		ctx.r5.u64 = var_r26 + ctx.r11.u64;
		// stw r6,84(r5)
		PPC_STORE_U32(ctx.r5.u32 + 84, ctx.r6.u32);
		// lfs f2,112(r28)
		temp.u32 = PPC_LOAD_U32(var_r28 + 112);
		ctx.f2.f64 = double(temp.f32);
		// lwz r10,184(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 184);
		// lbz r11,0(r29)
		ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 0);
		// add r30,r26,r10
		var_r30 = (uint32_t)(var_r26 + ctx.r10.u64);
		// lwz r25,-4(r29)
		var_r25 = (uint32_t)(PPC_LOAD_U32(var_r29 + -4));
		// stb r21,12(r30)
		PPC_STORE_U8(var_r30 + 12, (uint8_t)var_r21);
		// stb r11,13(r30)
		PPC_STORE_U8(var_r30 + 13, ctx.r11.u8);
		// bl 0x82431308
		atSingleton_1308_g(ctx, base);
		// mr r4,r25
		ctx.r4.u64 = var_r25;
		// lwz r11,80(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 80);
		// addi r3,r30,16
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 16;
		// ori r10,r11,1
		ctx.r10.u64 = ctx.r11.u64 | 1;
		// addi r24,r24,-1
		var_r24 = (uint32_t)(var_r24 + -1);
		// addi r23,r23,2
		var_r23 = (uint32_t)(var_r23 + 2);
		// std r4,80(r31)
		PPC_STORE_U64(var_r31 + 80, ctx.r4.u64);
		// addi r27,r27,12
		var_r27 = (uint32_t)(var_r27 + 12);
		// addi r29,r29,8
		var_r29 = (uint32_t)(var_r29 + 8);
		// stw r10,80(r30)
		PPC_STORE_U32(var_r30 + 80, ctx.r10.u32);
		// cmplwi cr6,r24,0
		ctx.cr6.compare<uint32_t>(var_r24, 0, ctx.xer);
		// lfd f0,80(r31)
		ctx.fpscr.disableFlushMode();
		ctx.f0.u64 = PPC_LOAD_U64(var_r31 + 80);
		// fcfid f13,f0
		ctx.f13.f64 = double(ctx.f0.s64);
		// fmul f12,f13,f1
		ctx.f12.f64 = ctx.f13.f64 * ctx.f1.f64;
		// fctidz f11,f12
		ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
		// stfiwx f11,0,r3
		PPC_STORE_U32(ctx.r3.u32, ctx.f11.u32);
		// lwz r11,184(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 184);
		// add r11,r26,r11
		ctx.r11.u64 = var_r26 + ctx.r11.u64;
		// addi r26,r26,88
		var_r26 = (uint32_t)(var_r26 + 88);
		// lwz r9,80(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 80);
		// stw r22,32(r11)
		PPC_STORE_U32(ctx.r11.u32 + 32, var_r22);
		// ori r8,r9,2
		ctx.r8.u64 = ctx.r9.u64 | 2;
		// stw r8,80(r11)
		PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r8.u32);
		// bne cr6,0x82466d58
		if (!ctx.cr6.eq) goto loc_82466D58;
	}
loc_82466EB4:
	// lbz r7,248(r28)
	ctx.r7.u64 = PPC_LOAD_U8(var_r28 + 248);
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r6,r7,0,30,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r6,0
	// beq cr6,0x82466ecc
	if (ctx.r6.u32 != 0) {
		// mr r5,r21
		ctx.r5.u64 = var_r21;
	}
loc_82466ECC:
	// addi r6,r28,180
	ctx.r6.s64 = (int64_t)(int32_t)var_r28 + 180;
	// mr r4,r18
	ctx.r4.u64 = var_r18;
	// mr r3,r19
	ctx.r3.u64 = var_r19;
	// bl 0x8246ac18
	aud_AC18(ctx, base);
	// addi r1,r31,224
	ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 224;
	// lfd f31,-128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x8242f8c0
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_58_6EE8"))) PPC_WEAK_FUNC(ph_vt5A60_58_6EE8);
PPC_FUNC_IMPL(__imp__ph_vt5A60_58_6EE8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r29,264
	var_r31 = (uint32_t)(var_r29 + 264);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x82466f20
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82466f30
		if (var_r30 == ctx.r10.u32) goto loc_82466F30;
	}
loc_82466F20:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
loc_82466F30:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// stw r9,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r9.u32);
	// lwz r3,180(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
	// bl 0x8246aef8
	ph_AEF8(ctx, base);
	// li r28,0
	var_r28 = 0;
	// cmpwi cr6,r3,0
	// beq cr6,0x82466f90
	if (ctx.r3.s32 != 0) {
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b708
		ph_B708(ctx, base);
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b7a0
		ke_B7A0(ctx, base);
		// lbz r8,52(r29)
		ctx.r8.u64 = PPC_LOAD_U8(var_r29 + 52);
		// cmplwi cr6,r8,0
		// beq cr6,0x82466f90
		if (ctx.r8.u32 == 0) goto loc_82466F90;
		// mr r30,r28
		var_r30 = (uint32_t)(var_r28);
	loc_82466F70:
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b408
		phInst_B408_p39(ctx, base);
		// addi r7,r30,1
		ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 1;
		// lbz r6,52(r29)
		ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 52);
		// clrlwi r30,r7,24
		var_r30 = (uint32_t)(ctx.r7.u32 & 0xFF);
		// cmplw cr6,r30,r6
		// blt cr6,0x82466f70
		if (var_r30 < ctx.r6.u32) goto loc_82466F70;
	}
loc_82466F90:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82466fd4
	if (ctx.r11.s32 != 0) {
		// lwz r5,8(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r5
		// bne cr6,0x82466fd4
		if (ctx.r10.u32 != ctx.r5.u32) {
			// addi r3,r29,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 16;
			// bl 0x824661e0
			ph_vt57D8_3_61E0(ctx, base);
			return;
		}
		// addic. r11,r11,-1
		ctx.xer.ca = ctx.r11.u32 > 0;
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne 0x82466fd4
		if (ctx.r11.s32 != 0) {
			// addi r3,r29,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 16;
			// bl 0x824661e0
			ph_vt57D8_3_61E0(ctx, base);
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// stw r28,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r28);
		// stb r28,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82466FD4:
	// addi r3,r29,16
	ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 16;
	// bl 0x824661e0
	ph_vt57D8_3_61E0(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_63_6FE8"))) PPC_WEAK_FUNC(ph_vt5A60_63_6FE8);
PPC_FUNC_IMPL(__imp__ph_vt5A60_63_6FE8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=160, savegprlr_25
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r25,0
	var_r25 = 0;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// mr r26,r25
	var_r26 = (uint32_t)(var_r25);
	// addi r30,r29,264
	var_r30 = (uint32_t)(var_r29 + 264);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r31,r13
	var_r31 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x8246702c
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 8);
		// cmplw cr6,r31,r10
		// beq cr6,0x8246703c
		if (var_r31 == ctx.r10.u32) goto loc_8246703C;
	}
loc_8246702C:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r28,12(r30)
	PPC_STORE_U8(var_r30 + 12, (uint8_t)var_r28);
	// stw r31,8(r30)
	PPC_STORE_U32(var_r30 + 8, var_r31);
loc_8246703C:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// stw r9,4(r30)
	PPC_STORE_U32(var_r30 + 4, ctx.r9.u32);
	// lwz r3,180(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
	// bl 0x8246b860
	phInst_B860_p39(ctx, base);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// cmpwi cr6,r28,0
	// bne cr6,0x82467094
	if ((int32_t)var_r28 == 0) {
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b708
		ph_B708(ctx, base);
		// mr r26,r3
		var_r26 = ctx.r3.u32;
		// cmpwi cr6,r26,0
		// bge cr6,0x8246708c
		if ((int32_t)var_r26 < 0) {
			// lwz r11,4(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
			// mr r10,r13
			ctx.r10.u64 = ctx.r13.u64;
			// cmpwi cr6,r11,0
			// beq cr6,0x82467148
			if (ctx.r11.s32 == 0) {
				// mr r3,r26
				ctx.r3.u64 = var_r26;
				return;
			}
			// lwz r8,8(r30)
			ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 8);
			// cmplw cr6,r10,r8
			ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
			// b 0x8246711c
			goto loc_8246711C;
		}
	loc_8246708C:
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b7a0
		ke_B7A0(ctx, base);
	}
loc_82467094:
	// lbz r7,52(r29)
	ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 52);
	// cmplwi cr6,r7,0
	// beq cr6,0x824670f0
	if (ctx.r7.u32 != 0) {
		// mr r31,r25
		var_r31 = (uint32_t)(var_r25);
	loc_824670A4:
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x8246b570
		phInst_B570_p39(ctx, base);
		// mr r26,r3
		var_r26 = ctx.r3.u32;
		// cmpwi cr6,r26,0
		// blt cr6,0x824670f0
		if ((int32_t)var_r26 < 0) goto loc_824670F0;
		// lwz r5,88(r1)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		// rlwinm r11,r31,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 3) & 0xFFFFFFF8;
		// lwz r3,84(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// addi r6,r31,1
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 1;
		// add r11,r11,r27
		ctx.r11.u64 = ctx.r11.u64 + var_r27;
		// rlwinm r4,r5,7,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 7) & 0xFFFFFF80;
		// clrlwi r31,r6,24
		var_r31 = (uint32_t)(ctx.r6.u32 & 0xFF);
		// stw r3,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r3.u32);
		// sth r4,4(r11)
		PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r4.u16);
		// lbz r8,52(r29)
		ctx.r8.u64 = PPC_LOAD_U8(var_r29 + 52);
		// cmplw cr6,r31,r8
		// blt cr6,0x824670a4
		if (var_r31 < ctx.r8.u32) goto loc_824670A4;
	}
loc_824670F0:
	// cmpwi cr6,r28,0
	// bne cr6,0x82467104
	if ((int32_t)var_r28 == 0) {
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b9e0
		ph_B9E0(ctx, base);
		// mr r26,r3
		var_r26 = ctx.r3.u32;
	}
loc_82467104:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82467148
	if (ctx.r11.s32 != 0) {
		// lwz r7,8(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 8);
		// cmplw cr6,r10,r7
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, ctx.xer);
	loc_8246711C:
		// bne cr6,0x82467148
		if (!ctx.cr6.eq) {
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			return;
		}
		// addic. r11,r11,-1
		ctx.xer.ca = ctx.r11.u32 > 0;
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,4(r30)
		PPC_STORE_U32(var_r30 + 4, ctx.r11.u32);
		// bne 0x82467148
		if (ctx.r11.s32 != 0) {
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			return;
		}
		// lbz r31,12(r30)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r30 + 12));
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stw r25,8(r30)
		PPC_STORE_U32(var_r30 + 8, var_r25);
		// stb r25,12(r30)
		PPC_STORE_U8(var_r30 + 12, (uint8_t)var_r25);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82467148:
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	return;
}

__attribute__((alias("__imp__ph_vt5A64_63_7158"))) PPC_WEAK_FUNC(ph_vt5A64_63_7158);
PPC_FUNC_IMPL(__imp__ph_vt5A64_63_7158) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// addi r31,r30,264
	var_r31 = (uint32_t)(var_r30 + 264);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x82467194
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r10
		// beq cr6,0x824671a4
		if (var_r29 == ctx.r10.u32) goto loc_824671A4;
	}
loc_82467194:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r27,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
loc_824671A4:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	// addi r9,r30,116
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 116;
	// mr r11,r28
	ctx.r11.u64 = var_r28;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// li r9,12
	ctx.r9.s64 = 12;
	// stw r8,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r8.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_824671C4:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x824671c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_824671C4;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82467220
	if (ctx.r11.s32 != 0) {
		// lwz r6,8(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r6
		// bne cr6,0x82467220
		if (ctx.r10.u32 != ctx.r6.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addic. r11,r11,-1
		ctx.xer.ca = ctx.r11.u32 > 0;
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne 0x82467220
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// li r11,0
		ctx.r11.s64 = 0;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82467220:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A60_23_7230"))) PPC_WEAK_FUNC(ph_vt5A60_23_7230);
PPC_FUNC_IMPL(__imp__ph_vt5A60_23_7230) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466ee8
	ph_vt5A60_58_6EE8(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_32_7238"))) PPC_WEAK_FUNC(ph_vt5A60_32_7238);
PPC_FUNC_IMPL(__imp__ph_vt5A60_32_7238) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466a50
	ph_vt5A8C_63_6A50(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_34_7240"))) PPC_WEAK_FUNC(ph_vt5A60_34_7240);
PPC_FUNC_IMPL(__imp__ph_vt5A60_34_7240) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466a98
	ph_vt5A7C_63_6A98(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_36_7248"))) PPC_WEAK_FUNC(ph_vt5A60_36_7248);
PPC_FUNC_IMPL(__imp__ph_vt5A60_36_7248) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466b90
	ph_vt5A84_63_6B90(ctx, base);
	return;
}

__attribute__((alias("__imp__atSingleton_7250_p42"))) PPC_WEAK_FUNC(atSingleton_7250_p42);
PPC_FUNC_IMPL(__imp__atSingleton_7250_p42) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 68);
	// lbz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 12);
	// mulli r11,r10,100
	ctx.r11.s64 = static_cast<int64_t>(ctx.r10.u64 * static_cast<uint64_t>(100));
	// mulli r10,r9,88
	ctx.r10.s64 = static_cast<int64_t>(ctx.r9.u64 * static_cast<uint64_t>(88));
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r8,r11,280
	ctx.r8.s64 = ctx.r11.s64 + 280;
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ke_7278"))) PPC_WEAK_FUNC(ke_7278);
PPC_FUNC_IMPL(__imp__ke_7278) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	// FRAME: size=192, savegprlr_22
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// li r23,0
	var_r23 = 0;
	// mr r24,r23
	var_r24 = (uint32_t)(var_r23);
	// lbz r11,52(r28)
	ctx.r11.u64 = PPC_LOAD_U8(var_r28 + 52);
	// cmplwi cr6,r11,0
	// beq cr6,0x824672c8
	if (ctx.r11.u32 != 0) {
		// mr r31,r23
		var_r31 = (uint32_t)(var_r23);
	loc_824672A0:
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// lwz r3,180(r28)
		ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
		// bl 0x8246b050
		phInst_B050_p39(ctx, base);
		// cmpwi cr6,r3,0
		// beq cr6,0x8246732c
		if (ctx.r3.s32 == 0) {
			// li r3,1
			ctx.r3.s64 = 1;
			return;
		}
		// addi r10,r31,1
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 1;
		// lbz r9,52(r28)
		ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 52);
		// clrlwi r31,r10,24
		var_r31 = (uint32_t)(ctx.r10.u32 & 0xFF);
		// cmplw cr6,r31,r9
		// blt cr6,0x824672a0
		if (var_r31 < ctx.r9.u32) goto loc_824672A0;
	}
loc_824672C8:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r27,r11,30832
	var_r27 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r31,r13
	var_r31 = ctx.r13.u32;
	// lwz r11,4(r27)
	ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x824672f4
	if (ctx.r11.s32 != 0) {
		// lwz r8,8(r27)
		ctx.r8.u64 = PPC_LOAD_U32(var_r27 + 8);
		// cmplw cr6,r31,r8
		// beq cr6,0x82467308
		if (var_r31 == ctx.r8.u32) goto loc_82467308;
	}
loc_824672F4:
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r31,8(r27)
	PPC_STORE_U32(var_r27 + 8, var_r31);
	// stb r30,12(r27)
	PPC_STORE_U8(var_r27 + 12, (uint8_t)var_r30);
	// lwz r11,4(r27)
	ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
loc_82467308:
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// addi r22,r28,20
	var_r22 = (uint32_t)(var_r28 + 20);
	// stw r9,4(r27)
	PPC_STORE_U32(var_r27 + 4, ctx.r9.u32);
	// lwz r11,0(r22)
	ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 0);
	// cmplw cr6,r22,r11
	// beq cr6,0x82467338
	if (var_r22 != ctx.r11.u32) {
		// lwz r7,8(r22)
		ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 8);
		// subf r11,r7,r11
		ctx.r11.s64 = ctx.r11.s64 - ctx.r7.s64;
		// b 0x8246733c
		goto loc_8246733C;
	loc_8246732C:
		// li r3,1
		ctx.r3.s64 = 1;
		return;
	}
loc_82467338:
	// mr r11,r23
	ctx.r11.u64 = var_r23;
loc_8246733C:
	// mr r29,r11
	var_r29 = ctx.r11.u32;
	// cmplwi cr6,r11,0
	// beq cr6,0x82467564
	if (ctx.r11.u32 != 0) {
		// li r25,4
		var_r25 = 4;
	loc_8246734C:
		// lwz r6,0(r29)
		ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 0);
		// cmplw cr6,r6,r29
		// beq cr6,0x82467564
		if (ctx.r6.u32 == var_r29) goto loc_82467564;
		// lwz r11,8(r22)
		ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 8);
		// lwzx r10,r11,r29
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r29);
		// cmplw cr6,r22,r10
		// subf r26,r11,r10
		var_r26 = (uint32_t)(ctx.r10.s64 - ctx.r11.s64);
		// bne cr6,0x82467370
		if (var_r22 == ctx.r10.u32) {
			// mr r26,r23
			var_r26 = (uint32_t)(var_r23);
		}
	loc_82467370:
		// lbz r5,96(r29)
		ctx.r5.u64 = PPC_LOAD_U8(var_r29 + 96);
		// cmplwi cr6,r5,0
		// beq cr6,0x824673f8
		if (ctx.r5.u32 != 0) {
			// lbz r4,52(r28)
			ctx.r4.u64 = PPC_LOAD_U8(var_r28 + 52);
			// mr r30,r23
			var_r30 = (uint32_t)(var_r23);
			// cmplwi cr6,r4,0
			// beq cr6,0x824673e0
			if (ctx.r4.u32 != 0) {
				// mr r31,r23
				var_r31 = (uint32_t)(var_r23);
			loc_82467390:
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// lwz r5,8(r29)
				ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 8);
				// lwz r3,180(r28)
				ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
				// bl 0x8246aff0
				ke_AFF0_h(ctx, base);
				// cmpwi cr6,r3,0
				// beq cr6,0x824673c0
				if (ctx.r3.s32 != 0) {
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// lwz r3,180(r28)
					ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
					// bl 0x8246b488
					ke_B488_h(ctx, base);
					// rlwinm r3,r3,0,28,28
					ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x8;
					// cmplwi cr6,r3,0
					// beq cr6,0x824673d8
					if (ctx.r3.u32 == 0) goto loc_824673D8;
				}
			loc_824673C0:
				// addi r11,r31,1
				ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 1;
				// lbz r10,52(r28)
				ctx.r10.u64 = PPC_LOAD_U8(var_r28 + 52);
				// clrlwi r31,r11,24
				var_r31 = (uint32_t)(ctx.r11.u32 & 0xFF);
				// cmplw cr6,r31,r10
				// blt cr6,0x82467390
				if (var_r31 < ctx.r10.u32) goto loc_82467390;
				// b 0x824673dc
				goto loc_824673DC;
			loc_824673D8:
				// li r30,1
				var_r30 = 1;
			loc_824673DC:
				// lwz r9,4(r27)
				ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 4);
			}
		loc_824673E0:
			// clrlwi r8,r30,24
			ctx.r8.u64 = var_r30 & 0xFF;
			// cmplwi cr6,r8,0
			// bne cr6,0x824673fc
			if (ctx.r8.u32 != 0) goto loc_824673FC;
			// stb r25,96(r29)
			PPC_STORE_U8(var_r29 + 96, (uint8_t)var_r25);
			// lwz r9,4(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 4);
			// b 0x824673fc
		} else {
		loc_824673F8:
			// li r24,1
			var_r24 = 1;
		}
	loc_824673FC:
		// lbz r7,96(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 96);
		// cmplwi cr6,r7,4
		// bne cr6,0x82467558
		if (ctx.r7.u32 == 4) {
			// cmplwi cr6,r26,0
			// bne cr6,0x8246742c
			if (var_r26 == 0) {
				// lbz r11,168(r28)
				ctx.r11.u64 = PPC_LOAD_U8(var_r28 + 168);
				// clrlwi r6,r11,31
				ctx.r6.u64 = ctx.r11.u32 & 0x1;
				// cmplwi cr6,r6,0
				// beq cr6,0x8246742c
				if (ctx.r6.u32 == 0) goto loc_8246742C;
				// rlwinm r5,r11,0,27,27
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
				// cmplwi cr6,r5,0
				// beq cr6,0x82467558
				if (ctx.r5.u32 == 0) goto loc_82467558;
			}
		loc_8246742C:
			// mr r11,r13
			ctx.r11.u64 = ctx.r13.u64;
			// cmpwi cr6,r9,0
			// beq cr6,0x82467474
			if (ctx.r9.s32 != 0) {
				// lwz r4,8(r27)
				ctx.r4.u64 = PPC_LOAD_U32(var_r27 + 8);
				// cmplw cr6,r11,r4
				// bne cr6,0x82467474
				if (ctx.r11.u32 != ctx.r4.u32) goto loc_82467474;
				// addi r11,r9,-1
				ctx.r11.s64 = ctx.r9.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r27)
				PPC_STORE_U32(var_r27 + 4, ctx.r11.u32);
				// bne cr6,0x82467474
				if (ctx.r11.s32 != 0) goto loc_82467474;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
				// lbz r31,12(r27)
				var_r31 = (uint32_t)(PPC_LOAD_U8(var_r27 + 12));
				// mr r3,r27
				ctx.r3.u64 = var_r27;
				// stb r11,12(r27)
				PPC_STORE_U8(var_r27 + 12, ctx.r11.u8);
				// stw r11,8(r27)
				PPC_STORE_U32(var_r27 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_82467474:
			// addi r31,r28,264
			var_r31 = (uint32_t)(var_r28 + 264);
			// mr r10,r13
			ctx.r10.u64 = ctx.r13.u64;
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// cmpwi cr6,r11,0
			// beq cr6,0x824674bc
			if (ctx.r11.s32 != 0) {
				// lwz r3,8(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r10,r3
				// bne cr6,0x824674bc
				if (ctx.r10.u32 != ctx.r3.u32) goto loc_824674BC;
				// addic. r11,r11,-1
				ctx.xer.ca = ctx.r11.u32 > 0;
				ctx.r11.s64 = ctx.r11.s64 + -1;
				// stw r11,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
				// bne 0x824674bc
				if (ctx.r11.s32 != 0) goto loc_824674BC;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lbz r30,12(r31)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
				// stw r23,8(r31)
				PPC_STORE_U32(var_r31 + 8, var_r23);
				// stb r23,12(r31)
				PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r23);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_824674BC:
			// addi r3,r28,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r28 + 16;
			// li r5,0
			ctx.r5.s64 = 0;
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// lwz r10,76(r11)
			// bctrl
			VCALL(ctx.r3.u32, 19, ctx, base);  // vtable slot 19 (byte +76)
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r9,4(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r9,0
			// beq cr6,0x824674fc
			if (ctx.r9.s32 != 0) {
				// lwz r8,8(r31)
				ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r30,r8
				// beq cr6,0x8246750c
				if (var_r30 == ctx.r8.u32) goto loc_8246750C;
			}
		loc_824674FC:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// stw r30,8(r31)
			PPC_STORE_U32(var_r31 + 8, var_r30);
			// stb r29,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		loc_8246750C:
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// addi r7,r11,1
			ctx.r7.s64 = ctx.r11.s64 + 1;
			// stw r7,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r7.u32);
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r11,4(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
			// mr r30,r3
			var_r30 = ctx.r3.u32;
			// mr r31,r13
			var_r31 = ctx.r13.u32;
			// cmpwi cr6,r11,0
			// beq cr6,0x8246753c
			if (ctx.r11.s32 != 0) {
				// lwz r6,8(r27)
				ctx.r6.u64 = PPC_LOAD_U32(var_r27 + 8);
				// cmplw cr6,r31,r6
				// beq cr6,0x82467550
				if (var_r31 == ctx.r6.u32) goto loc_82467550;
			}
		loc_8246753C:
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// stw r31,8(r27)
			PPC_STORE_U32(var_r27 + 8, var_r31);
			// stb r30,12(r27)
			PPC_STORE_U8(var_r27 + 12, (uint8_t)var_r30);
			// lwz r11,4(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
		loc_82467550:
			// addi r9,r11,1
			ctx.r9.s64 = ctx.r11.s64 + 1;
			// stw r9,4(r27)
			PPC_STORE_U32(var_r27 + 4, ctx.r9.u32);
		}
	loc_82467558:
		// mr r29,r26
		var_r29 = (uint32_t)(var_r26);
		// cmplwi cr6,r26,0
		// bne cr6,0x8246734c
		if (var_r26 != 0) goto loc_8246734C;
	}
loc_82467564:
	// mr r11,r13
	ctx.r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x824675ac
	if (ctx.r9.s32 != 0) {
		// lwz r5,8(r27)
		ctx.r5.u64 = PPC_LOAD_U32(var_r27 + 8);
		// cmplw cr6,r11,r5
		// bne cr6,0x824675ac
		if (ctx.r11.u32 != ctx.r5.u32) goto loc_824675AC;
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r27)
		PPC_STORE_U32(var_r27 + 4, ctx.r11.u32);
		// bne cr6,0x824675ac
		if (ctx.r11.s32 != 0) goto loc_824675AC;
		// mr r11,r23
		ctx.r11.u64 = var_r23;
		// lbz r31,12(r27)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r27 + 12));
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// stb r11,12(r27)
		PPC_STORE_U8(var_r27 + 12, ctx.r11.u8);
		// stw r11,8(r27)
		PPC_STORE_U32(var_r27 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824675AC:
	// clrlwi r4,r24,24
	ctx.r4.u64 = var_r24 & 0xFF;
	// cmplwi cr6,r4,0
	// beq cr6,0x824677b8
	if (ctx.r4.u32 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// mr r31,r13
		var_r31 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x824675dc
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 8);
			// cmplw cr6,r31,r9
			// beq cr6,0x824675f4
			if (var_r31 == ctx.r9.u32) goto loc_824675F4;
		}
	loc_824675DC:
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r9,r31
		ctx.r9.u64 = var_r31;
		// stw r9,8(r27)
		PPC_STORE_U32(var_r27 + 8, ctx.r9.u32);
		// stb r30,12(r27)
		PPC_STORE_U8(var_r27 + 12, (uint8_t)var_r30);
		// lwz r11,4(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
	loc_824675F4:
		// addi r10,r11,1
		ctx.r10.s64 = ctx.r11.s64 + 1;
		// stw r10,4(r27)
		PPC_STORE_U32(var_r27 + 4, ctx.r10.u32);
		// lwz r11,0(r22)
		ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 0);
		// cmplw cr6,r22,r11
		// beq cr6,0x82467614
		if (var_r22 != ctx.r11.u32) {
			// lwz r3,8(r22)
			ctx.r3.u64 = PPC_LOAD_U32(var_r22 + 8);
			// subf r30,r3,r11
			var_r30 = (uint32_t)(ctx.r11.s64 - ctx.r3.s64);
			// b 0x82467618
		} else {
		loc_82467614:
			// mr r30,r23
			var_r30 = (uint32_t)(var_r23);
		}
	loc_82467618:
		// cmplwi cr6,r30,0
		// beq cr6,0x82467774
		if (var_r30 != 0) {
			// li r24,3
			var_r24 = 3;
		loc_82467624:
			// lwz r11,0(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
			// cmplw cr6,r11,r30
			// beq cr6,0x8246776c
			if (ctx.r11.u32 == var_r30) goto loc_8246776C;
			// lwz r11,8(r22)
			ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 8);
			// lwzx r10,r11,r30
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r30);
			// cmplw cr6,r22,r10
			// subf r25,r11,r10
			var_r25 = (uint32_t)(ctx.r10.s64 - ctx.r11.s64);
			// bne cr6,0x82467648
			if (var_r22 == ctx.r10.u32) {
				// mr r25,r23
				var_r25 = (uint32_t)(var_r23);
			}
		loc_82467648:
			// lbz r10,96(r30)
			ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 96);
			// cmplwi cr6,r10,0
			// bne cr6,0x82467760
			if (ctx.r10.u32 == 0) {
				// lbz r9,52(r28)
				ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 52);
				// li r26,1
				var_r26 = 1;
				// cmplwi cr6,r9,0
				// beq cr6,0x82467750
				if (ctx.r9.u32 != 0) {
					// mr r31,r23
					var_r31 = (uint32_t)(var_r23);
				loc_82467668:
					// mr r29,r23
					var_r29 = (uint32_t)(var_r23);
					// cmplwi cr6,r31,0
					// beq cr6,0x82467684
					if (var_r31 != 0) {
						// mr r4,r31
						ctx.r4.u64 = var_r31;
						// lwz r3,180(r28)
						ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
						// bl 0x8246b0b8
						ke_B0B8_h(ctx, base);
						// mr r29,r3
						var_r29 = ctx.r3.u32;
					}
				loc_82467684:
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// lwz r6,12(r30)
					ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 12);
					// lwz r5,8(r30)
					ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 8);
					// lwz r3,180(r28)
					ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
					// bl 0x8246af18
					MmGetPhysicalAddress_AF18(ctx, base);
					// cmpwi cr6,r29,0
					// beq cr6,0x824676b8
					if ((int32_t)var_r29 != 0) {
						// rlwinm r11,r31,14,0,17
						ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 14) & 0xFFFFC000;
						// lwz r3,180(r28)
						ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
						// li r6,0
						ctx.r6.s64 = 0;
						// addi r5,r11,32
						ctx.r5.s64 = ctx.r11.s64 + 32;
						// mr r4,r31
						ctx.r4.u64 = var_r31;
						// bl 0x8246b5e0
						ke_B5E0_h(ctx, base);
					}
				loc_824676B8:
					// lwz r7,16(r30)
					ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 16);
					// cmplwi cr6,r7,0
					// beq cr6,0x82467724
					if (ctx.r7.u32 != 0) {
						// rlwinm r11,r31,1,0,30
						ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0xFFFFFFFE;
						// lwz r3,180(r28)
						ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
						// addi r10,r31,2
						ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 2;
						// add r8,r31,r11
						ctx.r8.u64 = var_r31 + ctx.r11.u64;
						// addi r9,r1,84
						ctx.r9.s64 = ctx.r1.s64 + 84;
						// rlwinm r11,r8,2,0,29
						ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
						// rlwinm r8,r10,1,0,30
						ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
						// add r11,r11,r30
						ctx.r11.u64 = ctx.r11.u64 + var_r30;
						// add r6,r10,r8
						ctx.r6.u64 = ctx.r10.u64 + ctx.r8.u64;
						// stw r23,0(r9)
						PPC_STORE_U32(ctx.r9.u32 + 0, var_r23);
						// addi r5,r1,80
						ctx.r5.s64 = ctx.r1.s64 + 80;
						// rlwinm r10,r6,2,0,29
						ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
						// stw r23,4(r9)
						PPC_STORE_U32(ctx.r9.u32 + 4, var_r23);
						// stb r7,90(r1)
						PPC_STORE_U8(ctx.r1.u32 + 90, ctx.r7.u8);
						// lwz r8,20(r11)
						ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
						// mr r4,r31
						ctx.r4.u64 = var_r31;
						// lbz r7,28(r11)
						ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 28);
						// lbz r11,29(r11)
						ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 29);
						// lwzx r6,r10,r30
						ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r30);
						// stw r8,80(r1)
						PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
						// stb r7,88(r1)
						PPC_STORE_U8(ctx.r1.u32 + 88, ctx.r7.u8);
						// stb r11,89(r1)
						PPC_STORE_U8(ctx.r1.u32 + 89, ctx.r11.u8);
						// stw r6,84(r1)
						PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
						// bl 0x8246b4d8
						phInst_B4D8_p39(ctx, base);
					}
				loc_82467724:
					// mr r4,r31
					ctx.r4.u64 = var_r31;
					// lwz r3,180(r28)
					ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
					// bl 0x8246b050
					phInst_B050_p39(ctx, base);
					// cmpwi cr6,r3,0
					// bne cr6,0x8246773c
					if (ctx.r3.s32 == 0) {
						// mr r26,r23
						var_r26 = (uint32_t)(var_r23);
					}
				loc_8246773C:
					// addi r10,r31,1
					ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 1;
					// lbz r9,52(r28)
					ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 52);
					// clrlwi r31,r10,24
					var_r31 = (uint32_t)(ctx.r10.u32 & 0xFF);
					// cmplw cr6,r31,r9
					// blt cr6,0x82467668
					if (var_r31 < ctx.r9.u32) goto loc_82467668;
				}
			loc_82467750:
				// clrlwi r8,r26,24
				ctx.r8.u64 = var_r26 & 0xFF;
				// stb r24,96(r30)
				PPC_STORE_U8(var_r30 + 96, (uint8_t)var_r24);
				// cmplwi cr6,r8,0
				// beq cr6,0x8246776c
				if (ctx.r8.u32 == 0) goto loc_8246776C;
			}
		loc_82467760:
			// mr r30,r25
			var_r30 = (uint32_t)(var_r25);
			// cmplwi cr6,r25,0
			// bne cr6,0x82467624
			if (var_r25 != 0) goto loc_82467624;
		loc_8246776C:
			// lwz r9,8(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 8);
			// lwz r10,4(r27)
			ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 4);
		}
	loc_82467774:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r10,0
		// beq cr6,0x824677b8
		if (ctx.r10.s32 == 0) goto loc_824677B8;
		// cmplw cr6,r11,r9
		// bne cr6,0x824677b8
		if (ctx.r11.u32 != ctx.r9.u32) goto loc_824677B8;
		// addi r11,r10,-1
		ctx.r11.s64 = ctx.r10.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r27)
		PPC_STORE_U32(var_r27 + 4, ctx.r11.u32);
		// bne cr6,0x824677b8
		if (ctx.r11.s32 != 0) goto loc_824677B8;
		// mr r11,r23
		ctx.r11.u64 = var_r23;
		// lbz r31,12(r27)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r27 + 12));
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// stb r11,12(r27)
		PPC_STORE_U8(var_r27 + 12, ctx.r11.u8);
		// stw r11,8(r27)
		PPC_STORE_U32(var_r27 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824677B8:
	// lwz r7,116(r28)
	ctx.r7.u64 = PPC_LOAD_U32(var_r28 + 116);
	// cmplwi cr6,r7,0
	// beq cr6,0x82467814
	if (ctx.r7.u32 != 0) {
		// lbz r6,52(r28)
		ctx.r6.u64 = PPC_LOAD_U8(var_r28 + 52);
		// cmplwi cr6,r6,0
		// beq cr6,0x82467810
		if (ctx.r6.u32 != 0) {
			// mr r31,r23
			var_r31 = (uint32_t)(var_r23);
		loc_824677D4:
			// addi r5,r31,15
			ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 15;
			// lwz r3,180(r28)
			ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
			// rlwinm r11,r31,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 3) & 0xFFFFFFF8;
			// rlwinm r10,r5,3,0,28
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
			// add r9,r11,r28
			ctx.r9.u64 = ctx.r11.u64 + var_r28;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lhzx r8,r10,r28
			ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + var_r28);
			// lwz r5,116(r9)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 116);
			// rlwinm r6,r8,25,7,31
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
			// bl 0x8246b5e0
			ke_B5E0_h(ctx, base);
			// addi r7,r31,1
			ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 1;
			// lbz r6,52(r28)
			ctx.r6.u64 = PPC_LOAD_U8(var_r28 + 52);
			// clrlwi r31,r7,24
			var_r31 = (uint32_t)(ctx.r7.u32 & 0xFF);
			// cmplw cr6,r31,r6
			// blt cr6,0x824677d4
			if (var_r31 < ctx.r6.u32) goto loc_824677D4;
		}
	loc_82467810:
		// stw r23,116(r28)
		PPC_STORE_U32(var_r28 + 116, var_r23);
	}
loc_82467814:
	// lbz r5,52(r28)
	ctx.r5.u64 = PPC_LOAD_U8(var_r28 + 52);
	// cmplwi cr6,r5,0
	// beq cr6,0x82467864
	if (ctx.r5.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
	// mr r31,r23
	var_r31 = (uint32_t)(var_r23);
loc_82467824:
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r3,180(r28)
	ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
	// bl 0x8246b0b8
	ke_B0B8_h(ctx, base);
	// cmpwi cr6,r3,0
	// bne cr6,0x82467850
	if (ctx.r3.s32 != 0) goto loc_82467850;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// lwz r3,180(r28)
	ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 180);
	// bl 0x8246b488
	ke_B488_h(ctx, base);
	// rlwinm r4,r3,0,28,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r4,0
	// beq cr6,0x8246732c
	if (ctx.r4.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
		return;
	}
loc_82467850:
	// addi r3,r31,1
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 1;
	// lbz r10,52(r28)
	ctx.r10.u64 = PPC_LOAD_U8(var_r28 + 52);
	// clrlwi r31,r3,24
	var_r31 = (uint32_t)(ctx.r3.u32 & 0xFF);
	// cmplw cr6,r31,r10
	// blt cr6,0x82467824
	if (var_r31 < ctx.r10.u32) goto loc_82467824;
loc_82467864:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A60_60_7870"))) PPC_WEAK_FUNC(ph_vt5A60_60_7870);
PPC_FUNC_IMPL(__imp__ph_vt5A60_60_7870) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r27,0
	var_r27 = 0;
	// lbz r11,248(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 248);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	// beq cr6,0x82467930
	if (ctx.r10.u32 != 0) {
		// addi r31,r29,264
		var_r31 = (uint32_t)(var_r29 + 264);
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r9,0
		// beq cr6,0x824678bc
		if (ctx.r9.s32 != 0) {
			// lwz r8,8(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r8
			// beq cr6,0x824678cc
			if (var_r30 == ctx.r8.u32) goto loc_824678CC;
		}
	loc_824678BC:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stb r28,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
	loc_824678CC:
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r7,r11,1
		ctx.r7.s64 = ctx.r11.s64 + 1;
		// stw r7,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r7.u32);
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246b638
		aud_B638(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82467928
		if (ctx.r11.s32 != 0) {
			// lwz r6,8(r31)
			ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r6
			// bne cr6,0x82467928
			if (ctx.r10.u32 != ctx.r6.u32) goto loc_82467928;
			// addic. r11,r11,-1
			ctx.xer.ca = ctx.r11.u32 > 0;
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne 0x82467928
			if (ctx.r11.s32 != 0) goto loc_82467928;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// stw r27,8(r31)
			PPC_STORE_U32(var_r31 + 8, var_r27);
			// stb r27,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82467928:
		// cmpwi cr6,r28,0
		// blt cr6,0x82467a28
		if ((int32_t)var_r28 < 0) {
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			return;
		}
	}
loc_82467930:
	// addi r30,r29,16
	var_r30 = (uint32_t)(var_r29 + 16);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82467960
	if (ctx.r11.s32 != 0) {
		// lwz r5,8(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r5
		// beq cr6,0x82467974
		if (var_r29 == ctx.r5.u32) goto loc_82467974;
	}
loc_82467960:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82467974:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 152);
	// clrlwi r4,r10,31
	ctx.r4.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r4,0
	// bne cr6,0x824679a8
	if (ctx.r4.u32 == 0) {
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// li r5,3
		ctx.r5.s64 = 3;
		// li r4,3
		ctx.r4.s64 = 3;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r10,68(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
		// mtctr r10
		ctx.ctr.u64 = ctx.r10.u64;
		// b 0x824679cc
	} else {
	loc_824679A8:
		// rlwinm r9,r10,0,29,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
		// cmplwi cr6,r9,0
		// beq cr6,0x824679d4
		if (ctx.r9.u32 == 0) goto loc_824679D4;
		// lwz r8,0(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 0);
		// li r5,2
		ctx.r5.s64 = 2;
		// li r4,6
		ctx.r4.s64 = 6;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r7,68(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 68);
		// mtctr r7
		ctx.ctr.u64 = ctx.r7.u64;
	}
loc_824679CC:
	// bctrl
	ctx.lr = 0x824679D0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_824679D4:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82467a1c
	if (ctx.r11.s32 != 0) {
		// lwz r6,8(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r6
		// bne cr6,0x82467a1c
		if (ctx.r10.u32 != ctx.r6.u32) {
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82467a1c
		if (ctx.r11.s32 != 0) {
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			return;
		}
		// mr r11,r27
		ctx.r11.u64 = var_r27;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82467A1C:
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	return;
}

__attribute__((alias("__imp__ph_vt5A60_61_7A38"))) PPC_WEAK_FUNC(ph_vt5A60_61_7A38);
PPC_FUNC_IMPL(__imp__ph_vt5A60_61_7A38) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// clrlwi r27,r4,31
	var_r27 = (uint32_t)(ctx.r4.u32 & 0x1);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r26,0
	var_r26 = 0;
	// cmplwi cr6,r27,0
	// beq cr6,0x82467af8
	if (var_r27 != 0) {
		// lbz r11,248(r29)
		ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 248);
		// rlwinm r10,r11,0,30,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r10,0
		// beq cr6,0x82467af8
		if (ctx.r10.u32 == 0) goto loc_82467AF8;
		// addi r31,r29,264
		var_r31 = (uint32_t)(var_r29 + 264);
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r9,0
		// beq cr6,0x82467a90
		if (ctx.r9.s32 != 0) {
			// lwz r8,8(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r8
			// beq cr6,0x82467aa0
			if (var_r30 == ctx.r8.u32) goto loc_82467AA0;
		}
	loc_82467A90:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stb r28,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
	loc_82467AA0:
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// addi r7,r11,1
		ctx.r7.s64 = ctx.r11.s64 + 1;
		// stw r7,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r7.u32);
		// lwz r3,180(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 180);
		// bl 0x8246ae80
		aud_AE80(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82467af8
		if (ctx.r11.s32 == 0) goto loc_82467AF8;
		// lwz r6,8(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r6
		// bne cr6,0x82467af8
		if (ctx.r10.u32 != ctx.r6.u32) goto loc_82467AF8;
		// addic. r11,r11,-1
		ctx.xer.ca = ctx.r11.u32 > 0;
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne 0x82467af8
		if (ctx.r11.s32 != 0) goto loc_82467AF8;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// stw r26,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r26);
		// stb r26,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r26);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82467AF8:
	// addi r30,r29,16
	var_r30 = (uint32_t)(var_r29 + 16);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82467b28
	if (ctx.r11.s32 != 0) {
		// lwz r5,8(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r5
		// beq cr6,0x82467b3c
		if (var_r29 == ctx.r5.u32) goto loc_82467B3C;
	}
loc_82467B28:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82467B3C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 152);
	// clrlwi r4,r10,31
	ctx.r4.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r4,0
	// beq cr6,0x82467bc0
	if (ctx.r4.u32 != 0) {
		// cmplwi cr6,r27,0
		// bne cr6,0x82467ba0
		if (var_r27 == 0) {
			// andi. r3,r10,18
			ctx.r3.u64 = ctx.r10.u64 & 18;
			ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
			// cmplwi cr6,r3,0
			// bne cr6,0x82467ba0
			if (ctx.r3.u32 != 0) goto loc_82467BA0;
			// rlwinm r10,r10,0,29,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
			// cmplwi cr6,r10,0
			// bne cr6,0x82467bc0
			if (ctx.r10.u32 != 0) goto loc_82467BC0;
			// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
			// li r5,4
			ctx.r5.s64 = 4;
			// li r4,4
			ctx.r4.s64 = 4;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r8,68(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
			// lis r11,-32256
			ctx.r11.s64 = -2113929216;
			// lhz r11,23128(r11)
			ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 23128);
			// sth r11,154(r30)
			PPC_STORE_U16(var_r30 + 154, ctx.r11.u16);
			// b 0x82467bbc
		} else {
		loc_82467BA0:
			// lwz r7,0(r30)
  // [ph4a] vtable load collapsed
			// li r5,0
			ctx.r5.s64 = 0;
			// li r4,87
			ctx.r4.s64 = 87;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r6,68(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		}
	loc_82467BBC:
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_82467BC0:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82467c08
	if (ctx.r11.s32 != 0) {
		// lwz r5,8(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r5
		// bne cr6,0x82467c08
		if (ctx.r10.u32 != ctx.r5.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82467c08
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// mr r11,r26
		ctx.r11.u64 = var_r26;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82467C08:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A60_28_7C18"))) PPC_WEAK_FUNC(ph_vt5A60_28_7C18);
PPC_FUNC_IMPL(__imp__ph_vt5A60_28_7C18) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82467870
	ph_vt5A60_60_7870(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_29_7C20"))) PPC_WEAK_FUNC(ph_vt5A60_29_7C20);
PPC_FUNC_IMPL(__imp__ph_vt5A60_29_7C20) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82467a38
	ph_vt5A60_61_7A38(ctx, base);
	return;
}

__attribute__((alias("__imp__atSingleton_7C28"))) PPC_WEAK_FUNC(atSingleton_7C28);
PPC_FUNC_IMPL(__imp__atSingleton_7C28) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// addi r30,r31,4
	var_r30 = (uint32_t)(var_r31 + 4);
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8245fef8
	atSingleton_FEF8_2h(ctx, base);
	// lis r11,-32256
	// lis r10,-32256
	// lis r9,-32256
	// li r29,0
	var_r29 = 0;
	// addi r11,r11,23336
	ctx.r11.s64 = ctx.r11.s64 + 23336;
	// addi r10,r10,23296
	ctx.r10.s64 = ctx.r10.s64 + 23296;
	// addi r9,r9,23216
	ctx.r9.s64 = ctx.r9.s64 + 23216;
	// mr r5,r28
	ctx.r5.u64 = var_r28;
	// stw r29,180(r31)
	PPC_STORE_U32(var_r31 + 180, var_r29);
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stw r10,0(r30)
	PPC_STORE_U32(var_r30 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r9.u32);
	// lbz r11,69(r27)
	ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 69);
	// stb r11,248(r31)
	PPC_STORE_U8(var_r31 + 248, ctx.r11.u8);
	// stw r29,268(r31)
	PPC_STORE_U32(var_r31 + 268, var_r29);
	// stw r29,272(r31)
	PPC_STORE_U32(var_r31 + 272, var_r29);
	// stb r29,276(r31)
	PPC_STORE_U8(var_r31 + 276, (uint8_t)var_r29);
	// stw r29,264(r31)
	PPC_STORE_U32(var_r31 + 264, var_r29);
	// bl 0x8245fcd8
	atSingleton_FCD8_2h(ctx, base);
	// lis r10,744
	ctx.r10.s64 = 48758784;
	// lbz r30,52(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 52));
	// ori r9,r10,47662
	ctx.r9.u64 = ctx.r10.u64 | 47662;
	// mulli r4,r30,88
	ctx.r4.s64 = static_cast<int64_t>(var_r30 * static_cast<uint64_t>(88));
	// cmplw cr6,r30,r9
	// ble cr6,0x82467cbc
	if (var_r30 > ctx.r9.u32) {
		// li r4,-1
	}
loc_82467CBC:
	// lwz r8,0(r28)
  // [ph4a] vtable load collapsed
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// lwz r7,20(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r28, 5, ctx, base);  // pattern-B slot 5 (byte +20)
	// cmplwi cr6,r3,0
	// beq cr6,0x82467d1c
	if (ctx.r3.u32 != 0) {
		// addi r7,r30,-1
		ctx.r7.s64 = (int64_t)(int32_t)var_r30 + -1;
		// mr r10,r3
		ctx.r10.u64 = ctx.r3.u64;
		// cmpwi cr6,r7,0
		// blt cr6,0x82467d18
	while (ctx.r7.s32 >= 0) {
		loc_82467CE8:
			// mr r11,r10
			ctx.r11.u64 = ctx.r10.u64;
			// mr r8,r29
			ctx.r8.u64 = var_r29;
			// li r9,19
			ctx.r9.s64 = 19;
			// mtctr r9
			ctx.ctr.u64 = ctx.r9.u64;
		loc_82467CF8:
			// stw r8,0(r11)
			PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
			// addi r11,r11,4
			ctx.r11.s64 = ctx.r11.s64 + 4;
			// bdnz 0x82467cf8
			--ctx.ctr.u64;
			if (ctx.ctr.u32 != 0) goto loc_82467CF8;
			// addi r7,r7,-1
			ctx.r7.s64 = ctx.r7.s64 + -1;
			// stw r29,76(r10)
			PPC_STORE_U32(ctx.r10.u32 + 76, var_r29);
			// addi r10,r10,88
			ctx.r10.s64 = ctx.r10.s64 + 88;
			// cmpwi cr6,r7,0
			// bge cr6,0x82467ce8
	}
	loc_82467D18:
		// mr r29,r3
		var_r29 = ctx.r3.u32;
	}
loc_82467D1C:
	// addi r6,r27,64
	ctx.r6.s64 = (int64_t)(int32_t)var_r27 + 64;
	// stw r29,184(r31)
	PPC_STORE_U32(var_r31 + 184, var_r29);
	// vspltisw v0,15
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0xF)));
	// addi r5,r31,188
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 188;
	// vspltisw v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_set1_epi32(int(0x4)));
	// li r9,14
	ctx.r9.s64 = 14;
	// vspltisw v11,1
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r10,r31,192
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 192;
	// addi r11,r31,48
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 48;
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vexptefp v12,v12
	ctx.fpscr.enableFlushMode();
	ctx.v12.f32[0] = exp2f(ctx.v12.f32[0]);
	ctx.v12.f32[1] = exp2f(ctx.v12.f32[1]);
	ctx.v12.f32[2] = exp2f(ctx.v12.f32[2]);
	ctx.v12.f32[3] = exp2f(ctx.v12.f32[3]);
	// vctsxs v12,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_vctsxs(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_set1_ps(16))));
	// vadduws v0,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_adds_epu32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vsraw v0,v0,v13
	ctx.v0.s32[0] = ctx.v0.s32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v0.s32[1] = ctx.v0.s32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v0.s32[2] = ctx.v0.s32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v0.s32[3] = ctx.v0.s32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vmaxsw v0,v0,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_max_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// stvewx v0,r0,r5
	ea = (ctx.r5.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82467D64:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82467d64
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82467D64;
	// li r11,1
	ctx.r11.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// sth r11,104(r31)
	PPC_STORE_U16(var_r31 + 104, ctx.r11.u16);
	// sth r11,106(r31)
	PPC_STORE_U16(var_r31 + 106, ctx.r11.u16);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_56_7D90"))) PPC_WEAK_FUNC(ph_vt5A60_56_7D90);
PPC_FUNC_IMPL(__imp__ph_vt5A60_56_7D90) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=208, savegprlr_22
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r23,0
	var_r23 = 0;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// addi r24,r31,264
	var_r24 = (uint32_t)(var_r31 + 264);
	// mr r26,r23
	var_r26 = (uint32_t)(var_r23);
	// mr r22,r23
	var_r22 = (uint32_t)(var_r23);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r24)
	ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x82467ddc
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r24)
		ctx.r10.u64 = PPC_LOAD_U32(var_r24 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82467dec
		if (var_r30 == ctx.r10.u32) goto loc_82467DEC;
	}
loc_82467DDC:
	// mr r3,r24
	ctx.r3.u64 = var_r24;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r29,12(r24)
	PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r29);
	// stw r30,8(r24)
	PPC_STORE_U32(var_r24 + 8, var_r30);
loc_82467DEC:
	// lwz r11,4(r24)
	ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// stw r9,4(r24)
	PPC_STORE_U32(var_r24 + 4, ctx.r9.u32);
	// lwz r3,180(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
	// bl 0x8246aef8
	ph_AEF8(ctx, base);
	// cmpwi cr6,r3,0
	// bne cr6,0x82467e24
	if (ctx.r3.s32 == 0) {
		// lwz r11,4(r24)
		ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82468884
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r8,8(r24)
		ctx.r8.u64 = PPC_LOAD_U32(var_r24 + 8);
		// cmplw cr6,r10,r8
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
		// b 0x82468858
	} else {
	loc_82467E24:
		// lwz r3,180(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
		// bl 0x8246b708
		ph_B708(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmpwi cr6,r30,0
		// bge cr6,0x82467e8c
		if ((int32_t)var_r30 < 0) {
			// lwz r11,4(r24)
			ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
			// mr r10,r13
			ctx.r10.u64 = ctx.r13.u64;
			// cmpwi cr6,r11,0
			// beq cr6,0x82467e7c
			if (ctx.r11.s32 == 0) {
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				return;
			}
			// lwz r7,8(r24)
			ctx.r7.u64 = PPC_LOAD_U32(var_r24 + 8);
			// cmplw cr6,r10,r7
			// bne cr6,0x82467e7c
			if (ctx.r10.u32 != ctx.r7.u32) {
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				return;
			}
			// addic. r11,r11,-1
			ctx.xer.ca = ctx.r11.u32 > 0;
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// stw r11,4(r24)
			PPC_STORE_U32(var_r24 + 4, ctx.r11.u32);
			// bne 0x82467e7c
			if (ctx.r11.s32 != 0) {
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				return;
			}
			// mr r3,r24
			ctx.r3.u64 = var_r24;
			// lbz r31,12(r24)
			var_r31 = (uint32_t)(PPC_LOAD_U8(var_r24 + 12));
			// stw r23,8(r24)
			PPC_STORE_U32(var_r24 + 8, var_r23);
			// stb r23,12(r24)
			PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r23);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		loc_82467E7C:
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
	loc_82467E8C:
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// addi r25,r11,30832
		var_r25 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// lwz r11,4(r25)
		ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x82467eb8
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r25)
			ctx.r10.u64 = PPC_LOAD_U32(var_r25 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x82467ecc
			if (var_r30 == ctx.r10.u32) goto loc_82467ECC;
		}
	loc_82467EB8:
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r25)
		PPC_STORE_U32(var_r25 + 8, var_r30);
		// stb r29,12(r25)
		PPC_STORE_U8(var_r25 + 12, (uint8_t)var_r29);
		// lwz r11,4(r25)
		ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 4);
	loc_82467ECC:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r4,r1,88
		ctx.r4.s64 = ctx.r1.s64 + 88;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// stw r11,4(r25)
		PPC_STORE_U32(var_r25 + 4, ctx.r11.u32);
		// bl 0x82465da8
		util_5DA8(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmpwi cr6,r29,0
		// blt cr6,0x82467f44
		if ((int32_t)var_r29 >= 0) {
			// lbz r6,52(r31)
			ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 52);
			// mr r10,r23
			ctx.r10.u64 = var_r23;
			// cmplwi cr6,r6,0
			// beq cr6,0x82467f24
			if (ctx.r6.u32 != 0) {
				// clrlwi r8,r6,24
				ctx.r8.u64 = ctx.r6.u32 & 0xFF;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
			loc_82467F04:
				// rlwinm r9,r11,3,0,28
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
				// addi r5,r11,1
				ctx.r5.s64 = ctx.r11.s64 + 1;
				// add r4,r9,r31
				ctx.r4.u64 = ctx.r9.u64 + var_r31;
				// clrlwi r11,r5,24
				ctx.r11.u64 = ctx.r5.u32 & 0xFF;
				// cmplw cr6,r11,r8
				// lbz r9,60(r4)
				ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 60);
				// add r10,r9,r10
				ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
				// blt cr6,0x82467f04
				if (ctx.r11.u32 < ctx.r8.u32) goto loc_82467F04;
			}
		loc_82467F24:
			// li r5,1
			ctx.r5.s64 = 1;
			// stb r10,89(r1)
			PPC_STORE_U8(ctx.r1.u32 + 89, ctx.r10.u8);
			// addi r4,r1,88
			ctx.r4.s64 = ctx.r1.s64 + 88;
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// bl 0x82466018
			phDemoWorld_6018_g(ctx, base);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// cmpwi cr6,r29,0
			// bge cr6,0x82467ff0
			if ((int32_t)var_r29 >= 0) goto loc_82467FF0;
		}
	loc_82467F44:
		// lwz r9,4(r25)
		ctx.r9.u64 = PPC_LOAD_U32(var_r25 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r9,0
		// beq cr6,0x82467f90
		if (ctx.r9.s32 != 0) {
			// lwz r11,8(r25)
			ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 8);
			// cmplw cr6,r10,r11
			// bne cr6,0x82467f90
			if (ctx.r10.u32 != ctx.r11.u32) goto loc_82467F90;
			// addi r11,r9,-1
			ctx.r11.s64 = ctx.r9.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r25)
			PPC_STORE_U32(var_r25 + 4, ctx.r11.u32);
			// bne cr6,0x82467f90
			if (ctx.r11.s32 != 0) goto loc_82467F90;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// lbz r30,12(r25)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r25 + 12));
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// stb r11,12(r25)
			PPC_STORE_U8(var_r25 + 12, ctx.r11.u8);
			// stw r11,8(r25)
			PPC_STORE_U32(var_r25 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82467F90:
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// cmpwi cr6,r29,0
		// beq cr6,0x824681d8
		if ((int32_t)var_r29 == 0) goto loc_824681D8;
	loc_82467F9C:
		// lwz r11,4(r24)
		ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82467e7c
		if (ctx.r11.s32 == 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// lwz r9,8(r24)
		ctx.r9.u64 = PPC_LOAD_U32(var_r24 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x82467e7c
		if (ctx.r10.u32 != ctx.r9.u32) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// addic. r11,r11,-1
		ctx.xer.ca = ctx.r11.u32 > 0;
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,4(r24)
		PPC_STORE_U32(var_r24 + 4, ctx.r11.u32);
		// bne 0x82467e7c
		if (ctx.r11.s32 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// mr r3,r24
		ctx.r3.u64 = var_r24;
		// lbz r31,12(r24)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r24 + 12));
		// stw r23,8(r24)
		PPC_STORE_U32(var_r24 + 8, var_r23);
		// stb r23,12(r24)
		PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r23);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		return;
	loc_82467FF0:
		// lbz r10,52(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 52);
		// cmplwi cr6,r10,0
		// beq cr6,0x82468050
		if (ctx.r10.u32 != 0) {
			// lwz r8,96(r1)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// li r6,256
			ctx.r6.s64 = 256;
		loc_82468008:
			// lwz r7,184(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 184);
			// mulli r10,r11,88
			ctx.r10.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(88));
			// addi r9,r11,1
			ctx.r9.s64 = ctx.r11.s64 + 1;
			// rlwinm r11,r11,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
			// add r10,r10,r7
			ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
			// add r7,r11,r31
			ctx.r7.u64 = ctx.r11.u64 + var_r31;
			// clrlwi r11,r9,24
			ctx.r11.u64 = ctx.r9.u32 & 0xFF;
			// stw r8,20(r10)
			PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r8.u32);
			// stw r6,24(r10)
			PPC_STORE_U32(ctx.r10.u32 + 24, ctx.r6.u32);
			// stw r23,28(r10)
			PPC_STORE_U32(ctx.r10.u32 + 28, var_r23);
			// lbz r5,60(r7)
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r7.u32 + 60);
			// lwz r3,96(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			// rotlwi r10,r5,10
			ctx.r10.u64 = __builtin_rotateleft32(ctx.r5.u32, 10);
			// lbz r4,52(r31)
			ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 52);
			// add r8,r10,r3
			ctx.r8.u64 = ctx.r10.u64 + ctx.r3.u64;
			// cmplw cr6,r11,r4
			// stw r8,96(r1)
			PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
			// blt cr6,0x82468008
			if (ctx.r11.u32 < ctx.r4.u32) goto loc_82468008;
		}
	loc_82468050:
		// lbz r11,168(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 168);
		// clrlwi r10,r11,31
		ctx.r10.u64 = ctx.r11.u32 & 0x1;
		// cmplwi cr6,r10,0
		// beq cr6,0x82468400
		if (ctx.r10.u32 != 0) {
			// rlwinm r9,r11,0,27,27
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
			// cmplwi cr6,r9,0
			// bne cr6,0x82468400
			if (ctx.r9.u32 != 0) goto loc_82468400;
			// rlwinm r8,r11,0,30,30
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
			// lis r11,-32256
			// cmplwi cr6,r8,0
			// lfs f31,15784(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
			var_f31 = double(temp.f32);
			// beq cr6,0x8246810c
			if (ctx.r8.u32 != 0) {
				// lbz r7,52(r31)
				ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 52);
				// cmplwi cr6,r7,0
				// beq cr6,0x824680f0
				if (ctx.r7.u32 != 0) {
					// mr r8,r23
					ctx.r8.u64 = var_r23;
				loc_82468090:
					// lwz r10,184(r31)
					ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 184);
					// mulli r11,r8,88
					ctx.r11.s64 = static_cast<int64_t>(ctx.r8.u64 * static_cast<uint64_t>(88));
					// add r6,r11,r10
					ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
					// lfs f0,108(r31)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(var_r31 + 108);
					ctx.f0.f64 = double(temp.f32);
					// mr r7,r23
					ctx.r7.u64 = var_r23;
					// li r9,6
					ctx.r9.s64 = 6;
					// stfs f0,40(r6)
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r6.u32 + 40, temp.u32);
					// lwz r10,184(r31)
					ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 184);
					// add r10,r11,r10
					ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
					// lfs f13,40(r10)
					temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
					ctx.f13.f64 = double(temp.f32);
					// stfs f13,36(r10)
					temp.f32 = float(ctx.f13.f64);
					PPC_STORE_U32(ctx.r10.u32 + 36, temp.u32);
					// lwz r10,184(r31)
					ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 184);
					// add r11,r11,r10
					ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
					// addi r10,r11,52
					ctx.r10.s64 = ctx.r11.s64 + 52;
					// mtctr r9
					ctx.ctr.u64 = ctx.r9.u64;
				loc_824680CC:
					// stw r7,0(r10)
					PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
					// addi r10,r10,4
					ctx.r10.s64 = ctx.r10.s64 + 4;
					// bdnz 0x824680cc
					--ctx.ctr.u64;
					if (ctx.ctr.u32 != 0) goto loc_824680CC;
					// addi r5,r8,1
					ctx.r5.s64 = ctx.r8.s64 + 1;
					// stfs f31,48(r11)
					ctx.fpscr.disableFlushMode();
					temp.f32 = float(var_f31);
					PPC_STORE_U32(ctx.r11.u32 + 48, temp.u32);
					// lbz r4,52(r31)
					ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 52);
					// clrlwi r8,r5,24
					ctx.r8.u64 = ctx.r5.u32 & 0xFF;
					// cmplw cr6,r8,r4
					// blt cr6,0x82468090
					if (ctx.r8.u32 < ctx.r4.u32) goto loc_82468090;
				}
			loc_824680F0:
				// addi r3,r31,16
				ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 16;
				// li r5,0
				ctx.r5.s64 = 0;
				// li r4,2
				ctx.r4.s64 = 2;
				// lwz r10,68(r11)
				// bctrl
				VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
			}
		loc_8246810C:
			// lbz r9,168(r31)
			ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 168);
			// rlwinm r8,r9,0,29,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4;
			// cmplwi cr6,r8,0
			// beq cr6,0x8246818c
			if (ctx.r8.u32 != 0) {
				// addi r11,r31,16
				ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
				// lwz r7,184(r31)
				ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 184);
				// lhz r6,154(r11)
				ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 154);
				// lfs f13,40(r7)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 40);
				ctx.f13.f64 = double(temp.f32);
				// addis r5,r6,1
				ctx.r5.s64 = ctx.r6.s64 + 65536;
				// addi r5,r5,-1
				ctx.r5.s64 = ctx.r5.s64 + -1;
				// clrlwi r10,r5,16
				ctx.r10.u64 = ctx.r5.u32 & 0xFFFF;
				// cmplwi cr6,r10,0
				// sth r10,154(r11)
				PPC_STORE_U16(ctx.r11.u32 + 154, ctx.r10.u16);
				// beq cr6,0x82468154
				if (ctx.r10.u32 != 0) {
					// lis r11,-32256
					ctx.r11.s64 = -2113929216;
					// lfs f0,23132(r11)
					temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23132);  /* glob:lbl_82005A5C @ 0x82005a5c */
					ctx.f0.f64 = double(temp.f32);
					// fmuls f0,f13,f0
					ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
					// b 0x82468158
				} else {
				loc_82468154:
					// fmr f0,f31
					ctx.fpscr.disableFlushMode();
					ctx.f0.f64 = var_f31;
				}
			loc_82468158:
				// lbz r3,52(r31)
				ctx.r3.u64 = PPC_LOAD_U8(var_r31 + 52);
				// cmplwi cr6,r3,0
				// beq cr6,0x8246818c
				if (ctx.r3.u32 == 0) goto loc_8246818C;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
			loc_82468168:
				// lwz r9,184(r31)
				ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 184);
				// mulli r10,r11,88
				ctx.r10.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(88));
				// add r7,r10,r9
				ctx.r7.u64 = ctx.r10.u64 + ctx.r9.u64;
				// addi r8,r11,1
				ctx.r8.s64 = ctx.r11.s64 + 1;
				// clrlwi r11,r8,24
				ctx.r11.u64 = ctx.r8.u32 & 0xFF;
				// stfs f0,40(r7)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f0.f64);
				PPC_STORE_U32(ctx.r7.u32 + 40, temp.u32);
				// lbz r6,52(r31)
				ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 52);
				// cmplw cr6,r11,r6
				// blt cr6,0x82468168
				if (ctx.r11.u32 < ctx.r6.u32) goto loc_82468168;
			}
		loc_8246818C:
			// lwz r9,4(r25)
			ctx.r9.u64 = PPC_LOAD_U32(var_r25 + 4);
			// mr r10,r13
			ctx.r10.u64 = ctx.r13.u64;
			// cmpwi cr6,r9,0
			// beq cr6,0x824681d8
			if (ctx.r9.s32 != 0) {
				// lwz r11,8(r25)
				ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 8);
				// cmplw cr6,r10,r11
				// bne cr6,0x824681d8
				if (ctx.r10.u32 != ctx.r11.u32) goto loc_824681D8;
				// addi r11,r9,-1
				ctx.r11.s64 = ctx.r9.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r25)
				PPC_STORE_U32(var_r25 + 4, ctx.r11.u32);
				// bne cr6,0x824681d8
				if (ctx.r11.s32 != 0) goto loc_824681D8;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
				// lbz r30,12(r25)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r25 + 12));
				// mr r3,r25
				ctx.r3.u64 = var_r25;
				// stb r11,12(r25)
				PPC_STORE_U8(var_r25 + 12, ctx.r11.u8);
				// stw r11,8(r25)
				PPC_STORE_U32(var_r25 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_824681D8:
			// lwz r3,180(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
			// bl 0x8246b7a0
			ke_B7A0(ctx, base);
			// lbz r8,52(r31)
			ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 52);
			// cmplwi cr6,r8,0
			// beq cr6,0x8246833c
			if (ctx.r8.u32 != 0) {
				// mr r27,r23
				var_r27 = (uint32_t)(var_r23);
			loc_824681F0:
				// mr r4,r27
				ctx.r4.u64 = var_r27;
				// lwz r3,180(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
				// bl 0x8246b118
				phInst_B118_p39(ctx, base);
				// mr r28,r3
				var_r28 = ctx.r3.u32;
				// mr r29,r23
				var_r29 = (uint32_t)(var_r23);
				// cmplwi cr6,r28,0
				// beq cr6,0x824682a0
				if (var_r28 != 0) {
					// addi r5,r1,80
					ctx.r5.s64 = ctx.r1.s64 + 80;
					// lwz r3,180(r31)
					ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
					// mr r4,r27
					ctx.r4.u64 = var_r27;
					// bl 0x8246b360
					phInst_B360_p39(ctx, base);
					// cmplwi cr6,r3,0
					// beq cr6,0x824682a0
					if (ctx.r3.u32 == 0) goto loc_824682A0;
					// mulli r30,r27,88
					var_r30 = (uint32_t)(static_cast<int64_t>(var_r27 * static_cast<uint64_t>(88)));
				loc_82468228:
					// lwz r11,184(r31)
					ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 184);
					// cmplwi cr6,r3,0
					// lwz r7,80(r1)
					ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
					// add r11,r30,r11
					ctx.r11.u64 = var_r30 + ctx.r11.u64;
					// stw r7,0(r11)
					PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
					// stw r3,4(r11)
					PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r3.u32);
					// beq cr6,0x82468248
					if (ctx.r3.u32 != 0) {
						// mr r3,r23
						ctx.r3.u64 = var_r23;
					}
				loc_82468248:
					// stw r3,8(r11)
					PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r3.u32);
					// lwz r11,184(r31)
					ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 184);
					// add r3,r30,r11
					ctx.r3.u64 = var_r30 + ctx.r11.u64;
					// bl 0x82470cc0
					ph_0CC0(ctx, base);
					// mr r5,r3
					ctx.r5.u64 = ctx.r3.u64;
					// addi r6,r1,80
					ctx.r6.s64 = ctx.r1.s64 + 80;
					// lwz r3,180(r31)
					ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
					// mr r4,r27
					ctx.r4.u64 = var_r27;
					// add r29,r5,r29
					var_r29 = (uint32_t)(ctx.r5.u64 + var_r29);
					// bl 0x8246b190
					phInst_B190_p39(ctx, base);
					// lwz r11,184(r31)
					ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 184);
					// add r11,r30,r11
					ctx.r11.u64 = var_r30 + ctx.r11.u64;
					// lwz r6,24(r11)
					ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
					// lwz r5,28(r11)
					ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
					// subf. r4,r5,r6
					ctx.r4.s64 = ctx.r6.s64 - ctx.r5.s64;
					// beq 0x824682a0
					if (ctx.r4.s32 == 0) goto loc_824682A0;
					// addi r5,r1,80
					ctx.r5.s64 = ctx.r1.s64 + 80;
					// lwz r3,180(r31)
					ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
					// mr r4,r27
					ctx.r4.u64 = var_r27;
					// bl 0x8246b360
					phInst_B360_p39(ctx, base);
					// cmplwi cr6,r3,0
					// bne cr6,0x82468228
					if (ctx.r3.u32 != 0) goto loc_82468228;
				}
			loc_824682A0:
				// addi r3,r27,125
				ctx.r3.s64 = (int64_t)(int32_t)var_r27 + 125;
				// subf r11,r29,r28
				ctx.r11.s64 = (int64_t)(int32_t)var_r28 - (int64_t)(int32_t)var_r29;
				// rlwinm r10,r3,1,0,30
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
				// lhzx r9,r10,r31
				ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + var_r31);
				// cmplw cr6,r11,r9
				// bgt cr6,0x824682bc
				if (ctx.r11.u32 <= ctx.r9.u32) {
					// li r22,1
					var_r22 = 1;
				}
			loc_824682BC:
				// li r4,0
				ctx.r4.s64 = 0;
				// lwz r3,180(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
				// bl 0x8246b550
				ph_B550_h(ctx, base);
				// lwz r10,184(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 184);
				// mulli r11,r27,88
				ctx.r11.s64 = static_cast<int64_t>(var_r27 * static_cast<uint64_t>(88));
				// stw r3,164(r31)
				PPC_STORE_U32(var_r31 + 164, ctx.r3.u32);
				// add r28,r11,r10
				var_r28 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
				// lwz r11,28(r28)
				ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 28);
				// lwz r8,24(r28)
				ctx.r8.u64 = PPC_LOAD_U32(var_r28 + 24);
				// lbz r30,13(r28)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r28 + 13));
				// subf r10,r11,r8
				ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
				// cmplwi cr6,r10,0
				// beq cr6,0x82468328
				if (ctx.r10.u32 != 0) {
					// cmplwi cr6,r30,0
					// beq cr6,0x82468324
					if (var_r30 != 0) {
						// rlwinm r26,r10,2,0,29
						var_r26 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC);
						// rlwinm r29,r11,2,0,29
						var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
					loc_82468300:
						// lwz r11,20(r28)
						ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 20);
						// mr r5,r26
						ctx.r5.u64 = var_r26;
						// li r4,0
						ctx.r4.s64 = 0;
						// add r3,r29,r11
						ctx.r3.u64 = var_r29 + ctx.r11.u64;
						// bl 0x8242fed0
						memset(ctx, base);
						// addi r30,r30,-1
						var_r30 = (uint32_t)(var_r30 + -1);
						// addi r29,r29,1024
						var_r29 = (uint32_t)(var_r29 + 1024);
						// cmplwi cr6,r30,0
						// bne cr6,0x82468300
						if (var_r30 != 0) goto loc_82468300;
					}
				loc_82468324:
					// li r26,1
					var_r26 = 1;
				}
			loc_82468328:
				// addi r7,r27,1
				ctx.r7.s64 = (int64_t)(int32_t)var_r27 + 1;
				// lbz r6,52(r31)
				ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 52);
				// clrlwi r27,r7,24
				var_r27 = (uint32_t)(ctx.r7.u32 & 0xFF);
				// cmplw cr6,r27,r6
				// blt cr6,0x824681f0
				if (var_r27 < ctx.r6.u32) goto loc_824681F0;
			}
		loc_8246833C:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x82467278
			ke_7278(ctx, base);
			// mr r28,r3
			var_r28 = ctx.r3.u32;
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r11,4(r25)
			ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r11,0
			// beq cr6,0x8246836c
			if (ctx.r11.s32 != 0) {
				// lwz r5,8(r25)
				ctx.r5.u64 = PPC_LOAD_U32(var_r25 + 8);
				// cmplw cr6,r30,r5
				// beq cr6,0x82468380
				if (var_r30 == ctx.r5.u32) goto loc_82468380;
			}
		loc_8246836C:
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// stw r30,8(r25)
			PPC_STORE_U32(var_r25 + 8, var_r30);
			// stb r29,12(r25)
			PPC_STORE_U8(var_r25 + 12, (uint8_t)var_r29);
			// lwz r11,4(r25)
			ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 4);
		loc_82468380:
			// addi r7,r11,1
			ctx.r7.s64 = ctx.r11.s64 + 1;
			// stw r7,4(r25)
			PPC_STORE_U32(var_r25 + 4, ctx.r7.u32);
			// lbz r4,52(r31)
			ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 52);
			// cmplwi cr6,r4,0
			// beq cr6,0x824683c4
			if (ctx.r4.u32 != 0) {
				// mr r11,r23
				ctx.r11.u64 = var_r23;
			loc_82468398:
				// lwz r9,184(r31)
				ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 184);
				// mulli r10,r11,88
				ctx.r10.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(88));
				// add r10,r10,r9
				ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
				// addi r3,r11,1
				ctx.r3.s64 = ctx.r11.s64 + 1;
				// clrlwi r11,r3,24
				ctx.r11.u64 = ctx.r3.u32 & 0xFF;
				// lfs f12,40(r10)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
				ctx.f12.f64 = double(temp.f32);
				// stfs f12,36(r10)
				temp.f32 = float(ctx.f12.f64);
				PPC_STORE_U32(ctx.r10.u32 + 36, temp.u32);
				// lbz r10,52(r31)
				ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 52);
				// cmplw cr6,r11,r10
				// blt cr6,0x82468398
				if (ctx.r11.u32 < ctx.r10.u32) goto loc_82468398;
				// lwz r7,4(r25)
				ctx.r7.u64 = PPC_LOAD_U32(var_r25 + 4);
			}
		loc_824683C4:
			// clrlwi r9,r26,24
			ctx.r9.u64 = var_r26 & 0xFF;
			// cmplwi cr6,r9,0
			// beq cr6,0x82468574
			if (ctx.r9.u32 == 0) goto loc_82468574;
			// lbz r11,168(r31)
			ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 168);
			// rlwinm r8,r11,0,28,28
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x8;
			// cmplwi cr6,r8,0
			// bne cr6,0x82468544
			if (ctx.r8.u32 != 0) goto loc_82468544;
			// mr r10,r23
			ctx.r10.u64 = var_r23;
			// mr r8,r23
			ctx.r8.u64 = var_r23;
			// addi r9,r31,20
			ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 20;
		loc_824683EC:
			// cmplwi cr6,r10,0
			// beq cr6,0x824684c8
			if (ctx.r10.u32 == 0) goto loc_824684C8;
			// lwz r11,8(r9)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
			// add r11,r11,r10
			ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
			// b 0x824684cc
			goto loc_824684CC;
		}
	loc_82468400:
		// lbz r5,52(r31)
		ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 52);
		// cmplwi cr6,r5,0
		// beq cr6,0x82468474
		if (ctx.r5.u32 != 0) {
			// mr r26,r23
			var_r26 = (uint32_t)(var_r23);
		loc_82468410:
			// lwz r10,184(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 184);
			// mulli r11,r26,88
			ctx.r11.s64 = static_cast<int64_t>(var_r26 * static_cast<uint64_t>(88));
			// add r28,r11,r10
			var_r28 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
			// lwz r11,24(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 24);
			// lbz r30,13(r28)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r28 + 13));
			// cmplwi cr6,r11,0
			// beq cr6,0x82468460
			if (ctx.r11.u32 != 0) {
				// cmplwi cr6,r30,0
				// beq cr6,0x82468460
				if (var_r30 == 0) goto loc_82468460;
				// rlwinm r27,r11,2,0,29
				var_r27 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
				// mr r29,r23
				var_r29 = (uint32_t)(var_r23);
			loc_8246843C:
				// lwz r11,20(r28)
				ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 20);
				// mr r5,r27
				ctx.r5.u64 = var_r27;
				// li r4,0
				ctx.r4.s64 = 0;
				// add r3,r29,r11
				ctx.r3.u64 = var_r29 + ctx.r11.u64;
				// bl 0x8242fed0
				memset(ctx, base);
				// addi r30,r30,-1
				var_r30 = (uint32_t)(var_r30 + -1);
				// addi r29,r29,1024
				var_r29 = (uint32_t)(var_r29 + 1024);
				// cmplwi cr6,r30,0
				// bne cr6,0x8246843c
				if (var_r30 != 0) goto loc_8246843C;
			}
		loc_82468460:
			// addi r4,r26,1
			ctx.r4.s64 = (int64_t)(int32_t)var_r26 + 1;
			// lbz r3,52(r31)
			ctx.r3.u64 = PPC_LOAD_U8(var_r31 + 52);
			// clrlwi r26,r4,24
			var_r26 = (uint32_t)(ctx.r4.u32 & 0xFF);
			// cmplw cr6,r26,r3
			// blt cr6,0x82468410
			if (var_r26 < ctx.r3.u32) goto loc_82468410;
		}
	loc_82468474:
		// lwz r9,4(r25)
		ctx.r9.u64 = PPC_LOAD_U32(var_r25 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r9,0
		// beq cr6,0x824684c0
		if (ctx.r9.s32 != 0) {
			// lwz r11,8(r25)
			ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 8);
			// cmplw cr6,r10,r11
			// bne cr6,0x824684c0
			if (ctx.r10.u32 != ctx.r11.u32) goto loc_824684C0;
			// addi r11,r9,-1
			ctx.r11.s64 = ctx.r9.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r25)
			PPC_STORE_U32(var_r25 + 4, ctx.r11.u32);
			// bne cr6,0x824684c0
			if (ctx.r11.s32 != 0) goto loc_824684C0;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// lbz r31,12(r25)
			var_r31 = (uint32_t)(PPC_LOAD_U8(var_r25 + 12));
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// stb r11,12(r25)
			PPC_STORE_U8(var_r25 + 12, ctx.r11.u8);
			// stw r11,8(r25)
			PPC_STORE_U32(var_r25 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_824684C0:
		// li r30,1
		var_r30 = 1;
		// b 0x82467f9c
		goto loc_82467F9C;
	loc_824684C8:
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	loc_824684CC:
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r9,r11
		// beq cr6,0x824684e4
		if (ctx.r9.u32 != ctx.r11.u32) {
			// lwz r6,8(r9)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
			// subf r11,r6,r11
			ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
			// b 0x824684e8
		} else {
		loc_824684E4:
			// mr r11,r23
			ctx.r11.u64 = var_r23;
		}
	loc_824684E8:
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
		// cmplwi cr6,r11,0
		// beq cr6,0x82468518
		if (ctx.r11.u32 == 0) goto loc_82468518;
		// lbz r5,96(r11)
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 96);
		// li r11,1
		ctx.r11.s64 = 1;
		// cmplwi cr6,r5,4
		// bne cr6,0x82468508
		if (ctx.r5.u32 == 4) {
			// mr r11,r23
			ctx.r11.u64 = var_r23;
		}
	loc_82468508:
		// clrlwi r4,r8,24
		ctx.r4.u64 = ctx.r8.u32 & 0xFF;
		// or r3,r4,r11
		ctx.r3.u64 = ctx.r4.u64 | ctx.r11.u64;
		// clrlwi r8,r3,24
		ctx.r8.u64 = ctx.r3.u32 & 0xFF;
		// b 0x824683ec
		goto loc_824683EC;
	loc_82468518:
		// clrlwi r11,r8,24
		ctx.r11.u64 = ctx.r8.u32 & 0xFF;
		// cmplwi cr6,r11,0
		// bne cr6,0x82468574
		if (ctx.r11.u32 == 0) {
			// addi r3,r31,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 16;
			// li r5,16
			ctx.r5.s64 = 16;
			// li r4,16
			ctx.r4.s64 = 16;
			// lwz r9,68(r10)
			// bctrl
			VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
			// b 0x82468570
			goto loc_82468570;
		loc_82468544:
			// cmpwi cr6,r28,0
			// bne cr6,0x82468574
			if ((int32_t)var_r28 != 0) goto loc_82468574;
			// rlwinm r8,r11,0,27,27
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
			// cmplwi cr6,r8,0
			// bne cr6,0x82468574
			if (ctx.r8.u32 != 0) goto loc_82468574;
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,44(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 11, ctx, base);  // pattern-B slot 11 (byte +44)
		loc_82468570:
			// lwz r7,4(r25)
			ctx.r7.u64 = PPC_LOAD_U32(var_r25 + 4);
		}
	loc_82468574:
		// lbz r11,168(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 168);
		// rlwinm r5,r11,0,29,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
		// cmplwi cr6,r5,0
		// beq cr6,0x824685b8
		if (ctx.r5.u32 != 0) {
			// lhz r4,170(r31)
			ctx.r4.u64 = PPC_LOAD_U16(var_r31 + 170);
			// cmplwi cr6,r4,0
			// beq cr6,0x8246859c
			if (ctx.r4.u32 != 0) {
				// rlwinm r3,r11,0,27,27
				ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
				// cmplwi cr6,r3,0
				// beq cr6,0x824685b8
				if (ctx.r3.u32 == 0) goto loc_824685B8;
			}
		loc_8246859C:
			// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r10,44(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 11, ctx, base);  // pattern-B slot 11 (byte +44)
			// lwz r7,4(r25)
			ctx.r7.u64 = PPC_LOAD_U32(var_r25 + 4);
		}
	loc_824685B8:
		// lbz r11,168(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 168);
		// clrlwi r9,r11,31
		ctx.r9.u64 = ctx.r11.u32 & 0x1;
		// cmplwi cr6,r9,0
		// beq cr6,0x824685d4
		if (ctx.r9.u32 != 0) {
			// rlwinm r8,r11,0,27,27
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
			// cmplwi cr6,r8,0
			// beq cr6,0x82468774
			if (ctx.r8.u32 == 0) goto loc_82468774;
		}
	loc_824685D4:
		// addi r27,r31,20
		var_r27 = (uint32_t)(var_r31 + 20);
		// lwz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
		// cmplw cr6,r27,r11
		// beq cr6,0x824685f0
		if (var_r27 != ctx.r11.u32) {
			// lwz r6,8(r27)
			ctx.r6.u64 = PPC_LOAD_U32(var_r27 + 8);
			// subf r11,r6,r11
			ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
			// b 0x824685f4
		} else {
		loc_824685F0:
			// mr r11,r23
			ctx.r11.u64 = var_r23;
		}
	loc_824685F4:
		// mr r28,r11
		var_r28 = ctx.r11.u32;
		// cmplwi cr6,r11,0
		// beq cr6,0x82468774
	while (var_r28 != 0) {
		loc_82468600:
			// mr r29,r28
			var_r29 = (uint32_t)(var_r28);
			// cmplwi cr6,r28,0
			// beq cr6,0x82468618
			if (var_r28 != 0) {
				// lwz r11,8(r27)
				ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 8);
				// add r11,r11,r28
				ctx.r11.u64 = ctx.r11.u64 + var_r28;
				// b 0x8246861c
			} else {
			loc_82468618:
				// mr r11,r27
				ctx.r11.u64 = var_r27;
			}
		loc_8246861C:
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// cmplw cr6,r27,r11
			// beq cr6,0x82468634
			if (var_r27 != ctx.r11.u32) {
				// lwz r5,8(r27)
				ctx.r5.u64 = PPC_LOAD_U32(var_r27 + 8);
				// subf r28,r5,r11
				var_r28 = (uint32_t)(ctx.r11.s64 - ctx.r5.s64);
				// b 0x82468638
			} else {
			loc_82468634:
				// mr r28,r23
				var_r28 = (uint32_t)(var_r23);
			}
		loc_82468638:
			// lbz r4,96(r29)
			ctx.r4.u64 = PPC_LOAD_U8(var_r29 + 96);
			// cmplwi cr6,r4,4
			// bne cr6,0x8246876c
			if (ctx.r4.u32 != 4) goto loc_8246876C;
			// mr r11,r13
			ctx.r11.u64 = ctx.r13.u64;
			// cmpwi cr6,r7,0
			// beq cr6,0x8246868c
			if (ctx.r7.s32 != 0) {
				// lwz r3,8(r25)
				ctx.r3.u64 = PPC_LOAD_U32(var_r25 + 8);
				// cmplw cr6,r11,r3
				// bne cr6,0x8246868c
				if (ctx.r11.u32 != ctx.r3.u32) goto loc_8246868C;
				// addi r11,r7,-1
				ctx.r11.s64 = ctx.r7.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r25)
				PPC_STORE_U32(var_r25 + 4, ctx.r11.u32);
				// bne cr6,0x8246868c
				if (ctx.r11.s32 != 0) goto loc_8246868C;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
				// lbz r30,12(r25)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r25 + 12));
				// mr r3,r25
				ctx.r3.u64 = var_r25;
				// stb r11,12(r25)
				PPC_STORE_U8(var_r25 + 12, ctx.r11.u8);
				// stw r11,8(r25)
				PPC_STORE_U32(var_r25 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_8246868C:
			// lwz r11,4(r24)
			ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
			// mr r10,r13
			ctx.r10.u64 = ctx.r13.u64;
			// cmpwi cr6,r11,0
			// beq cr6,0x824686d0
			if (ctx.r11.s32 != 0) {
				// lwz r9,8(r24)
				ctx.r9.u64 = PPC_LOAD_U32(var_r24 + 8);
				// cmplw cr6,r10,r9
				// bne cr6,0x824686d0
				if (ctx.r10.u32 != ctx.r9.u32) goto loc_824686D0;
				// addic. r11,r11,-1
				ctx.xer.ca = ctx.r11.u32 > 0;
				ctx.r11.s64 = ctx.r11.s64 + -1;
				// stw r11,4(r24)
				PPC_STORE_U32(var_r24 + 4, ctx.r11.u32);
				// bne 0x824686d0
				if (ctx.r11.s32 != 0) goto loc_824686D0;
				// mr r3,r24
				ctx.r3.u64 = var_r24;
				// lbz r30,12(r24)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r24 + 12));
				// stw r23,8(r24)
				PPC_STORE_U32(var_r24 + 8, var_r23);
				// stb r23,12(r24)
				PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r23);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_824686D0:
			// addi r3,r31,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 16;
			// li r5,0
			ctx.r5.s64 = 0;
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// lwz r7,76(r8)
			// bctrl
			VCALL(ctx.r3.u32, 19, ctx, base);  // vtable slot 19 (byte +76)
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r6,4(r24)
			ctx.r6.u64 = PPC_LOAD_U32(var_r24 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r6,0
			// beq cr6,0x82468710
			if (ctx.r6.s32 != 0) {
				// lwz r5,8(r24)
				ctx.r5.u64 = PPC_LOAD_U32(var_r24 + 8);
				// cmplw cr6,r30,r5
				// beq cr6,0x82468720
				if (var_r30 == ctx.r5.u32) goto loc_82468720;
			}
		loc_82468710:
			// mr r3,r24
			ctx.r3.u64 = var_r24;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// stw r30,8(r24)
			PPC_STORE_U32(var_r24 + 8, var_r30);
			// stb r29,12(r24)
			PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r29);
		loc_82468720:
			// lwz r11,4(r24)
			ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
			// addi r4,r11,1
			ctx.r4.s64 = ctx.r11.s64 + 1;
			// stw r4,4(r24)
			PPC_STORE_U32(var_r24 + 4, ctx.r4.u32);
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r11,4(r25)
			ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r11,0
			// beq cr6,0x82468750
			if (ctx.r11.s32 != 0) {
				// lwz r3,8(r25)
				ctx.r3.u64 = PPC_LOAD_U32(var_r25 + 8);
				// cmplw cr6,r30,r3
				// beq cr6,0x82468764
				if (var_r30 == ctx.r3.u32) goto loc_82468764;
			}
		loc_82468750:
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// stw r30,8(r25)
			PPC_STORE_U32(var_r25 + 8, var_r30);
			// stb r29,12(r25)
			PPC_STORE_U8(var_r25 + 12, (uint8_t)var_r29);
			// lwz r11,4(r25)
			ctx.r11.u64 = PPC_LOAD_U32(var_r25 + 4);
		loc_82468764:
			// addi r7,r11,1
			ctx.r7.s64 = ctx.r11.s64 + 1;
			// stw r7,4(r25)
			PPC_STORE_U32(var_r25 + 4, ctx.r7.u32);
		loc_8246876C:
			// cmplwi cr6,r28,0
			// bne cr6,0x82468600
	}
	loc_82468774:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r7,0
		// beq cr6,0x824687bc
		if (ctx.r7.s32 != 0) {
			// lwz r10,8(r25)
			ctx.r10.u64 = PPC_LOAD_U32(var_r25 + 8);
			// cmplw cr6,r11,r10
			// bne cr6,0x824687bc
			if (ctx.r11.u32 != ctx.r10.u32) goto loc_824687BC;
			// addi r11,r7,-1
			ctx.r11.s64 = ctx.r7.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r25)
			PPC_STORE_U32(var_r25 + 4, ctx.r11.u32);
			// bne cr6,0x824687bc
			if (ctx.r11.s32 != 0) goto loc_824687BC;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// lbz r30,12(r25)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r25 + 12));
			// mr r3,r25
			ctx.r3.u64 = var_r25;
			// stb r11,12(r25)
			PPC_STORE_U8(var_r25 + 12, ctx.r11.u8);
			// stw r11,8(r25)
			PPC_STORE_U32(var_r25 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_824687BC:
		// clrlwi r9,r22,24
		ctx.r9.u64 = var_r22 & 0xFF;
		// cmplwi cr6,r9,0
		// beq cr6,0x82468840
		if (!(ctx.r9.u32 == 0)) {
			// lwz r3,180(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
			// bl 0x8246aef8
			ph_AEF8(ctx, base);
		} else {
			if (!(ctx.r3.s32 == 0)) {
				// lwz r3,180(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 180);
				// bl 0x8246b9e0
				ph_B9E0(ctx, base);
				// mr r30,r3
				var_r30 = ctx.r3.u32;
			} else {
				if (!((int32_t)var_r30 >= 0)) {
					// lwz r11,4(r24)
					ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
					// mr r10,r13
					ctx.r10.u64 = ctx.r13.u64;
					// cmpwi cr6,r11,0
					// beq cr6,0x82467e7c
					if (ctx.r11.s32 == 0) {
					// mr r3,r30
					ctx.r3.u64 = var_r30;
					return;
					}
					// lwz r8,8(r24)
					ctx.r8.u64 = PPC_LOAD_U32(var_r24 + 8);
					// cmplw cr6,r10,r8
					// bne cr6,0x82467e7c
					if (ctx.r10.u32 != ctx.r8.u32) {
					// mr r3,r30
					ctx.r3.u64 = var_r30;
					return;
					}
					// addic. r11,r11,-1
					ctx.xer.ca = ctx.r11.u32 > 0;
					ctx.r11.s64 = ctx.r11.s64 + -1;
					// stw r11,4(r24)
					PPC_STORE_U32(var_r24 + 4, ctx.r11.u32);
					// bne 0x82467e7c
					if (ctx.r11.s32 != 0) {
					// mr r3,r30
					ctx.r3.u64 = var_r30;
					return;
					}
					// mr r3,r24
					ctx.r3.u64 = var_r24;
					// lbz r31,12(r24)
					var_r31 = (uint32_t)(PPC_LOAD_U8(var_r24 + 12));
					// stw r23,8(r24)
					PPC_STORE_U32(var_r24 + 8, var_r23);
					// stb r23,12(r24)
					PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r23);
					// bl 0x8258631c
					__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
					// mr r3,r31
					ctx.r3.u64 = var_r31;
					// bl 0x8258654c
					__imp__KfLowerIrql(ctx, base);
					// mr r3,r30
					ctx.r3.u64 = var_r30;
					return;
				}
			}
		}
	loc_82468840:
		// lwz r11,4(r24)
		ctx.r11.u64 = PPC_LOAD_U32(var_r24 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82468884
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r7,8(r24)
		ctx.r7.u64 = PPC_LOAD_U32(var_r24 + 8);
		// cmplw cr6,r10,r7
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, ctx.xer);
	}
loc_82468858:
	// bne cr6,0x82468884
	if (ctx.cr6.eq) {
		// addic. r11,r11,-1
		ctx.xer.ca = ctx.r11.u32 > 0;
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// stw r11,4(r24)
		PPC_STORE_U32(var_r24 + 4, ctx.r11.u32);
		// bne 0x82468884
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r31,12(r24)
		var_r31 = (uint32_t)(PPC_LOAD_U8(var_r24 + 12));
		// mr r3,r24
		ctx.r3.u64 = var_r24;
		// stw r23,8(r24)
		PPC_STORE_U32(var_r24 + 8, var_r23);
		// stb r23,12(r24)
		PPC_STORE_U8(var_r24 + 12, (uint8_t)var_r23);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82468884:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__atSingleton_8898_fw"))) PPC_WEAK_FUNC(atSingleton_8898_fw);
PPC_FUNC_IMPL(__imp__atSingleton_8898_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// li r4,280
	ctx.r4.s64 = 280;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,20(r11)
	// bctrl
	VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	// cmplwi cr6,r3,0
	// beq cr6,0x824688e8
	if (ctx.r3.u32 != 0) {
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x82467c28
		atSingleton_7C28(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmplwi cr6,r31,0
		// bne cr6,0x824688f8
		if (var_r31 != 0) goto loc_824688F8;
	}
loc_824688E8:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
loc_824688F8:
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,104(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 104);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82468924
	if ((int32_t)var_r30 >= 0) {
		// stw r31,0(r29)
		PPC_STORE_U32(var_r29 + 0,/* atSingleton::vtable@+0x0 */ var_r31);
		return;
	}
loc_82468924:
	// addi r3,r31,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 4;
	// lwz r6,12(r7)
	// bctrl
	VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__ph_vt5B98_20_8948"))) PPC_WEAK_FUNC(ph_vt5B98_20_8948);
PPC_FUNC_IMPL(__imp__ph_vt5B98_20_8948) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82468a48
	atSingleton_8A48_p42(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5B98_0_8950"))) PPC_WEAK_FUNC(ph_vt5B98_0_8950);
PPC_FUNC_IMPL(__imp__ph_vt5B98_0_8950) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82468a48
	atSingleton_8A48_p42(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt57D8_27_8958"))) PPC_WEAK_FUNC(ph_vt57D8_27_8958);
PPC_FUNC_IMPL(__imp__ph_vt57D8_27_8958) {
	PPC_FUNC_PROLOGUE();
	// li r11,4
	ctx.r11.s64 = 4;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r11.u8);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lhz r11,23128(r11)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 23128);
	// stb r10,4(r4)
	PPC_STORE_U8(ctx.r4.u32 + 4, ctx.r10.u8);
	// sth r11,2(r4)
	PPC_STORE_U16(ctx.r4.u32 + 2, ctx.r11.u16);
	// blr
	return;
}

__attribute__((alias("__imp__CPeakMeterEffect_vfn_5"))) PPC_WEAK_FUNC(CPeakMeterEffect_vfn_5);
PPC_FUNC_IMPL(__imp__CPeakMeterEffect_vfn_5) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5B98_37_8990"))) PPC_WEAK_FUNC(ph_vt5B98_37_8990);
PPC_FUNC_IMPL(__imp__ph_vt5B98_37_8990) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// b 0x82466378
	ph_vt57D8_2_6378(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5B98_38_8998"))) PPC_WEAK_FUNC(ph_vt5B98_38_8998);
PPC_FUNC_IMPL(__imp__ph_vt5B98_38_8998) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// b 0x824661e0
	ph_vt57D8_3_61E0(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_59_89A0"))) PPC_WEAK_FUNC(ph_vt5A60_59_89A0);
PPC_FUNC_IMPL(__imp__ph_vt5A60_59_89A0) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,168(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 168);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A78_63_89B0"))) PPC_WEAK_FUNC(ph_vt5A78_63_89B0);
PPC_FUNC_IMPL(__imp__ph_vt5A78_63_89B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,108(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A80_63_89C0"))) PPC_WEAK_FUNC(ph_vt5A80_63_89C0);
PPC_FUNC_IMPL(__imp__ph_vt5A80_63_89C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,112(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5B98_43_89D0"))) PPC_WEAK_FUNC(ph_vt5B98_43_89D0);
PPC_FUNC_IMPL(__imp__ph_vt5B98_43_89D0) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lhz r10,104(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 104);
	// lwz r9,116(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 116);
	// mullw r8,r10,r9
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5B98_44_89F0"))) PPC_WEAK_FUNC(ph_vt5B98_44_89F0);
PPC_FUNC_IMPL(__imp__ph_vt5B98_44_89F0) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// li r3,0
	ctx.r3.s64 = 0;
	// lhz r9,104(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 104);
	// divwu r8,r10,r9
	ctx.r8.u32 = ctx.r9.u32 ? ctx.r10.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// stw r8,116(r11)
	PPC_STORE_U32(ctx.r11.u32 + 116, ctx.r8.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A74_63_8A10"))) PPC_WEAK_FUNC(ph_vt5A74_63_8A10);
PPC_FUNC_IMPL(__imp__ph_vt5A74_63_8A10) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,164(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 164);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt57D8_36_8A20"))) PPC_WEAK_FUNC(ph_vt57D8_36_8A20);
PPC_FUNC_IMPL(__imp__ph_vt57D8_36_8A20) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r3,-4
	ctx.r11.s64 = ctx.r3.s64 + -4;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt57D8_46_8A30"))) PPC_WEAK_FUNC(ph_vt57D8_46_8A30);
PPC_FUNC_IMPL(__imp__ph_vt57D8_46_8A30) {
	PPC_FUNC_PROLOGUE();
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A60_53_8A38"))) PPC_WEAK_FUNC(ph_vt5A60_53_8A38);
PPC_FUNC_IMPL(__imp__ph_vt5A60_53_8A38) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// b 0x8245fd08
	// FATAL: unresolved function 0x8245FD08 (no CallTarget in FunctionNode)
	REX_FATAL("Unresolved call from 0x82468A3C to 0x8245FD08");
	return;
}

__attribute__((alias("__imp__atSingleton_8A48_p42"))) PPC_WEAK_FUNC(atSingleton_8A48_p42);
PPC_FUNC_IMPL(__imp__atSingleton_8A48_p42) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32256
	// lis r10,-32256
	// lis r9,-32256
	// addi r3,r31,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 4;
	// addi r11,r11,23568
	ctx.r11.s64 = ctx.r11.s64 + 23568;
	// addi r10,r10,23528
	ctx.r10.s64 = ctx.r10.s64 + 23528;
	// addi r9,r9,23448
	ctx.r9.s64 = ctx.r9.s64 + 23448;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r9.u32);
	// bl 0x82460148
	atSingleton_0148_p39(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5B98_54_8AA0"))) PPC_WEAK_FUNC(ph_vt5B98_54_8AA0);
PPC_FUNC_IMPL(__imp__ph_vt5B98_54_8AA0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r3,r31,16
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 16;
	// bl 0x824662f8
	ph_vt57D8_12_62F8(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x82468aec
	if (ctx.r3.s32 >= 0) {
		// lbz r9,192(r31)
		ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 192);
		// lis r11,-32256
		// lbz r8,193(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 193);
		// li r10,1
		ctx.r10.s64 = 1;
		// addi r11,r11,23672
		ctx.r11.s64 = ctx.r11.s64 + 23672;
		// rotlwi r7,r9,2
		ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
		// lwzx r6,r7,r11
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r11.u32);
		// sth r10,106(r31)
		PPC_STORE_U16(var_r31 + 106, ctx.r10.u16);
		// mullw r5,r6,r8
		ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r8.s32);
		// sth r5,104(r31)
		PPC_STORE_U16(var_r31 + 104, ctx.r5.u16);
	}
loc_82468AEC:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5B98_50_8B00"))) PPC_WEAK_FUNC(ph_vt5B98_50_8B00);
PPC_FUNC_IMPL(__imp__ph_vt5B98_50_8B00) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468b44
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82468b58
		if (var_r30 == ctx.r10.u32) goto loc_82468B58;
	}
loc_82468B44:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82468B58:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// stfs f31,108(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r29 + 108, temp.u32);
	// stfs f31,220(r29)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r29 + 220, temp.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468bb0
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x82468bb0
		if (ctx.r10.u32 != ctx.r9.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82468bb0
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82468BB0:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5B98_52_8BC0"))) PPC_WEAK_FUNC(ph_vt5B98_52_8BC0);
PPC_FUNC_IMPL(__imp__ph_vt5B98_52_8BC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// fmr f31,f1
	var_f31 = ctx.f1.f64;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468c04
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82468c18
		if (var_r30 == ctx.r10.u32) goto loc_82468C18;
	}
loc_82468C04:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82468C18:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// fmr f2,f31
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = var_f31;
	// addi r30,r29,180
	var_r30 = (uint32_t)(var_r29 + 180);
	// lis r10,-32248
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// stfs f31,112(r29)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r29 + 112, temp.u32);
	// lbz r11,52(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 52);
	// lbz r9,48(r29)
	ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 48);
	// lfd f1,-25408(r10)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25408);
	// lwz r28,56(r29)
	var_r28 = (uint32_t)(PPC_LOAD_U32(var_r29 + 56));
	// stb r11,13(r30)
	PPC_STORE_U8(var_r30 + 13, ctx.r11.u8);
	// stb r9,12(r30)
	PPC_STORE_U8(var_r30 + 12, ctx.r9.u8);
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// mr r8,r28
	ctx.r8.u64 = var_r28;
	// lwz r6,80(r30)
	ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 80);
	// addi r7,r30,16
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 16;
	// ori r5,r6,1
	ctx.r5.u64 = ctx.r6.u64 | 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// stw r5,80(r30)
	PPC_STORE_U32(var_r30 + 80, ctx.r5.u32);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// fmul f12,f13,f1
	ctx.f12.f64 = ctx.f13.f64 * ctx.f1.f64;
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f11.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468cc0
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x82468cc0
		if (ctx.r10.u32 != ctx.r9.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82468cc0
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82468CC0:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt57D8_35_8CD0"))) PPC_WEAK_FUNC(ph_vt57D8_35_8CD0);
PPC_FUNC_IMPL(__imp__ph_vt57D8_35_8CD0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82468980
	CPeakMeterEffect_vfn_5(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_47_8CD8"))) PPC_WEAK_FUNC(ph_vt5A60_47_8CD8);
PPC_FUNC_IMPL(__imp__ph_vt5A60_47_8CD8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82468958
	ph_vt57D8_27_8958(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_25_8D08"))) PPC_WEAK_FUNC(ph_vt5A60_25_8D08);
PPC_FUNC_IMPL(__imp__ph_vt5A60_25_8D08) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466930
	ph_vt5A6C_63_6930(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_26_8D10"))) PPC_WEAK_FUNC(ph_vt5A60_26_8D10);
PPC_FUNC_IMPL(__imp__ph_vt5A60_26_8D10) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82466980
	ph_vt5A70_63_6980(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_27_8D18"))) PPC_WEAK_FUNC(ph_vt5A60_27_8D18);
PPC_FUNC_IMPL(__imp__ph_vt5A60_27_8D18) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x824689a0
	ph_vt5A60_59_89A0(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_33_8D30"))) PPC_WEAK_FUNC(ph_vt5A60_33_8D30);
PPC_FUNC_IMPL(__imp__ph_vt5A60_33_8D30) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x824689b0
	ph_vt5A78_63_89B0(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_35_8D40"))) PPC_WEAK_FUNC(ph_vt5A60_35_8D40);
PPC_FUNC_IMPL(__imp__ph_vt5A60_35_8D40) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x824689c0
	ph_vt5A80_63_89C0(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5B98_40_8D50"))) PPC_WEAK_FUNC(ph_vt5B98_40_8D50);
PPC_FUNC_IMPL(__imp__ph_vt5B98_40_8D50) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_28
	// addi r30,r3,16
	var_r30 = (uint32_t)(ctx.r3.s64 + 16);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468d8c
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r10
		// beq cr6,0x82468da0
		if (var_r29 == ctx.r10.u32) goto loc_82468DA0;
	}
loc_82468D8C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82468DA0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 152);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x82468dd4
	if (ctx.r9.u32 == 0) {
		// lwz r8,0(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 0);
		// li r5,3
		ctx.r5.s64 = 3;
		// li r4,3
		ctx.r4.s64 = 3;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r7,68(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 68);
		// mtctr r7
		ctx.ctr.u64 = ctx.r7.u64;
		// b 0x82468df8
	} else {
	loc_82468DD4:
		// rlwinm r6,r10,0,29,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
		// cmplwi cr6,r6,0
		// beq cr6,0x82468e00
		if (ctx.r6.u32 == 0) goto loc_82468E00;
		// lwz r11,0(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
		// li r5,2
		ctx.r5.s64 = 2;
		// li r4,6
		ctx.r4.s64 = 6;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r10,68(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
		// mtctr r10
		ctx.ctr.u64 = ctx.r10.u64;
	}
loc_82468DF8:
	// bctrl
	ctx.lr = 0x82468DFC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82468E00:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82468e44
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x82468e44
		if (ctx.r10.u32 != ctx.r9.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82468e44
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82468E44:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5B98_41_8E50"))) PPC_WEAK_FUNC(ph_vt5B98_41_8E50);
PPC_FUNC_IMPL(__imp__ph_vt5B98_41_8E50) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// addi r30,r3,16
	var_r30 = (uint32_t)(ctx.r3.s64 + 16);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468e90
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r10
		// beq cr6,0x82468ea4
		if (var_r29 == ctx.r10.u32) goto loc_82468EA4;
	}
loc_82468E90:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82468EA4:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r10,152(r30)
	ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 152);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// beq cr6,0x82468f2c
	if (ctx.r9.u32 != 0) {
		// clrlwi r8,r27,31
		ctx.r8.u64 = var_r27 & 0x1;
		// cmplwi cr6,r8,0
		// bne cr6,0x82468f0c
		if (ctx.r8.u32 == 0) {
			// andi. r7,r10,18
			ctx.r7.u64 = ctx.r10.u64 & 18;
			ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
			// cmplwi cr6,r7,0
			// bne cr6,0x82468f0c
			if (ctx.r7.u32 != 0) goto loc_82468F0C;
			// rlwinm r6,r10,0,29,29
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
			// cmplwi cr6,r6,0
			// bne cr6,0x82468f2c
			if (ctx.r6.u32 != 0) goto loc_82468F2C;
			// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
			// li r5,4
			ctx.r5.s64 = 4;
			// li r4,4
			ctx.r4.s64 = 4;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r10,68(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
			// lis r11,-32256
			ctx.r11.s64 = -2113929216;
			// lhz r11,23128(r11)
			ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 23128);
			// sth r11,154(r30)
			PPC_STORE_U16(var_r30 + 154, ctx.r11.u16);
			// b 0x82468f28
		} else {
		loc_82468F0C:
			// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
			// li r5,0
			ctx.r5.s64 = 0;
			// li r4,87
			ctx.r4.s64 = 87;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r8,68(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		}
	loc_82468F28:
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_82468F2C:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82468f70
	if (ctx.r11.s32 != 0) {
		// lwz r7,8(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r7
		// bne cr6,0x82468f70
		if (ctx.r10.u32 != ctx.r7.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82468f70
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82468F70:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5A60_62_8F80"))) PPC_WEAK_FUNC(ph_vt5A60_62_8F80);
PPC_FUNC_IMPL(__imp__ph_vt5A60_62_8F80) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// addi r29,r3,16
	var_r29 = (uint32_t)(ctx.r3.s64 + 16);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82468fbc
	if (ctx.r11.s32 != 0) {
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r8
		// beq cr6,0x82468fd4
		if (var_r30 == ctx.r8.u32) goto loc_82468FD4;
	}
loc_82468FBC:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = var_r30;
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82468FD4:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r29,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 4;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r10,r9
	// beq cr6,0x82469020
	if (ctx.r10.u32 != ctx.r9.u32) {
		// lwz r10,8(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// subf r10,r10,r9
		ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
		// cmplwi cr6,r10,0
		// beq cr6,0x82469020
		if (ctx.r10.u32 == 0) goto loc_82469020;
		// lwz r9,0(r29)
  // [ph4a] vtable load collapsed
		// li r5,8
		ctx.r5.s64 = 8;
		// li r4,8
		ctx.r4.s64 = 8;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r8,68(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_82469020:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82469060
	if (ctx.r11.s32 != 0) {
		// cmplw cr6,r10,r8
		// bne cr6,0x82469060
		if (ctx.r10.u32 != ctx.r8.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82469060
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82469060:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__phInst_9070_p42"))) PPC_WEAK_FUNC(phInst_9070_p42);
PPC_FUNC_IMPL(__imp__phInst_9070_p42) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,68(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 68);
	// li r3,0
	ctx.r3.s64 = 0;
	// mulli r11,r11,100
	ctx.r11.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(100));
	// addi r10,r11,268
	ctx.r10.s64 = ctx.r11.s64 + 268;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5B98_8_9088"))) PPC_WEAK_FUNC(ph_vt5B98_8_9088);
PPC_FUNC_IMPL(__imp__ph_vt5B98_8_9088) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82468d50
	ph_vt5B98_40_8D50(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5B98_9_9090"))) PPC_WEAK_FUNC(ph_vt5B98_9_9090);
PPC_FUNC_IMPL(__imp__ph_vt5B98_9_9090) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82468e50
	ph_vt5B98_41_8E50(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5A60_30_9098"))) PPC_WEAK_FUNC(ph_vt5A60_30_9098);
PPC_FUNC_IMPL(__imp__ph_vt5A60_30_9098) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82468f80
	ph_vt5A60_62_8F80(ctx, base);
	return;
}

__attribute__((alias("__imp__atSingleton_90A0"))) PPC_WEAK_FUNC(atSingleton_90A0);
PPC_FUNC_IMPL(__imp__atSingleton_90A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_27
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// addi r29,r31,4
	var_r29 = (uint32_t)(var_r31 + 4);
	// mr r27,r5
	var_r27 = ctx.r5.u32;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x8245fef8
	atSingleton_FEF8_2h(ctx, base);
	// lis r11,-32256
	// lis r10,-32256
	// lis r9,-32256
	// addi r11,r11,23568
	ctx.r11.s64 = ctx.r11.s64 + 23568;
	// addi r10,r10,23528
	ctx.r10.s64 = ctx.r10.s64 + 23528;
	// addi r9,r9,23448
	ctx.r9.s64 = ctx.r9.s64 + 23448;
	// addi r30,r31,180
	var_r30 = (uint32_t)(var_r31 + 180);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// li r8,19
	ctx.r8.s64 = 19;
	// stw r10,0(r29)
	PPC_STORE_U32(var_r29 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// mr r11,r30
	ctx.r11.u64 = var_r30;
	// stw r9,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r9.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_824690FC:
	// stw r7,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x824690fc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_824690FC;
	// mr r5,r27
	ctx.r5.u64 = var_r27;
	// stw r7,76(r30)
	PPC_STORE_U32(var_r30 + 76, ctx.r7.u32);
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bl 0x8245fcd8
	atSingleton_FCD8_2h(ctx, base);
	// lbz r9,69(r28)
	ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 69);
	// lbz r11,52(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 52);
	// lis r10,-32248
	// lbz r8,48(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 48);
	// lfs f2,112(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 112);
	ctx.f2.f64 = double(temp.f32);
	// lwz r29,56(r31)
	var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 56));
	// stw r9,264(r31)
	PPC_STORE_U32(var_r31 + 264, ctx.r9.u32);
	// lfd f1,-25408(r10)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25408);
	// stb r11,13(r30)
	PPC_STORE_U8(var_r30 + 13, ctx.r11.u8);
	// stb r8,12(r30)
	PPC_STORE_U8(var_r30 + 12, ctx.r8.u8);
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// mr r7,r29
	ctx.r7.u64 = var_r29;
	// lwz r9,80(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 80);
	// addi r6,r30,16
	ctx.r6.s64 = (int64_t)(int32_t)var_r30 + 16;
	// ori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 | 1;
	// lis r5,0
	ctx.r5.s64 = 0;
	// lis r11,-32256
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// ori r4,r5,48000
	ctx.r4.u64 = ctx.r5.u64 | 48000;
	// addi r11,r11,23672
	ctx.r11.s64 = ctx.r11.s64 + 23672;
	// stw r8,80(r30)
	PPC_STORE_U32(var_r30 + 80, ctx.r8.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// fmul f12,f13,f1
	ctx.f12.f64 = ctx.f13.f64 * ctx.f1.f64;
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f11.u32);
	// lwz r7,80(r30)
	ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 80);
	// stw r4,32(r30)
	PPC_STORE_U32(var_r30 + 32, ctx.r4.u32);
	// ori r6,r7,2
	ctx.r6.u64 = ctx.r7.u64 | 2;
	// stw r6,80(r30)
	PPC_STORE_U32(var_r30 + 80, ctx.r6.u32);
	// lbz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U8(var_r30 + 12);
	// lbz r4,13(r30)
	ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 13);
	// rotlwi r9,r5,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// lwzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// sth r10,106(r31)
	PPC_STORE_U16(var_r31 + 106, ctx.r10.u16);
	// mullw r7,r8,r4
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// sth r7,104(r31)
	PPC_STORE_U16(var_r31 + 104, ctx.r7.u16);
	return;
}

__attribute__((alias("__imp__ph_vt5B98_36_91C0"))) PPC_WEAK_FUNC(ph_vt5B98_36_91C0);
PPC_FUNC_IMPL(__imp__ph_vt5B98_36_91C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=176, savegprlr_25
	// li r25,0
	var_r25 = 0;
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// mr r27,r25
	var_r27 = (uint32_t)(var_r25);
	// cmplwi cr6,r5,0
	// addi r28,r5,-8
	var_r28 = (uint32_t)(ctx.r5.s64 + -8);
	// bne cr6,0x824691ec
	if (ctx.r5.u32 == 0) {
		// mr r28,r25
		var_r28 = (uint32_t)(var_r25);
	}
loc_824691EC:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82469218
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8246922c
		if (var_r30 == ctx.r10.u32) goto loc_8246922C;
	}
loc_82469218:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246922C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// bl 0x82465d80
	ph_5D80(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmpwi cr6,r29,0
	// blt cr6,0x82469270
	if ((int32_t)var_r29 >= 0) {
		// lbz r11,52(r26)
		ctx.r11.u64 = PPC_LOAD_U8(var_r26 + 52);
		// li r5,1
		ctx.r5.s64 = 1;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// stb r11,81(r1)
		PPC_STORE_U8(ctx.r1.u32 + 81, ctx.r11.u8);
		// bl 0x82465ef0
		ph_5EF0(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmpwi cr6,r29,0
		// bge cr6,0x82469338
		if ((int32_t)var_r29 >= 0) goto loc_82469338;
	}
loc_82469270:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x824692bc
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x824692bc
		if (ctx.r10.u32 != ctx.r11.u32) goto loc_824692BC;
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x824692bc
		if (ctx.r11.s32 != 0) goto loc_824692BC;
		// mr r11,r25
		ctx.r11.u64 = var_r25;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_824692BC:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// cmpwi cr6,r29,0
	// bne cr6,0x8246980c
	if ((int32_t)var_r29 == 0) {
	loc_824692C8:
		// mr r29,r27
		var_r29 = (uint32_t)(var_r27);
		// cmplwi cr6,r27,0
		// beq cr6,0x82469530
		if (var_r27 == 0) goto loc_82469530;
		// lwz r3,184(r26)
		ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 184);
		// lwz r11,188(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 188);
		// subf. r10,r11,r3
		ctx.r10.s64 = ctx.r3.s64 - ctx.r11.s64;
		// bne 0x82469530
		if (ctx.r10.s32 != 0) goto loc_82469530;
		// lwz r11,16(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 16);
		// cmplwi cr6,r11,0
		// beq cr6,0x82469510
		if (ctx.r11.u32 == 0) goto loc_82469510;
		// cmpwi cr6,r11,-1
		// beq cr6,0x82469300
		if (ctx.r11.s32 != -1) {
			// addi r9,r11,-1
			ctx.r9.s64 = ctx.r11.s64 + -1;
			// stw r9,16(r27)
			PPC_STORE_U32(var_r27 + 16, ctx.r9.u32);
		}
	loc_82469300:
		// lwz r8,20(r27)
		ctx.r8.u64 = PPC_LOAD_U32(var_r27 + 20);
		// stw r8,116(r26)
		PPC_STORE_U32(var_r26 + 116, ctx.r8.u32);
		// lwz r11,176(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 176);
		// cmplwi cr6,r11,0
		// beq cr6,0x82469530
		if (ctx.r11.u32 == 0) goto loc_82469530;
		// lwz r7,12(r26)
		ctx.r7.u64 = PPC_LOAD_U32(var_r26 + 12);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// lwz r6,92(r27)
		ctx.r6.u64 = PPC_LOAD_U32(var_r27 + 92);
		// stw r25,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, var_r25);
		// stw r7,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
		// stw r6,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
		// b 0x82469530
		goto loc_82469530;
	loc_82469338:
		// lwz r10,88(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		// addi r30,r26,180
		var_r30 = (uint32_t)(var_r26 + 180);
		// li r9,256
		ctx.r9.s64 = 256;
		// stw r10,20(r30)
		PPC_STORE_U32(var_r30 + 20, ctx.r10.u32);
		// stw r9,24(r30)
		PPC_STORE_U32(var_r30 + 24, ctx.r9.u32);
		// stw r25,28(r30)
		PPC_STORE_U32(var_r30 + 28, var_r25);
		// lbz r11,168(r26)
		ctx.r11.u64 = PPC_LOAD_U8(var_r26 + 168);
		// clrlwi r8,r11,31
		ctx.r8.u64 = ctx.r11.u32 & 0x1;
		// cmplwi cr6,r8,0
		// beq cr6,0x82469468
		if (ctx.r8.u32 == 0) goto loc_82469468;
		// rlwinm r7,r11,0,27,27
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
		// cmplwi cr6,r7,0
		// bne cr6,0x82469468
		if (ctx.r7.u32 != 0) goto loc_82469468;
		// rlwinm r6,r11,0,30,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
		// lis r11,-32256
		// cmplwi cr6,r6,0
		// lfs f31,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
		var_f31 = double(temp.f32);
		// beq cr6,0x824693cc
		if (ctx.r6.u32 != 0) {
			// lfs f0,108(r26)
			temp.u32 = PPC_LOAD_U32(var_r26 + 108);
			ctx.f0.f64 = double(temp.f32);
			// mr r9,r25
			ctx.r9.u64 = var_r25;
			// stfs f0,220(r26)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r26 + 220, temp.u32);
			// li r10,6
			ctx.r10.s64 = 6;
			// lfs f13,40(r30)
			temp.u32 = PPC_LOAD_U32(var_r30 + 40);
			ctx.f13.f64 = double(temp.f32);
			// addi r11,r30,52
			ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 52;
			// stfs f13,36(r30)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(var_r30 + 36, temp.u32);
			// mtctr r10
			ctx.ctr.u64 = ctx.r10.u64;
		loc_824693A0:
			// stw r9,0(r11)
			PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
			// addi r11,r11,4
			ctx.r11.s64 = ctx.r11.s64 + 4;
			// bdnz 0x824693a0
			--ctx.ctr.u64;
			if (ctx.ctr.u32 != 0) goto loc_824693A0;
			// addi r3,r26,16
			ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 16;
			// stfs f31,48(r30)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(var_f31);
			PPC_STORE_U32(var_r30 + 48, temp.u32);
			// li r5,0
			ctx.r5.s64 = 0;
			// li r4,2
			ctx.r4.s64 = 2;
			// lwz r10,68(r11)
			// bctrl
			VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
		}
	loc_824693CC:
		// lbz r9,168(r26)
		ctx.r9.u64 = PPC_LOAD_U8(var_r26 + 168);
		// rlwinm r8,r9,0,29,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x4;
		// cmplwi cr6,r8,0
		// beq cr6,0x82469418
		if (ctx.r8.u32 != 0) {
			// addi r11,r26,16
			ctx.r11.s64 = (int64_t)(int32_t)var_r26 + 16;
			// lfs f13,220(r26)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r26 + 220);
			ctx.f13.f64 = double(temp.f32);
			// lhz r7,154(r11)
			ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 154);
			// addis r6,r7,1
			ctx.r6.s64 = ctx.r7.s64 + 65536;
			// addi r6,r6,-1
			ctx.r6.s64 = ctx.r6.s64 + -1;
			// clrlwi r10,r6,16
			ctx.r10.u64 = ctx.r6.u32 & 0xFFFF;
			// cmplwi cr6,r10,0
			// sth r10,154(r11)
			PPC_STORE_U16(ctx.r11.u32 + 154, ctx.r10.u16);
			// beq cr6,0x82469410
			if (ctx.r10.u32 != 0) {
				// lis r11,-32256
				ctx.r11.s64 = -2113929216;
				// lfs f0,23132(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23132);  /* glob:lbl_82005A5C @ 0x82005a5c */
				ctx.f0.f64 = double(temp.f32);
				// fmuls f0,f13,f0
				ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
				// b 0x82469414
			} else {
			loc_82469410:
				// fmr f0,f31
				ctx.fpscr.disableFlushMode();
				ctx.f0.f64 = var_f31;
			}
		loc_82469414:
			// stfs f0,220(r26)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(var_r26 + 220, temp.u32);
		}
	loc_82469418:
		// lwz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r9,0
		// beq cr6,0x824692c8
		if (ctx.r9.s32 == 0) goto loc_824692C8;
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x824692c8
		if (ctx.r10.u32 != ctx.r11.u32) goto loc_824692C8;
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x824692c8
		if (ctx.r11.s32 != 0) goto loc_824692C8;
		// mr r11,r25
		ctx.r11.u64 = var_r25;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// b 0x824692c8
		goto loc_824692C8;
	loc_82469468:
		// lwz r11,28(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 28);
		// lwz r4,24(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 24);
		// lbz r29,13(r30)
		var_r29 = (uint32_t)(PPC_LOAD_U8(var_r30 + 13));
		// subf r10,r11,r4
		ctx.r10.s64 = ctx.r4.s64 - ctx.r11.s64;
		// cmplwi cr6,r10,0
		// beq cr6,0x824694b4
		if (ctx.r10.u32 != 0) {
			// cmplwi cr6,r29,0
			// beq cr6,0x824694b4
			if (var_r29 == 0) goto loc_824694B4;
			// rlwinm r27,r10,2,0,29
			var_r27 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC);
			// rlwinm r28,r11,2,0,29
			var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
		loc_82469490:
			// lwz r11,20(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
			// mr r5,r27
			ctx.r5.u64 = var_r27;
			// li r4,0
			ctx.r4.s64 = 0;
			// add r3,r11,r28
			ctx.r3.u64 = ctx.r11.u64 + var_r28;
			// bl 0x8242fed0
			memset(ctx, base);
			// addi r29,r29,-1
			var_r29 = (uint32_t)(var_r29 + -1);
			// addi r28,r28,1024
			var_r28 = (uint32_t)(var_r28 + 1024);
			// cmplwi cr6,r29,0
			// bne cr6,0x82469490
			if (var_r29 != 0) goto loc_82469490;
		}
	loc_824694B4:
		// lwz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r9,0
		// beq cr6,0x82469500
		if (ctx.r9.s32 != 0) {
			// lwz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r11
			// bne cr6,0x82469500
			if (ctx.r10.u32 != ctx.r11.u32) {
				// li r3,1
				ctx.r3.s64 = 1;
				return;
			}
			// addi r11,r9,-1
			ctx.r11.s64 = ctx.r9.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x82469500
			if (ctx.r11.s32 != 0) {
				// li r3,1
				ctx.r3.s64 = 1;
				return;
			}
			// mr r11,r25
			ctx.r11.u64 = var_r25;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82469500:
		// li r3,1
		ctx.r3.s64 = 1;
		return;
	loc_82469510:
		// addi r3,r26,16
		ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 16;
		// li r5,0
		ctx.r5.s64 = 0;
		// mr r4,r27
		ctx.r4.u64 = var_r27;
		// lwz r10,76(r11)
		// bctrl
		VCALL(ctx.r3.u32, 19, ctx, base);  // vtable slot 19 (byte +76)
		// mr r29,r25
		var_r29 = (uint32_t)(var_r25);
	loc_82469530:
		// addi r28,r26,180
		var_r28 = (uint32_t)(var_r26 + 180);
		// lwz r9,24(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 24);
		// lwz r8,28(r28)
		ctx.r8.u64 = PPC_LOAD_U32(var_r28 + 28);
		// subf. r7,r8,r9
		ctx.r7.s64 = ctx.r9.s64 - ctx.r8.s64;
		// bne 0x8246954c
		if (ctx.r7.s32 == 0) {
			// mr r11,r25
			ctx.r11.u64 = var_r25;
			// b 0x8246960c
		} else {
		loc_8246954C:
			// cmplwi cr6,r29,0
			// bne cr6,0x82469608
			if (var_r29 == 0) {
				// bl 0x8258653c
				__imp__KeRaiseIrqlToDpcLevel(ctx, base);
				// lwz r10,4(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
				// mr r29,r3
				var_r29 = ctx.r3.u32;
				// mr r30,r13
				var_r30 = ctx.r13.u32;
				// cmpwi cr6,r10,0
				// beq cr6,0x82469578
				if (ctx.r10.s32 != 0) {
					// lwz r8,8(r31)
					ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
					// cmplw cr6,r30,r8
					// beq cr6,0x82469598
					if (var_r30 == ctx.r8.u32) goto loc_82469598;
				}
			loc_82469578:
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x8258634c
				__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
				// mr r8,r30
				ctx.r8.u64 = var_r30;
				// mr r30,r29
				var_r30 = (uint32_t)(var_r29);
				// stw r8,8(r31)
				PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
				// stb r30,12(r31)
				PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
				// lwz r10,4(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
				// b 0x8246959c
				goto loc_8246959C;
			loc_82469598:
				// lbz r30,12(r31)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			loc_8246959C:
				// addi r9,r10,1
				ctx.r9.s64 = ctx.r10.s64 + 1;
				// addi r11,r26,20
				ctx.r11.s64 = (int64_t)(int32_t)var_r26 + 20;
				// stw r9,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r9.u32);
				// lwz r10,0(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
				// cmplw cr6,r11,r10
				// beq cr6,0x824695c0
				if (ctx.r11.u32 != ctx.r10.u32) {
					// lwz r6,8(r11)
					ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
					// subf r11,r6,r10
					ctx.r11.s64 = ctx.r10.s64 - ctx.r6.s64;
					// b 0x824695c4
				} else {
				loc_824695C0:
					// mr r11,r25
					ctx.r11.u64 = var_r25;
				}
			loc_824695C4:
				// mr r29,r11
				var_r29 = ctx.r11.u32;
				// mr r11,r13
				ctx.r11.u64 = ctx.r13.u64;
				// cmpwi cr6,r9,0
				// beq cr6,0x82469608
				if (ctx.r9.s32 == 0) goto loc_82469608;
				// cmplw cr6,r11,r8
				// bne cr6,0x82469608
				if (ctx.r11.u32 != ctx.r8.u32) goto loc_82469608;
				// addi r11,r9,-1
				ctx.r11.s64 = ctx.r9.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
				// bne cr6,0x82469608
				if (ctx.r11.s32 != 0) goto loc_82469608;
				// mr r11,r25
				ctx.r11.u64 = var_r25;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// stb r11,12(r31)
				PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
				// stw r11,8(r31)
				PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_82469608:
			// mr r11,r29
			ctx.r11.u64 = var_r29;
		}
	loc_8246960C:
		// mr r27,r11
		var_r27 = ctx.r11.u32;
		// cmplwi cr6,r11,0
		// beq cr6,0x8246967c
		if (ctx.r11.u32 == 0) goto loc_8246967C;
		// lwz r5,16(r11)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
		// cmplwi cr6,r5,0
		// beq cr6,0x82469634
		if (ctx.r5.u32 != 0) {
			// lwz r10,24(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
			// lwz r9,20(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
			// add r10,r10,r9
			ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
			// b 0x82469638
		} else {
		loc_82469634:
			// lwz r10,12(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
		}
	loc_82469638:
		// lwz r4,8(r11)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
		// lwz r11,116(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 116);
		// stw r10,4(r28)
		PPC_STORE_U32(var_r28 + 4, ctx.r10.u32);
		// cmplw cr6,r11,r10
		// stw r4,0(r28)
		PPC_STORE_U32(var_r28 + 0, ctx.r4.u32);
		// blt cr6,0x82469654
		if (ctx.r11.u32 >= ctx.r10.u32) {
			// mr r11,r10
			ctx.r11.u64 = ctx.r10.u64;
		}
	loc_82469654:
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// stw r11,8(r28)
		PPC_STORE_U32(var_r28 + 8, ctx.r11.u32);
		// bl 0x82470cc0
		ph_0CC0(ctx, base);
		// lwz r10,116(r26)
		ctx.r10.u64 = PPC_LOAD_U32(var_r26 + 116);
		// lwz r11,164(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 164);
		// add r10,r3,r10
		ctx.r10.u64 = ctx.r3.u64 + ctx.r10.u64;
		// add r9,r11,r3
		ctx.r9.u64 = ctx.r11.u64 + ctx.r3.u64;
		// stw r10,116(r26)
		PPC_STORE_U32(var_r26 + 116, ctx.r10.u32);
		// stw r9,164(r26)
		PPC_STORE_U32(var_r26 + 164, ctx.r9.u32);
		// b 0x824692c8
		goto loc_824692C8;
	loc_8246967C:
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x824696a0
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x824696b4
			if (var_r30 == ctx.r10.u32) goto loc_824696B4;
		}
	loc_824696A0:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_824696B4:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r30,r26,180
		var_r30 = (uint32_t)(var_r26 + 180);
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lfs f12,40(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 40);
		ctx.f12.f64 = double(temp.f32);
		// stfs f12,36(r30)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r30 + 36, temp.u32);
		// lwz r11,28(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 28);
		// lwz r8,24(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 24);
		// lbz r29,13(r30)
		var_r29 = (uint32_t)(PPC_LOAD_U8(var_r30 + 13));
		// subf r10,r11,r8
		ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
		// cmplwi cr6,r10,0
		// beq cr6,0x8246977c
		if (ctx.r10.u32 != 0) {
			// cmplwi cr6,r29,0
			// beq cr6,0x82469714
			if (var_r29 != 0) {
				// rlwinm r27,r10,2,0,29
				var_r27 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC);
				// rlwinm r28,r11,2,0,29
				var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC);
			loc_824696F0:
				// lwz r11,20(r30)
				ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 20);
				// mr r5,r27
				ctx.r5.u64 = var_r27;
				// li r4,0
				ctx.r4.s64 = 0;
				// add r3,r11,r28
				ctx.r3.u64 = ctx.r11.u64 + var_r28;
				// bl 0x8242fed0
				memset(ctx, base);
				// addi r29,r29,-1
				var_r29 = (uint32_t)(var_r29 + -1);
				// addi r28,r28,1024
				var_r28 = (uint32_t)(var_r28 + 1024);
				// cmplwi cr6,r29,0
				// bne cr6,0x824696f0
				if (var_r29 != 0) goto loc_824696F0;
			}
		loc_82469714:
			// lbz r7,168(r26)
			ctx.r7.u64 = PPC_LOAD_U8(var_r26 + 168);
			// rlwinm r6,r7,0,28,28
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x8;
			// cmplwi cr6,r6,0
			// bne cr6,0x82469764
			if (ctx.r6.u32 == 0) {
				// addi r11,r26,20
				ctx.r11.s64 = (int64_t)(int32_t)var_r26 + 20;
				// lwz r10,0(r11)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
				// cmplw cr6,r11,r10
				// beq cr6,0x82469744
				if (ctx.r11.u32 != ctx.r10.u32) {
					// lwz r5,8(r11)
					ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
					// subf r11,r5,r10
					ctx.r11.s64 = ctx.r10.s64 - ctx.r5.s64;
					// cmplwi cr6,r11,0
					// bne cr6,0x8246977c
					if (ctx.r11.u32 != 0) goto loc_8246977C;
				}
			loc_82469744:
				// addi r3,r26,16
				ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 16;
				// li r5,16
				ctx.r5.s64 = 16;
				// li r4,16
				ctx.r4.s64 = 16;
				// lwz r10,68(r11)
				// bctrl
				VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
				// b 0x8246977c
			} else {
			loc_82469764:
				// lwz r9,0(r26)
  // [ph4a] vtable load collapsed
				// li r4,1
				ctx.r4.s64 = 1;
				// mr r3,r26
				ctx.r3.u64 = var_r26;
				// lwz r8,44(r9)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r26, 11, ctx, base);  // pattern-B slot 11 (byte +44)
			}
		}
	loc_8246977C:
		// lbz r11,168(r26)
		ctx.r11.u64 = PPC_LOAD_U8(var_r26 + 168);
		// rlwinm r7,r11,0,29,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
		// cmplwi cr6,r7,0
		// beq cr6,0x824697bc
		if (ctx.r7.u32 != 0) {
			// lhz r6,170(r26)
			ctx.r6.u64 = PPC_LOAD_U16(var_r26 + 170);
			// cmplwi cr6,r6,0
			// beq cr6,0x824697a4
			if (ctx.r6.u32 != 0) {
				// rlwinm r5,r11,0,27,27
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
				// cmplwi cr6,r5,0
				// beq cr6,0x824697bc
				if (ctx.r5.u32 == 0) goto loc_824697BC;
			}
		loc_824697A4:
			// lwz r11,0(r26)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// lwz r10,44(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r26, 11, ctx, base);  // pattern-B slot 11 (byte +44)
		}
	loc_824697BC:
		// lwz r9,4(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r9,0
		// beq cr6,0x82469808
		if (ctx.r9.s32 != 0) {
			// lwz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r11
			// bne cr6,0x82469808
			if (ctx.r10.u32 != ctx.r11.u32) goto loc_82469808;
			// addi r11,r9,-1
			ctx.r11.s64 = ctx.r9.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x82469808
			if (ctx.r11.s32 != 0) goto loc_82469808;
			// mr r11,r25
			ctx.r11.u64 = var_r25;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82469808:
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8246980C:
	return;
}

__attribute__((alias("__imp__atSingleton_9818_2hr"))) PPC_WEAK_FUNC(atSingleton_9818_2hr);
PPC_FUNC_IMPL(__imp__atSingleton_9818_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// li r4,268
	ctx.r4.s64 = 268;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,20(r11)
	// bctrl
	VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	// cmplwi cr6,r3,0
	// beq cr6,0x82469878
	if (ctx.r3.u32 != 0) {
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x824690a0
		atSingleton_90A0(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// cmplwi cr6,r11,0
		// beq cr6,0x82469878
		if (ctx.r11.u32 == 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// li r3,0
		ctx.r3.s64 = 0;
		// stw r11,0(r29)
		PPC_STORE_U32(var_r29 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
		return;
	}
loc_82469878:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
}

__attribute__((alias("__imp__ph_vt5C84_3_9888"))) PPC_WEAK_FUNC(ph_vt5C84_3_9888);
PPC_FUNC_IMPL(__imp__ph_vt5C84_3_9888) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r30,8(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
	// cmplwi cr6,r30,0
	// beq cr6,0x824698c0
	if (var_r30 != 0) {
		// rotlwi r3,r30,0
		ctx.r3.u64 = var_r30;
		// lwz r10,0(r11)
		// bctrl
		DTOR(ctx.r3.u32, ctx, base);  // vtable slot 0 (destructor)
	}
loc_824698C0:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,0(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 0, ctx, base);  // pattern-B slot 0 (byte +0)
	// cmplwi cr6,r30,0
	// beq cr6,0x824698f4
	if (var_r30 != 0) {
		// lwz r7,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r6,4(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	}
loc_824698F4:
	// blr
	return;
}

__attribute__((alias("__imp__ke_9910"))) PPC_WEAK_FUNC(ke_9910);
PPC_FUNC_IMPL(__imp__ke_9910) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=160, savegprlr_24
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// mr r25,r5
	var_r25 = ctx.r5.u32;
	// mr r24,r6
	var_r24 = ctx.r6.u32;
	// mr r26,r7
	var_r26 = ctx.r7.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246995c
	if (ctx.r11.s32 != 0) {
		// lwz r7,8(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r7
		// beq cr6,0x8246997c
		if (var_r30 == ctx.r7.u32) goto loc_8246997C;
	}
loc_8246995C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r7,r30
	ctx.r7.u64 = var_r30;
	// mr r30,r29
	var_r30 = (uint32_t)(var_r29);
	// stw r7,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r7.u32);
	// stb r30,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// b 0x82469980
	goto loc_82469980;
loc_8246997C:
	// lbz r30,12(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
loc_82469980:
	// clrlwi r10,r27,24
	ctx.r10.u64 = var_r27 & 0xFF;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r10,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0xFFFFFFC0;
	// li r11,0
	ctx.r11.s64 = 0;
	// add r9,r9,r28
	ctx.r9.u64 = ctx.r9.u64 + var_r28;
	// stw r8,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r8.u32);
	// addi r9,r9,92
	ctx.r9.s64 = ctx.r9.s64 + 92;
loc_8246999C:
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplwi cr6,r6,0
	// beq cr6,0x824699bc
	if (ctx.r6.u32 != 0) {
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r9,r9,8
		ctx.r9.s64 = ctx.r9.s64 + 8;
		// cmplwi cr6,r11,8
		// blt cr6,0x8246999c
		if (ctx.r11.u32 < 8) goto loc_8246999C;
		// b 0x824699d8
	} else {
	loc_824699BC:
		// rlwinm r10,r10,3,0,28
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// rlwinm r11,r5,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// addi r11,r11,92
		ctx.r11.s64 = ctx.r11.s64 + 92;
		// cmplwi cr6,r11,0
		// bne cr6,0x82469a24
		if (ctx.r11.u32 != 0) goto loc_82469A24;
	}
loc_824699D8:
	// mr r11,r13
	ctx.r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r8,0
	// beq cr6,0x82469a14
	if (ctx.r8.s32 != 0) {
		// cmplw cr6,r11,r7
		// bne cr6,0x82469a14
		if (ctx.r11.u32 != ctx.r7.u32) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// addi r11,r8,-1
		ctx.r11.s64 = ctx.r8.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82469a14
		if (ctx.r11.s32 != 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82469A14:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
loc_82469A24:
	// cmplwi cr6,r26,0
	// stw r25,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r25);
	// stw r24,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, var_r24);
	// beq cr6,0x82469a38
	if (var_r26 != 0) {
		// stw r11,0(r26)
		PPC_STORE_U32(var_r26 + 0, ctx.r11.u32);
	}
loc_82469A38:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x82469a80
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x82469a80
		if (ctx.r10.u32 != ctx.r11.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82469a80
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82469A80:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ke_9A90"))) PPC_WEAK_FUNC(ke_9A90);
PPC_FUNC_IMPL(__imp__ke_9A90) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r11,r27,92
	ctx.r11.s64 = (int64_t)(int32_t)var_r27 + 92;
	// cmplw cr6,r4,r11
	// blt cr6,0x82469abc
	if (ctx.r4.u32 >= ctx.r11.u32) {
		// addi r10,r27,220
		ctx.r10.s64 = (int64_t)(int32_t)var_r27 + 220;
		// mr r26,r4
		var_r26 = ctx.r4.u32;
		// cmplw cr6,r4,r10
		// blt cr6,0x82469ac0
		if (ctx.r4.u32 < ctx.r10.u32) goto loc_82469AC0;
	}
loc_82469ABC:
	// li r26,0
	var_r26 = 0;
loc_82469AC0:
	// mr r28,r13
	var_r28 = ctx.r13.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82469af0
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82469b08
		if (var_r30 == ctx.r10.u32) goto loc_82469B08;
	}
loc_82469AF0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// stw r10,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82469B08:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lwz r9,224(r27)
	ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 224);
	// cmplw cr6,r28,r9
	// beq cr6,0x82469bbc
	if (var_r28 != ctx.r9.u32) {
		// lwz r8,220(r27)
		ctx.r8.u64 = PPC_LOAD_U32(var_r27 + 220);
		// cmplw cr6,r26,r8
		// bne cr6,0x82469bbc
	while (var_r26 == ctx.r7.u32) {
		loc_82469B28:
			// mr r9,r13
			ctx.r9.u64 = ctx.r13.u64;
			// cmpwi cr6,r11,0
			// beq cr6,0x82469b68
			if (ctx.r11.s32 != 0) {
				// cmplw cr6,r9,r10
				// bne cr6,0x82469b68
				if (ctx.r9.u32 != ctx.r10.u32) goto loc_82469B68;
				// addi r11,r11,-1
				ctx.r11.s64 = ctx.r11.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
				// bne cr6,0x82469b68
				if (ctx.r11.s32 != 0) goto loc_82469B68;
				// lbz r30,12(r31)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// stb r11,12(r31)
				PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
				// stw r11,8(r31)
				PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_82469B68:
			// bl 0x8258659c
			__imp__NtYieldExecution(ctx, base);
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r11,0
			// beq cr6,0x82469b90
			if (ctx.r11.s32 != 0) {
				// lwz r10,8(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r30,r10
				// beq cr6,0x82469ba8
				if (var_r30 == ctx.r10.u32) goto loc_82469BA8;
			}
		loc_82469B90:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// mr r10,r30
			ctx.r10.u64 = var_r30;
			// stw r10,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
			// stb r29,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		loc_82469BA8:
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// lwz r7,220(r27)
			ctx.r7.u64 = PPC_LOAD_U32(var_r27 + 220);
			// cmplw cr6,r26,r7
			// beq cr6,0x82469b28
	}
	}
loc_82469BBC:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r6,0(r26)
	PPC_STORE_U32(var_r26 + 0, ctx.r6.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82469c0c
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x82469c0c
		if (ctx.r10.u32 != ctx.r9.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82469c0c
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82469C0C:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ke_9C18"))) PPC_WEAK_FUNC(ke_9C18);
PPC_FUNC_IMPL(__imp__ke_9C18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// rlwinm r11,r4,6,18,25
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 6) & 0x3FC0;
	// li r26,8
	var_r26 = 8;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + var_r27;
	// addi r28,r11,92
	var_r28 = (uint32_t)(ctx.r11.s64 + 92);  // addr:0x825e005c
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
loc_82469C40:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x82469c64
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82469c78
		if (var_r30 == ctx.r10.u32) goto loc_82469C78;
	}
loc_82469C64:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82469C78:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// stw r28,220(r27)
	PPC_STORE_U32(var_r27 + 220, var_r28);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmplwi cr6,r11,0
	// beq cr6,0x82469cec
	if (ctx.r11.u32 != 0) {
		// cmpwi cr6,r9,0
		// beq cr6,0x82469cd8
		if (ctx.r9.s32 != 0) {
			// lwz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r11
			// bne cr6,0x82469cd8
			if (ctx.r10.u32 != ctx.r11.u32) goto loc_82469CD8;
			// addi r11,r9,-1
			ctx.r11.s64 = ctx.r9.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x82469cd8
			if (ctx.r11.s32 != 0) goto loc_82469CD8;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82469CD8:
		// lwz r3,4(r28)
		ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 4);
		// lwz r10,0(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 0);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
		// b 0x82469d2c
	} else {
	loc_82469CEC:
		// cmpwi cr6,r9,0
		// beq cr6,0x82469d2c
		if (ctx.r9.s32 == 0) goto loc_82469D2C;
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x82469d2c
		if (ctx.r10.u32 != ctx.r11.u32) goto loc_82469D2C;
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82469d2c
		if (ctx.r11.s32 != 0) goto loc_82469D2C;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82469D2C:
	// addi r26,r26,-1
	var_r26 = (uint32_t)(var_r26 + -1);
	// addi r28,r28,8
	var_r28 = (uint32_t)(var_r28 + 8);
	// cmplwi cr6,r26,0
	// bne cr6,0x82469c40
	if (var_r26 != 0) goto loc_82469C40;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x82469d60
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82469d74
		if (var_r30 == ctx.r10.u32) goto loc_82469D74;
	}
loc_82469D60:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_82469D74:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// stw r9,220(r27)
	PPC_STORE_U32(var_r27 + 220, ctx.r9.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82469dcc
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x82469dcc
		if (ctx.r10.u32 != ctx.r9.u32) {
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82469dcc
		if (ctx.r11.s32 != 0) {
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82469DCC:
	return;
}

__attribute__((alias("__imp__ph_vt5C84_0_9DD8"))) PPC_WEAK_FUNC(ph_vt5C84_0_9DD8);
PPC_FUNC_IMPL(__imp__ph_vt5C84_0_9DD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32256
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r11,r11,23684
	ctx.r11.s64 = ctx.r11.s64 + 23684;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
	// cmplwi cr6,r3,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82469e24
	if (ctx.r3.u32 != 0) {
		// lwz r10,4(r11)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// li r9,0
		ctx.r9.s64 = 0;
		// stw r9,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	}
loc_82469E24:
	// lis r11,-32256
	// clrlwi r8,r30,31
	ctx.r8.u64 = var_r30 & 0x1;
	// addi r11,r11,15792
	ctx.r11.s64 = ctx.r11.s64 + 15792;
	// cmplwi cr6,r8,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x82469e50
	if (ctx.r8.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_82469E50:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_9E70_2h"))) PPC_WEAK_FUNC(phInst_9E70_2h);
PPC_FUNC_IMPL(__imp__phInst_9E70_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmplwi cr6,r11,0
	// bne cr6,0x82469ea0
	if (ctx.r11.u32 == 0) {
		// lis r11,-32256
		// addi r11,r11,22920
		ctx.r11.s64 = ctx.r11.s64 + 22920;
	}
loc_82469EA0:
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8246c1a0
	phInst_C1A0_2hr(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x82469f3c
	if (ctx.r3.s32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// lbz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 0);
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// cmplwi cr6,r9,6
		// std r11,0(r10)
		PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r11.u64);
		// std r11,8(r10)
		PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r11.u64);
		// stw r11,16(r10)
		PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r11.u32);
		// stb r11,96(r1)
		PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r11.u8);
		// stb r11,100(r1)
		PPC_STORE_U8(ctx.r1.u32 + 100, ctx.r11.u8);
		// bgt cr6,0x82469ee4
		if (ctx.r9.u32 <= 6) {
			// li r9,6
			ctx.r9.s64 = 6;
		}
	loc_82469EE4:
		// lis r10,0
		ctx.r10.s64 = 0;
		// stb r9,101(r1)
		PPC_STORE_U8(ctx.r1.u32 + 101, ctx.r9.u8);
		// addi r4,r1,84
		ctx.r4.s64 = ctx.r1.s64 + 84;
		// ori r9,r10,48000
		ctx.r9.u64 = ctx.r10.u64 | 48000;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// stw r9,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
		// bl 0x82466030
		phInst_6030_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x82469f3c
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// lwz r8,84(r1)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// lbz r11,1(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 1);
		// lwz r7,88(r1)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		// rlwinm r10,r8,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// cmplwi cr6,r11,0
		// add r10,r10,r7
		ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
		// beq cr6,0x82469f38
		if (ctx.r11.u32 != 0) {
			// rlwinm r9,r11,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// add r6,r11,r9
			ctx.r6.u64 = ctx.r11.u64 + ctx.r9.u64;
			// rlwinm r11,r6,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
			// add r11,r11,r10
			ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
			// addi r10,r11,4
			ctx.r10.s64 = ctx.r11.s64 + 4;
		}
	loc_82469F38:
		// stw r10,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r10.u32);
	}
loc_82469F3C:
	// blr
	return;
}

__attribute__((alias("__imp__ke_9F58"))) PPC_WEAK_FUNC(ke_9F58);
PPC_FUNC_IMPL(__imp__ke_9F58) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	// FRAME: size=176, savegprlr_21
	// lis r11,-32165
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// addi r21,r11,8960
	var_r21 = (uint32_t)(ctx.r11.s64 + 8960);  // lbl_825B2300 @ 0x825b2300
	// addi r3,r21,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r21 + 4;
	// bl 0x82585e0c
	__imp__RtlEnterCriticalSection(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	// stw r13,224(r26)
	PPC_STORE_U32(var_r26 + 224, ctx.r13.u32);
	// bl 0x82469c18
	ke_9C18(ctx, base);
	// lwz r11,24(r26)
	ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 24);
	// addi r22,r26,80
	var_r22 = (uint32_t)(var_r26 + 80);
	// mr r4,r22
	ctx.r4.u64 = var_r22;
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	// lwz r3,16(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// bl 0x825865bc
	__imp__XAudioGetVoiceCategoryVolumeChangeMask(ctx, base);
	// li r23,0
	var_r23 = 0;
	// addi r30,r26,72
	var_r30 = (uint32_t)(var_r26 + 72);
	// mr r31,r23
	var_r31 = (uint32_t)(var_r23);
	// li r29,1
	var_r29 = 1;
loc_82469FB0:
	// cmpwi cr6,r3,0
	// blt cr6,0x82469fe8
	if (ctx.r3.s32 < 0) goto loc_82469FE8;
	// lwz r9,0(r22)
	ctx.r9.u64 = PPC_LOAD_U32(var_r22 + 0);
	// slw r8,r29,r31
	ctx.r8.u64 = (uint8_t)var_r31 & 0x20 ? 0 : (var_r29 << ((uint8_t)var_r31 & 0x3F));
	// and r7,r8,r9
	ctx.r7.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	// beq cr6,0x82469fd8
	if (ctx.r7.u32 != 0) {
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x825865ac
		__imp__XAudioGetVoiceCategoryVolume(ctx, base);
	}
loc_82469FD8:
	// addi r31,r31,1
	var_r31 = (uint32_t)(var_r31 + 1);
	// addi r30,r30,4
	var_r30 = (uint32_t)(var_r30 + 4);
	// cmplwi cr6,r31,2
	// blt cr6,0x82469fb0
	if (var_r31 < 2) goto loc_82469FB0;
loc_82469FE8:
	// addi r27,r26,52
	var_r27 = (uint32_t)(var_r26 + 52);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246a018
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8246a02c
		if (var_r30 == ctx.r10.u32) goto loc_8246A02C;
	}
loc_8246A018:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246A02C:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
	// cmplw cr6,r27,r11
	// beq cr6,0x8246a04c
	if (var_r27 != ctx.r11.u32) {
		// lwz r6,8(r27)
		ctx.r6.u64 = PPC_LOAD_U32(var_r27 + 8);
		// subf r29,r6,r11
		var_r29 = (uint32_t)(ctx.r11.s64 - ctx.r6.s64);
		// b 0x8246a050
	} else {
	loc_8246A04C:
		// mr r29,r23
		var_r29 = (uint32_t)(var_r23);
	}
loc_8246A050:
	// cmplwi cr6,r29,0
	// beq cr6,0x8246a140
while (!ctx.cr6.eq) {
	loc_8246A058:
		// cmplwi cr6,r29,0
		// beq cr6,0x8246a06c
		if (var_r29 != 0) {
			// lwz r11,8(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 8);
			// add r11,r11,r29
			ctx.r11.u64 = ctx.r11.u64 + var_r29;
			// b 0x8246a070
		} else {
		loc_8246A06C:
			// mr r11,r27
			ctx.r11.u64 = var_r27;
		}
	loc_8246A070:
		// lwz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r27,r11
		// beq cr6,0x8246a088
		if (var_r27 != ctx.r11.u32) {
			// lwz r5,8(r27)
			ctx.r5.u64 = PPC_LOAD_U32(var_r27 + 8);
			// subf r28,r5,r11
			var_r28 = (uint32_t)(ctx.r11.s64 - ctx.r5.s64);
			// b 0x8246a08c
		} else {
		loc_8246A088:
			// mr r28,r23
			var_r28 = (uint32_t)(var_r23);
		}
	loc_8246A08C:
		// stw r29,84(r26)
		PPC_STORE_U32(var_r26 + 84, var_r29);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// stw r28,88(r26)
		PPC_STORE_U32(var_r26 + 88, var_r28);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8246a0e0
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r9
			// bne cr6,0x8246a0e0
			if (ctx.r10.u32 != ctx.r9.u32) goto loc_8246A0E0;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246a0e0
			if (ctx.r11.s32 != 0) goto loc_8246A0E0;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_8246A0E0:
		// lwz r4,0(r29)
  // [ph4a] vtable load collapsed
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r11,68(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246a118
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8246a12c
			if (var_r30 == ctx.r10.u32) goto loc_8246A12C;
		}
	loc_8246A118:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8246A12C:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// mr r29,r28
		var_r29 = (uint32_t)(var_r28);
		// cmplwi cr6,r28,0
		ctx.cr6.compare<uint32_t>(var_r28, 0, ctx.xer);
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246a058
}
loc_8246A140:
	// stw r23,84(r26)
	PPC_STORE_U32(var_r26 + 84, var_r23);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246a190
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x8246a190
		if (ctx.r10.u32 != ctx.r9.u32) goto loc_8246A190;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246a190
		if (ctx.r11.s32 != 0) goto loc_8246A190;
		// mr r11,r23
		ctx.r11.u64 = var_r23;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246A190:
	// lbz r10,68(r26)
	ctx.r10.u64 = PPC_LOAD_U8(var_r26 + 68);
	// mr r24,r23
	var_r24 = (uint32_t)(var_r23);
	// cmplwi cr6,r10,0
	// beq cr6,0x8246a360
	if (ctx.r10.u32 != 0) {
		// mr r25,r23
		var_r25 = (uint32_t)(var_r23);
	loc_8246A1A4:
		// lwz r11,64(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 64);
		// add r11,r25,r11
		ctx.r11.u64 = var_r25 + ctx.r11.u64;
		// addi r27,r11,12
		var_r27 = (uint32_t)(ctx.r11.s64 + 12);  // addr:0x825e000c
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246a1d4
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8246a1e8
			if (var_r30 == ctx.r10.u32) goto loc_8246A1E8;
		}
	loc_8246A1D4:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8246A1E8:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lwz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
		// cmplw cr6,r27,r11
		// beq cr6,0x8246a208
		if (var_r27 != ctx.r11.u32) {
			// lwz r9,8(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 8);
			// subf r29,r9,r11
			var_r29 = (uint32_t)(ctx.r11.s64 - ctx.r9.s64);
			// b 0x8246a20c
		} else {
		loc_8246A208:
			// mr r29,r23
			var_r29 = (uint32_t)(var_r23);
		}
	loc_8246A20C:
		// cmplwi cr6,r29,0
		// beq cr6,0x8246a2fc
	while (!ctx.cr6.eq) {
		loc_8246A214:
			// cmplwi cr6,r29,0
			// beq cr6,0x8246a228
			if (var_r29 != 0) {
				// lwz r11,8(r27)
				ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 8);
				// add r11,r11,r29
				ctx.r11.u64 = ctx.r11.u64 + var_r29;
				// b 0x8246a22c
			} else {
			loc_8246A228:
				// mr r11,r27
				ctx.r11.u64 = var_r27;
			}
		loc_8246A22C:
			// lwz r11,0(r11)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// cmplw cr6,r27,r11
			// beq cr6,0x8246a244
			if (var_r27 != ctx.r11.u32) {
				// lwz r8,8(r27)
				ctx.r8.u64 = PPC_LOAD_U32(var_r27 + 8);
				// subf r28,r8,r11
				var_r28 = (uint32_t)(ctx.r11.s64 - ctx.r8.s64);
				// b 0x8246a248
			} else {
			loc_8246A244:
				// mr r28,r23
				var_r28 = (uint32_t)(var_r23);
			}
		loc_8246A248:
			// stw r29,84(r26)
			PPC_STORE_U32(var_r26 + 84, var_r29);
			// mr r10,r13
			ctx.r10.u64 = ctx.r13.u64;
			// stw r28,88(r26)
			PPC_STORE_U32(var_r26 + 88, var_r28);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// cmpwi cr6,r11,0
			// beq cr6,0x8246a29c
			if (ctx.r11.s32 != 0) {
				// lwz r9,8(r31)
				ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r10,r9
				// bne cr6,0x8246a29c
				if (ctx.r10.u32 != ctx.r9.u32) goto loc_8246A29C;
				// addi r11,r11,-1
				ctx.r11.s64 = ctx.r11.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
				// bne cr6,0x8246a29c
				if (ctx.r11.s32 != 0) goto loc_8246A29C;
				// mr r11,r23
				ctx.r11.u64 = var_r23;
				// lbz r30,12(r31)
				var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// stb r11,12(r31)
				PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
				// stw r11,8(r31)
				PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_8246A29C:
			// lwz r7,0(r29)
  // [ph4a] vtable load collapsed
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// lwz r6,68(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r29, 17, ctx, base);  // pattern-B slot 17 (byte +68)
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r11,0
			// beq cr6,0x8246a2d4
			if (ctx.r11.s32 != 0) {
				// lwz r10,8(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r30,r10
				// beq cr6,0x8246a2e8
				if (var_r30 == ctx.r10.u32) goto loc_8246A2E8;
			}
		loc_8246A2D4:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// stw r30,8(r31)
			PPC_STORE_U32(var_r31 + 8, var_r30);
			// stb r29,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		loc_8246A2E8:
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// mr r29,r28
			var_r29 = (uint32_t)(var_r28);
			// cmplwi cr6,r28,0
			ctx.cr6.compare<uint32_t>(var_r28, 0, ctx.xer);
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246a214
	}
	loc_8246A2FC:
		// stw r23,84(r26)
		PPC_STORE_U32(var_r26 + 84, var_r23);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8246a34c
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r9
			// bne cr6,0x8246a34c
			if (ctx.r10.u32 != ctx.r9.u32) goto loc_8246A34C;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246a34c
			if (ctx.r11.s32 != 0) goto loc_8246A34C;
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_8246A34C:
		// lbz r5,68(r26)
		ctx.r5.u64 = PPC_LOAD_U8(var_r26 + 68);
		// addi r24,r24,1
		var_r24 = (uint32_t)(var_r24 + 1);
		// addi r25,r25,24
		var_r25 = (uint32_t)(var_r25 + 24);
		// cmplw cr6,r24,r5
		// blt cr6,0x8246a1a4
		if (var_r24 < ctx.r5.u32) goto loc_8246A1A4;
	}
loc_8246A360:
	// lwz r3,24(r26)
	ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 24);
	// lwz r11,68(r4)
	// bctrl
	VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	// stw r23,0(r22)
	PPC_STORE_U32(var_r22 + 0, var_r23);
	// bl 0x82469c18
	ke_9C18(ctx, base);
	// addi r3,r21,4
	ctx.r3.s64 = (int64_t)(int32_t)var_r21 + 4;
	// stw r23,224(r26)
	PPC_STORE_U32(var_r26 + 224, var_r23);
	// bl 0x82585dfc
	__imp__RtlLeaveCriticalSection(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__phInst_A3A0_p33"))) PPC_WEAK_FUNC(phInst_A3A0_p33);
PPC_FUNC_IMPL(__imp__phInst_A3A0_p33) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82460100
	atSingleton_0100_p39(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246a410
while (ctx.r3.u32 != 0) {
	loc_8246A3C8:
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// add r11,r11,r3
		ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r10,r11
		// beq cr6,0x8246a3f8
		if (ctx.r10.u32 == ctx.r11.u32) goto loc_8246A3F8;
		// lwz r9,4(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r9,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
		// lwz r8,4(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r7,0(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// stw r7,0(r8)
		PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
		// stw r11,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
		// stw r11,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
	loc_8246A3F8:
		// li r5,1
		ctx.r5.s64 = 1;
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82460100
		atSingleton_0100_p39(ctx, base);
		// cmplwi cr6,r3,0
		// bne cr6,0x8246a3c8
}
loc_8246A410:
	// blr
	return;
}

__attribute__((alias("__imp__ke_A428"))) PPC_WEAK_FUNC(ke_A428);
PPC_FUNC_IMPL(__imp__ke_A428) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	// FRAME: size=176, savegprlr_21
	// lis r11,-32256
	// mr r23,r3
	var_r23 = ctx.r3.u32;
	// addi r11,r11,23704
	ctx.r11.s64 = ctx.r11.s64 + 23704;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// lwz r11,24(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 24);
	// cmplwi cr6,r11,0
	// beq cr6,0x8246a468
	if (ctx.r11.u32 != 0) {
		// lwz r3,68(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
		// li r4,0
		ctx.r4.s64 = 0;
		// lwz r10,28(r11)
		// bctrl
		VCALL(ctx.r3.u32, 7, ctx, base);  // vtable slot 7 (byte +28)
	}
loc_8246A468:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246a494
	if (ctx.r11.s32 != 0) {
		// lwz r29,8(r31)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
		// cmplw cr6,r30,r29
		// beq cr6,0x8246a4b0
		if (var_r30 == var_r29) goto loc_8246A4B0;
	}
loc_8246A494:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r29,r30
	var_r29 = (uint32_t)(var_r30);
	// stw r29,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r29);
	// stb r27,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// b 0x8246a4b4
	goto loc_8246A4B4;
loc_8246A4B0:
	// lbz r27,12(r31)
	var_r27 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
loc_8246A4B4:
	// addi r30,r11,1
	var_r30 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x825e0001
	// addi r21,r23,40
	var_r21 = (uint32_t)(var_r23 + 40);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r21
	ctx.r3.u64 = var_r21;
	// stw r30,4(r31)
	PPC_STORE_U32(var_r31 + 4, var_r30);
	// bl 0x82460100
	atSingleton_0100_p39(ctx, base);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// li r22,0
	var_r22 = 0;
	// cmplwi cr6,r28,0
	// beq cr6,0x8246a5a0
while (var_r28 != 0) {
	loc_8246A4E0:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r30,0
		// beq cr6,0x8246a524
		if ((int32_t)var_r30 != 0) {
			// cmplw cr6,r11,r29
			// bne cr6,0x8246a524
			if (ctx.r11.u32 != var_r29) goto loc_8246A524;
			// addi r11,r30,-1
			ctx.r11.s64 = (int64_t)(int32_t)var_r30 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246a524
			if (ctx.r11.s32 != 0) goto loc_8246A524;
			// mr r11,r22
			ctx.r11.u64 = var_r22;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// mr r30,r27
			var_r30 = (uint32_t)(var_r27);
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_8246A524:
		// lwz r9,0(r28)
  // [ph4a] vtable load collapsed
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r8,12(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r28, 3, ctx, base);  // pattern-B slot 3 (byte +12)
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r27,r3
		var_r27 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246a55c
		if (ctx.r11.s32 != 0) {
			// lwz r29,8(r31)
			var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
			// cmplw cr6,r30,r29
			// beq cr6,0x8246a578
			if (var_r30 == var_r29) goto loc_8246A578;
		}
	loc_8246A55C:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r29,r30
		var_r29 = (uint32_t)(var_r30);
		// stw r29,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r29);
		// stb r27,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// b 0x8246a57c
		goto loc_8246A57C;
	loc_8246A578:
		// lbz r27,12(r31)
		var_r27 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
	loc_8246A57C:
		// addi r30,r11,1
		var_r30 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x825e0001
		// li r5,1
		ctx.r5.s64 = 1;
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		// stw r30,4(r31)
		PPC_STORE_U32(var_r31 + 4, var_r30);
		// bl 0x82460100
		atSingleton_0100_p39(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// cmplwi cr6,r28,0
		// bne cr6,0x8246a4e0
}
loc_8246A5A0:
	// mr r11,r13
	ctx.r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r30,0
	// beq cr6,0x8246a5e0
	if ((int32_t)var_r30 != 0) {
		// cmplw cr6,r11,r29
		// bne cr6,0x8246a5e0
		if (ctx.r11.u32 != var_r29) goto loc_8246A5E0;
		// addi r11,r30,-1
		ctx.r11.s64 = (int64_t)(int32_t)var_r30 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246a5e0
		if (ctx.r11.s32 != 0) goto loc_8246A5E0;
		// mr r11,r22
		ctx.r11.u64 = var_r22;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246A5E0:
	// lbz r7,68(r23)
	ctx.r7.u64 = PPC_LOAD_U8(var_r23 + 68);
	// mr r24,r22
	var_r24 = (uint32_t)(var_r22);
	// cmplwi cr6,r7,0
	// beq cr6,0x8246a774
	if (ctx.r7.u32 != 0) {
		// mr r25,r22
		var_r25 = (uint32_t)(var_r22);
	loc_8246A5F4:
		// lwz r11,64(r23)
		ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 64);
		// add r26,r25,r11
		var_r26 = (uint32_t)(var_r25 + ctx.r11.u64);
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r27,r3
		var_r27 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246a620
		if (ctx.r11.s32 != 0) {
			// lwz r29,8(r31)
			var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
			// cmplw cr6,r30,r29
			// beq cr6,0x8246a63c
			if (var_r30 == var_r29) goto loc_8246A63C;
		}
	loc_8246A620:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r29,r30
		var_r29 = (uint32_t)(var_r30);
		// stw r29,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r29);
		// stb r27,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// b 0x8246a640
		goto loc_8246A640;
	loc_8246A63C:
		// lbz r27,12(r31)
		var_r27 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
	loc_8246A640:
		// addi r30,r11,1
		var_r30 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x825e0001
		// li r5,1
		ctx.r5.s64 = 1;
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// stw r30,4(r31)
		PPC_STORE_U32(var_r31 + 4, var_r30);
		// bl 0x82460100
		atSingleton_0100_p39(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// cmplwi cr6,r28,0
		// beq cr6,0x8246a720
	while (var_r28 != 0) {
		loc_8246A664:
			// mr r11,r13
			ctx.r11.u64 = ctx.r13.u64;
			// cmpwi cr6,r30,0
			// beq cr6,0x8246a6a4
			if ((int32_t)var_r30 != 0) {
				// cmplw cr6,r11,r29
				// bne cr6,0x8246a6a4
				if (ctx.r11.u32 != var_r29) goto loc_8246A6A4;
				// addi r11,r30,-1
				ctx.r11.s64 = (int64_t)(int32_t)var_r30 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
				// bne cr6,0x8246a6a4
				if (ctx.r11.s32 != 0) goto loc_8246A6A4;
				// mr r11,r22
				ctx.r11.u64 = var_r22;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// stb r11,12(r31)
				PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
				// stw r11,8(r31)
				PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r27
				ctx.r3.u64 = var_r27;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_8246A6A4:
			// lwz r6,0(r28)
  // [ph4a] vtable load collapsed
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// lwz r5,12(r6)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r28, 3, ctx, base);  // pattern-B slot 3 (byte +12)
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// mr r27,r3
			var_r27 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r11,0
			// beq cr6,0x8246a6dc
			if (ctx.r11.s32 != 0) {
				// lwz r29,8(r31)
				var_r29 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
				// cmplw cr6,r30,r29
				// beq cr6,0x8246a6f8
				if (var_r30 == var_r29) goto loc_8246A6F8;
			}
		loc_8246A6DC:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// mr r29,r30
			var_r29 = (uint32_t)(var_r30);
			// stw r29,8(r31)
			PPC_STORE_U32(var_r31 + 8, var_r29);
			// stb r27,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
			// b 0x8246a6fc
			goto loc_8246A6FC;
		loc_8246A6F8:
			// lbz r27,12(r31)
			var_r27 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		loc_8246A6FC:
			// addi r30,r11,1
			var_r30 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x825e0001
			// li r5,1
			ctx.r5.s64 = 1;
			// li r4,0
			ctx.r4.s64 = 0;
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// stw r30,4(r31)
			PPC_STORE_U32(var_r31 + 4, var_r30);
			// bl 0x82460100
			atSingleton_0100_p39(ctx, base);
			// mr r28,r3
			var_r28 = ctx.r3.u32;
			// cmplwi cr6,r28,0
			// bne cr6,0x8246a664
	}
	loc_8246A720:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r30,0
		// beq cr6,0x8246a760
		if ((int32_t)var_r30 != 0) {
			// cmplw cr6,r11,r29
			// bne cr6,0x8246a760
			if (ctx.r11.u32 != var_r29) goto loc_8246A760;
			// addi r11,r30,-1
			ctx.r11.s64 = (int64_t)(int32_t)var_r30 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246a760
			if (ctx.r11.s32 != 0) goto loc_8246A760;
			// mr r11,r22
			ctx.r11.u64 = var_r22;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_8246A760:
		// lbz r4,68(r23)
		ctx.r4.u64 = PPC_LOAD_U8(var_r23 + 68);
		// addi r24,r24,1
		var_r24 = (uint32_t)(var_r24 + 1);
		// addi r25,r25,24
		var_r25 = (uint32_t)(var_r25 + 24);
		// cmplw cr6,r24,r4
		// blt cr6,0x8246a5f4
		if (var_r24 < ctx.r4.u32) goto loc_8246A5F4;
	}
loc_8246A774:
	// lwz r3,24(r23)
	ctx.r3.u64 = PPC_LOAD_U32(var_r23 + 24);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246a794
	if (ctx.r3.u32 != 0) {
		// lwz r10,12(r11)
		// bctrl
		VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
		// stw r22,24(r23)
		PPC_STORE_U32(var_r23 + 24, var_r22);
	}
loc_8246A794:
	// lis r10,-32162
	// mr r11,r22
	ctx.r11.u64 = var_r22;
	// stw r11,30860(r10)
	PPC_STORE_U32(ctx.r10.u32 + 30860, ctx.r11.u32);
	// lwz r3,20(r23)
	ctx.r3.u64 = PPC_LOAD_U32(var_r23 + 20);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246a7b4
	if (ctx.r3.u32 != 0) {
		// bl 0x82465d60
		phInst_5D60_p39(ctx, base);
		// stw r22,20(r23)
		PPC_STORE_U32(var_r23 + 20, var_r22);
	}
loc_8246A7B4:
	// addi r31,r23,12
	var_r31 = (uint32_t)(var_r23 + 12);
	// li r30,2
	var_r30 = 2;
loc_8246A7BC:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246a7d0
	if (ctx.r3.u32 != 0) {
		// bl 0x82465d60
		phInst_5D60_p39(ctx, base);
		// stw r22,0(r31)
		PPC_STORE_U32(var_r31 + 0, var_r22);
	}
loc_8246A7D0:
	// addi r30,r30,-1
	var_r30 = (uint32_t)(var_r30 + -1);
	// addi r31,r31,4
	var_r31 = (uint32_t)(var_r31 + 4);
	// cmplwi cr6,r30,0
	// bne cr6,0x8246a7bc
	if (var_r30 != 0) goto loc_8246A7BC;
	// addi r3,r21,12
	ctx.r3.s64 = (int64_t)(int32_t)var_r21 + 12;
	// bl 0x8246a3a0
	phInst_A3A0_p33(ctx, base);
	// mr r3,r21
	ctx.r3.u64 = var_r21;
	// bl 0x8246a3a0
	phInst_A3A0_p33(ctx, base);
	// addi r3,r23,28
	ctx.r3.s64 = (int64_t)(int32_t)var_r23 + 28;
	// bl 0x8246a3a0
	phInst_A3A0_p33(ctx, base);
	// lis r11,-32256
	// lwz r3,8(r23)
	ctx.r3.u64 = PPC_LOAD_U32(var_r23 + 8);
	// addi r11,r11,23684
	ctx.r11.s64 = ctx.r11.s64 + 23684;
	// cmplwi cr6,r3,0
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// beq cr6,0x8246a824
	if (ctx.r3.u32 != 0) {
		// lwz r8,4(r9)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// stw r22,8(r23)
		PPC_STORE_U32(var_r23 + 8, var_r22);
	}
loc_8246A824:
	// lis r11,-32256
	// addi r11,r11,15792
	ctx.r11.s64 = ctx.r11.s64 + 15792;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phInst_A838_2h"))) PPC_WEAK_FUNC(phInst_A838_2h);
PPC_FUNC_IMPL(__imp__phInst_A838_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=160, savegprlr_28
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r31,0
	var_r31 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lbz r11,1(r28)
	ctx.r11.u64 = PPC_LOAD_U8(var_r28 + 1);
	// mr r30,r11
	var_r30 = ctx.r11.u32;
	// cmplwi cr6,r30,0
	// stb r11,68(r29)
	PPC_STORE_U8(var_r29 + 68, ctx.r11.u8);
	// beq cr6,0x8246a928
	if (var_r30 != 0) {
		// lis r11,2730
		ctx.r11.s64 = 178913280;
		// ori r10,r11,43690
		ctx.r10.u64 = ctx.r11.u64 | 43690;
		// cmplw cr6,r30,r10
		// bgt cr6,0x8246a894
		if (var_r30 <= ctx.r10.u32) {
			// rlwinm r11,r30,1,0,30
			ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 1) & 0xFFFFFFFE;
			// li r9,-5
			// add r8,r30,r11
			ctx.r8.u64 = var_r30 + ctx.r11.u64;
			// rlwinm r11,r8,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
			// cmplw cr6,r11,r9
			// addi r4,r11,4
			ctx.r4.s64 = ctx.r11.s64 + 4;
			// ble cr6,0x8246a898
			if (ctx.r11.u32 <= ctx.r9.u32) goto loc_8246A898;
		}
	loc_8246A894:
		// li r4,-1
	loc_8246A898:
		// lwz r3,8(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 8);
		// lwz r6,20(r7)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// cmplwi cr6,r3,0
		// beq cr6,0x8246a914
		if (ctx.r3.u32 != 0) {
			// addi r5,r3,4
			ctx.r5.s64 = ctx.r3.s64 + 4;
			// stw r30,0(r3)
			PPC_STORE_U32(ctx.r3.u32 + 0, var_r30);
			// addi r8,r30,-1
			ctx.r8.s64 = (int64_t)(int32_t)var_r30 + -1;
			// mr r10,r5
			ctx.r10.u64 = ctx.r5.u64;
			// cmpwi cr6,r8,0
			// blt cr6,0x8246a90c
			if (ctx.r8.s32 >= 0) {
				// addi r11,r10,16
				ctx.r11.s64 = ctx.r10.s64 + 16;
				// li r6,16
				ctx.r6.s64 = 16;
				// li r7,24
				ctx.r7.s64 = 24;
			loc_8246A8D8:
				// mr r4,r10
				ctx.r4.u64 = ctx.r10.u64;
				// stw r10,0(r10)
				PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
				// addi r9,r11,-4
				ctx.r9.s64 = ctx.r11.s64 + -4;
				// stw r6,-8(r11)
				PPC_STORE_U32(ctx.r11.u32 + -8, ctx.r6.u32);
				// addi r8,r8,-1
				ctx.r8.s64 = ctx.r8.s64 + -1;
				// addi r10,r10,24
				ctx.r10.s64 = ctx.r10.s64 + 24;
				// cmpwi cr6,r8,0
				// stw r4,-12(r11)
				PPC_STORE_U32(ctx.r11.u32 + -12, ctx.r4.u32);
				// stw r7,4(r11)
				PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
				// stw r9,0(r11)
				PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
				// addi r11,r11,24
				ctx.r11.s64 = ctx.r11.s64 + 24;
				// stw r9,0(r9)
				PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
				// bge cr6,0x8246a8d8
				if (ctx.r8.s32 >= 0) goto loc_8246A8D8;
			}
		loc_8246A90C:
			// mr r11,r5
			ctx.r11.u64 = ctx.r5.u64;
			// b 0x8246a918
		} else {
		loc_8246A914:
			// mr r11,r31
			ctx.r11.u64 = var_r31;
		}
	loc_8246A918:
		// cmplwi cr6,r11,0
		// stw r11,64(r29)
		PPC_STORE_U32(var_r29 + 64, ctx.r11.u32);
		// beq cr6,0x8246aa04
		if (ctx.r11.u32 == 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8246A928:
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lbz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U8(var_r28 + 0);
	// cmplwi cr6,r10,6
	// std r31,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, var_r31);
	// std r31,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, var_r31);
	// stw r31,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r31);
	// stb r31,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, (uint8_t)var_r31);
	// stb r31,100(r1)
	PPC_STORE_U8(ctx.r1.u32 + 100, (uint8_t)var_r31);
	// bgt cr6,0x8246a950
	if (ctx.r10.u32 <= 6) {
		// li r10,6
		ctx.r10.s64 = 6;
	}
loc_8246A950:
	// stb r10,101(r1)
	PPC_STORE_U8(ctx.r1.u32 + 101, ctx.r10.u8);
	// lis r10,0
	ctx.r10.s64 = 0;
	// addi r30,r29,12
	var_r30 = (uint32_t)(var_r29 + 12);
	// ori r9,r10,48000
	ctx.r9.u64 = ctx.r10.u64 | 48000;
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
loc_8246A964:
	// cmpwi cr6,r3,0
	// blt cr6,0x8246a9fc
	if (ctx.r3.s32 < 0) {
		return;
	}
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// lwz r4,8(r29)
	ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 8);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82466088
	phInst_6088_2h(ctx, base);
	// addi r31,r31,1
	var_r31 = (uint32_t)(var_r31 + 1);
	// addi r30,r30,4
	var_r30 = (uint32_t)(var_r30 + 4);
	// cmplwi cr6,r31,2
	// blt cr6,0x8246a964
	if (var_r31 < 2) goto loc_8246A964;
	// cmpwi cr6,r3,0
	// blt cr6,0x8246a9fc
	if (ctx.r3.s32 >= 0) {
		// lwz r11,4(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 4);
		// cmplwi cr6,r11,0
		// bne cr6,0x8246a9a8
		if (ctx.r11.u32 == 0) {
			// lis r11,-32256
			// addi r11,r11,22920
			ctx.r11.s64 = ctx.r11.s64 + 22920;
		}
	loc_8246A9A8:
		// addi r5,r29,20
		ctx.r5.s64 = (int64_t)(int32_t)var_r29 + 20;
		// lwz r4,8(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 8);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stw r11,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
		// bl 0x8246c1e0
		phInst_C1E0_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8246a9fc
		if (ctx.r3.s32 < 0) {
			return;
		}
		// addi r31,r29,24
		var_r31 = (uint32_t)(var_r29 + 24);
		// lwz r3,8(r28)
		ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 8);
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x8247e880
		phInst_E880(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8246a9fc
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r11,0(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lis r10,-32184
		ctx.r10.s64 = -2109210624;
		// addi r4,r10,-7488
		ctx.r4.s64 = ctx.r10.s64 + -7488;
		// lwz r3,68(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
		// lwz r7,28(r8)
		// bctrl
		VCALL(ctx.r3.u32, 7, ctx, base);  // vtable slot 7 (byte +28)
	}
loc_8246A9FC:
	return;
}

__attribute__((alias("__imp__phInst_AA18_2hr"))) PPC_WEAK_FUNC(phInst_AA18_2hr);
PPC_FUNC_IMPL(__imp__phInst_AA18_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,23684
	ctx.r11.s64 = ctx.r11.s64 + 23684;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r4,0
	// stw r4,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r4.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// beq cr6,0x8246aa60
	if (ctx.r4.u32 != 0) {
		// lwz r9,0(r4)
  // [ph4a] vtable load collapsed
		// mr r3,r4
		ctx.r3.u64 = ctx.r4.u64;
		// lwz r8,0(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r4.u32, 0, ctx, base);  // pattern-B slot 0 (byte +0)
	}
loc_8246AA60:
	// lis r9,-32256
	// addi r10,r31,28
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 28;
	// addi r9,r9,23704
	ctx.r9.s64 = ctx.r9.s64 + 23704;
	// li r7,24
	ctx.r7.s64 = 24;
	// addi r11,r31,40
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 40;
	// li r6,16
	ctx.r6.s64 = 16;
	// lis r8,16256
	ctx.r8.s64 = 1065353216;
	// stw r9,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r9.u32);
	// addi r9,r11,12
	ctx.r9.s64 = ctx.r11.s64 + 12;
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// stw r6,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
	// lis r11,-32162
	// stw r7,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r7.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// stw r9,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r9.u32);
	// stw r8,72(r31)
	PPC_STORE_U32(var_r31 + 72, ctx.r8.u32);
	// stw r31,30860(r11)
	PPC_STORE_U32(ctx.r11.u32 + 30860, var_r31);
	// stw r8,76(r31)
	PPC_STORE_U32(var_r31 + 76, ctx.r8.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5C84_5_AAD0"))) PPC_WEAK_FUNC(ph_vt5C84_5_AAD0);
PPC_FUNC_IMPL(__imp__ph_vt5C84_5_AAD0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8246a428
	ke_A428(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_AB00"))) PPC_WEAK_FUNC(phInst_AB00);
PPC_FUNC_IMPL(__imp__phInst_AB00) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// stw r4,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// bl 0x82469e70
	phInst_9E70_2h(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246ab34
	if (ctx.r3.s32 >= 0) {
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r4,r11,228
		ctx.r4.s64 = ctx.r11.s64 + 228;
		// b 0x8246ab38
	} else {
	loc_8246AB34:
		// lwz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	}
loc_8246AB38:
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmpwi cr6,r3,0
	// blt cr6,0x8246abd4
	if (ctx.r3.s32 >= 0) {
		// lis r3,24962
		ctx.r3.s64 = 1635909632;
		// addi r5,r1,156
		ctx.r5.s64 = ctx.r1.s64 + 156;
		// ori r3,r3,5
		ctx.r3.u64 = ctx.r3.u64 | 5;
		// bl 0x8247ee00
		phInst_EE00(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x8246abd4
		if ((int32_t)var_r31 < 0) goto loc_8246ABD4;
		// lwz r3,156(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
		// li r4,228
		ctx.r4.s64 = 228;
		// lwz r9,20(r10)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// cmplwi cr6,r3,0
		// beq cr6,0x8246ab94
		if (ctx.r3.u32 != 0) {
			// lwz r4,156(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
			// bl 0x8246aa18
			phInst_AA18_2hr(ctx, base);
			// mr r30,r3
			var_r30 = ctx.r3.u32;
			// cmplwi cr6,r30,0
			// bne cr6,0x8246aba0
			if (var_r30 != 0) goto loc_8246ABA0;
		}
	loc_8246AB94:
		// lis r31,-32761
		var_r31 = (uint32_t)(-2147024896);
		// ori r31,r31,14
		var_r31 = (uint32_t)(var_r31 | 14);
		// b 0x8246abd4
		goto loc_8246ABD4;
	loc_8246ABA0:
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8246a838
		phInst_A838_2h(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x8246abc0
		if ((int32_t)var_r31 >= 0) {
			// stw r30,0(r28)
			PPC_STORE_U32(var_r28 + 0, var_r30);
			// b 0x8246abd4
		} else {
		loc_8246ABC0:
			// lwz r8,0(r30)
  // [ph4a] vtable load collapsed
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r7,12(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 3, ctx, base);  // pattern-B slot 3 (byte +12)
		}
	}
loc_8246ABD4:
	// lwz r3,156(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246abf0
	if (ctx.r3.u32 != 0) {
		// lwz r5,4(r6)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	}
loc_8246ABF0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__phInst_AC00_2hr"))) PPC_WEAK_FUNC(phInst_AC00_2hr);
PPC_FUNC_IMPL(__imp__phInst_AC00_2hr) {
	PPC_FUNC_PROLOGUE();
	// lis r11,32746
	ctx.r11.s64 = 2146041856;
	// ori r10,r11,6144
	ctx.r10.u64 = ctx.r11.u64 | 6144;
	// lwbrx r11,0,r10
	ctx.r11.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r10.u32));
	// lis r10,-32162
	ctx.r10.s64 = -2107768832;
	// stw r11,30864(r10)
	PPC_STORE_U32(ctx.r10.u32 + 30864, ctx.r11.u32);  /* glob:lbl_825E7890 @ 0x825e7890 */
	// blr
	return;
}

__attribute__((alias("__imp__aud_AC18"))) PPC_WEAK_FUNC(aud_AC18);
PPC_FUNC_IMPL(__imp__aud_AC18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	// FRAME: size=160, savegprlr_23
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r25,r4
	var_r25 = ctx.r4.u32;
	// rlwinm r11,r28,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 1) & 0xFFFFFFFE;
	// mr r24,r5
	var_r24 = ctx.r5.u32;
	// add r11,r28,r11
	ctx.r11.u64 = var_r28 + ctx.r11.u64;
	// mr r23,r6
	var_r23 = ctx.r6.u32;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// li r31,0
	var_r31 = 0;
	// addi r10,r11,143
	ctx.r10.s64 = ctx.r11.s64 + 143;
	// cmplwi cr6,r28,0
	// rlwinm r29,r10,0,0,24
	var_r29 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80);
	// beq cr6,0x8246ac84
	if (var_r28 != 0) {
		// addi r11,r25,8
		ctx.r11.s64 = (int64_t)(int32_t)var_r25 + 8;
		// mr r10,r28
		ctx.r10.u64 = var_r28;
	loc_8246AC5C:
		// lwz r9,-4(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// lbz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// addi r11,r11,12
		ctx.r11.s64 = ctx.r11.s64 + 12;
		// addi r7,r9,128
		ctx.r7.s64 = ctx.r9.s64 + 128;
		// cmplwi cr6,r10,0
		// mullw r6,r7,r8
		ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
		// rlwinm r9,r6,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
		// add r31,r9,r31
		var_r31 = (uint32_t)(ctx.r9.u64 + var_r31);
		// bne cr6,0x8246ac5c
		if (ctx.r10.u32 != 0) goto loc_8246AC5C;
	}
loc_8246AC84:
	// lis r11,-32162
	// add r30,r29,r31
	var_r30 = (uint32_t)(var_r29 + var_r31);
	// lis r5,-22654
	// addi r3,r11,30856
	ctx.r3.s64 = ctx.r11.s64 + 30856;
	// ori r5,r5,7
	ctx.r5.u64 = ctx.r5.u64 | 7;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// bl 0x824667c0
	util_67C0(ctx, base);
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// cmplwi cr6,r26,0
	// bne cr6,0x8246acbc
	if (var_r26 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
loc_8246ACBC:
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	// bl 0x825681d0
	fiDeviceLocal_81D0_p39(ctx, base);
	// addi r5,r26,16
	ctx.r5.s64 = (int64_t)(int32_t)var_r26 + 16;
	// add r29,r29,r26
	var_r29 = (uint32_t)(var_r29 + var_r26);
	// stw r28,0(r26)
	PPC_STORE_U32(var_r26 + 0, var_r28);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(var_r31, 0, ctx.xer);
	// stw r5,8(r26)
	PPC_STORE_U32(var_r26 + 8, ctx.r5.u32);
	// beq cr6,0x8246acf8
while (ctx.r11.u32 < var_r31) {
	loc_8246ACE8:
		// dcbf r11,r29
		// addi r11,r11,128
		ctx.r11.s64 = ctx.r11.s64 + 128;
		// cmplw cr6,r11,r31
		// blt cr6,0x8246ace8
}
loc_8246ACF8:
	// oris r4,r24,3
	ctx.r4.u64 = var_r24 | 196608;
	// cmplwi cr6,r28,0
	// stw r4,4(r26)
	PPC_STORE_U32(var_r26 + 4, ctx.r4.u32);
	// beq cr6,0x8246adec
	if (var_r28 != 0) {
		// li r27,0
		var_r27 = 0;
		// addi r30,r25,8
		var_r30 = (uint32_t)(var_r25 + 8);
	loc_8246AD10:
		// lwz r11,8(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 8);
		// add r31,r11,r27
		var_r31 = (uint32_t)(ctx.r11.u64 + var_r27);
		// stw r29,68(r31)
		PPC_STORE_U32(var_r31 + 68, var_r29);
		// lwz r11,-8(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + -8);
		// cmplwi cr6,r11,24000
		// bgt cr6,0x8246ad30
		if (ctx.r11.u32 <= 24000) {
			// li r10,0
			ctx.r10.s64 = 0;
			// b 0x8246ad50
		} else {
		loc_8246AD30:
			// cmplwi cr6,r11,32000
			// bgt cr6,0x8246ad40
			if (ctx.r11.u32 <= 32000) {
				// li r10,1
				ctx.r10.s64 = 1;
				// b 0x8246ad50
			} else {
			loc_8246AD40:
				// cmplwi cr6,r11,44100
				// li r10,2
				ctx.r10.s64 = 2;
				// ble cr6,0x8246ad50
				if (ctx.r11.u32 <= 44100) goto loc_8246AD50;
				// li r10,3
				ctx.r10.s64 = 3;
			}
		}
	loc_8246AD50:
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// rlwimi r11,r10,27,3,4
		ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 27) & 0x18000000) | (ctx.r11.u64 & 0xFFFFFFFFE7FFFFFF);
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r10,0(r30)
		ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 0);
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// rlwimi r11,r10,29,2,2
		ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 29) & 0x20000000) | (ctx.r11.u64 & 0xFFFFFFFFDFFFFFFF);
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r8,1(r30)
		ctx.r8.u64 = PPC_LOAD_U8(var_r30 + 1);
		// rlwimi r9,r8,20,8,11
		ctx.r9.u64 = (__builtin_rotateleft32(ctx.r8.u32, 20) & 0xF00000) | (ctx.r9.u64 & 0xFFFFFFFFFF0FFFFF);
		// stw r9,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r9.u32);
		// bl 0x8258630c
		__imp__MmGetPhysicalAddress(ctx, base);
		// stw r3,28(r31)
		PPC_STORE_U32(var_r31 + 28, ctx.r3.u32);
		// lwz r3,-4(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + -4);
		// lbz r4,0(r30)
		ctx.r4.u64 = PPC_LOAD_U8(var_r30 + 0);
		// lwz r7,0(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lwz r6,4(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mullw r11,r4,r3
		ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r3.s32);
		// rlwimi r7,r11,15,5,9
		ctx.r7.u64 = (__builtin_rotateleft32(ctx.r11.u32, 15) & 0x7C00000) | (ctx.r7.u64 & 0xFFFFFFFFF83FFFFF);
		// oris r5,r6,32768
		ctx.r5.u64 = ctx.r6.u64 | 2147483648;
		// stw r7,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r7.u32);
		// stw r5,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r5.u32);
		// lbz r10,0(r30)
		ctx.r10.u64 = PPC_LOAD_U8(var_r30 + 0);
		// lwz r9,-4(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + -4);
		// mullw r8,r10,r9
		ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
		// rlwinm r11,r8,1,0,30
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// add r29,r11,r29
		var_r29 = (uint32_t)(ctx.r11.u64 + var_r29);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8258630c
		__imp__MmGetPhysicalAddress(ctx, base);
		// stw r3,32(r31)
		PPC_STORE_U32(var_r31 + 32, ctx.r3.u32);
		// lbz r7,0(r30)
		ctx.r7.u64 = PPC_LOAD_U8(var_r30 + 0);
		// addi r28,r28,-1
		var_r28 = (uint32_t)(var_r28 + -1);
		// addi r27,r27,96
		var_r27 = (uint32_t)(var_r27 + 96);
		// rotlwi r11,r7,8
		ctx.r11.u64 = __builtin_rotateleft32(ctx.r7.u32, 8);
		// cmplwi cr6,r28,0
		// addi r30,r30,12
		var_r30 = (uint32_t)(var_r30 + 12);
		// add r29,r11,r29
		var_r29 = (uint32_t)(ctx.r11.u64 + var_r29);
		// bne cr6,0x8246ad10
		if (var_r28 != 0) goto loc_8246AD10;
	}
loc_8246ADEC:
	// clrlwi r6,r24,31
	ctx.r6.u64 = var_r24 & 0x1;
	// cmplwi cr6,r6,0
	// bne cr6,0x8246ae20
	if (ctx.r6.u32 == 0) {
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x8246b638
		aud_B638(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// bge cr6,0x8246ae20
		if ((int32_t)var_r31 >= 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// stw r26,0(r23)
			PPC_STORE_U32(var_r23 + 0, var_r26);
			return;
		}
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x8246ae30
		aud_AE30(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		return;
	}
loc_8246AE20:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r26,0(r23)
	PPC_STORE_U32(var_r23 + 0, var_r26);
	return;
}

__attribute__((alias("__imp__aud_AE30"))) PPC_WEAK_FUNC(aud_AE30);
PPC_FUNC_IMPL(__imp__aud_AE30) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8246ae80
	aud_AE80(ctx, base);
	// cmplwi cr6,r31,0
	// beq cr6,0x8246ae68
	if (var_r31 != 0) {
		// lis r11,-32162
		// lis r5,-22654
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// ori r5,r5,7
		ctx.r5.u64 = ctx.r5.u64 | 7;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_8246AE68:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__aud_AE80"))) PPC_WEAK_FUNC(aud_AE80);
PPC_FUNC_IMPL(__imp__aud_AE80) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// mr r29,r28
	var_r29 = (uint32_t)(var_r28);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
	// cmplwi cr6,r11,0
	// ble cr6,0x8246aee0
	if (ctx.r11.u32 > 0) {
		// mr r31,r28
		var_r31 = (uint32_t)(var_r28);
	loc_8246AEA8:
		// lwz r11,8(r30)
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 8);
		// add r10,r31,r11
		ctx.r10.u64 = var_r31 + ctx.r11.u64;
		// lwz r3,64(r10)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
		// cmplwi cr6,r3,0
		// beq cr6,0x8246aecc
		if (ctx.r3.u32 != 0) {
			// bl 0x825865cc
			__imp__XMAReleaseContext(ctx, base);
			// lwz r11,8(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 8);
			// add r9,r31,r11
			ctx.r9.u64 = var_r31 + ctx.r11.u64;
			// stw r28,64(r9)
			PPC_STORE_U32(ctx.r9.u32 + 64, var_r28);
		}
	loc_8246AECC:
		// lwz r8,0(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 0);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r31,r31,96
		var_r31 = (uint32_t)(var_r31 + 96);
		// cmplw cr6,r29,r8
		// blt cr6,0x8246aea8
		if (var_r29 < ctx.r8.u32) goto loc_8246AEA8;
	}
loc_8246AEE0:
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r6,r7,0,14,12
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFBFFFF;
	// stw r6,4(r30)
	PPC_STORE_U32(var_r30 + 4, ctx.r6.u32);
	return;
}

__attribute__((alias("__imp__ph_AEF8"))) PPC_WEAK_FUNC(ph_AEF8);
PPC_FUNC_IMPL(__imp__ph_AEF8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r3,1
	ctx.r3.s64 = 1;
	// rlwinm r10,r11,0,13,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40000;
	// cmplwi cr6,r10,0
	// bnelr cr6
	if (ctx.r10.u32 != 0) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__MmGetPhysicalAddress_AF18"))) PPC_WEAK_FUNC(MmGetPhysicalAddress_AF18);
PPC_FUNC_IMPL(__imp__MmGetPhysicalAddress_AF18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_28
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r30,r6,21,11,31
	var_r30 = (uint32_t)(__builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 21) & 0x1FFFFF);
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r31,r11,r10
	var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// rlwinm r28,r10,12,30,31
	var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3);
	// clrlwi r9,r28,31
	ctx.r9.u64 = var_r28 & 0x1;
	// cmplwi cr6,r9,0
	// bne cr6,0x8246af78
	if (ctx.r9.u32 == 0) {
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8258630c
		__imp__MmGetPhysicalAddress(ctx, base);
		// lwz r8,0(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
		// stw r3,20(r31)
		PPC_STORE_U32(var_r31 + 20, ctx.r3.u32);
		// rlwimi r30,r8,0,0,19
		var_r30 = (uint32_t)((ctx.r8.u32 & 0xFFFFF000) | (var_r30 & 0xFFFFFFFF00000FFF));
		// oris r7,r30,16
		ctx.r7.u64 = var_r30 | 1048576;
		// stw r7,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r7.u32);
		// stw r29,80(r31)
		PPC_STORE_U32(var_r31 + 80, var_r29);
		// b 0x8246afac
	} else {
	loc_8246AF78:
		// rlwinm r6,r28,0,30,30
		ctx.r6.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 0) & 0x2;
		// cmplwi cr6,r6,0
		// bne cr6,0x8246afdc
		if (ctx.r6.u32 != 0) {
			// lis r3,-32761
			// ori r3,r3,5
			ctx.r3.u64 = ctx.r3.u64 | 5;
			return;
		}
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8258630c
		__imp__MmGetPhysicalAddress(ctx, base);
		// lwz r5,0(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 0);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// oris r4,r5,32
		ctx.r4.u64 = ctx.r5.u64 | 2097152;
		// stw r3,24(r31)
		PPC_STORE_U32(var_r31 + 24, ctx.r3.u32);
		// rlwimi r30,r11,0,0,19
		var_r30 = (uint32_t)((ctx.r11.u32 & 0xFFFFF000) | (var_r30 & 0xFFFFFFFF00000FFF));
		// stw r4,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r4.u32);
		// stw r30,4(r31)
		PPC_STORE_U32(var_r31 + 4, var_r30);
		// stw r29,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r29);
	}
loc_8246AFAC:
	// cmplwi cr6,r28,0
	// bne cr6,0x8246afd0
	if (var_r28 == 0) {
		// lwz r3,0(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
		// bl 0x8246b618
		MmGetPhysicalAddress_B618_2hr(ctx, base);
		// cmplwi cr6,r3,0
		// beq cr6,0x8246afd0
		if (ctx.r3.u32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// rlwimi r3,r10,0,0,5
		ctx.r3.u64 = (ctx.r10.u32 & 0xFC000000) | (ctx.r3.u64 & 0xFFFFFFFF03FFFFFF);
		// stw r3,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r3.u32);
	}
loc_8246AFD0:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ke_AFF0_h"))) PPC_WEAK_FUNC(ke_AFF0_h);
PPC_FUNC_IMPL(__imp__ke_AFF0_h) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r10,r10,12,30,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// beq cr6,0x8246b02c
	if (ctx.r9.u32 != 0) {
		// lwz r8,80(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 80);
		// cmplw cr6,r8,r5
		// bne cr6,0x8246b02c
		if (ctx.r8.u32 != ctx.r5.u32) goto loc_8246B02C;
		// li r3,1
		ctx.r3.s64 = 1;
		// blr
		return;
	}
loc_8246B02C:
	// rlwinm r7,r10,0,30,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r7,0
	// beq cr6,0x8246b048
	if (ctx.r7.u32 != 0) {
		// lwz r6,84(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 84);
		// li r3,1
		ctx.r3.s64 = 1;
		// cmplw cr6,r6,r5
		// beqlr cr6
		if (ctx.r6.u32 == ctx.r5.u32) return;
	}
loc_8246B048:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B050_p39"))) PPC_WEAK_FUNC(phInst_B050_p39);
PPC_FUNC_IMPL(__imp__phInst_B050_p39) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r10,0
	// beq cr6,0x8246b084
	if (ctx.r10.u32 != 0) {
		// add r7,r4,r11
		ctx.r7.u64 = ctx.r4.u64 + ctx.r11.u64;
		// lwz r9,8(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// lis r8,48
		ctx.r8.s64 = 3145728;
		// rlwinm r6,r7,5,0,26
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0xFFFFFFE0;
		// lwzx r5,r6,r9
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r9.u32);
		// rlwinm r4,r5,0,10,11
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x300000;
		// cmplw cr6,r4,r8
		ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r8.u32, ctx.xer);
		// b 0x8246b0a8
	} else {
	loc_8246B084:
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// lwz r10,8(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// lis r3,48
		ctx.r3.s64 = 3145728;
		// rlwinm r11,r11,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
		// add r10,r11,r10
		ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
		// lwz r9,64(r10)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
		// lwz r8,0(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		// rlwinm r7,r8,0,10,11
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x300000;
		// cmplw cr6,r7,r3
		ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r3.u32, ctx.xer);
	}
loc_8246B0A8:
	// li r3,1
	ctx.r3.s64 = 1;
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__ke_B0B8_h"))) PPC_WEAK_FUNC(ke_B0B8_h);
PPC_FUNC_IMPL(__imp__ke_B0B8_h) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r10,0
	// beq cr6,0x8246b0e4
	if (ctx.r10.u32 != 0) {
		// add r8,r4,r11
		ctx.r8.u64 = ctx.r4.u64 + ctx.r11.u64;
		// lwz r9,8(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// rlwinm r7,r8,5,0,26
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
		// lwzx r6,r7,r9
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
		// rlwinm r11,r6,12,30,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0x3;
		// b 0x8246b100
	} else {
	loc_8246B0E4:
		// add r5,r4,r11
		ctx.r5.u64 = ctx.r4.u64 + ctx.r11.u64;
		// lwz r10,8(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// rlwinm r11,r5,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 5) & 0xFFFFFFE0;
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// lwz r3,64(r4)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 64);
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// rlwinm r11,r11,12,30,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0x3;
	}
loc_8246B100:
	// cmplwi cr6,r11,0
	// li r3,1
	ctx.r3.s64 = 1;
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B118_p39"))) PPC_WEAK_FUNC(phInst_B118_p39);
PPC_FUNC_IMPL(__imp__phInst_B118_p39) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r10,r7,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	// beq cr6,0x8246b16c
	if (ctx.r10.u32 != 0) {
		// lwz r9,36(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
		// lwz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// clrlwi r10,r9,27
		ctx.r10.u64 = ctx.r9.u32 & 0x1F;
		// rlwinm r9,r8,5,27,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0x1F;
		// cmplw cr6,r9,r10
		// blt cr6,0x8246b15c
		if (ctx.r9.u32 >= ctx.r10.u32) {
			// subf r10,r10,r9
			ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
			// b 0x8246b174
			// lhz r5,78(r11)
			ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 78);
			// rlwinm r4,r10,8,0,23
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
			// rlwinm r11,r7,3,31,31
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x1;
			// subf r3,r5,r4
			ctx.r3.s64 = ctx.r4.s64 - ctx.r5.s64;
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// srw r3,r3,r11
			ctx.r3.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r3.u32 >> (ctx.r11.u8 & 0x3F));
			// blr
			return;
		}
	loc_8246B15C:
		// rlwinm r8,r8,10,27,31
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0x1F;
		// subf r10,r10,r8
		ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
		// add r10,r10,r9
		ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
		// b 0x8246b174
	} else {
	loc_8246B16C:
		// lwz r6,0(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// rlwinm r10,r6,10,27,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 10) & 0x1F;
	}
loc_8246B174:
	// lhz r5,78(r11)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 78);
	// rlwinm r4,r10,8,0,23
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r11,r7,3,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x1;
	// subf r3,r5,r4
	ctx.r3.s64 = ctx.r4.s64 - ctx.r5.s64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// srw r3,r3,r11
	ctx.r3.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r3.u32 >> (ctx.r11.u8 & 0x3F));
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B190_p39"))) PPC_WEAK_FUNC(phInst_B190_p39);
PPC_FUNC_IMPL(__imp__phInst_B190_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r30);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r31,0
	var_r31 = 0;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// rlwinm r4,r10,5,0,26
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lwz r7,36(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// lwz r8,68(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	// rlwinm r10,r7,8,19,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 8) & 0x1F00;
	// lhz r9,78(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 78);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmplwi cr6,r9,0
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r10,r8,3,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0x1;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// slw r10,r5,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r7.u8 & 0x3F));
	// beq cr6,0x8246b274
	if (ctx.r9.u32 != 0) {
		// subfic r8,r9,256
		ctx.xer.ca = ctx.r9.u32 <= 256;
		ctx.r8.s64 = 256 - ctx.r9.s64;
		// cmplw cr6,r10,r8
		// bge cr6,0x8246b208
		if (ctx.r10.u32 < ctx.r8.u32) {
			// lwz r8,8(r3)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			// add r6,r10,r9
			ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
			// mr r31,r10
			var_r31 = ctx.r10.u32;
			// add r5,r4,r8
			ctx.r5.u64 = ctx.r4.u64 + ctx.r8.u64;
			// sth r6,78(r5)
			PPC_STORE_U16(ctx.r5.u32 + 78, ctx.r6.u16);
			// b 0x8246b270
		} else {
		loc_8246B208:
			// lwz r9,8(r3)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
			// li r6,0
			ctx.r6.s64 = 0;
			// mr r31,r8
			var_r31 = ctx.r8.u32;
			// add r7,r4,r9
			ctx.r7.u64 = ctx.r4.u64 + ctx.r9.u64;
			// subf r10,r8,r10
			ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
			// sth r6,78(r7)
			PPC_STORE_U16(ctx.r7.u32 + 78, ctx.r6.u16);
			// lwz r8,36(r11)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
			// lwz r5,0(r11)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// clrlwi r9,r8,27
			ctx.r9.u64 = ctx.r8.u32 & 0x1F;
			// rlwinm r7,r5,10,27,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 10) & 0x1F;
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// cmplw cr6,r9,r7
			// blt cr6,0x8246b240
			if (ctx.r9.u32 >= ctx.r7.u32) {
				// li r9,0
				ctx.r9.s64 = 0;
			}
		loc_8246B240:
			// lwz r6,4(r11)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// rlwimi r8,r9,0,27,31
			ctx.r8.u64 = (ctx.r9.u32 & 0x1F) | (ctx.r8.u64 & 0xFFFFFFFFFFFFFFE0);
			// cmplwi cr6,r9,0
			// oris r5,r6,32768
			ctx.r5.u64 = ctx.r6.u64 | 2147483648;
			// li r9,1
			ctx.r9.s64 = 1;
			// stw r8,36(r11)
			PPC_STORE_U32(ctx.r11.u32 + 36, ctx.r8.u32);
			// stw r5,4(r11)
			PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
			// beq cr6,0x8246b264
			if (ctx.r9.u32 != 0) {
				// li r9,0
				ctx.r9.s64 = 0;
			}
		loc_8246B264:
			// clrlwi r8,r9,24
			ctx.r8.u64 = ctx.r9.u32 & 0xFF;
			// cmplwi cr6,r8,0
			// beq cr6,0x8246b274
			if (ctx.r8.u32 == 0) goto loc_8246B274;
		}
	loc_8246B270:
		// li r10,0
		ctx.r10.s64 = 0;
	}
loc_8246B274:
	// lwz r5,36(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// rlwinm r8,r10,24,8,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFFFFFF;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// clrlwi r30,r10,24
	var_r30 = (uint32_t)(ctx.r10.u32 & 0xFF);
	// clrlwi r10,r5,27
	ctx.r10.u64 = ctx.r5.u32 & 0x1F;
	// rlwinm r7,r6,5,27,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0x1F;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplw cr6,r7,r10
	// ble cr6,0x8246b2a0
	if (ctx.r7.u32 > ctx.r10.u32) {
		// subf r9,r10,r7
		ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
		// b 0x8246b2c8
	} else {
	loc_8246B2A0:
		// bge cr6,0x8246b2b0
		if (ctx.cr6.lt) {
			// rlwinm r7,r6,10,27,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 10) & 0x1F;
			// subf r9,r10,r7
			ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
			// b 0x8246b2c8
		} else {
		loc_8246B2B0:
			// lwz r7,4(r11)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// rlwinm r7,r7,0,0,0
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x80000000;
			// cmplwi cr6,r7,0
			// bne cr6,0x8246b2c8
			if (ctx.r7.u32 != 0) goto loc_8246B2C8;
			// rlwinm r9,r6,10,27,31
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 10) & 0x1F;
			// subf r9,r10,r9
			ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
		}
	}
loc_8246B2C8:
	// cmplwi cr6,r8,0
	// beq cr6,0x8246b310
	if (ctx.r8.u32 != 0) {
		// cmplw cr6,r8,r9
		// blt cr6,0x8246b2dc
		if (ctx.r8.u32 >= ctx.r9.u32) {
			// mr r8,r9
			ctx.r8.u64 = ctx.r9.u64;
		}
	loc_8246B2DC:
		// add r10,r10,r8
		ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
		// rlwinm r7,r8,8,0,23
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 8) & 0xFFFFFF00;
		// rlwinm r6,r6,10,27,31
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 10) & 0x1F;
		// add r31,r7,r31
		var_r31 = (uint32_t)(ctx.r7.u64 + var_r31);
		// subf r9,r8,r9
		ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
		// cmplw cr6,r10,r6
		// blt cr6,0x8246b2fc
		if (ctx.r10.u32 >= ctx.r6.u32) {
			// li r10,0
			ctx.r10.s64 = 0;
		}
	loc_8246B2FC:
		// lwz r8,4(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// rlwimi r5,r10,0,27,31
		ctx.r5.u64 = (ctx.r10.u32 & 0x1F) | (ctx.r5.u64 & 0xFFFFFFFFFFFFFFE0);
		// oris r7,r8,32768
		ctx.r7.u64 = ctx.r8.u64 | 2147483648;
		// stw r5,36(r11)
		PPC_STORE_U32(ctx.r11.u32 + 36, ctx.r5.u32);
		// stw r7,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	}
loc_8246B310:
	// cmplwi cr6,r30,0
	// beq cr6,0x8246b330
	if (var_r30 != 0) {
		// cmplwi cr6,r9,0
		// beq cr6,0x8246b330
		if (ctx.r9.u32 == 0) goto loc_8246B330;
		// lwz r10,8(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// add r31,r30,r31
		var_r31 = (uint32_t)(var_r30 + var_r31);
		// add r5,r4,r10
		ctx.r5.u64 = ctx.r4.u64 + ctx.r10.u64;
		// sth r30,78(r5)
		PPC_STORE_U16(ctx.r5.u32 + 78, (uint16_t)var_r30);
	}
loc_8246B330:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r10,r10,3,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0x1;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// srw r3,r31,r9
	ctx.r3.u64 = ctx.r9.u8 & 0x20 ? 0 : (var_r31 >> (ctx.r9.u8 & 0x3F));
	// add r8,r3,r10
	ctx.r8.u64 = ctx.r3.u64 + ctx.r10.u64;
	// stw r8,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r8.u32);
	// ld r30,-16(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B360_p39"))) PPC_WEAK_FUNC(phInst_B360_p39);
PPC_FUNC_IMPL(__imp__phInst_B360_p39) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r9,36(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// lwz r8,68(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	// rlwinm r9,r9,8,19,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0x1F00;
	// lhz r10,78(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 78);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmplwi cr6,r10,0
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r8,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r8.u32);
	// lwz r6,36(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// clrlwi r11,r6,27
	ctx.r11.u64 = ctx.r6.u32 & 0x1F;
	// rlwinm r8,r9,5,27,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0x1F;
	// rlwinm r6,r9,10,27,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0x1F;
	// rlwinm r9,r7,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0x1;
	// beq cr6,0x8246b3c4
	if (ctx.r10.u32 != 0) {
		// subfic r3,r10,256
		ctx.xer.ca = ctx.r10.u32 <= 256;
		ctx.r3.s64 = 256 - ctx.r10.s64;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// li r9,1
		ctx.r9.s64 = 1;
	}
loc_8246B3C4:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplw cr6,r8,r11
	// ble cr6,0x8246b3d8
	if (ctx.r8.u32 > ctx.r11.u32) {
		// subf r10,r11,r8
		ctx.r10.s64 = ctx.r8.s64 - ctx.r11.s64;
		// b 0x8246b3ec
	} else {
	loc_8246B3D8:
		// blt cr6,0x8246b3e8
		if (!(ctx.cr6.lt)) {
			// clrlwi r5,r9,24
			ctx.r5.u64 = ctx.r9.u32 & 0xFF;
			// cmplwi cr6,r5,0
			// bne cr6,0x8246b3ec
			if (ctx.r5.u32 != 0) {
				// rlwinm r9,r7,3,31,31
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x1;
				// rlwinm r11,r10,8,0,23
				ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
				// addi r4,r9,1
				ctx.r4.s64 = ctx.r9.s64 + 1;
				// add r3,r11,r3
				ctx.r3.u64 = ctx.r11.u64 + ctx.r3.u64;
				// srw r3,r3,r4
				ctx.r3.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r3.u32 >> (ctx.r4.u8 & 0x3F));
				// blr
				return;
			}
		}
	loc_8246B3E8:
		// subf r10,r11,r6
		ctx.r10.s64 = ctx.r6.s64 - ctx.r11.s64;
	}
loc_8246B3EC:
	// rlwinm r9,r7,3,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0x1;
	// rlwinm r11,r10,8,0,23
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// addi r4,r9,1
	ctx.r4.s64 = ctx.r9.s64 + 1;
	// add r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 + ctx.r3.u64;
	// srw r3,r3,r4
	ctx.r3.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r3.u32 >> (ctx.r4.u8 & 0x3F));
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B408_p39"))) PPC_WEAK_FUNC(phInst_B408_p39);
PPC_FUNC_IMPL(__imp__phInst_B408_p39) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r10,0
	ctx.r10.s64 = 0;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r8,r9,0,5,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x7FFF000;
	// lwz r6,16(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lwz r5,36(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// rlwinm r4,r7,0,1,19
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x7FFFF000;
	// rlwinm r8,r8,0,12,9
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFCFFFFF;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// rlwinm r7,r5,0,0,26
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// stw r10,80(r11)
	PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r10.u32);
	// stw r10,84(r11)
	PPC_STORE_U32(ctx.r11.u32 + 84, ctx.r10.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// clrlwi r8,r6,1
	ctx.r8.u64 = ctx.r6.u32 & 0x7FFFFFFF;
	// clrlwi r6,r9,6
	ctx.r6.u64 = ctx.r9.u32 & 0x3FFFFFF;
	// stw r10,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r10.u32);
	// sth r10,78(r11)
	PPC_STORE_U16(ctx.r11.u32 + 78, ctx.r10.u16);
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// stw r4,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r4.u32);
	// stw r8,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r8.u32);
	// stw r7,36(r11)
	PPC_STORE_U32(ctx.r11.u32 + 36, ctx.r7.u32);
	// stw r6,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ke_B488_h"))) PPC_WEAK_FUNC(ke_B488_h);
PPC_FUNC_IMPL(__imp__ke_B488_h) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r10,0
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// beq cr6,0x8246b4b8
	if (ctx.r10.u32 != 0) {
		// add r9,r4,r11
		ctx.r9.u64 = ctx.r4.u64 + ctx.r11.u64;
		// rlwinm r11,r9,5,0,26
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
		// add r8,r11,r10
		ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
		// lwz r7,8(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// rlwinm r3,r7,6,27,31
		ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0x1F;
		// blr
		return;
	}
loc_8246B4B8:
	// add r6,r4,r11
	ctx.r6.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r6,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 5) & 0xFFFFFFE0;
	// add r5,r11,r10
	ctx.r5.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r4,64(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// lwz r3,8(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// rlwinm r3,r3,6,27,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 6) & 0x1F;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B4D8_p39"))) PPC_WEAK_FUNC(phInst_B4D8_p39);
PPC_FUNC_IMPL(__imp__phInst_B4D8_p39) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,10(r5)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 10);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r3,0
	ctx.r3.s64 = 0;
	// rotlwi r9,r11,12
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 12);
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r4,r11
	ctx.r8.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r8,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r6,r7,0,20,11
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFF00FFF;
	// lwz r8,12(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// or r4,r9,r6
	ctx.r4.u64 = ctx.r9.u64 | ctx.r6.u64;
	// stw r4,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r4.u32);
	// lbz r9,8(r5)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r5.u32 + 8);
	// rlwimi r10,r9,12,18,19
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 12) & 0x3000) | (ctx.r10.u64 & 0xFFFFFFFFFFFFCFFF);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// lbz r4,9(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + 9);
	// rlwimi r6,r4,17,12,14
	ctx.r6.u64 = (__builtin_rotateleft32(ctx.r4.u32, 17) & 0xE0000) | (ctx.r6.u64 & 0xFFFFFFFFFFF1FFFF);
	// stw r6,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r6.u32);
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// rlwimi r8,r10,0,6,31
	ctx.r8.u64 = (ctx.r10.u32 & 0x3FFFFFF) | (ctx.r8.u64 & 0xFFFFFFFFFC000000);
	// stw r8,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r8.u32);
	// lwz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// rlwimi r9,r7,0,0,5
	ctx.r9.u64 = (ctx.r7.u32 & 0xFC000000) | (ctx.r9.u64 & 0xFFFFFFFF03FFFFFF);
	// stw r9,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_B550_h"))) PPC_WEAK_FUNC(ph_B550_h);
PPC_FUNC_IMPL(__imp__ph_B550_h) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r3,72(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 72);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_B570_p39"))) PPC_WEAK_FUNC(phInst_B570_p39);
PPC_FUNC_IMPL(__imp__phInst_B570_p39) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r9,r10,0,10,11
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x300000;
	// cmplwi cr6,r9,0
	// bne cr6,0x8246b5ac
	if (ctx.r9.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
		// stw r11,4(r5)
		PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r11.u32);
		// stw r11,8(r5)
		PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r11.u32);
		// blr
		return;
	}
loc_8246B5AC:
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// rlwinm r10,r8,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// addi r7,r10,20
	ctx.r7.s64 = ctx.r10.s64 + 20;
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r4,r6,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	// stw r4,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r4.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// clrlwi r9,r10,6
	ctx.r9.u64 = ctx.r10.u32 & 0x3FFFFFF;
	// stw r9,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r9.u32);
	// lbz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// clrlwi r7,r8,29
	ctx.r7.u64 = ctx.r8.u32 & 0x7;
	// stw r7,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ke_B5E0_h"))) PPC_WEAK_FUNC(ke_B5E0_h);
PPC_FUNC_IMPL(__imp__ke_B5E0_h) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,5,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwimi r5,r10,0,0,5
	ctx.r5.u64 = (ctx.r10.u32 & 0xFC000000) | (ctx.r5.u64 & 0xFFFFFFFF03FFFFFF);
	// rlwimi r9,r6,24,5,7
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r6.u32, 24) & 0x7000000) | (ctx.r9.u64 & 0xFFFFFFFFF8FFFFFF);
	// stw r5,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r5.u32);
	// stw r9,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__MmGetPhysicalAddress_B618_2hr"))) PPC_WEAK_FUNC(MmGetPhysicalAddress_B618_2hr);
PPC_FUNC_IMPL(__imp__MmGetPhysicalAddress_B618_2hr) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r10,r11,0,4,4
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x8000000;
	// cmplwi cr6,r10,0
	// beqlr cr6
	if (ctx.r10.u32 == 0) return;
	// rlwinm r11,r11,21,17,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 21) & 0x7FFF;
	// addi r3,r11,32
	ctx.r3.s64 = ctx.r11.s64 + 32;
	// blr
	return;
}

__attribute__((alias("__imp__aud_B638"))) PPC_WEAK_FUNC(aud_B638);
PPC_FUNC_IMPL(__imp__aud_B638) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=160, savegprlr_23
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// li r23,0
	var_r23 = 0;
	// li r24,0
	var_r24 = 0;
	// lwz r11,0(r26)
	ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 0);
	// cmplwi cr6,r11,0
	// ble cr6,0x8246b6ec
	if (ctx.r11.u32 > 0) {
		// lis r10,8186
		ctx.r10.s64 = 536477696;
		// li r25,0
		var_r25 = 0;
		// lis r28,-32162
		var_r28 = (uint32_t)(-2107768832);
		// li r27,1
		var_r27 = 1;
		// ori r29,r10,34464
		var_r29 = (uint32_t)(ctx.r10.u64 | 34464);
	loc_8246B670:
		// lwz r11,8(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 8);
		// add r30,r25,r11
		var_r30 = (uint32_t)(var_r25 + ctx.r11.u64);
		// addi r31,r30,64
		var_r31 = (uint32_t)(var_r30 + 64);
		// lwz r9,0(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
		// cmplwi cr6,r9,0
		// bne cr6,0x8246b6d8
		if (ctx.r9.u32 == 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x825865dc
			__imp__XMACreateContext(ctx, base);
			// mr r23,r3
			var_r23 = ctx.r3.u32;
			// cmpwi cr6,r23,0
			// blt cr6,0x8246b6f8
			if ((int32_t)var_r23 < 0) {
				// mr r3,r23
				ctx.r3.u64 = var_r23;
				return;
			}
			// lwz r3,0(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
			// bl 0x8258630c
			__imp__MmGetPhysicalAddress(ctx, base);
			// lwz r11,30864(r28)
			ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 30864);
			// subf r8,r11,r3
			ctx.r8.s64 = ctx.r3.s64 - ctx.r11.s64;
			// srawi r7,r8,6
			ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3F) != 0);
			ctx.r7.s64 = ctx.r8.s32 >> 6;
			// clrlwi r11,r7,16
			ctx.r11.u64 = ctx.r7.u32 & 0xFFFF;
			// mr r10,r11
			ctx.r10.u64 = ctx.r11.u64;
			// clrlwi r6,r10,27
			ctx.r6.u64 = ctx.r10.u32 & 0x1F;
			// rlwinm r5,r10,27,5,31
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
			// sth r11,76(r30)
			PPC_STORE_U16(var_r30 + 76, ctx.r11.u16);
			// add r4,r5,r29
			ctx.r4.u64 = ctx.r5.u64 + var_r29;
			// rlwinm r3,r4,2,0,29
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// slw r11,r27,r6
			ctx.r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (var_r27 << (ctx.r6.u8 & 0x3F));
			// stwbrx r11,0,r3
			PPC_MM_STORE_U32(ctx.r3.u32, __builtin_bswap32(ctx.r11.u32));
			// eieio
		}
	loc_8246B6D8:
		// lwz r10,0(r26)
		ctx.r10.u64 = PPC_LOAD_U32(var_r26 + 0);
		// addi r24,r24,1
		var_r24 = (uint32_t)(var_r24 + 1);
		// addi r25,r25,96
		var_r25 = (uint32_t)(var_r25 + 96);
		// cmplw cr6,r24,r10
		// blt cr6,0x8246b670
		if (var_r24 < ctx.r10.u32) goto loc_8246B670;
	}
loc_8246B6EC:
	// lwz r9,4(r26)
	ctx.r9.u64 = PPC_LOAD_U32(var_r26 + 4);
	// oris r8,r9,4
	ctx.r8.u64 = ctx.r9.u64 | 262144;
	// stw r8,4(r26)
	PPC_STORE_U32(var_r26 + 4, ctx.r8.u32);
loc_8246B6F8:
	// mr r3,r23
	ctx.r3.u64 = var_r23;
	return;
}

__attribute__((alias("__imp__ph_B708"))) PPC_WEAK_FUNC(ph_B708);
PPC_FUNC_IMPL(__imp__ph_B708) {
	PPC_FUNC_PROLOGUE();
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// rlwinm r10,r11,0,14,15
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x30000;
	// cmplwi cr6,r10,0
	// beq cr6,0x8246b72c
	if (ctx.r10.u32 != 0) {
		// oris r9,r11,1
		ctx.r9.u64 = ctx.r11.u64 | 65536;
		// li r3,0
		ctx.r3.s64 = 0;
		// stw r9,4(r8)
		PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r9.u32);
		// blr
		return;
	}
loc_8246B72C:
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r7,0
	// ble cr6,0x8246b788
	if (ctx.r7.u32 > 0) {
		// lis r5,8186
		ctx.r5.s64 = 536477696;
		// li r10,0
		ctx.r10.s64 = 0;
		// li r6,1
		ctx.r6.s64 = 1;
		// ori r7,r5,34448
		ctx.r7.u64 = ctx.r5.u64 | 34448;
	loc_8246B74C:
		// lwz r11,8(r8)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// lhz r11,76(r4)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 76);
		// clrlwi r3,r11,27
		ctx.r3.u64 = ctx.r11.u32 & 0x1F;
		// rlwinm r11,r11,27,5,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x7FFFFFF;
		// add r5,r11,r7
		ctx.r5.u64 = ctx.r11.u64 + ctx.r7.u64;
		// rlwinm r4,r5,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// slw r3,r6,r3
		ctx.r3.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r6.u32 << (ctx.r3.u8 & 0x3F));
		// stwbrx r3,0,r4
		PPC_MM_STORE_U32(ctx.r4.u32, __builtin_bswap32(ctx.r3.u32));
		// eieio
		// lwz r11,0(r8)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// addi r10,r10,96
		ctx.r10.s64 = ctx.r10.s64 + 96;
		// cmplw cr6,r9,r11
		// blt cr6,0x8246b74c
		if (ctx.r9.u32 < ctx.r11.u32) goto loc_8246B74C;
	}
loc_8246B788:
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// oris r9,r10,1
	ctx.r9.u64 = ctx.r10.u64 | 65536;
	// stw r9,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ke_B7A0"))) PPC_WEAK_FUNC(ke_B7A0);
PPC_FUNC_IMPL(__imp__ke_B7A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mftb r29
	var_r29 = (uint32_t)(PPC_QUERY_TIMEBASE());
	// bl 0x8246b860
	phInst_B860_p39(ctx, base);
	// cmpwi cr6,r3,0
	// bne cr6,0x8246b7e8
while (ctx.r3.s32 == 0) {
	loc_8246B7C0:
		// mftb r30
		var_r30 = (uint32_t)(PPC_QUERY_TIMEBASE());
		// bl 0x825865ec
		__imp__KeQueryPerformanceFrequency(ctx, base);
		// rldicl r11,r3,61,3
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 61) & 0x1FFFFFFFFFFFFFFF;
		// subf r10,r29,r30
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 - (int64_t)(int32_t)var_r29;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// cmpld cr6,r10,r11
		// bgt cr6,0x8246b7f4
		if (ctx.r10.u64 > ctx.r11.u64) goto loc_8246B7F4;
		// bl 0x8246b860
		phInst_B860_p39(ctx, base);
		// cmpwi cr6,r3,0
		// beq cr6,0x8246b7c0
}
loc_8246B7E8:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
loc_8246B7F4:
	// bl 0x8240e6d0
	nop_8240E6D0(ctx, base);
	// li r3,1000
	ctx.r3.s64 = 1000;
	// bl 0x82566c80
	pg_6C80_g(ctx, base);
	// lis r9,32746
	ctx.r9.s64 = 2146041856;
	// li r30,0
	var_r30 = 0;
	// ori r11,r9,6148
	ctx.r11.u64 = ctx.r9.u64 | 6148;
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// stwx r30,0,r8
	PPC_MM_STORE_U32(ctx.r8.u32, var_r30);
	// eieio
	// lis r7,768
	ctx.r7.s64 = 50331648;
	// stwx r7,0,r11
	PPC_MM_STORE_U32(ctx.r11.u32, ctx.r7.u32);
	// eieio
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8246b860
	phInst_B860_p39(ctx, base);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
	// cmplwi cr6,r6,0
	// ble cr6,0x8246b854
while (var_r30 < ctx.r5.u32) {
	loc_8246B838:
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8246b408
		phInst_B408_p39(ctx, base);
		// lwz r5,0(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// cmplw cr6,r30,r5
		// blt cr6,0x8246b838
}
loc_8246B854:
	// li r3,1
	ctx.r3.s64 = 1;
	return;
}

__attribute__((alias("__imp__phInst_B860_p39"))) PPC_WEAK_FUNC(phInst_B860_p39);
PPC_FUNC_IMPL(__imp__phInst_B860_p39) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// stw r9,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r9.u32);
	// rlwinm r10,r11,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r10,0
	// bne cr6,0x8246b9cc
	if (ctx.r10.u32 == 0) {
		// lis r8,32746
		ctx.r8.s64 = 2146041856;
		// li r5,0
		ctx.r5.s64 = 0;
		// ori r7,r8,6168
		ctx.r7.u64 = ctx.r8.u64 | 6168;
		// lwz r8,0(r9)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		// mr r10,r5
		ctx.r10.u64 = ctx.r5.u64;
		// cmplwi cr6,r8,0
		// lwbrx r11,0,r7
		ctx.r11.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r7.u32));
		// xori r7,r11,512
		ctx.r7.u64 = ctx.r11.u64 ^ 512;
		// beq cr6,0x8246b8c4
		if (ctx.r8.u32 != 0) {
			// lwz r11,8(r9)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
			// addi r11,r11,76
			ctx.r11.s64 = ctx.r11.s64 + 76;
		loc_8246B8A4:
			// lhz r6,0(r11)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
			// cmplw cr6,r6,r7
			// beq cr6,0x8246b9d4
			if (ctx.r6.u32 == ctx.r7.u32) {
				// li r3,0
				ctx.r3.s64 = 0;
				// blr
				return;
			}
			// lwz r4,0(r9)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// addi r11,r11,96
			ctx.r11.s64 = ctx.r11.s64 + 96;
			// cmplw cr6,r10,r4
			// blt cr6,0x8246b8a4
			if (ctx.r10.u32 < ctx.r4.u32) goto loc_8246B8A4;
		}
	loc_8246B8C4:
		// mr r11,r5
		ctx.r11.u64 = ctx.r5.u64;
		// cmplwi cr6,r8,0
		// stw r11,-12(r1)
		PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r11.u32);
		// beq cr6,0x8246b9c0
		while (ctx.r8.u32 < 0) {
		loc_8246B8D4:
			// lwz r10,8(r9)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
			// rlwinm r9,r11,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// add r3,r11,r9
			ctx.r3.u64 = ctx.r11.u64 + ctx.r9.u64;
			// rlwinm r11,r3,5,0,26
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 5) & 0xFFFFFFE0;
			// add r10,r11,r10
			ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lwz r11,64(r10)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
			// stw r10,-16(r1)
			PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r10.u32);
			// addi r9,r11,16
			ctx.r9.s64 = ctx.r11.s64 + 16;
			// addi r8,r11,32
			ctx.r8.s64 = ctx.r11.s64 + 32;
			// lvx128 v0,r0,r11
			ea = (ctx.r11.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v13,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lvx128 v12,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v0,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r7,-16(r1)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
			// addi r6,r7,16
			ctx.r6.s64 = ctx.r7.s64 + 16;
			// stvx v13,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r4,-16(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
			// addi r3,r4,32
			ctx.r3.s64 = ctx.r4.s64 + 32;
			// stvx v12,r0,r3
			ea = (ctx.r3.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// lwz r11,-16(r1)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
			// lwz r8,-12(r1)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
			// lwz r10,0(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// rlwinm r6,r10,12,30,31
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3;
			// clrlwi r9,r6,31
			ctx.r9.u64 = ctx.r6.u32 & 0x1;
			// cmplwi cr6,r9,0
			// lwz r9,20(r1)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
			// bne cr6,0x8246b964
			if (ctx.r9.u32 == 0) {
				// rlwinm r7,r10,0,0,19
				ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFF000;
				// stw r5,20(r11)
				PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r5.u32);
				// rlwinm r10,r8,1,0,30
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
				// add r4,r8,r10
				ctx.r4.u64 = ctx.r8.u64 + ctx.r10.u64;
				// stw r7,0(r11)
				PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
				// rlwinm r10,r4,5,0,26
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 5) & 0xFFFFFFE0;
				// lwz r7,8(r9)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
				// add r3,r10,r7
				ctx.r3.u64 = ctx.r10.u64 + ctx.r7.u64;
				// stw r5,80(r3)
				PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r5.u32);
			}
		loc_8246B964:
			// rlwinm r10,r6,0,30,30
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x2;
			// cmplwi cr6,r10,0
			// bne cr6,0x8246b998
			if (ctx.r10.u32 == 0) {
				// lwz r7,4(r11)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				// rlwinm r10,r8,1,0,30
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
				// stw r5,24(r11)
				PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r5.u32);
				// rlwinm r4,r7,0,0,19
				ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFF000;
				// add r3,r8,r10
				ctx.r3.u64 = ctx.r8.u64 + ctx.r10.u64;
				// rlwinm r10,r3,5,0,26
				ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 5) & 0xFFFFFFE0;
				// stw r4,4(r11)
				PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r4.u32);
				// lwz r7,8(r9)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
				// add r10,r10,r7
				ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
				// stw r5,84(r10)
				PPC_STORE_U32(ctx.r10.u32 + 84, ctx.r5.u32);
			}
		loc_8246B998:
			// cmplwi cr6,r6,0
			// bne cr6,0x8246b9ac
			if (ctx.r6.u32 != 0) goto loc_8246B9AC;
			// lwz r7,16(r11)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
			// clrlwi r6,r7,1
			ctx.r6.u64 = ctx.r7.u32 & 0x7FFFFFFF;
			// stw r6,16(r11)
			PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r6.u32);
		loc_8246B9AC:
			// addi r11,r8,1
			ctx.r11.s64 = ctx.r8.s64 + 1;
			// lwz r4,0(r9)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
			// cmplw cr6,r11,r4
			ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
			// stw r11,-12(r1)
			PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r11.u32);
			// blt cr6,0x8246b8d4
	}
	loc_8246B9C0:
		// lwz r11,4(r9)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
		// oris r10,r11,2
		ctx.r10.u64 = ctx.r11.u64 | 131072;
		// stw r10,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	}
loc_8246B9CC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr
	return;
}

__attribute__((alias("__imp__ph_B9E0"))) PPC_WEAK_FUNC(ph_B9E0);
PPC_FUNC_IMPL(__imp__ph_B9E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// lwz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stw r8,20(r1)
	PPC_STORE_U32(ctx.r1.u32 + 20, ctx.r8.u32);
	// cmplwi cr6,r5,0
	// beq cr6,0x8246ba34
	if (ctx.r5.u32 != 0) {
		// lwz r7,8(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// rotlwi r6,r5,0
		ctx.r6.u64 = ctx.r5.u32;
	loc_8246B9FC:
		// lwz r10,0(r7)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		// li r11,0
		ctx.r11.s64 = 0;
		// lwz r9,68(r7)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
		// rlwinm r10,r10,18,19,23
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x1F00;
		// cmplwi cr6,r10,0
		// beq cr6,0x8246ba24
	while (ctx.r11.u32 < ctx.r10.u32) {
		loc_8246BA14:
			// dcbf r11,r9
			// addi r11,r11,128
			ctx.r11.s64 = ctx.r11.s64 + 128;
			// cmplw cr6,r11,r10
			// blt cr6,0x8246ba14
	}
	loc_8246BA24:
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// addi r7,r7,96
		ctx.r7.s64 = ctx.r7.s64 + 96;
		// cmplwi cr6,r6,0
		// bne cr6,0x8246b9fc
		if (ctx.r6.u32 != 0) goto loc_8246B9FC;
	}
loc_8246BA34:
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// rlwinm r7,r9,0,14,14
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r7,0
	// beq cr6,0x8246baac
	if (ctx.r7.u32 != 0) {
		// li r9,0
		ctx.r9.s64 = 0;
		// cmplwi cr6,r5,0
		// beq cr6,0x8246baac
		if (ctx.r5.u32 == 0) goto loc_8246BAAC;
		// li r10,0
		ctx.r10.s64 = 0;
	loc_8246BA54:
		// lwz r11,8(r8)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
		// addi r10,r10,96
		ctx.r10.s64 = ctx.r10.s64 + 96;
		// addi r6,r11,16
		ctx.r6.s64 = ctx.r11.s64 + 16;
		// addi r5,r11,32
		ctx.r5.s64 = ctx.r11.s64 + 32;
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,64(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 64);
		// lvx128 v13,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stw r11,-16(r1)
		PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r11.u32);
		// stvx v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r4,-16(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		// addi r3,r4,16
		ctx.r3.s64 = ctx.r4.s64 + 16;
		// stvx v13,r0,r3
		ea = (ctx.r3.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r11,-16(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
		// addi r8,r11,32
		ctx.r8.s64 = ctx.r11.s64 + 32;
		// stvx v12,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lwz r8,20(r1)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 20);
		// lwz r7,0(r8)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// cmplw cr6,r9,r7
		// blt cr6,0x8246ba54
		if (ctx.r9.u32 < ctx.r7.u32) goto loc_8246BA54;
	}
loc_8246BAAC:
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r6,0
	// ble cr6,0x8246bb08
	if (ctx.r6.u32 > 0) {
		// lis r4,8186
		ctx.r4.s64 = 536477696;
		// li r10,0
		ctx.r10.s64 = 0;
		// li r5,1
		ctx.r5.s64 = 1;
		// ori r6,r4,34384
		ctx.r6.u64 = ctx.r4.u64 | 34384;
	loc_8246BACC:
		// lwz r11,8(r8)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
		// add r3,r10,r11
		ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
		// lhz r11,76(r3)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 76);
		// rlwinm r7,r11,27,5,31
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x7FFFFFF;
		// clrlwi r11,r11,27
		ctx.r11.u64 = ctx.r11.u32 & 0x1F;
		// add r7,r7,r6
		ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
		// rlwinm r4,r7,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// slw r11,r5,r11
		ctx.r11.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r11.u8 & 0x3F));
		// stwbrx r11,0,r4
		PPC_MM_STORE_U32(ctx.r4.u32, __builtin_bswap32(ctx.r11.u32));
		// eieio
		// lwz r3,0(r8)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// addi r10,r10,96
		ctx.r10.s64 = ctx.r10.s64 + 96;
		// cmplw cr6,r9,r3
		// blt cr6,0x8246bacc
		if (ctx.r9.u32 < ctx.r3.u32) goto loc_8246BACC;
	}
loc_8246BB08:
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r10,r11,0,16,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFCFFFF;
	// stw r10,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_BB20_2hr"))) PPC_WEAK_FUNC(phInst_BB20_2hr);
PPC_FUNC_IMPL(__imp__phInst_BB20_2hr) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x8246bb30
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8246BB30:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInst_BB40_2hr"))) PPC_WEAK_FUNC(phInst_BB40_2hr);
PPC_FUNC_IMPL(__imp__phInst_BB40_2hr) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x8246bb50
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8246BB50:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__RtlEnterCriticalSection_BB60_2hr"))) PPC_WEAK_FUNC(RtlEnterCriticalSection_BB60_2hr);
PPC_FUNC_IMPL(__imp__RtlEnterCriticalSection_BB60_2hr) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r3,r3,-8
	ctx.r3.s64 = ctx.r3.s64 + -8;
	// bne cr6,0x8246bb70
	if (ctx.r3.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
	}
loc_8246BB70:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__ph_BB80"))) PPC_WEAK_FUNC(ph_BB80);
PPC_FUNC_IMPL(__imp__ph_BB80) {
	PPC_FUNC_PROLOGUE();
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r11,127
	// bgt cr6,0x8246bbbc
	if (ctx.r11.u32 <= 127) {
		// lwz r10,8(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// cmplw cr6,r11,r10
		// bge cr6,0x8246bbf0
		if (!(ctx.r11.u32 >= ctx.r10.u32)) {
			// lwz r10,12(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
			// rlwinm r11,r11,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
			// add r11,r11,r10
			ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lwz r9,4(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		} else {
			if (!(ctx.r9.u32 == 0)) {
				// lwz r8,0(r11)
				ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
				// cmplwi cr6,r8,0
				ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
				// b 0x8246bbec
				} else {
				loc_8246BBBC:
				// lwz r7,16(r3)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
				// addi r11,r11,-128
				ctx.r11.s64 = ctx.r11.s64 + -128;
			} else {
				if (!(ctx.r11.u32 >= ctx.r7.u32)) {
					// lwz r10,20(r3)
					ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
					// rlwinm r11,r11,3,0,28
					ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
					// add r11,r11,r10
					ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
					// lwz r6,4(r11)
					ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
				} else {
					if (!(ctx.r6.u32 == 0)) {
						// lwz r5,0(r11)
						ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
						// cmplwi cr6,r5,0
						ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
						}
						loc_8246BBEC:
						// bne cr6,0x8246bbf4
						if (ctx.cr6.eq) {
					}
				}
			}
		}
	loc_8246BBF0:
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8246BBF4:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// blr
	return;
}

__attribute__((alias("__imp__msgMsgSink_vfn_118"))) PPC_WEAK_FUNC(msgMsgSink_vfn_118);
PPC_FUNC_IMPL(__imp__msgMsgSink_vfn_118) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* msgMsgSink::flags@+0x4 */;
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4,/* msgMsgSink::flags@+0x4 */ ctx.r11.u32);
	// bne 0x8246bc40
	if (ctx.r11.s32 == 0) {
		// lwz r9,12(r10)
		// bctrl
		msgMsgSink_vfn_3(ctx, base);  // vtable slot 3 (byte +12)  // msgMsgSink::vfn_3
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8246BC40:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5A40_5_BC58"))) PPC_WEAK_FUNC(ph_vt5A40_5_BC58);
PPC_FUNC_IMPL(__imp__ph_vt5A40_5_BC58) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	// addi r11,r3,8
	ctx.r11.s64 = ctx.r3.s64 + 8;
	// bne cr6,0x8246bc68
	if (ctx.r3.u32 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	}
loc_8246BC68:
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5C84_15_BC70"))) PPC_WEAK_FUNC(ph_vt5C84_15_BC70);
PPC_FUNC_IMPL(__imp__ph_vt5C84_15_BC70) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// lbz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 0);
	// bl 0x8246bb80
	ph_BB80(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246bca8
	if (ctx.r3.u32 == 0) {
		// lis r3,-32768
		// ori r3,r3,16385
		ctx.r3.u64 = ctx.r3.u64 | 16385;
		// b 0x8246bcbc
	} else {
	loc_8246BCA8:
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	}
loc_8246BCBC:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5C84_16_BCD8"))) PPC_WEAK_FUNC(ph_vt5C84_16_BCD8);
PPC_FUNC_IMPL(__imp__ph_vt5C84_16_BCD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// lbz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 0);
	// bl 0x8246bb80
	ph_BB80(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246bd10
	if (ctx.r3.u32 == 0) {
		// lis r3,-32768
		// ori r3,r3,16385
		ctx.r3.u64 = ctx.r3.u64 | 16385;
		return;
	}
loc_8246BD10:
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r5,r29
	ctx.r5.u64 = var_r29;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__ph_vt5C84_17_BD30"))) PPC_WEAK_FUNC(ph_vt5C84_17_BD30);
PPC_FUNC_IMPL(__imp__ph_vt5C84_17_BD30) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=144, savegprlr_25
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// li r25,0
	var_r25 = 0;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r26,r25
	var_r26 = (uint32_t)(var_r25);
	// mr r11,r25
	ctx.r11.u64 = var_r25;
	// lbz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U8(var_r27 + 0);
	// cmplwi cr6,r10,0
	// beq cr6,0x8246be04
	if (ctx.r10.u32 != 0) {
		// mr r8,r25
		ctx.r8.u64 = var_r25;
	loc_8246BD60:
		// cmplwi cr6,r11,0
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// bne cr6,0x8246bd70
		if (ctx.r11.u32 == 0) {
			// lwz r11,20(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
		}
	loc_8246BD70:
		// lwz r9,16(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 16);
		// lwz r7,20(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 20);
		// rlwinm r10,r9,3,0,28
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
		// add r6,r10,r7
		ctx.r6.u64 = ctx.r10.u64 + ctx.r7.u64;
		// cmplw cr6,r11,r6
		// bge cr6,0x8246bdbc
	while (ctx.r11.u32 < ctx.r10.u32) {
		loc_8246BD88:
			// lwz r5,4(r11)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// cmplwi cr6,r5,0
			// bne cr6,0x8246bda0
			if (ctx.r5.u32 != 0) goto loc_8246BDA0;
			// lwz r4,0(r11)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// cmplwi cr6,r4,0
			// beq cr6,0x8246bdc0
			if (ctx.r4.u32 == 0) goto loc_8246BDC0;
		loc_8246BDA0:
			// lwz r3,16(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 16);
			// addi r11,r11,8
			ctx.r11.s64 = ctx.r11.s64 + 8;
			// lwz r9,20(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 20);
			// rlwinm r10,r3,3,0,28
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
			// add r10,r10,r9
			ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
			// cmplw cr6,r11,r10
			// blt cr6,0x8246bd88
	}
	loc_8246BDBC:
		// mr r11,r25
		ctx.r11.u64 = var_r25;
	loc_8246BDC0:
		// cmplwi cr6,r11,0
		// beq cr6,0x8246be04
		if (ctx.r11.u32 == 0) goto loc_8246BE04;
		// lwz r10,4(r27)
		ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 4);
		// subf r9,r7,r11
		ctx.r9.s64 = ctx.r11.s64 - ctx.r7.s64;
		// addi r26,r26,1
		var_r26 = (uint32_t)(var_r26 + 1);
		// add r10,r10,r8
		ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
		// srawi r9,r9,3
		ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
		ctx.r9.s64 = ctx.r9.s32 >> 3;
		// addi r8,r8,12
		ctx.r8.s64 = ctx.r8.s64 + 12;
		// addi r7,r9,128
		ctx.r7.s64 = ctx.r9.s64 + 128;
		// lwz r5,4(r10)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// stb r7,0(r10)
		PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
		// stw r5,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
		// lwz r4,8(r10)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// stw r4,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r4.u32);
		// lbz r3,0(r27)
		ctx.r3.u64 = PPC_LOAD_U8(var_r27 + 0);
		// cmplw cr6,r26,r3
		// blt cr6,0x8246bd60
		if (var_r26 < ctx.r3.u32) goto loc_8246BD60;
	}
loc_8246BE04:
	// lbz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 0);
	// subf r30,r26,r11
	var_r30 = (uint32_t)(ctx.r11.s64 - (int64_t)(int32_t)var_r26);
	// cmplwi cr6,r30,0
	// bne cr6,0x8246be20
	if (var_r30 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_8246BE20:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 16);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + var_r30;
	// cmplwi cr6,r11,128
	// ble cr6,0x8246be40
	if (ctx.r11.u32 > 128) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
loc_8246BE40:
	// lis r10,-32162
	// lis r5,24962
	ctx.r5.s64 = 1635909632;
	// addi r28,r10,30856
	var_r28 = (uint32_t)(ctx.r10.s64 + 30856);  // lbl_825E7888 @ 0x825e7888
	// ori r5,r5,4
	ctx.r5.u64 = ctx.r5.u64 | 4;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// bl 0x824667c0
	util_67C0(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmplwi cr6,r29,0
	// bne cr6,0x8246be7c
	if (var_r29 == 0) {
		// lis r25,-32761
		var_r25 = (uint32_t)(-2147024896);
		// ori r25,r25,14
		var_r25 = (uint32_t)(var_r25 | 14);
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		return;
	}
loc_8246BE7C:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 16);
	// cmplwi cr6,r11,0
	// beq cr6,0x8246beb8
	if (ctx.r11.u32 != 0) {
		// rlwinm r5,r11,3,0,28
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// lwz r4,20(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 20);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x82434100
		memcpy(ctx, base);
		// lwz r4,20(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 20);
		// cmplwi cr6,r4,0
		// beq cr6,0x8246beb8
		if (ctx.r4.u32 == 0) goto loc_8246BEB8;
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// ori r5,r5,4
		ctx.r5.u64 = ctx.r5.u64 | 4;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
		// stw r25,20(r31)
		PPC_STORE_U32(var_r31 + 20, var_r25);
	}
loc_8246BEB8:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 16);
	// cmplwi cr6,r30,0
	// stw r29,20(r31)
	PPC_STORE_U32(var_r31 + 20, var_r29);
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + var_r30;
	// mr r11,r25
	ctx.r11.u64 = var_r25;
	// stw r10,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r10.u32);
	// beq cr6,0x8246bf30
	if (var_r30 != 0) {
		// rlwinm r10,r26,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(var_r26 | (var_r26 << 32), 1) & 0xFFFFFFFE;
		// add r9,r26,r10
		ctx.r9.u64 = var_r26 + ctx.r10.u64;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_8246BEE0:
		// lwz r7,16(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 16);
		// lwz r6,20(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 20);
		// subf r9,r30,r7
		ctx.r9.s64 = ctx.r7.s64 - (int64_t)(int32_t)var_r30;
		// lwz r10,4(r27)
		ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 4);
		// add r5,r9,r11
		ctx.r5.u64 = ctx.r9.u64 + ctx.r11.u64;
		// add r10,r10,r8
		ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
		// rlwinm r9,r5,3,0,28
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// add r9,r9,r29
		ctx.r9.u64 = ctx.r9.u64 + var_r29;
		// addi r8,r8,12
		ctx.r8.s64 = ctx.r8.s64 + 12;
		// subf r3,r6,r9
		ctx.r3.s64 = ctx.r9.s64 - ctx.r6.s64;
		// lwz r4,4(r10)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// cmplw cr6,r11,r30
		// srawi r7,r3,3
		ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
		ctx.r7.s64 = ctx.r3.s32 >> 3;
		// addi r7,r7,128
		ctx.r7.s64 = ctx.r7.s64 + 128;
		// stb r7,0(r10)
		PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r7.u8);
		// stw r4,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r4.u32);
		// lwz r5,8(r10)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// stw r5,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r5.u32);
		// blt cr6,0x8246bee0
		if (ctx.r11.u32 < var_r30) goto loc_8246BEE0;
	}
loc_8246BF30:
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	return;
}

__attribute__((alias("__imp__ph_vt5C84_18_BF40"))) PPC_WEAK_FUNC(ph_vt5C84_18_BF40);
PPC_FUNC_IMPL(__imp__ph_vt5C84_18_BF40) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// cmplwi cr6,r11,0
	// beq cr6,0x8246bfb0
while (ctx.r9.u32 < ctx.r11.u32) {
	loc_8246BF54:
		// lwz r10,4(r4)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r7,16(r3)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
		// lbzx r11,r9,r10
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r10.u32);
		// addi r11,r11,-128
		ctx.r11.s64 = ctx.r11.s64 + -128;
		// cmplw cr6,r11,r7
		// bge cr6,0x8246bfa0
		if (!(ctx.r11.u32 >= ctx.r7.u32)) {
			// lwz r10,20(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
			// rlwinm r11,r11,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
			// add r11,r11,r10
			ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lwz r6,4(r11)
			ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		} else {
			if (!(ctx.r6.u32 == 0)) {
				// lwz r5,0(r11)
				ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			} else {
				if (!(ctx.r11.u32 == 0)) {
					// stw r8,0(r11)
					PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
					// stw r8,4(r11)
					PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
				}
			}
		}
	loc_8246BFA0:
		// lbz r11,0(r4)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r11
		// blt cr6,0x8246bf54
}
loc_8246BFB0:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_BFB8_2hr"))) PPC_WEAK_FUNC(phInst_BFB8_2hr);
PPC_FUNC_IMPL(__imp__phInst_BFB8_2hr) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	// beqlr cr6
	if (ctx.r10.u32 == 0) return;
	// lbz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r11,0
	// beq cr6,0x8246c010
	if (ctx.r11.u32 != 0) {
		// lwz r10,4(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
	loc_8246BFEC:
		// lbz r11,0(r10)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplw cr6,r11,r9
		// ble cr6,0x8246c000
		if (ctx.r11.u32 > ctx.r9.u32) {
			// mr r9,r11
			ctx.r9.u64 = ctx.r11.u64;
		}
	loc_8246C000:
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// addi r10,r10,12
		ctx.r10.s64 = ctx.r10.s64 + 12;
		// cmplwi cr6,r8,0
		// bne cr6,0x8246bfec
		if (ctx.r8.u32 != 0) goto loc_8246BFEC;
	}
loc_8246C010:
	// rlwinm r3,r9,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_C018_wrh"))) PPC_WEAK_FUNC(phInst_C018_wrh);
PPC_FUNC_IMPL(__imp__phInst_C018_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,23724
	ctx.r11.s64 = ctx.r11.s64 + 23724;
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	// beq cr6,0x8246c084
	if (ctx.r10.u32 != 0) {
		// lwz r8,4(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	loc_8246C060:
		// lbz r11,0(r8)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplw cr6,r11,r9
		// ble cr6,0x8246c074
		if (ctx.r11.u32 > ctx.r9.u32) {
			// mr r9,r11
			ctx.r9.u64 = ctx.r11.u64;
		}
	loc_8246C074:
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// addi r8,r8,12
		ctx.r8.s64 = ctx.r8.s64 + 12;
		// cmplwi cr6,r10,0
		// bne cr6,0x8246c060
		if (ctx.r10.u32 != 0) goto loc_8246C060;
	}
loc_8246C084:
	// cmplwi cr6,r9,0
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// beq cr6,0x8246c0ac
	if (ctx.r9.u32 != 0) {
		// lwz r8,0(r5)
  // [ph4a] vtable load collapsed
		// rlwinm r4,r9,3,0,28
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
		// mr r3,r5
		ctx.r3.u64 = ctx.r5.u64;
		// lwz r7,20(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r5.u32, 5, ctx, base);  // pattern-B slot 5 (byte +20)
		// stw r3,12(r31)
		PPC_STORE_U32(var_r31 + 12, ctx.r3.u32);
	}
loc_8246C0AC:
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 0);
	// li r9,0
	ctx.r9.s64 = 0;
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplwi cr6,r6,0
	// beq cr6,0x8246c114
	if (ctx.r6.u32 != 0) {
		// li r11,0
		ctx.r11.s64 = 0;
	loc_8246C0C4:
		// lwz r10,4(r10)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// lwz r5,12(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 12);
		// add r10,r10,r11
		ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
		// lbz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// lwz r4,4(r10)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// rotlwi r10,r8,3
		ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
		// stwx r4,r5,r10
		PPC_STORE_U32(ctx.r5.u32 + ctx.r10.u32, ctx.r4.u32);
		// lwz r3,0(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lwz r8,12(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 12);
		// add r8,r8,r10
		ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
		// lwz r10,4(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// add r7,r10,r11
		ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
		// addi r11,r11,12
		ctx.r11.s64 = ctx.r11.s64 + 12;
		// lwz r6,8(r7)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
		// stw r6,4(r8)
		PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
		// lwz r10,0(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lbz r5,0(r10)
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
		// cmplw cr6,r9,r5
		// blt cr6,0x8246c0c4
		if (ctx.r9.u32 < ctx.r5.u32) goto loc_8246C0C4;
	}
loc_8246C114:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5C84_10_C130"))) PPC_WEAK_FUNC(ph_vt5C84_10_C130);
PPC_FUNC_IMPL(__imp__ph_vt5C84_10_C130) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32256
	// addi r11,r11,23724
	ctx.r11.s64 = ctx.r11.s64 + 23724;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 20);
	// cmplwi cr6,r4,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x8246c178
	if (ctx.r4.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// ori r5,r5,4
		ctx.r5.u64 = ctx.r5.u64 | 4;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,20(r31)
		PPC_STORE_U32(var_r31 + 20, ctx.r11.u32);
	}
loc_8246C178:
	// lis r11,-32256
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,15792
	ctx.r11.s64 = ctx.r11.s64 + 15792;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_C1A0_2hr"))) PPC_WEAK_FUNC(phInst_C1A0_2hr);
PPC_FUNC_IMPL(__imp__phInst_C1A0_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// bl 0x8246bfb8
	phInst_BFB8_2hr(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_C1E0_2hr"))) PPC_WEAK_FUNC(phInst_C1E0_2hr);
PPC_FUNC_IMPL(__imp__phInst_C1E0_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,20(r11)
	// bctrl
	VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	// cmplwi cr6,r3,0
	// beq cr6,0x8246c244
	if (ctx.r3.u32 != 0) {
		// mr r5,r31
		ctx.r5.u64 = var_r31;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x8246c018
		phInst_C018_wrh(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// cmplwi cr6,r11,0
		// beq cr6,0x8246c244
		if (ctx.r11.u32 == 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// addi r9,r11,8
		ctx.r9.s64 = ctx.r11.s64 + 8;
		// li r3,0
		ctx.r3.s64 = 0;
		// stw r9,0(r29)
		PPC_STORE_U32(var_r29 + 0, ctx.r9.u32);
		return;
	}
loc_8246C244:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
}

__attribute__((alias("__imp__phInst_C258_w"))) PPC_WEAK_FUNC(phInst_C258_w);
PPC_FUNC_IMPL(__imp__phInst_C258_w) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,3
	// bne cr6,0x8246c2a0
	if (ctx.r11.u32 == 3) {
		// lbz r8,4(r3)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// li r10,0
		ctx.r10.s64 = 0;
		// cmplwi cr6,r8,0
		// beq cr6,0x8246c298
		if (ctx.r8.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		loc_8246C278:
			// rlwinm r9,r11,3,0,28
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
			// addi r7,r11,1
			ctx.r7.s64 = ctx.r11.s64 + 1;
			// add r6,r9,r3
			ctx.r6.u64 = ctx.r9.u64 + ctx.r3.u64;
			// clrlwi r11,r7,24
			ctx.r11.u64 = ctx.r7.u32 & 0xFF;
			// cmplw cr6,r11,r8
			// lbz r9,12(r6)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + 12);
			// add r10,r9,r10
			ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
			// blt cr6,0x8246c278
			if (ctx.r11.u32 < ctx.r8.u32) goto loc_8246C278;
		}
	loc_8246C298:
		// stb r10,1(r4)
		PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r10.u8);
		// b 0x8246c2a8
	} else {
	loc_8246C2A0:
		// lbz r3,4(r3)
		ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
		// stb r3,1(r4)
		PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r3.u8);
	}
loc_8246C2A8:
	// lis r11,0
	ctx.r11.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// ori r10,r11,48000
	ctx.r10.u64 = ctx.r11.u64 | 48000;
	// stb r9,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r9.u8);
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_C2C0_h"))) PPC_WEAK_FUNC(phInst_C2C0_h);
PPC_FUNC_IMPL(__imp__phInst_C2C0_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// addi r10,r31,84
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 84;
	// mr r11,r30
	ctx.r11.u64 = var_r30;
	// li r9,14
	ctx.r9.s64 = 14;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8246C2E8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8246c2e8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8246C2E8;
	// lwz r3,76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 76);
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lwz r7,96(r8)
	// bctrl
	VCALL(ctx.r3.u32, 24, ctx, base);  // vtable slot 24 (byte +96)
	// cmpwi cr6,r3,0
	// blt cr6,0x8246c33c
	if (ctx.r3.s32 >= 0) {
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8246c258
		phInst_C258_w(ctx, base);
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8247eef0
		ph_vt5CD8_21_EEF0(ctx, base);
	}
loc_8246C33C:
	return;
}

__attribute__((alias("__imp__phInst_C348_h"))) PPC_WEAK_FUNC(phInst_C348_h);
PPC_FUNC_IMPL(__imp__phInst_C348_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,60(r11)
	// bctrl
	phInst_vfn_15(ctx, base);  // vtable slot 15 (byte +60)  // phInst::vfn_15
	// cmpwi cr6,r3,0
	// blt cr6,0x8246c39c
	if (ctx.r3.s32 >= 0) {
		// lwz r11,76(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 76);
		// cmplwi cr6,r11,0
		// beq cr6,0x8246c39c
		if (ctx.r11.u32 == 0) {
			// blr
			return;
		}
		// lwz r9,0(r11)
  // [ph4a] vtable load collapsed
		// mr r3,r11
		ctx.r3.u64 = ctx.r11.u64;
		// lwz r8,32(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r11.u32, 8, ctx, base);  // pattern-B slot 8 (byte +32)
	}
loc_8246C39C:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_12_C3B0"))) PPC_WEAK_FUNC(ph_vt5CD8_12_C3B0);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_12_C3B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_27
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246c3f0
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8246c404
		if (var_r30 == ctx.r10.u32) goto loc_8246C404;
	}
loc_8246C3F0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246C404:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lwz r3,76(r28)
	ctx.r3.u64 = PPC_LOAD_U32(var_r28 + 76);
	// lwz r10,36(r11)
	// bctrl
	VCALL(ctx.r3.u32, 9, ctx, base);  // vtable slot 9 (byte +36)
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmpwi cr6,r29,0
	// blt cr6,0x8246c440
	if ((int32_t)var_r29 >= 0) {
		// lbz r9,61(r28)
		ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 61);
		// lbz r7,80(r1)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
		// or r6,r9,r7
		ctx.r6.u64 = ctx.r9.u64 | ctx.r7.u64;
		// stb r6,0(r27)
		PPC_STORE_U8(var_r27 + 0, ctx.r6.u8);
	}
loc_8246C440:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	// beq cr6,0x8246c488
	if (ctx.r9.s32 != 0) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r11
		// bne cr6,0x8246c488
		if (ctx.r10.u32 != ctx.r11.u32) {
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			return;
		}
		// addi r11,r9,-1
		ctx.r11.s64 = ctx.r9.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246c488
		if (ctx.r11.s32 != 0) {
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246C488:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_20_C498"))) PPC_WEAK_FUNC(ph_vt5CD8_20_C498);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_20_C498) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r9,1
	ctx.r9.s64 = 1;
	// lis r11,-32162
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lbz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 144);
	// lwz r11,30860(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// lwz r8,80(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	// slw r7,r9,r10
	ctx.r7.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// cmpwi cr6,r6,0
	// beq cr6,0x8246c4e8
	if (ctx.r6.s32 != 0) {
		// lwz r4,88(r5)
		// bctrl
		VCALL(ctx.r3.u32, 22, ctx, base);  // vtable slot 22 (byte +88)
		// cmpwi cr6,r3,0
		// blt cr6,0x8246c568
		if (ctx.r3.s32 < 0) {
			return;
		}
	}
loc_8246C4E8:
	// addi r29,r31,76
	var_r29 = (uint32_t)(var_r31 + 76);
	// mr r5,r30
	ctx.r5.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// bl 0x8247fe38
	ph_FE38(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246c568
	if (ctx.r3.s32 >= 0) {
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8247f420
		ph_vt5D38_20_F420(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8246c568
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r3,0(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lwz r10,36(r11)
		// bctrl
		VCALL(ctx.r3.u32, 9, ctx, base);  // vtable slot 9 (byte +36)
		// cmpwi cr6,r3,0
		// blt cr6,0x8246c568
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lbz r9,80(r1)
		ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
		// clrlwi r8,r9,31
		ctx.r8.u64 = ctx.r9.u32 & 0x1;
		// cmplwi cr6,r8,0
		// bne cr6,0x8246c568
		if (ctx.r8.u32 != 0) {
			return;
		}
		// lbz r7,61(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 61);
		// rlwinm r6,r7,0,29,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x4;
		// cmplwi cr6,r6,0
		// bne cr6,0x8246c568
		if (ctx.r6.u32 != 0) {
			return;
		}
		// li r5,1
		ctx.r5.s64 = 1;
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82480160
		ph_vt5D38_15_0160(ctx, base);
	}
loc_8246C568:
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_22_C570"))) PPC_WEAK_FUNC(ph_vt5CD8_22_C570);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_22_C570) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// cmplwi cr6,r11,2
	// bge cr6,0x8246c594
	if (ctx.r11.u32 < 2) {
		// addi r11,r11,18
		ctx.r11.s64 = ctx.r11.s64 + 18;
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// lis r11,-32162
		ctx.r11.s64 = -2107768832;
		// lwz r11,30860(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
		// lfsx f0,r10,r11
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
		ctx.f0.f64 = double(temp.f32);
		// b 0x8246c59c
	} else {
	loc_8246C594:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15788(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);  /* glob:lbl_82003DAC @ 0x82003dac */
		ctx.f0.f64 = double(temp.f32);
	}
loc_8246C59C:
	// lwz r11,76(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// lfs f13,140(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,80(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 80);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInst_C5C0_2hr"))) PPC_WEAK_FUNC(phInst_C5C0_2hr);
PPC_FUNC_IMPL(__imp__phInst_C5C0_2hr) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_8246C5E0:
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x8246c5e0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8246C5E0;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x8246c258
	phInst_C258_w(ctx, base);
	// lwz r11,68(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r11.u32);
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// lwz r11,30860(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// stw r10,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r10.u32);
	// lwz r9,76(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 76);
	// stw r9,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r9.u32);
	// lwz r8,88(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 88);
	// stw r8,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r8.u32);
	// lbz r11,57(r5)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 57);
	// cmplwi cr6,r11,0
	// bne cr6,0x8246c62c
	if (ctx.r11.u32 == 0) {
		// li r11,1
		ctx.r11.s64 = 1;
	}
loc_8246C62C:
	// stb r11,24(r4)
	PPC_STORE_U8(ctx.r4.u32 + 24, ctx.r11.u8);
	// lbz r11,58(r5)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + 58);
	// cmplwi cr6,r11,0
	// bne cr6,0x8246c640
	if (ctx.r11.u32 == 0) {
		// li r11,6
		ctx.r11.s64 = 6;
	}
loc_8246C640:
	// stb r11,25(r4)
	PPC_STORE_U8(ctx.r4.u32 + 25, ctx.r11.u8);
	// lwz r5,72(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + 72);
	// stw r5,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r5.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_C660"))) PPC_WEAK_FUNC(phInst_C660);
PPC_FUNC_IMPL(__imp__phInst_C660) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=240, savegprlr_29
	// lis r11,-32162
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,30860(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// lwz r30,20(r11)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 20));
	// bl 0x8246c5c0
	phInst_C5C0_2hr(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8247ef40
	phInst_EF40(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246c744
	if (ctx.r3.s32 >= 0) {
		// addi r11,r1,128
		ctx.r11.s64 = ctx.r1.s64 + 128;
		// li r10,0
		ctx.r10.s64 = 0;
		// li r9,10
		ctx.r9.s64 = 10;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_8246C6AC:
		// std r10,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// bdnz 0x8246c6ac
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_8246C6AC;
		// lwz r11,88(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 88);
		// li r9,14
		ctx.r9.s64 = 14;
		// stb r10,128(r1)
		PPC_STORE_U8(ctx.r1.u32 + 128, ctx.r10.u8);
		// addi r10,r1,136
		ctx.r10.s64 = ctx.r1.s64 + 136;
		// stw r11,132(r1)
		PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
		// mr r11,r31
		ctx.r11.u64 = var_r31;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_8246C6D4:
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bdnz 0x8246c6d4
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_8246C6D4;
		// lbz r8,59(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 59);
		// addi r5,r1,84
		ctx.r5.s64 = ctx.r1.s64 + 84;
		// lbz r7,64(r31)
		ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 64);
		// addi r4,r1,128
		ctx.r4.s64 = ctx.r1.s64 + 128;
		// lwz r6,80(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 80);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r11,84(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 84);
		// lfs f0,60(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 60);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,192(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
		// stb r8,196(r1)
		PPC_STORE_U8(ctx.r1.u32 + 196, ctx.r8.u8);
		// stb r7,197(r1)
		PPC_STORE_U8(ctx.r1.u32 + 197, ctx.r7.u8);
		// stw r6,200(r1)
		PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r6.u32);
		// stw r11,204(r1)
		PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r11.u32);
		// bl 0x8246bb20
		phInst_BB20_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// li r3,0
		ctx.r3.s64 = 0;
		// blt cr6,0x8246c748
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lwz r11,84(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// add r10,r11,r10
		ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
		// stw r10,0(r29)
		PPC_STORE_U32(var_r29 + 0, ctx.r10.u32);
		return;
	}
loc_8246C744:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8246C748:
	return;
}

__attribute__((alias("__imp__ke_C750"))) PPC_WEAK_FUNC(ke_C750);
PPC_FUNC_IMPL(__imp__ke_C750) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,-32256
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r11,r11,23768
	ctx.r11.s64 = ctx.r11.s64 + 23768;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r11.u32);
	// lwz r11,60(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 60);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246c7a4
	if (ctx.r3.s32 >= 0) {
		// lwz r3,76(r27)
		ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 76);
		// cmplwi cr6,r3,0
		// beq cr6,0x8246c7a4
		if (ctx.r3.u32 == 0) goto loc_8246C7A4;
		// lwz r9,32(r10)
		// bctrl
		VCALL(ctx.r3.u32, 8, ctx, base);  // vtable slot 8 (byte +32)
	}
loc_8246C7A4:
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// lwz r28,30860(r11)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 30860));  /* glob:lbl_825E788C @ 0x825e788c */
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246c7d8
	if (ctx.r11.s32 != 0) {
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r8
		// beq cr6,0x8246c7f0
		if (var_r30 == ctx.r8.u32) goto loc_8246C7F0;
	}
loc_8246C7D8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = var_r30;
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246C7F0:
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// lwz r11,48(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 48);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + var_r27;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r9,r11
	// beq cr6,0x8246c830
	if (ctx.r9.u32 != ctx.r11.u32) {
		// lwz r8,4(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r8,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
		// lwz r7,4(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r6,0(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// stw r6,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
		// stw r11,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
		// stw r11,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_8246C830:
	// mr r11,r13
	ctx.r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	// beq cr6,0x8246c870
	if (ctx.r10.s32 != 0) {
		// cmplw cr6,r11,r8
		// bne cr6,0x8246c870
		if (ctx.r11.u32 != ctx.r8.u32) goto loc_8246C870;
		// addi r11,r10,-1
		ctx.r11.s64 = ctx.r10.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246c870
		if (ctx.r11.s32 != 0) goto loc_8246C870;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246C870:
	// lwz r3,76(r27)
	ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 76);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246c894
	if (ctx.r3.u32 != 0) {
		// lwz r4,4(r5)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// li r3,0
		ctx.r3.s64 = 0;
		// stw r3,76(r27)
		PPC_STORE_U32(var_r27 + 76, ctx.r3.u32);
	}
loc_8246C894:
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x8247f2b8
	ke_F2B8(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_11_C8A8"))) PPC_WEAK_FUNC(ph_vt5CD8_11_C8A8);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_11_C8A8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,48(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_13_C8B8"))) PPC_WEAK_FUNC(ph_vt5CD8_13_C8B8);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_13_C8B8) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 12);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_16_C8C8"))) PPC_WEAK_FUNC(ph_vt5CD8_16_C8C8);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_16_C8C8) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r3,52
	ctx.r11.s64 = ctx.r3.s64 + 52;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r9,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_14_C8E8"))) PPC_WEAK_FUNC(ph_vt5CD8_14_C8E8);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_14_C8E8) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32768
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_C8F8"))) PPC_WEAK_FUNC(phInst_C8F8);
PPC_FUNC_IMPL(__imp__phInst_C8F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=224, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r10,r30,84
	ctx.r10.s64 = (int64_t)(int32_t)var_r30 + 84;
	// mr r11,r31
	ctx.r11.u64 = var_r31;
	// li r9,14
	ctx.r9.s64 = 14;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8246C924:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8246c924
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8246C924;
	// lbz r8,56(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 56);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stb r8,144(r30)
	PPC_STORE_U8(var_r30 + 144, ctx.r8.u8);
	// bl 0x8246c5c0
	phInst_C5C0_2hr(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8247eff8
	phInst_EFF8_2hr(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246c9e8
	if (ctx.r3.s32 >= 0) {
		// addi r11,r1,112
		ctx.r11.s64 = ctx.r1.s64 + 112;
		// li r10,0
		ctx.r10.s64 = 0;
		// li r9,10
		ctx.r9.s64 = 10;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_8246C970:
		// std r10,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// bdnz 0x8246c970
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_8246C970;
		// lwz r7,88(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 88);
		// mr r11,r31
		ctx.r11.u64 = var_r31;
		// stb r10,112(r1)
		PPC_STORE_U8(ctx.r1.u32 + 112, ctx.r10.u8);
		// li r9,14
		ctx.r9.s64 = 14;
		// addi r10,r1,120
		ctx.r10.s64 = ctx.r1.s64 + 120;
		// stw r7,116(r1)
		PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_8246C998:
		// lwz r6,0(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// stw r6,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// bdnz 0x8246c998
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_8246C998;
		// lbz r4,59(r31)
		ctx.r4.u64 = PPC_LOAD_U8(var_r31 + 59);
		// li r6,255
		ctx.r6.s64 = 255;
		// lbz r11,64(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 64);
		// addi r5,r1,112
		ctx.r5.s64 = ctx.r1.s64 + 112;
		// lwz r10,80(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 80);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r9,84(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 84);
		// lfs f0,60(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 60);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,176(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
		// stb r4,180(r1)
		PPC_STORE_U8(ctx.r1.u32 + 180, ctx.r4.u8);
		// addi r4,r30,76
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 76;
		// stb r11,181(r1)
		PPC_STORE_U8(ctx.r1.u32 + 181, ctx.r11.u8);
		// stw r10,184(r1)
		PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r10.u32);
		// stw r9,188(r1)
		PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r9.u32);
		// bl 0x8247f9b0
		phInst_F9B0_2h(ctx, base);
	}
loc_8246C9E8:
	// blr
	return;
}

__attribute__((alias("__imp__ke_CA00"))) PPC_WEAK_FUNC(ke_CA00);
PPC_FUNC_IMPL(__imp__ke_CA00) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246ca40
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8246ca60
		if (var_r30 == ctx.r10.u32) goto loc_8246CA60;
	}
loc_8246CA40:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// mr r30,r29
	var_r30 = (uint32_t)(var_r29);
	// stw r10,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
	// stb r30,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// b 0x8246ca64
	goto loc_8246CA64;
loc_8246CA60:
	// lbz r30,12(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
loc_8246CA64:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r9,61(r27)
	ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 61);
	// rlwinm r8,r9,0,25,25
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x40;
	// cmplwi cr6,r8,0
	// beq cr6,0x8246cb40
	if (ctx.r8.u32 != 0) {
		// cmpwi cr6,r28,0
		// lis r28,-32162
		var_r28 = (uint32_t)(-2107768832);
		// bne cr6,0x8246cb8c
		if ((int32_t)var_r28 != 0) goto loc_8246CB8C;
	loc_8246CA88:
		// lwz r29,30860(r28)
		var_r29 = (uint32_t)(PPC_LOAD_U32(var_r28 + 30860));
		// lwz r7,84(r29)
		ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 84);
		// cmplw cr6,r27,r7
		// beq cr6,0x8246caa4
		if (var_r27 != ctx.r7.u32) {
			// lwz r6,88(r29)
			ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 88);
			// cmplw cr6,r27,r6
			// bne cr6,0x8246cb90
			if (var_r27 != ctx.r6.u32) goto loc_8246CB90;
		}
	loc_8246CAA4:
		// mr r9,r13
		ctx.r9.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246cae0
		if (ctx.r11.s32 != 0) {
			// cmplw cr6,r9,r10
			// bne cr6,0x8246cae0
			if (ctx.r9.u32 != ctx.r10.u32) goto loc_8246CAE0;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246cae0
			if (ctx.r11.s32 != 0) goto loc_8246CAE0;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_8246CAE0:
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246cb04
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8246cb24
			if (var_r30 == ctx.r10.u32) goto loc_8246CB24;
		}
	loc_8246CB04:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r10,r30
		ctx.r10.u64 = var_r30;
		// mr r30,r29
		var_r30 = (uint32_t)(var_r29);
		// stw r10,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
		// stb r30,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// b 0x8246cb28
		goto loc_8246CB28;
	loc_8246CB24:
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
	loc_8246CB28:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r5,61(r27)
		ctx.r5.u64 = PPC_LOAD_U8(var_r27 + 61);
		// rlwinm r4,r5,0,25,25
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x40;
		// cmplwi cr6,r4,0
		// bne cr6,0x8246ca88
		if (ctx.r4.u32 != 0) goto loc_8246CA88;
	}
loc_8246CB40:
	// mr r9,r13
	ctx.r9.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cb7c
	if (ctx.r11.s32 != 0) {
		// cmplw cr6,r9,r10
		// bne cr6,0x8246cb7c
		if (ctx.r9.u32 != ctx.r10.u32) {
			// lis r3,-32768
			// ori r3,r3,16388
			ctx.r3.u64 = ctx.r3.u64 | 16388;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246cb7c
		if (ctx.r11.s32 != 0) {
			// lis r3,-32768
			// ori r3,r3,16388
			ctx.r3.u64 = ctx.r3.u64 | 16388;
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246CB7C:
	// lis r3,-32768
	// ori r3,r3,16388
	ctx.r3.u64 = ctx.r3.u64 | 16388;
	return;
loc_8246CB8C:
	// lwz r29,30860(r28)
	var_r29 = (uint32_t)(PPC_LOAD_U32(var_r28 + 30860));
loc_8246CB90:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cbb4
	if (ctx.r11.s32 != 0) {
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r8
		// beq cr6,0x8246cbcc
		if (var_r30 == ctx.r8.u32) goto loc_8246CBCC;
	}
loc_8246CBB4:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = var_r30;
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246CBCC:
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// lwz r11,36(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 36);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + var_r27;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r9,r11
	// beq cr6,0x8246cc0c
	if (ctx.r9.u32 != ctx.r11.u32) {
		// lwz r3,4(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r3,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r3.u32);
		// lwz r10,4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// stw r9,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
		// stw r11,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
		// stw r11,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_8246CC0C:
	// mr r11,r13
	ctx.r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	// beq cr6,0x8246cc4c
	if (ctx.r10.s32 != 0) {
		// cmplw cr6,r11,r8
		// bne cr6,0x8246cc4c
		if (ctx.r11.u32 != ctx.r8.u32) goto loc_8246CC4C;
		// addi r11,r10,-1
		ctx.r11.s64 = ctx.r10.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246cc4c
		if (ctx.r11.s32 != 0) goto loc_8246CC4C;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246CC4C:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cc70
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8246cc84
		if (var_r30 == ctx.r10.u32) goto loc_8246CC84;
	}
loc_8246CC70:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246CC84:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r8,61(r27)
	ctx.r8.u64 = PPC_LOAD_U8(var_r27 + 61);
	// andi. r7,r8,191
	ctx.r7.u64 = ctx.r8.u64 & 191;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stb r7,61(r27)
	PPC_STORE_U8(var_r27 + 61, ctx.r7.u8);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cce4
	if (ctx.r11.s32 != 0) {
		// lwz r6,8(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r6
		// bne cr6,0x8246cce4
		if (ctx.r10.u32 != ctx.r6.u32) goto loc_8246CCE4;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246cce4
		if (ctx.r11.s32 != 0) goto loc_8246CCE4;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_8246CCE4:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cd28
	if (ctx.r11.s32 != 0) {
		// lwz r5,8(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r5
		// bne cr6,0x8246cd28
		if (ctx.r10.u32 != ctx.r5.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246cd28
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246CD28:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ke_CD38"))) PPC_WEAK_FUNC(ke_CD38);
PPC_FUNC_IMPL(__imp__ke_CD38) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_27
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x8247eeb0
	ke_EEB0(ctx, base);
	// lis r11,-32256
	// lis r10,-32256
	// addi r11,r11,23768
	ctx.r11.s64 = ctx.r11.s64 + 23768;
	// lfs f0,15788(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15788);
	ctx.f0.f64 = double(temp.f32);
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// lis r11,-32162
	// stfs f0,140(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r30 + 140, temp.u32);
	// lwz r11,30860(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// addi r29,r11,40
	var_r29 = (uint32_t)(ctx.r11.s64 + 40);  // addr:0x825e0028
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r28,r13
	var_r28 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cd9c
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r28,r10
		// beq cr6,0x8246cdb0
		if (var_r28 == ctx.r10.u32) goto loc_8246CDB0;
	}
loc_8246CD9C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r28,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r28);
	// stb r27,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246CDB0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lwz r11,8(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + var_r30;
	// stw r29,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r29);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 4);
	// stw r9,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
	// stw r11,4(r29)
	PPC_STORE_U32(var_r29 + 4, ctx.r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246ce20
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x8246ce20
		if (ctx.r10.u32 != ctx.r9.u32) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246ce20
		if (ctx.r11.s32 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// lbz r29,12(r31)
		var_r29 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246CE20:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_0_CE30"))) PPC_WEAK_FUNC(ph_vt5CD8_0_CE30);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_0_CE30) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8246c750
	ke_C750(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ke_CE60"))) PPC_WEAK_FUNC(ke_CE60);
PPC_FUNC_IMPL(__imp__ke_CE60) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// li r26,0
	var_r26 = 0;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246cea0
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8246ceb4
		if (var_r30 == ctx.r10.u32) goto loc_8246CEB4;
	}
loc_8246CEA0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246CEB4:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r10,61(r27)
	ctx.r10.u64 = PPC_LOAD_U8(var_r27 + 61);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	// beq cr6,0x8246ced8
	if (ctx.r9.u32 != 0) {
		// lis r26,-32768
		var_r26 = (uint32_t)(-2147483648);
		// ori r26,r26,65535
		var_r26 = (uint32_t)(var_r26 | 65535);
		// b 0x8246d030
	} else {
	loc_8246CED8:
		// rlwinm r8,r10,0,25,25
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
		// cmplwi cr6,r8,0
		// bne cr6,0x8246d030
		if (ctx.r8.u32 != 0) goto loc_8246D030;
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246cf08
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8246cf1c
			if (var_r30 == ctx.r10.u32) goto loc_8246CF1C;
		}
	loc_8246CF08:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8246CF1C:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r7,61(r27)
		ctx.r7.u64 = PPC_LOAD_U8(var_r27 + 61);
		// ori r6,r7,64
		ctx.r6.u64 = ctx.r7.u64 | 64;
		// stb r6,61(r27)
		PPC_STORE_U8(var_r27 + 61, ctx.r6.u8);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8246cf78
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r9
			// bne cr6,0x8246cf78
			if (ctx.r10.u32 != ctx.r9.u32) goto loc_8246CF78;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8246cf78
			if (ctx.r11.s32 != 0) goto loc_8246CF78;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_8246CF78:
		// lis r11,-32162
		ctx.r11.s64 = -2107768832;
		// lwz r29,30860(r11)
		var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 30860));  /* glob:lbl_825E788C @ 0x825e788c */
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8246cfa4
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8246cfb8
			if (var_r30 == ctx.r10.u32) goto loc_8246CFB8;
		}
	loc_8246CFA4:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r28,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8246CFB8:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r29,28
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 28;
		// mr r9,r13
		ctx.r9.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lwz r11,8(r10)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// add r11,r11,r27
		ctx.r11.u64 = ctx.r11.u64 + var_r27;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// lwz r5,4(r10)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// stw r5,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
		// stw r11,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r11.u32);
		// lwz r4,4(r11)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r11,0(r4)
		PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8246d030
		if (ctx.r11.s32 == 0) goto loc_8246D030;
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r9,r3
		// bne cr6,0x8246d030
		if (ctx.r9.u32 != ctx.r3.u32) goto loc_8246D030;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246d030
		if (ctx.r11.s32 != 0) goto loc_8246D030;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_8246D030:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x8246d074
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x8246d074
		if (ctx.r10.u32 != ctx.r9.u32) {
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246d074
		if (ctx.r11.s32 != 0) {
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246D074:
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_15_D080"))) PPC_WEAK_FUNC(ph_vt5CD8_15_D080);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_15_D080) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x8246ca00
	ke_CA00(ctx, base);
	// lis r11,-32768
	ctx.r11.s64 = -2147483648;
	// ori r10,r11,16388
	ctx.r10.u64 = ctx.r11.u64 | 16388;
	// cmpw cr6,r3,r10
	// bne cr6,0x8246d130
	if (ctx.r3.s32 == ctx.r10.s32) {
		// lwz r3,76(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 76);
		// cmplwi cr6,r3,0
		// beq cr6,0x8246d120
		if (ctx.r3.u32 != 0) {
			// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// lwz r8,44(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 11, ctx, base);  // pattern-B slot 11 (byte +44)
			// cmpwi cr6,r3,0
			// blt cr6,0x8246d130
			if (ctx.r3.s32 < 0) {
				return;
			}
			// clrlwi r29,r31,24
			var_r29 = (uint32_t)(var_r31 & 0xFF);
			// clrlwi r7,r29,31
			ctx.r7.u64 = var_r29 & 0x1;
			// cmplwi cr6,r7,0
			// bne cr6,0x8246d120
			if (ctx.r7.u32 != 0) goto loc_8246D120;
			// lwz r3,76(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 76);
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwz r5,36(r6)
			// bctrl
			VCALL(ctx.r3.u32, 9, ctx, base);  // vtable slot 9 (byte +36)
			// cmpwi cr6,r3,0
			// blt cr6,0x8246d130
			if (ctx.r3.s32 < 0) {
				return;
			}
			// lbz r4,80(r1)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
			// clrlwi r3,r4,31
			ctx.r3.u64 = ctx.r4.u32 & 0x1;
			// cmplwi cr6,r3,0
			// bne cr6,0x8246d120
			if (ctx.r3.u32 != 0) goto loc_8246D120;
			// clrlwi r11,r29,24
			ctx.r11.u64 = var_r29 & 0xFF;
			// ori r31,r11,1
			var_r31 = (uint32_t)(ctx.r11.u64 | 1);
		}
	loc_8246D120:
		// mr r5,r28
		ctx.r5.u64 = var_r28;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82480160
		ph_vt5D38_15_0160(ctx, base);
	}
loc_8246D130:
	return;
}

__attribute__((alias("__imp__phInst_D138_g"))) PPC_WEAK_FUNC(phInst_D138_g);
PPC_FUNC_IMPL(__imp__phInst_D138_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// stw r4,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// bl 0x8246c660
	phInst_C660(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246d16c
	if (ctx.r3.s32 >= 0) {
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r4,r11,148
		ctx.r4.s64 = ctx.r11.s64 + 148;
		// b 0x8246d170
	} else {
	loc_8246D16C:
		// lwz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	}
loc_8246D170:
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmpwi cr6,r3,0
	// blt cr6,0x8246d210
	if (ctx.r3.s32 >= 0) {
		// lis r3,24962
		ctx.r3.s64 = 1635909632;
		// addi r5,r1,156
		ctx.r5.s64 = ctx.r1.s64 + 156;
		// ori r3,r3,6
		ctx.r3.u64 = ctx.r3.u64 | 6;
		// bl 0x8247ee00
		phInst_EE00(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x8246d210
		if ((int32_t)var_r31 < 0) goto loc_8246D210;
		// lwz r3,156(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
		// li r4,148
		ctx.r4.s64 = 148;
		// lwz r9,20(r10)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// cmplwi cr6,r3,0
		// beq cr6,0x8246d1d0
		if (ctx.r3.u32 != 0) {
			// li r5,0
			ctx.r5.s64 = 0;
			// lwz r4,156(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
			// bl 0x8246cd38
			ke_CD38(ctx, base);
			// mr r30,r3
			var_r30 = ctx.r3.u32;
			// cmplwi cr6,r30,0
			// bne cr6,0x8246d1dc
			if (var_r30 != 0) goto loc_8246D1DC;
		}
	loc_8246D1D0:
		// lis r31,-32761
		var_r31 = (uint32_t)(-2147024896);
		// ori r31,r31,14
		var_r31 = (uint32_t)(var_r31 | 14);
		// b 0x8246d210
		goto loc_8246D210;
	loc_8246D1DC:
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8246c8f8
		phInst_C8F8(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x8246d1fc
		if ((int32_t)var_r31 >= 0) {
			// stw r30,0(r28)
			PPC_STORE_U32(var_r28 + 0, var_r30);
			// b 0x8246d210
		} else {
		loc_8246D1FC:
			// lwz r8,0(r30)
  // [ph4a] vtable load collapsed
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r7,12(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 3, ctx, base);  // pattern-B slot 3 (byte +12)
		}
	}
loc_8246D210:
	// lwz r3,156(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246d22c
	if (ctx.r3.u32 != 0) {
		// lwz r5,4(r6)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	}
loc_8246D22C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__ke_D238"))) PPC_WEAK_FUNC(ke_D238);
PPC_FUNC_IMPL(__imp__ke_D238) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r11,0
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// beq cr6,0x8246d274
	if (ctx.r11.u32 != 0) {
		// bl 0x8246ce60
		ke_CE60(ctx, base);
		// blr
		return;
	}
loc_8246D274:
	// bl 0x8246ca00
	ke_CA00(ctx, base);
	// lis r10,-32768
	ctx.r10.s64 = -2147483648;
	// ori r9,r10,16388
	ctx.r9.u64 = ctx.r10.u64 | 16388;
	// cmpw cr6,r3,r9
	// beq cr6,0x8246d290
	if (ctx.r3.s32 != ctx.r9.s32) {
		// cmpwi cr6,r3,0
		// blt cr6,0x8246d2c8
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
	}
loc_8246D290:
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r7,88(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 22, ctx, base);  // pattern-B slot 22 (byte +88)
	// lwz r3,76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 76);
	// lwz r5,40(r6)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// cmpwi cr6,r3,0
	// blt cr6,0x8246d2c8
	if (ctx.r3.s32 >= 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8247ff40
		ph_vt5D38_14_FF40(ctx, base);
	}
loc_8246D2C8:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5D38_17_D2E0"))) PPC_WEAK_FUNC(ph_vt5D38_17_D2E0);
PPC_FUNC_IMPL(__imp__ph_vt5D38_17_D2E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8247fba0
	ph_vt5CD8_17_FBA0(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x8246d328
	if ((int32_t)var_r30 >= 0) {
		// lwz r11,32(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 32);
		// cmplwi cr6,r11,0
		// addi r3,r11,-8
		ctx.r3.s64 = ctx.r11.s64 + -8;
		// bne cr6,0x8246d31c
		if (ctx.r11.u32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
		}
	loc_8246D31C:
		// li r5,3
		ctx.r5.s64 = 3;
		// addi r4,r31,52
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 52;
		// bl 0x82465ef0
		ph_5EF0(ctx, base);
	}
loc_8246D328:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_D348_w"))) PPC_WEAK_FUNC(phInst_D348_w);
PPC_FUNC_IMPL(__imp__phInst_D348_w) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_8246D358:
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x8246d358
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8246D358;
	// lis r11,0
	ctx.r11.s64 = 0;
	// stb r9,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r9.u8);
	// lbz r9,1(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// ori r10,r11,48000
	ctx.r10.u64 = ctx.r11.u64 | 48000;
	// stb r9,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r9.u8);
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// stw r8,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r8.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// stw r7,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r7.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// stw r6,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r6.u32);
	// lbz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	// bne cr6,0x8246d3a4
	if (ctx.r11.u32 == 0) {
		// li r11,1
		ctx.r11.s64 = 1;
	}
loc_8246D3A4:
	// stb r11,24(r4)
	PPC_STORE_U8(ctx.r4.u32 + 24, ctx.r11.u8);
	// lbz r11,9(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 9);
	// cmplwi cr6,r11,0
	// bne cr6,0x8246d3b8
	if (ctx.r11.u32 == 0) {
		// li r11,6
		ctx.r11.s64 = 6;
	}
loc_8246D3B8:
	// stb r11,25(r4)
	PPC_STORE_U8(ctx.r4.u32 + 25, ctx.r11.u8);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5D38_21_D3C8"))) PPC_WEAK_FUNC(ph_vt5D38_21_D3C8);
PPC_FUNC_IMPL(__imp__ph_vt5D38_21_D3C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r9,0
	ctx.r9.s64 = 0;
	// lbz r11,1(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r8,r9,48000
	ctx.r8.u64 = ctx.r9.u64 | 48000;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, ctx.r11.u8);
	// stb r10,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r10.u8);
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// bl 0x8247eef0
	ph_vt5CD8_21_EEF0(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x8246d420
	if ((int32_t)var_r30 >= 0) {
		// li r5,3
		ctx.r5.s64 = 3;
		// lwz r3,32(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 32);
		// addi r4,r31,52
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 52;
		// bl 0x82466018
		phDemoWorld_6018_g(ctx, base);
	}
loc_8246D420:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// blr
	return;
}

__attribute__((alias("__imp__ke_D440"))) PPC_WEAK_FUNC(ke_D440);
PPC_FUNC_IMPL(__imp__ke_D440) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,-32256
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r11,r11,23864
	ctx.r11.s64 = ctx.r11.s64 + 23864;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// bl 0x82480160
	ph_vt5D38_15_0160(ctx, base);
	// lis r10,-32162
	// lbz r11,76(r30)
	ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 76);
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r10,30860(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 30860);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,64(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// add r27,r11,r10
	var_r27 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r29,r13
	var_r29 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246d4b4
	if (ctx.r11.s32 != 0) {
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r29,r8
		// beq cr6,0x8246d4cc
		if (var_r29 == ctx.r8.u32) goto loc_8246D4CC;
	}
loc_8246D4B4:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r29
	ctx.r8.u64 = var_r29;
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r28);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246D4CC:
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// lwz r11,8(r27)
	ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + var_r30;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r9,r11
	// beq cr6,0x8246d50c
	if (ctx.r9.u32 != ctx.r11.u32) {
		// lwz r10,4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r10,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
		// lwz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r9,4(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r8,0(r9)
		PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
		// stw r11,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
		// stw r11,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
		// lwz r8,8(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_8246D50C:
	// mr r11,r13
	ctx.r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	// beq cr6,0x8246d54c
	if (ctx.r10.s32 != 0) {
		// cmplw cr6,r11,r8
		// bne cr6,0x8246d54c
		if (ctx.r11.u32 != ctx.r8.u32) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8247f2b8
			ke_F2B8(ctx, base);
			return;
		}
		// addi r11,r10,-1
		ctx.r11.s64 = ctx.r10.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246d54c
		if (ctx.r11.s32 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8247f2b8
			ke_F2B8(ctx, base);
			return;
		}
		// lbz r29,12(r31)
		var_r29 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246D54C:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8247f2b8
	ke_F2B8(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5D38_12_D560"))) PPC_WEAK_FUNC(ph_vt5D38_12_D560);
PPC_FUNC_IMPL(__imp__ph_vt5D38_12_D560) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,61(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 61);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__thunk_ph_vt5CD8_9"))) PPC_WEAK_FUNC(thunk_ph_vt5CD8_9);
PPC_FUNC_IMPL(__imp__thunk_ph_vt5CD8_9) {
	PPC_FUNC_PROLOGUE();
	// b 0x8247f358
	ph_vt5CD8_9_F358(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_D578_h"))) PPC_WEAK_FUNC(phInst_D578_h);
PPC_FUNC_IMPL(__imp__phInst_D578_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=144, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x8246d348
	phInst_D348_w(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x8247eff8
	phInst_EFF8_2hr(ctx, base);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmpwi cr6,r31,0
	// blt cr6,0x8246d5c4
	if ((int32_t)var_r31 >= 0) {
		// li r5,3
		ctx.r5.s64 = 3;
		// lwz r3,32(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 32);
		// li r4,0
		ctx.r4.s64 = 0;
		// bl 0x82466018
		phDemoWorld_6018_g(ctx, base);
	}
loc_8246D5C4:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ke_D5E0"))) PPC_WEAK_FUNC(ke_D5E0);
PPC_FUNC_IMPL(__imp__ke_D5E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x8247eeb0
	ke_EEB0(ctx, base);
	// lis r11,-32256
	// addi r11,r11,23864
	ctx.r11.s64 = ctx.r11.s64 + 23864;
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// lbz r11,10(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 10);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// stb r11,76(r30)
	PPC_STORE_U8(var_r30 + 76, ctx.r11.u8);
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// lwz r11,30860(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 64);
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r11,r9
	var_r29 = (uint32_t)(ctx.r11.u64 + ctx.r9.u64);
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r28,r13
	var_r28 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246d660
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r28,r10
		// beq cr6,0x8246d674
		if (var_r28 == ctx.r10.u32) goto loc_8246D674;
	}
loc_8246D660:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r28,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r28);
	// stb r27,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8246D674:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lwz r11,8(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + var_r30;
	// stw r29,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r29);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 4);
	// stw r9,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
	// stw r11,4(r29)
	PPC_STORE_U32(var_r29 + 4, ctx.r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8246d6e4
	if (ctx.r11.s32 != 0) {
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x8246d6e4
		if (ctx.r10.u32 != ctx.r9.u32) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8246d6e4
		if (ctx.r11.s32 != 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
		// lbz r29,12(r31)
		var_r29 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8246D6E4:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__ph_vt5D38_0_D6F0"))) PPC_WEAK_FUNC(ph_vt5D38_0_D6F0);
PPC_FUNC_IMPL(__imp__ph_vt5D38_0_D6F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8246d440
	ke_D440(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_D720_g"))) PPC_WEAK_FUNC(phInst_D720_g);
PPC_FUNC_IMPL(__imp__phInst_D720_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=176, savegprlr_28
	// stw r4,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r4.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// bl 0x8246d348
	phInst_D348_w(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8247ef40
	phInst_EF40(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8246d760
	if (ctx.r3.s32 >= 0) {
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r10,r11,80
		ctx.r10.s64 = ctx.r11.s64 + 80;
		// stw r10,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	}
loc_8246D760:
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r3,0
	// blt cr6,0x8246d804
	if (ctx.r3.s32 >= 0) {
		// lis r3,24962
		ctx.r3.s64 = 1635909632;
		// lwz r4,80(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r5,r1,204
		ctx.r5.s64 = ctx.r1.s64 + 204;
		// ori r3,r3,6
		ctx.r3.u64 = ctx.r3.u64 | 6;
		// bl 0x8247ee00
		phInst_EE00(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmpwi cr6,r30,0
		// blt cr6,0x8246d804
		if ((int32_t)var_r30 < 0) goto loc_8246D804;
		// lwz r3,204(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
		// li r4,80
		ctx.r4.s64 = 80;
		// lwz r8,20(r9)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// cmplwi cr6,r3,0
		// beq cr6,0x8246d7c4
		if (ctx.r3.u32 != 0) {
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// lwz r5,204(r1)
			ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
			// bl 0x8246d5e0
			ke_D5E0(ctx, base);
			// mr r31,r3
			var_r31 = ctx.r3.u32;
			// cmplwi cr6,r31,0
			// bne cr6,0x8246d7d0
			if (var_r31 != 0) goto loc_8246D7D0;
		}
	loc_8246D7C4:
		// lis r30,-32761
		var_r30 = (uint32_t)(-2147024896);
		// ori r30,r30,14
		var_r30 = (uint32_t)(var_r30 | 14);
		// b 0x8246d804
		goto loc_8246D804;
	loc_8246D7D0:
		// mr r4,r29
		ctx.r4.u64 = var_r29;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8246d578
		phInst_D578_h(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmpwi cr6,r30,0
		// blt cr6,0x8246d7f0
		if ((int32_t)var_r30 >= 0) {
			// stw r31,0(r28)
			PPC_STORE_U32(var_r28 + 0, var_r31);
			// b 0x8246d804
		} else {
		loc_8246D7F0:
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,12(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 3, ctx, base);  // pattern-B slot 3 (byte +12)
		}
	}
loc_8246D804:
	// lwz r3,204(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// cmplwi cr6,r3,0
	// beq cr6,0x8246d820
	if (ctx.r3.u32 != 0) {
		// lwz r4,4(r5)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	}
loc_8246D820:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__atSingleton_D830_p42"))) PPC_WEAK_FUNC(atSingleton_D830_p42);
PPC_FUNC_IMPL(__imp__atSingleton_D830_p42) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	// lis r9,-32256
	// li r10,5
	ctx.r10.s64 = 5;
	// li r8,6
	ctx.r8.s64 = 6;
	// lfs f12,23972(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23972);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	// lfs f11,23980(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 23980);
	ctx.f11.f64 = double(temp.f32);
	// lis r9,-32256
	// stfs f12,52(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r10.u32);
	// stfs f11,72(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 72, temp.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ ctx.r10.u32);
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// lfs f0,23976(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r11,8
	ctx.r11.s64 = 8;
	// lfs f13,23984(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 23984);
	ctx.f13.f64 = double(temp.f32);
	// li r9,4
	ctx.r9.s64 = 4;
	// stfs f0,56(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 56, temp.u32);
	// stw r8,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r8.u32);
	// stfs f0,60(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 60, temp.u32);
	// stw r10,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r10.u32);
	// stfs f0,64(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 64, temp.u32);
	// stfs f0,68(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 68, temp.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r11.u32);
	// stfs f13,76(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 76, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// stfs f13,80(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 80, temp.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r11.u32);
	// stw r9,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r9.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r11.u32);
	// stw r9,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_D8B0_g"))) PPC_WEAK_FUNC(phBoundCapsule_D8B0_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_D8B0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f29 = 0.0;
	double var_f26 = 0.0;
	double var_f28 = 0.0;
	double var_f31 = 0.0;
	double var_f27 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82436610
	__savefpr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32256
	// lfs f0,12(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f29,f0,f13
	var_f29 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f26,23976(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23976);
	var_f26 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,23980(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23980);  /* glob:lbl_82005DAC @ 0x82005dac */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f29,f26
	// bge cr6,0x8246d8f8
	if (var_f29 < var_f26) {
		// fdivs f28,f13,f29
		var_f28 = double(float(ctx.f13.f64 / var_f29));
		// b 0x8246d8fc
	} else {
	loc_8246D8F8:
		// fmr f28,f0
		ctx.fpscr.disableFlushMode();
		var_f28 = ctx.f0.f64;
	}
loc_8246D8FC:
	// lis r11,-32256
	// fsubs f12,f0,f28
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f0.f64 - var_f28));
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,0(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f10.f64 = double(temp.f32);
	// lfd f0,27208(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 27208);  /* glob:lbl_82006A48 @ 0x82006a48 */
	// lis r11,-32248
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// fmuls f9,f12,f29
	ctx.f9.f64 = double(float(ctx.f12.f64 * var_f29));
	// lfd f31,-25752(r11)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25752);
	// lis r11,-32256
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// fdiv f8,f11,f10
	ctx.f8.f64 = ctx.f11.f64 / ctx.f10.f64;
	// lfs f0,19100(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 19100);  /* glob:lbl_82004A9C @ 0x82004a9c */
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f9,f0
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// frsp f27,f8
	var_f27 = double(float(ctx.f8.f64));
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// lis r11,-32248
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// lfd f30,-25168(r11)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25168);
	// fmr f2,f30
	ctx.f2.f64 = var_f30;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// lis r11,-32256
	// fmuls f7,f28,f29
	ctx.f7.f64 = double(float(var_f28 * var_f29));
	// fmr f1,f31
	ctx.f1.f64 = var_f31;
	// lfs f13,27204(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27204);  /* glob:lbl_82006A44 @ 0x82006a44 */
	ctx.f13.f64 = double(temp.f32);
	// frsp f31,f0
	var_f31 = double(float(ctx.f0.f64));
	// fmuls f2,f7,f13
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// stfs f1,24(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// fmr f2,f30
	ctx.f2.f64 = var_f30;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// lis r11,-32248
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// stfs f6,24(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 24, temp.u32);
	// lfs f0,-23752(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23752);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	// bge cr6,0x8246d9f8
	if (var_f31 < ctx.f0.f64) {
		// fmr f1,f27
		ctx.f1.f64 = var_f27;
		// bl 0x824302b0
		phBoundCapsule_02B0_g(ctx, base);
		// frsp f30,f1
		ctx.fpscr.disableFlushMode();
		var_f30 = double(float(ctx.f1.f64));
		// lis r11,-32248
		// fmuls f5,f31,f31
		ctx.f5.f64 = double(float(var_f31 * var_f31));
		// lfd f0,-25408(r11)
		ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25408);
		// lis r11,-32248
		// fmuls f4,f30,f30
		ctx.f4.f64 = double(float(var_f30 * var_f30));
		// lfd f29,-25848(r11)
		ctx.f29.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
		// fsub f3,f29,f30
		ctx.f3.f64 = var_f29 - var_f30;
		// fsub f2,f29,f4
		ctx.f2.f64 = var_f29 - ctx.f4.f64;
		// fmul f1,f3,f31
		ctx.f1.f64 = ctx.f3.f64 * var_f31;
		// fmul f13,f2,f5
		ctx.f13.f64 = ctx.f2.f64 * ctx.f5.f64;
		// fmsub f1,f1,f0,f13
		ctx.f1.f64 = ctx.f1.f64 * ctx.f0.f64 - ctx.f13.f64;
		// bl 0x824301d0
		phBoundCapsule_01D0_g(ctx, base);
		// fmuls f12,f30,f31
		ctx.fpscr.disableFlushMode();
		ctx.f12.f64 = double(float(var_f30 * var_f31));
		// fsub f10,f29,f31
		ctx.f10.f64 = var_f29 - var_f31;
		// fsub f11,f29,f12
		ctx.f11.f64 = var_f29 - ctx.f12.f64;
		// frsp f9,f10
		ctx.f9.f64 = double(float(ctx.f10.f64));
		// fsub f8,f11,f1
		ctx.f8.f64 = ctx.f11.f64 - ctx.f1.f64;
		// frsp f7,f8
		ctx.f7.f64 = double(float(ctx.f8.f64));
		// fdivs f6,f7,f9
		ctx.f6.f64 = double(float(ctx.f7.f64 / ctx.f9.f64));
		// stfs f6,28(r31)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(var_r31 + 28, temp.u32);
		// b 0x8246d9fc
	} else {
	loc_8246D9F8:
		// stfs f26,28(r31)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(var_f26);
		PPC_STORE_U32(var_r31 + 28, temp.u32);
	}
loc_8246D9FC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x8243665c
	__restfpr_26(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DA18_2h"))) PPC_WEAK_FUNC(atSingleton_DA18_2h);
PPC_FUNC_IMPL(__imp__atSingleton_DA18_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,2048
	ctx.r10.s64 = 2048;
loc_8246DA20:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246da20
	if (!ctx.cr6.eq) goto loc_8246DA20;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DA80_2h"))) PPC_WEAK_FUNC(atSingleton_DA80_2h);
PPC_FUNC_IMPL(__imp__atSingleton_DA80_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,64
	ctx.r10.s64 = 64;
loc_8246DA88:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246da88
	if (!ctx.cr6.eq) goto loc_8246DA88;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DAE8_2h"))) PPC_WEAK_FUNC(atSingleton_DAE8_2h);
PPC_FUNC_IMPL(__imp__atSingleton_DAE8_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,16
	ctx.r10.s64 = 16;
loc_8246DAF0:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246daf0
	if (!ctx.cr6.eq) goto loc_8246DAF0;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DB50_2h_DB50_1"))) PPC_WEAK_FUNC(atSingleton_DB50_2h_DB50_1);
PPC_FUNC_IMPL(__imp__atSingleton_DB50_2h_DB50_1) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,256
	ctx.r10.s64 = 256;
loc_8246DB58:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246db58
	if (!ctx.cr6.eq) goto loc_8246DB58;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DBB8_2h"))) PPC_WEAK_FUNC(atSingleton_DBB8_2h);
PPC_FUNC_IMPL(__imp__atSingleton_DBB8_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,32
	ctx.r10.s64 = 32;
loc_8246DBC0:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246dbc0
	if (!ctx.cr6.eq) goto loc_8246DBC0;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DC20_2h"))) PPC_WEAK_FUNC(atSingleton_DC20_2h);
PPC_FUNC_IMPL(__imp__atSingleton_DC20_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,128
	ctx.r10.s64 = 128;
loc_8246DC28:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246dc28
	if (!ctx.cr6.eq) goto loc_8246DC28;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_DC88_2h"))) PPC_WEAK_FUNC(atSingleton_DC88_2h);
PPC_FUNC_IMPL(__imp__atSingleton_DC88_2h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// li r10,512
	ctx.r10.s64 = 512;
loc_8246DC90:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stfs f0,-4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4)/* atSingleton::flags@+0x4 */;
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// bne cr6,0x8246dc90
	if (!ctx.cr6.eq) goto loc_8246DC90;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_DCF0_h"))) PPC_WEAK_FUNC(game_DCF0_h);
PPC_FUNC_IMPL(__imp__game_DCF0_h) {
	PPC_FUNC_PROLOGUE();
	// stw r4,100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 100, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_DCF8_h"))) PPC_WEAK_FUNC(game_DCF8_h);
PPC_FUNC_IMPL(__imp__game_DCF8_h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,18992(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 18992);  /* glob:lbl_82004A30 @ 0x82004a30 */
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f0,92(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 92, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundCapsule_DD10_g"))) PPC_WEAK_FUNC(phBoundCapsule_DD10_g);
PPC_FUNC_IMPL(__imp__phBoundCapsule_DD10_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r15 = 0;
	uint32_t var_r14 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	double var_f25 = 0.0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	double var_f28 = 0.0;
	double var_f27 = 0.0;
	double var_f31 = 0.0;
	double var_f26 = 0.0;
	double var_f24 = 0.0;
	double var_f23 = 0.0;
	double var_f20 = 0.0;
	double var_f19 = 0.0;
	double var_f18 = 0.0;
	double var_f22 = 0.0;
	double var_f21 = 0.0;
	double var_f16 = 0.0;
	double var_f17 = 0.0;
	double var_f15 = 0.0;
	double var_f14 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f860
	ctx.lr = 0x8246DD18;
	__savegprlr_14(ctx, base);
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x824365e0
	__savefpr_14(ctx, base);
	// stwu r1,-432(r1)
	ea = -432 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r10,r31,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 4;
	// mr r11,r29
	ctx.r11.u64 = var_r29;
	// li r9,21
	ctx.r9.s64 = 21;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8246DD3C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8246dd3c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8246DD3C;
	// lis r10,-32256
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4)/* phBoundCapsule::flags@+0x4 */;
	// cmplwi cr6,r11,0
	// lfs f0,27216(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27216);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// bne cr6,0x8246dd70
	if (ctx.r11.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
		// b 0x8246dda0
	} else {
	loc_8246DD70:
		// clrldi r8,r11,32
		ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
		// lfs f13,88(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 88);
		ctx.f13.f64 = double(temp.f32);
		// addi r7,r1,88
		ctx.r7.s64 = ctx.r1.s64 + 88;
		// std r8,120(r1)
		PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r8.u64);
		// lfd f12,120(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// frsp f10,f11
		ctx.f10.f64 = double(float(ctx.f11.f64));
		// fmuls f9,f13,f10
		ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
		// fmuls f8,f9,f0
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fctidz f7,f8
		ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f8.f64));
		// stfiwx f7,0,r7
		PPC_STORE_U32(ctx.r7.u32, ctx.f7.u32);
		// lwz r3,88(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	}
loc_8246DDA0:
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,4095
	// ble cr6,0x8246ddb0
	if (ctx.r3.u32 > 4095) {
		// li r3,4095
		ctx.r3.s64 = 4095;
	}
loc_8246DDB0:
	// lis r4,2
	ctx.r4.s64 = 131072;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// ori r11,r4,61380
	ctx.r11.u64 = ctx.r4.u64 | 61380;
	// lis r10,3
	ctx.r10.s64 = 196608;
	// lis r8,4
	ctx.r8.s64 = 262144;
	// ori r5,r6,40712
	ctx.r5.u64 = ctx.r6.u64 | 40712;
	// ori r9,r10,41056
	ctx.r9.u64 = ctx.r10.u64 | 41056;
	// stwx r3,r31,r11
	PPC_STORE_U32(var_r31 + ctx.r11.u32, ctx.r3.u32);
	// lis r11,-32256
	// ori r7,r8,20736
	ctx.r7.u64 = ctx.r8.u64 | 20736;
	// stwx r3,r31,r5
	PPC_STORE_U32(var_r31 + ctx.r5.u32, ctx.r3.u32);
	// stwx r3,r31,r9
	PPC_STORE_U32(var_r31 + ctx.r9.u32, ctx.r3.u32);
	// lfs f10,27328(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27328);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	// stwx r3,r31,r7
	PPC_STORE_U32(var_r31 + ctx.r7.u32, ctx.r3.u32);
	// lfs f0,80(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 80);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r11,23992
	var_r30 = (uint32_t)(ctx.r11.s64 + 23992);  // lbl_82005DB8 @ 0x82005db8
	// fcmpu cr6,f0,f10
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,23980(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23980);  /* glob:lbl_82005DAC @ 0x82005dac */
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// blt cr6,0x8246de30
	if (ctx.f0.f64 >= ctx.f10.f64) {
		// bso cr6,0x8246de30
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8246DE08, "bso");
		// lis r11,-32256
		// lfs f5,88(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 88);
		ctx.f5.f64 = double(temp.f32);
		// lfs f12,18992(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 18992);  /* glob:lbl_82004A30 @ 0x82004a30 */
		ctx.f12.f64 = double(temp.f32);
		// lis r11,-32248
		// fmuls f6,f0,f12
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
		// lfs f0,-24960(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24960);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f4,f6,f5
		ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
		// fmuls f25,f4,f0
		var_f25 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// b 0x8246de60
	} else {
	loc_8246DE30:
		// lis r11,-32248
		// lfs f12,4(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
		ctx.f12.f64 = double(temp.f32);
		// lfs f1,88(r31)
		temp.u32 = PPC_LOAD_U32(var_r31 + 88);
		ctx.f1.f64 = double(temp.f32);
		// lfs f11,-25668(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25668);
		ctx.f11.f64 = double(temp.f32);
		// lis r11,-32256
		// fmadds f3,f0,f12,f11
		ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f11.f64));
		// lfs f0,18992(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 18992);  /* glob:lbl_82004A30 @ 0x82004a30 */
		ctx.f0.f64 = double(temp.f32);
		// lis r11,-32248
		// fmuls f2,f3,f0
		ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// lfs f0,-24960(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24960);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f12,f2,f1
		ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
		// fmuls f25,f12,f0
		var_f25 = double(float(ctx.f12.f64 * ctx.f0.f64));
	}
loc_8246DE60:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,27324(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27324);  /* glob:lbl_82006ABC @ 0x82006abc */
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f25,f0
	// bge cr6,0x8246de74
	if (var_f25 < ctx.f0.f64) {
		// fmr f25,f0
		var_f25 = ctx.f0.f64;
	}
loc_8246DE74:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f12,16056(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16056);  /* glob:lbl_82003EB8 @ 0x82003eb8 */
	ctx.f12.f64 = double(temp.f32);
	// fmuls f30,f25,f12
	var_f30 = double(float(var_f25 * ctx.f12.f64));
	// fcmpu cr6,f30,f0
	// bge cr6,0x8246de90
	if (var_f30 < ctx.f0.f64) {
		// fmr f30,f0
		var_f30 = ctx.f0.f64;
		// b 0x8246de9c
	} else {
	loc_8246DE90:
		// fcmpu cr6,f30,f13
		ctx.fpscr.disableFlushMode();
		// ble cr6,0x8246de9c
		if (var_f30 <= ctx.f13.f64) goto loc_8246DE9C;
		// fmr f30,f13
		var_f30 = ctx.f13.f64;
	}
loc_8246DE9C:
	// lis r11,-32248
	// lfs f0,76(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 76);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,-24384(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24384);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f12
	// blt cr6,0x8246ded4
	if (ctx.f0.f64 >= ctx.f12.f64) {
		// bso cr6,0x8246ded4
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8246DEB0, "bso");
		// fsubs f11,f0,f12
		ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// lis r11,-32256
		// fmr f29,f13
		var_f29 = ctx.f13.f64;
		// fmr f28,f13
		var_f28 = ctx.f13.f64;
		// fmr f27,f13
		var_f27 = ctx.f13.f64;
		// lfs f0,27320(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27320);  /* glob:lbl_82006AB8 @ 0x82006ab8 */
		ctx.f0.f64 = double(temp.f32);
		// fmuls f31,f11,f0
		var_f31 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// b 0x8246df54
	} else {
	loc_8246DED4:
		// fcmpu cr6,f0,f10
		ctx.fpscr.disableFlushMode();
		// blt cr6,0x8246df04
		if (ctx.f0.f64 >= ctx.f10.f64) {
			// bso cr6,0x8246df04
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x8246DEDC, "bso");
			// lis r11,-32256
			// fsubs f10,f0,f10
			ctx.f10.f64 = double(float(ctx.f0.f64 - ctx.f10.f64));
			// fmr f28,f13
			var_f28 = ctx.f13.f64;
			// fmr f27,f13
			var_f27 = ctx.f13.f64;
			// lfs f31,23976(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23976);  /* glob:lbl_82005DA8 @ 0x82005da8 */
			var_f31 = double(temp.f32);
			// lis r11,-32256
			ctx.r11.s64 = -2113929216;
			// lfs f0,27320(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27320);  /* glob:lbl_82006AB8 @ 0x82006ab8 */
			ctx.f0.f64 = double(temp.f32);
			// fmuls f29,f10,f0
			var_f29 = double(float(ctx.f10.f64 * ctx.f0.f64));
			// b 0x8246df54
		} else {
		loc_8246DF04:
			// lis r11,-32256
			ctx.r11.s64 = -2113929216;
			// lfs f12,27316(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27316);  /* glob:lbl_82006AB4 @ 0x82006ab4 */
			ctx.f12.f64 = double(temp.f32);
			// fcmpu cr6,f0,f12
			// blt cr6,0x8246df38
			if (ctx.f0.f64 >= ctx.f12.f64) {
				// bso cr6,0x8246df38
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x8246DF14, "bso");
				// lis r11,-32256
				// fsubs f9,f0,f12
				ctx.f9.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
				// fmr f27,f13
				var_f27 = ctx.f13.f64;
				// lfs f31,23976(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23976);  /* glob:lbl_82005DA8 @ 0x82005da8 */
				var_f31 = double(temp.f32);
				// lis r11,-32256
				ctx.r11.s64 = -2113929216;
				// lfs f0,27320(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27320);  /* glob:lbl_82006AB8 @ 0x82006ab8 */
				ctx.f0.f64 = double(temp.f32);
				// fmuls f28,f9,f0
				var_f28 = double(float(ctx.f9.f64 * ctx.f0.f64));
				// b 0x8246df50
			} else {
			loc_8246DF38:
				// lis r11,-32256
				ctx.r11.s64 = -2113929216;
				// lfs f31,23976(r11)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23976);  /* glob:lbl_82005DA8 @ 0x82005da8 */
				var_f31 = double(temp.f32);
				// lis r11,-32256
				// fmr f28,f31
				var_f28 = var_f31;
				// lfs f13,27320(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27320);  /* glob:lbl_82006AB8 @ 0x82006ab8 */
				ctx.f13.f64 = double(temp.f32);
				// fmuls f27,f0,f13
				var_f27 = double(float(ctx.f0.f64 * ctx.f13.f64));
			}
		loc_8246DF50:
			// fmr f29,f31
			ctx.fpscr.disableFlushMode();
			var_f29 = var_f31;
		}
	}
loc_8246DF54:
	// lis r11,-32256
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// lfs f0,27312(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27312);  /* glob:lbl_82006AB0 @ 0x82006ab0 */
	ctx.f0.f64 = double(temp.f32);
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(var_f30 * ctx.f0.f64));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f8.f64));
	// stfiwx f7,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f7.u32);
	// lwz r27,88(r1)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// lis r11,-32256
	// lis r4,2
	ctx.r4.s64 = 131072;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// addi r8,r3,-1
	ctx.r8.s64 = ctx.r3.s64 + -1;
	// lfs f0,27308(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27308);  /* glob:lbl_82006AAC @ 0x82006aac */
	ctx.f0.f64 = double(temp.f32);
	// ori r11,r4,240
	ctx.r11.u64 = ctx.r4.u64 | 240;
	// fmuls f6,f30,f0
	ctx.f6.f64 = double(float(var_f30 * ctx.f0.f64));
	// ori r9,r10,252
	ctx.r9.u64 = ctx.r10.u64 | 252;
	// stwx r3,r31,r11
	PPC_STORE_U32(var_r31 + ctx.r11.u32, ctx.r3.u32);
	// stwx r8,r31,r9
	PPC_STORE_U32(var_r31 + ctx.r9.u32, ctx.r8.u32);
	// fctidz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f5.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// ori r6,r7,2868
	ctx.r6.u64 = ctx.r7.u64 | 2868;
	// ori r4,r5,2880
	ctx.r4.u64 = ctx.r5.u64 | 2880;
	// stwx r3,r31,r6
	PPC_STORE_U32(var_r31 + ctx.r6.u32, ctx.r3.u32);
	// stwx r11,r31,r4
	PPC_STORE_U32(var_r31 + ctx.r4.u32, ctx.r11.u32);
	// li r11,1020
	ctx.r11.s64 = 1020;
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 4)/* phBoundCapsule::flags@+0x4 */;
	// cmplwi cr6,r10,0
	// beq cr6,0x8246dfe4
	if (ctx.r10.u32 != 0) {
		// li r11,9
		ctx.r11.s64 = 9;
	}
loc_8246DFE4:
	// clrldi r9,r11,32
	ctx.r9.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// std r9,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r9.u64);
	// lfd f4,120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f30
	ctx.f1.f64 = double(float(ctx.f2.f64 * var_f30));
	// fctidz f0,f1
	ctx.f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f0.u32);
	// lwz r28,88(r1)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// ori r5,r6,14224
	ctx.r5.u64 = ctx.r6.u64 | 14224;
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// stwx r7,r31,r5
	PPC_STORE_U32(var_r31 + ctx.r5.u32, ctx.r7.u32);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// lis r11,-32256
	// lis r9,2
	ctx.r9.s64 = 131072;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r7,r3,-1
	ctx.r7.s64 = ctx.r3.s64 + -1;
	// ori r8,r9,18372
	ctx.r8.u64 = ctx.r9.u64 | 18372;
	// lfs f0,27304(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27304);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// fmuls f13,f30,f0
	ctx.f13.f64 = double(float(var_f30 * ctx.f0.f64));
	// ori r10,r11,18360
	ctx.r10.u64 = ctx.r11.u64 | 18360;
	// stwx r7,r31,r8
	PPC_STORE_U32(var_r31 + ctx.r8.u32, ctx.r7.u32);
	// stwx r3,r31,r10
	PPC_STORE_U32(var_r31 + ctx.r10.u32, ctx.r3.u32);
	// fctidz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f12.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lis r4,2
	ctx.r4.s64 = 131072;
	// ori r5,r6,20988
	ctx.r5.u64 = ctx.r6.u64 | 20988;
	// ori r10,r4,21000
	ctx.r10.u64 = ctx.r4.u64 | 21000;
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// stwx r11,r31,r5
	PPC_STORE_U32(var_r31 + ctx.r5.u32, ctx.r11.u32);
	// stwx r9,r31,r10
	PPC_STORE_U32(var_r31 + ctx.r10.u32, ctx.r9.u32);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// fmuls f26,f31,f30
	ctx.fpscr.disableFlushMode();
	var_f26 = double(float(var_f31 * var_f30));
	// lis r11,-32256
	// lis r7,2
	ctx.r7.s64 = 131072;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// ori r6,r7,32344
	ctx.r6.u64 = ctx.r7.u64 | 32344;
	// lfs f0,27300(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27300);
	ctx.f0.f64 = double(temp.f32);
	// stwx r3,r31,r6
	PPC_STORE_U32(var_r31 + ctx.r6.u32, ctx.r3.u32);
	// fmuls f11,f26,f0
	ctx.f11.f64 = double(float(var_f26 * ctx.f0.f64));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfiwx f10,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f10.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e0cc
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E0CC:
	// lis r11,-32256
	// addis r15,r31,3
	var_r15 = (uint32_t)(var_r31 + 196608);
	// addis r14,r31,3
	var_r14 = (uint32_t)(var_r31 + 196608);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// addi r15,r15,-29064
	var_r15 = (uint32_t)(var_r15 + -29064);
	// lfs f0,27296(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27296);
	ctx.f0.f64 = double(temp.f32);
	// addi r14,r14,-26992
	var_r14 = (uint32_t)(var_r14 + -26992);
	// fmuls f9,f30,f0
	ctx.f9.f64 = double(float(var_f30 * ctx.f0.f64));
	// stw r3,0(r15)
	PPC_STORE_U32(var_r15 + 0, ctx.r3.u32);
	// stw r15,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, var_r15);
	// stw r14,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, var_r14);
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// stfiwx f8,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f8.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// fmuls f7,f31,f25
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(var_f31 * var_f25));
	// lis r11,-32256
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stw r3,0(r14)
	PPC_STORE_U32(var_r14 + 0, ctx.r3.u32);
	// lfs f0,27292(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27292);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fctidz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f5.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e13c
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E13C:
	// lis r11,-32256
	// addis r16,r31,3
	var_r16 = (uint32_t)(var_r31 + 196608);
	// addi r16,r16,-24836
	var_r16 = (uint32_t)(var_r16 + -24836);
	// lfs f0,27288(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27288);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,88
	ctx.r11.s64 = ctx.r1.s64 + 88;
	// fmuls f4,f26,f0
	ctx.f4.f64 = double(float(var_f26 * ctx.f0.f64));
	// stw r3,0(r16)
	PPC_STORE_U32(var_r16 + 0, ctx.r3.u32);
	// fctidz f3,f4
	ctx.f3.s64 = (ctx.f4.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f4.f64));
	// stfiwx f3,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f3.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e174
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E174:
	// lis r11,-32256
	// addis r18,r31,3
	var_r18 = (uint32_t)(var_r31 + 196608);
	// addis r17,r31,3
	var_r17 = (uint32_t)(var_r31 + 196608);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// addi r18,r18,-8396
	var_r18 = (uint32_t)(var_r18 + -8396);
	// lfs f0,27284(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27284);
	ctx.f0.f64 = double(temp.f32);
	// addi r17,r17,-6324
	var_r17 = (uint32_t)(var_r17 + -6324);
	// fmuls f2,f30,f0
	ctx.f2.f64 = double(float(var_f30 * ctx.f0.f64));
	// stw r3,0(r18)
	PPC_STORE_U32(var_r18 + 0, ctx.r3.u32);
	// stw r18,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, var_r18);
	// stw r17,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, var_r17);
	// fctidz f1,f2
	ctx.f1.s64 = (ctx.f2.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f2.f64));
	// stfiwx f1,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f1.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// fmuls f13,f29,f25
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(var_f29 * var_f25));
	// lis r11,-32256
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// stw r3,0(r17)
	PPC_STORE_U32(var_r17 + 0, ctx.r3.u32);
	// lfs f0,27280(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27280);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f11.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e1e4
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E1E4:
	// fmuls f28,f28,f30
	ctx.fpscr.disableFlushMode();
	var_f28 = double(float(var_f28 * var_f30));
	// lis r11,-32256
	// addis r19,r31,3
	var_r19 = (uint32_t)(var_r31 + 196608);
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r19,r19,-4168
	var_r19 = (uint32_t)(var_r19 + -4168);
	// lfs f0,27276(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27276);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,0(r19)
	PPC_STORE_U32(var_r19 + 0, ctx.r3.u32);
	// fmuls f10,f28,f0
	ctx.f10.f64 = double(float(var_f28 * ctx.f0.f64));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e220
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E220:
	// fmuls f31,f29,f30
	ctx.fpscr.disableFlushMode();
	var_f31 = double(float(var_f29 * var_f30));
	// lis r11,-32256
	// addis r20,r31,3
	var_r20 = (uint32_t)(var_r31 + 196608);
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r20,r20,12272
	var_r20 = (uint32_t)(var_r20 + 12272);
	// lfs f0,27272(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27272);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r3.u32);
	// fmuls f8,f31,f0
	ctx.f8.f64 = double(float(var_f31 * ctx.f0.f64));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f8.f64));
	// stfiwx f7,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f7.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e25c
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E25C:
	// lis r11,-32256
	// addis r22,r31,3
	var_r22 = (uint32_t)(var_r31 + 196608);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r3,-1
	ctx.r5.s64 = ctx.r3.s64 + -1;
	// addi r22,r22,20488
	var_r22 = (uint32_t)(var_r22 + 20488);
	// lfs f0,27268(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27268);
	ctx.f0.f64 = double(temp.f32);
	// addis r21,r31,3
	var_r21 = (uint32_t)(var_r31 + 196608);
	// fmuls f6,f30,f0
	ctx.f6.f64 = double(float(var_f30 * ctx.f0.f64));
	// addi r21,r21,28708
	var_r21 = (uint32_t)(var_r21 + 28708);
	// stw r5,0(r22)
	PPC_STORE_U32(var_r22 + 0, ctx.r5.u32);
	// fctidz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f5.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// lis r11,-32256
	// stw r3,0(r21)
	PPC_STORE_U32(var_r21 + 0, ctx.r3.u32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// lfs f0,27264(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27264);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f26,f0
	ctx.f4.f64 = double(float(var_f26 * ctx.f0.f64));
	// fctidz f3,f4
	ctx.f3.s64 = (ctx.f4.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f4.f64));
	// stfiwx f3,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f3.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e2c4
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E2C4:
	// fmuls f2,f27,f25
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = double(float(var_f27 * var_f25));
	// lis r11,-32256
	// addis r23,r31,4
	var_r23 = (uint32_t)(var_r31 + 262144);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// addi r23,r23,-28612
	var_r23 = (uint32_t)(var_r23 + -28612);
	// lfs f0,27260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27260);
	ctx.f0.f64 = double(temp.f32);
	// stw r3,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r3.u32);
	// stw r23,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, var_r23);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fctidz f0,f1
	ctx.f0.s64 = (ctx.f1.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f1.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f0.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e304
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E304:
	// lis r11,-32256
	// addis r24,r31,4
	var_r24 = (uint32_t)(var_r31 + 262144);
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r24,r24,-24492
	var_r24 = (uint32_t)(var_r24 + -24492);
	// lfs f0,27256(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27256);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f28,f0
	ctx.f13.f64 = double(float(var_f28 * ctx.f0.f64));
	// stw r3,0(r24)
	PPC_STORE_U32(var_r24 + 0, ctx.r3.u32);
	// fctidz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f12.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e33c
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E33C:
	// lis r11,-32256
	// addis r25,r31,4
	var_r25 = (uint32_t)(var_r31 + 262144);
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r25,r25,-8048
	var_r25 = (uint32_t)(var_r25 + -8048);
	// lfs f0,27252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27252);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f31,f0
	ctx.f11.f64 = double(float(var_f31 * ctx.f0.f64));
	// stw r3,0(r25)
	PPC_STORE_U32(var_r25 + 0, ctx.r3.u32);
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfiwx f10,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f10.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e374
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E374:
	// lis r11,-32256
	// addis r26,r31,4
	var_r26 = (uint32_t)(var_r31 + 262144);
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r26,r26,168
	var_r26 = (uint32_t)(var_r26 + 168);
	// lfs f0,27248(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27248);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f9,f26,f0
	ctx.f9.f64 = double(float(var_f26 * ctx.f0.f64));
	// stw r3,0(r26)
	PPC_STORE_U32(var_r26 + 0, ctx.r3.u32);
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e3ac
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E3AC:
	// lis r11,-32256
	// addis r28,r31,4
	var_r28 = (uint32_t)(var_r31 + 262144);
	// addis r27,r31,4
	var_r27 = (uint32_t)(var_r31 + 262144);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r28,r28,8388
	var_r28 = (uint32_t)(var_r28 + 8388);
	// lfs f0,27244(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27244);
	ctx.f0.f64 = double(temp.f32);
	// addi r27,r27,16604
	var_r27 = (uint32_t)(var_r27 + 16604);
	// fmuls f7,f26,f0
	ctx.f7.f64 = double(float(var_f26 * ctx.f0.f64));
	// stw r3,0(r28)
	PPC_STORE_U32(var_r28 + 0, ctx.r3.u32);
	// stw r27,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, var_r27);
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f7.f64));
	// stfiwx f6,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f6.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// lis r11,-32256
	// stw r3,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r3.u32);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lfs f0,27240(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27240);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f25,f0
	ctx.f5.f64 = double(float(var_f25 * ctx.f0.f64));
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f5.f64));
	// stfiwx f4,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f4.u32);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82480730
	phBoundCapsule_0730_wrh(ctx, base);
	// cmplwi cr6,r3,0
	// bne cr6,0x8246e414
	if (ctx.r3.u32 == 0) {
		// li r3,1
		ctx.r3.s64 = 1;
	}
loc_8246E414:
	// lis r4,4
	ctx.r4.s64 = 262144;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 0);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lis r7,-32248
	// ori r9,r4,20724
	ctx.r9.u64 = ctx.r4.u64 | 20724;
	// lwz r8,0(r25)
	ctx.r8.u64 = PPC_LOAD_U32(var_r25 + 0);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r6,0(r23)
	ctx.r6.u64 = PPC_LOAD_U32(var_r23 + 0);
	// lwz r5,0(r21)
	ctx.r5.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lfs f3,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 88);
	ctx.f3.f64 = double(temp.f32);
	// lwz r4,0(r22)
	ctx.r4.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lfd f30,-25752(r7)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r7.u32 + -25752);
	// lwz r7,0(r24)
	ctx.r7.u64 = PPC_LOAD_U32(var_r24 + 0);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// fmr f1,f30
	ctx.f1.f64 = var_f30;
	// lwz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U32(var_r26 + 0);
	// lwz r28,0(r20)
	var_r28 = (uint32_t)(PPC_LOAD_U32(var_r20 + 0));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r27,0(r19)
	var_r27 = (uint32_t)(PPC_LOAD_U32(var_r19 + 0));
	// lwz r26,0(r17)
	var_r26 = (uint32_t)(PPC_LOAD_U32(var_r17 + 0));
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwz r25,0(r18)
	var_r25 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
	// lwz r24,0(r16)
	var_r24 = (uint32_t)(PPC_LOAD_U32(var_r16 + 0));
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// lwz r23,0(r14)
	var_r23 = (uint32_t)(PPC_LOAD_U32(var_r14 + 0));
	// lwz r21,88(r1)
	var_r21 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 88));
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// lwz r22,0(r15)
	var_r22 = (uint32_t)(PPC_LOAD_U32(var_r15 + 0));
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// stwx r3,r31,r21
	PPC_STORE_U32(var_r31 + var_r21, ctx.r3.u32);
	// lfs f2,72(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 72);
	ctx.f2.f64 = double(temp.f32);
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + var_r28;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + var_r27;
	// add r11,r11,r26
	ctx.r11.u64 = ctx.r11.u64 + var_r26;
	// add r11,r11,r25
	ctx.r11.u64 = ctx.r11.u64 + var_r25;
	// add r11,r11,r24
	ctx.r11.u64 = ctx.r11.u64 + var_r24;
	// add r11,r11,r23
	ctx.r11.u64 = ctx.r11.u64 + var_r23;
	// add r11,r11,r22
	ctx.r11.u64 = ctx.r11.u64 + var_r22;
	// add r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 + ctx.r3.u64;
	// rlwinm r11,r3,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x7FFFFFFF;
	// clrldi r10,r11,32
	ctx.r10.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// lis r11,-32256
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f0,88(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfd f13,27232(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + 27232);
	// fdivs f11,f12,f3
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// fdiv f0,f2,f11
	ctx.f0.f64 = ctx.f2.f64 / ctx.f11.f64;
	// fdiv f2,f13,f0
	ctx.f2.f64 = ctx.f13.f64 / ctx.f0.f64;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfd f0,-25168(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25168);  /* glob:lbl_82079DB0 @ 0x82079db0 */
	// fmul f1,f1,f0
	ctx.f1.f64 = ctx.f1.f64 * ctx.f0.f64;
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// lis r11,-32256
	// lwz r9,16(r29)
	ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 16);
	// addi r8,r30,888
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 + 888;
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// lwz r7,8(r29)
	ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 8);
	// addi r5,r30,192
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 + 192;
	// lwz r6,12(r29)
	ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 12);
	// addi r4,r30,504
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 504;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,27228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27228);  /* glob:lbl_82086A5C @ 0x82086a5c */
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r30,632
	ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 632;
	// addi r7,r30,760
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 + 760;
	// lfsx f25,r10,r4
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r4.u32);
	var_f25 = double(temp.f32);
	// addi r28,r30,376
	var_r28 = (uint32_t)(var_r30 + 376);
	// lfsx f28,r11,r8
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
	var_f28 = double(temp.f32);
	// lis r8,-32256
	// lfsx f9,r11,r5
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r5.u32);
	ctx.f9.f64 = double(temp.f32);
	// lfsx f24,r9,r3
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r3.u32);
	var_f24 = double(temp.f32);
	// lfsx f23,r10,r7
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	var_f23 = double(temp.f32);
	// lfs f13,27200(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27200);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32256
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f13,27224(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27224);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32256
	// lfsx f8,r11,r28
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + var_r28);
	ctx.f8.f64 = double(temp.f32);
	// addi r28,r30,504
	var_r28 = (uint32_t)(var_r30 + 504);
	// lwz r11,28(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 28);
	// addi r23,r30,632
	var_r23 = (uint32_t)(var_r30 + 632);
	// lwz r24,24(r29)
	var_r24 = (uint32_t)(PPC_LOAD_U32(var_r29 + 24));
	// addi r22,r30,760
	var_r22 = (uint32_t)(var_r30 + 760);
	// lwz r7,32(r29)
	ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 32);
	// lwz r4,20(r29)
	ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 20);
	// addi r15,r30,3144
	var_r15 = (uint32_t)(var_r30 + 3144);
	// lfs f12,27220(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27220);
	ctx.f12.f64 = double(temp.f32);
	// addi r27,r30,376
	var_r27 = (uint32_t)(var_r30 + 376);
	// fnmsubs f12,f11,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfsx f20,r9,r28
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r28);
	var_f20 = double(temp.f32);
	// lfsx f19,r10,r23
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r23);
	var_f19 = double(temp.f32);
	// rlwinm r10,r24,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 2) & 0xFFFFFFFC;
	// lfsx f18,r9,r22
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r22);
	var_f18 = double(temp.f32);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,40(r29)
	ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 40);
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,36(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 36);
	// addi r14,r30,3144
	var_r14 = (uint32_t)(var_r30 + 3144);
	// lwz r5,44(r29)
	ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 44);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lfsx f0,r10,r15
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r15);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r11,r6,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 0)/* phBoundCapsule::vtable@+0x0 */;
	// lfsx f6,r8,r27
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r27);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r6,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
	// fmuls f22,f28,f13
	var_f22 = double(float(var_f28 * ctx.f13.f64));
	// fmuls f26,f6,f12
	var_f26 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfsx f31,r9,r14
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r14);
	var_f31 = double(temp.f32);
	// fmuls f29,f8,f12
	var_f29 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// add r9,r11,r5
	ctx.r9.u64 = ctx.r11.u64 + ctx.r5.u64;
	// fneg f7,f10
	ctx.f7.u64 = ctx.f10.u64 ^ 0x8000000000000000;
	// addi r25,r30,192
	var_r25 = (uint32_t)(var_r30 + 192);
	// fmuls f21,f9,f12
	var_f21 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// addi r26,r30,888
	var_r26 = (uint32_t)(var_r30 + 888);
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r21,r30,1016
	var_r21 = (uint32_t)(var_r30 + 1016);
	// lfsx f5,r8,r25
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r25);
	ctx.f5.f64 = double(temp.f32);
	// addi r20,r30,316
	var_r20 = (uint32_t)(var_r30 + 316);
	// lfsx f27,r8,r26
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r26);
	var_f27 = double(temp.f32);
	// addi r19,r30,1536
	var_r19 = (uint32_t)(var_r30 + 1536);
	// fmuls f16,f5,f12
	var_f16 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// addi r18,r30,2056
	var_r18 = (uint32_t)(var_r30 + 2056);
	// fmuls f17,f29,f13
	var_f17 = double(float(var_f29 * ctx.f13.f64));
	// addi r17,r30,2600
	var_r17 = (uint32_t)(var_r30 + 2600);
	// fmuls f15,f27,f13
	var_f15 = double(float(var_f27 * ctx.f13.f64));
	// addi r16,r30,132
	var_r16 = (uint32_t)(var_r30 + 132);
	// lfsx f6,r11,r21
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + var_r21);
	ctx.f6.f64 = double(temp.f32);
	// lfsx f5,r10,r20
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r20);
	ctx.f5.f64 = double(temp.f32);
	// cmplwi cr6,r4,0
	// lfsx f4,r11,r19
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + var_r19);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f14,f26,f13
	var_f14 = double(float(var_f26 * ctx.f13.f64));
	// lfsx f3,r9,r18
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r18);
	ctx.f3.f64 = double(temp.f32);
	// lfsx f2,r9,r17
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + var_r17);
	ctx.f2.f64 = double(temp.f32);
	// lfsx f1,r10,r16
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r16);
	ctx.f1.f64 = double(temp.f32);
	// bne cr6,0x8246e660
	if (ctx.r4.u32 == 0) {
		// li r10,1
		ctx.r10.s64 = 1;
		// b 0x8246e694
	} else {
	loc_8246E660:
		// clrldi r8,r4,32
		ctx.r8.u64 = ctx.r4.u64 & 0xFFFFFFFF;
		// lfs f13,88(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 88);
		ctx.f13.f64 = double(temp.f32);
		// addi r7,r1,88
		ctx.r7.s64 = ctx.r1.s64 + 88;
		// std r8,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
		// lfd f12,88(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f12
		ctx.f9.f64 = double(ctx.f12.s64);
		// lfs f12,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f12.f64 = double(temp.f32);
		// frsp f8,f9
		ctx.f8.f64 = double(float(ctx.f9.f64));
		// fmuls f13,f8,f13
		ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
		// fmuls f9,f13,f12
		ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
		// fctidz f8,f9
		ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
		// stfiwx f8,0,r7
		PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
		// lwz r10,88(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	}
loc_8246E694:
	// addi r11,r31,104
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 104;
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r6,r5
	// bne cr6,0x8246e6b8
	if (ctx.r6.u32 == ctx.r5.u32) {
		// lfs f13,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f13.f64 = double(temp.f32);
		// stw r10,8(r11)
		PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
		// stfs f13,4(r11)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	}
loc_8246E6B8:
	// addis r11,r31,1
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 65536;
	// addi r11,r11,136
	ctx.r11.s64 = ctx.r11.s64 + 136;
	// lwz r4,12(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r3,8(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r4,r3
	// bne cr6,0x8246e6e0
	if (ctx.r4.u32 == ctx.r3.u32) {
		// lfs f12,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f12.f64 = double(temp.f32);
		// stw r10,8(r11)
		PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
		// stfs f12,4(r11)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	}
loc_8246E6E0:
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lfs f12,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 0)/* phBoundCapsule::vtable@+0x0 */;
	ctx.f12.f64 = double(temp.f32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// lfs f9,4(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 4)/* phBoundCapsule::flags@+0x4 */;
	ctx.f9.f64 = double(temp.f32);
	// ori r5,r6,244
	ctx.r5.u64 = ctx.r6.u64 | 244;
	// fneg f13,f0
	ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lfs f8,20(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 20);
	ctx.f8.f64 = double(temp.f32);
	// ori r3,r4,256
	ctx.r3.u64 = ctx.r4.u64 | 256;
	// ori r4,r6,268
	ctx.r4.u64 = ctx.r6.u64 | 268;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// stfsx f12,r31,r5
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + ctx.r5.u32, temp.u32);
	// ori r28,r6,2872
	var_r28 = (uint32_t)(ctx.r6.u64 | 2872);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfsx f9,r31,r3
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + ctx.r3.u32, temp.u32);
	// stfsx f25,r31,r4
	temp.f32 = float(var_f25);
	PPC_STORE_U32(var_r31 + ctx.r4.u32, temp.u32);
	// addis r10,r31,2
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// ori r27,r6,2884
	var_r27 = (uint32_t)(ctx.r6.u64 | 2884);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// addi r11,r11,2332
	ctx.r11.s64 = ctx.r11.s64 + 2332;
	// ori r26,r6,2896
	var_r26 = (uint32_t)(ctx.r6.u64 | 2896);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// addis r9,r31,2
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// ori r4,r6,232
	ctx.r4.u64 = ctx.r6.u64 | 232;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// ori r25,r6,236
	var_r25 = (uint32_t)(ctx.r6.u64 | 236);
	// stfsx f12,r31,r28
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + var_r28, temp.u32);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfsx f8,r31,r27
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + var_r27, temp.u32);
	// lis r3,2
	ctx.r3.s64 = 131072;
	// stfsx f22,r31,r26
	temp.f32 = float(var_f22);
	PPC_STORE_U32(var_r31 + var_r26, temp.u32);
	// ori r24,r6,18364
	var_r24 = (uint32_t)(ctx.r6.u64 | 18364);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// addi r10,r10,11104
	ctx.r10.s64 = ctx.r10.s64 + 11104;
	// ori r23,r6,18376
	var_r23 = (uint32_t)(ctx.r6.u64 | 18376);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// addis r8,r31,2
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// ori r22,r6,18388
	var_r22 = (uint32_t)(ctx.r6.u64 | 18388);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addi r9,r9,12152
	ctx.r9.s64 = ctx.r9.s64 + 12152;
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// ori r21,r6,20992
	var_r21 = (uint32_t)(ctx.r6.u64 | 20992);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// ori r5,r5,14228
	ctx.r5.u64 = ctx.r5.u64 | 14228;
	// ori r20,r6,21004
	var_r20 = (uint32_t)(ctx.r6.u64 | 21004);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfs f0,8(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// ori r3,r3,14240
	ctx.r3.u64 = ctx.r3.u64 | 14240;
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// ori r19,r6,21016
	var_r19 = (uint32_t)(ctx.r6.u64 | 21016);
	// stfsx f24,r31,r4
	temp.f32 = float(var_f24);
	PPC_STORE_U32(var_r31 + ctx.r4.u32, temp.u32);
	// addis r7,r31,2
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// stfsx f21,r31,r5
	temp.f32 = float(var_f21);
	PPC_STORE_U32(var_r31 + ctx.r5.u32, temp.u32);
	// addi r8,r8,20452
	ctx.r8.s64 = ctx.r8.s64 + 20452;
	// stfsx f12,r31,r24
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + var_r24, temp.u32);
	// addis r6,r31,2
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// stfsx f23,r31,r25
	temp.f32 = float(var_f23);
	PPC_STORE_U32(var_r31 + var_r25, temp.u32);
	// addi r7,r7,29224
	ctx.r7.s64 = ctx.r7.s64 + 29224;
	// stfsx f29,r31,r3
	temp.f32 = float(var_f29);
	PPC_STORE_U32(var_r31 + ctx.r3.u32, temp.u32);
	// addi r6,r6,30272
	ctx.r6.s64 = ctx.r6.s64 + 30272;
	// stfsx f9,r31,r23
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + var_r23, temp.u32);
	// stfsx f20,r31,r22
	temp.f32 = float(var_f20);
	PPC_STORE_U32(var_r31 + var_r22, temp.u32);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// stfs f0,8(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// lis r3,2
	ctx.r3.s64 = 131072;
	// stfs f13,4(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// ori r4,r5,32348
	ctx.r4.u64 = ctx.r5.u64 | 32348;
	// stfsx f12,r31,r21
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + var_r21, temp.u32);
	// ori r11,r3,32360
	ctx.r11.u64 = ctx.r3.u64 | 32360;
	// stfsx f8,r31,r20
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + var_r20, temp.u32);
	// fneg f12,f31
	ctx.f12.u64 = ctx.f31.u64 ^ 0x8000000000000000;
	// stfsx f15,r31,r19
	temp.f32 = float(var_f15);
	PPC_STORE_U32(var_r31 + var_r19, temp.u32);
	// stfs f0,8(r7)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// stfs f13,4(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f0,8(r6)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 8, temp.u32);
	// stfs f13,4(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// stfsx f26,r31,r11
	temp.f32 = float(var_f26);
	PPC_STORE_U32(var_r31 + ctx.r11.u32, temp.u32);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// stfsx f16,r31,r4
	temp.f32 = float(var_f16);
	PPC_STORE_U32(var_r31 + ctx.r4.u32, temp.u32);
	// ori r9,r10,18352
	ctx.r9.u64 = ctx.r10.u64 | 18352;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// ori r7,r8,18356
	ctx.r7.u64 = ctx.r8.u64 | 18356;
	// lis r3,2
	ctx.r3.s64 = 131072;
	// stfsx f19,r31,r9
	temp.f32 = float(var_f19);
	PPC_STORE_U32(var_r31 + ctx.r9.u32, temp.u32);
	// ori r9,r11,40680
	ctx.r9.u64 = ctx.r11.u64 | 40680;
	// lis r11,2
	ctx.r11.s64 = 131072;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// ori r28,r11,61312
	var_r28 = (uint32_t)(ctx.r11.u64 | 61312);
	// stfsx f18,r31,r7
	temp.f32 = float(var_f18);
	PPC_STORE_U32(var_r31 + ctx.r7.u32, temp.u32);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// lis r4,2
	ctx.r4.s64 = 131072;
	// ori r27,r11,61316
	var_r27 = (uint32_t)(ctx.r11.u64 | 61316);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// ori r5,r6,40620
	ctx.r5.u64 = ctx.r6.u64 | 40620;
	// ori r26,r11,61320
	var_r26 = (uint32_t)(ctx.r11.u64 | 61320);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// ori r6,r8,40648
	ctx.r6.u64 = ctx.r8.u64 | 40648;
	// ori r25,r11,61348
	var_r25 = (uint32_t)(ctx.r11.u64 | 61348);
	// lis r11,2
	ctx.r11.s64 = 131072;
	// ori r10,r3,40644
	ctx.r10.u64 = ctx.r3.u64 | 40644;
	// ori r24,r11,61352
	var_r24 = (uint32_t)(ctx.r11.u64 | 61352);
	// lis r11,3
	ctx.r11.s64 = 196608;
	// ori r3,r4,40652
	ctx.r3.u64 = ctx.r4.u64 | 40652;
	// ori r23,r11,12252
	var_r23 = (uint32_t)(ctx.r11.u64 | 12252);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lis r4,2
	ctx.r4.s64 = 131072;
	// ori r7,r8,40684
	ctx.r7.u64 = ctx.r8.u64 | 40684;
	// ori r8,r4,57120
	ctx.r8.u64 = ctx.r4.u64 | 57120;
	// stfs f31,8(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r11,104(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// ori r4,r4,61288
	ctx.r4.u64 = ctx.r4.u64 | 61288;
	// stfs f31,8(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r11,108(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// stfsx f1,r31,r7
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + ctx.r7.u32, temp.u32);
	// stfsx f6,r31,r5
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + ctx.r5.u32, temp.u32);
	// stfsx f5,r31,r10
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r31 + ctx.r10.u32, temp.u32);
	// lis r10,3
	ctx.r10.s64 = 196608;
	// stfsx f4,r31,r6
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r31 + ctx.r6.u32, temp.u32);
	// lis r6,4
	ctx.r6.s64 = 262144;
	// stfsx f3,r31,r3
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + ctx.r3.u32, temp.u32);
	// stfsx f2,r31,r9
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r31 + ctx.r9.u32, temp.u32);
	// ori r9,r10,57468
	ctx.r9.u64 = ctx.r10.u64 | 57468;
	// stfsx f11,r31,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + ctx.r8.u32, temp.u32);
	// lis r8,3
	ctx.r8.s64 = 196608;
	// stfs f31,8(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// ori r5,r6,37148
	ctx.r5.u64 = ctx.r6.u64 | 37148;
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// ori r7,r8,57472
	ctx.r7.u64 = ctx.r8.u64 | 57472;
	// stfs f31,8(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r11,116(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stfsx f6,r31,r4
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + ctx.r4.u32, temp.u32);
	// lis r4,4
	ctx.r4.s64 = 262144;
	// stfsx f1,r31,r24
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + var_r24, temp.u32);
	// fmr f1,f30
	ctx.f1.f64 = var_f30;
	// stfsx f5,r31,r28
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r31 + var_r28, temp.u32);
	// ori r3,r4,37152
	ctx.r3.u64 = ctx.r4.u64 | 37152;
	// stfsx f4,r31,r27
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r31 + var_r27, temp.u32);
	// stfsx f3,r31,r26
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + var_r26, temp.u32);
	// stfsx f2,r31,r25
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r31 + var_r25, temp.u32);
	// stfsx f11,r31,r23
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + var_r23, temp.u32);
	// stfs f31,8(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lwz r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stfsx f10,r31,r9
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + ctx.r9.u32, temp.u32);
	// stfsx f10,r31,r7
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + ctx.r7.u32, temp.u32);
	// stfs f31,8(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfsx f7,r31,r5
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + ctx.r5.u32, temp.u32);
	// stfsx f7,r31,r3
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + ctx.r3.u32, temp.u32);
	// lis r11,-32248
	// lfs f11,68(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 68);
	ctx.f11.f64 = double(temp.f32);
	// lfd f29,-25760(r11)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25760);
	// fmul f2,f11,f29
	ctx.f2.f64 = ctx.f11.f64 * var_f29;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// lis r11,2
	ctx.r11.s64 = 131072;
	// fmr f1,f30
	ctx.f1.f64 = var_f30;
	// ori r10,r11,40716
	ctx.r10.u64 = ctx.r11.u64 | 40716;
	// frsp f10,f0
	ctx.f10.f64 = double(float(ctx.f0.f64));
	// fmuls f9,f10,f28
	ctx.f9.f64 = double(float(ctx.f10.f64 * var_f28));
	// stfsx f9,r31,r10
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + ctx.r10.u32, temp.u32);
	// lfs f8,68(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 68);
	ctx.f8.f64 = double(temp.f32);
	// fmul f2,f8,f29
	ctx.f2.f64 = ctx.f8.f64 * var_f29;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// fmr f1,f30
	ctx.f1.f64 = var_f30;
	// ori r8,r9,61384
	ctx.r8.u64 = ctx.r9.u64 | 61384;
	// frsp f7,f0
	ctx.f7.f64 = double(float(ctx.f0.f64));
	// fmuls f6,f7,f27
	ctx.f6.f64 = double(float(ctx.f7.f64 * var_f27));
	// stfsx f6,r31,r8
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + ctx.r8.u32, temp.u32);
	// lfs f5,68(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 68);
	ctx.f5.f64 = double(temp.f32);
	// fmul f2,f5,f29
	ctx.f2.f64 = ctx.f5.f64 * var_f29;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// lis r7,3
	ctx.r7.s64 = 196608;
	// fmr f1,f30
	ctx.f1.f64 = var_f30;
	// ori r6,r7,41060
	ctx.r6.u64 = ctx.r7.u64 | 41060;
	// frsp f4,f0
	ctx.f4.f64 = double(float(ctx.f0.f64));
	// fmuls f3,f4,f17
	ctx.f3.f64 = double(float(ctx.f4.f64 * var_f17));
	// stfsx f3,r31,r6
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + ctx.r6.u32, temp.u32);
	// lfs f2,68(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 68);
	ctx.f2.f64 = double(temp.f32);
	// fmul f2,f2,f29
	ctx.f2.f64 = ctx.f2.f64 * var_f29;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// fmr f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f1.f64;
	// lis r5,4
	ctx.r5.s64 = 262144;
	// fmr f1,f30
	ctx.f1.f64 = var_f30;
	// ori r4,r5,20740
	ctx.r4.u64 = ctx.r5.u64 | 20740;
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// fmuls f13,f0,f14
	ctx.f13.f64 = double(float(ctx.f0.f64 * var_f14));
	// stfsx f13,r31,r4
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + ctx.r4.u32, temp.u32);
	// lfs f12,64(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 64);
	ctx.f12.f64 = double(temp.f32);
	// fmul f2,f12,f29
	ctx.f2.f64 = ctx.f12.f64 * var_f29;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// lfs f0,20(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 20);
	ctx.f0.f64 = double(temp.f32);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(var_f31 * ctx.f0.f64));
	// addis r10,r31,5
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addi r11,r11,-28368
	ctx.r11.s64 = ctx.r11.s64 + -28368;
	// frsp f11,f1
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// stfs f11,96(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 96, temp.u32);
	// addis r9,r31,5
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addi r10,r10,-26296
	ctx.r10.s64 = ctx.r10.s64 + -26296;
	// addi r9,r9,-24224
	ctx.r9.s64 = ctx.r9.s64 + -24224;
	// addis r30,r31,2
	var_r30 = (uint32_t)(var_r31 + 131072);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// addi r30,r30,168
	var_r30 = (uint32_t)(var_r30 + 168);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// fneg f13,f0
	ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// stfs f0,8(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r11,r11,-22152
	ctx.r11.s64 = ctx.r11.s64 + -22152;
	// stfs f0,8(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f10,52(r29)
	temp.u32 = PPC_LOAD_U32(var_r29 + 52);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,4(r30)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r30 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f9,56(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 56);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,8(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f8,60(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 60);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,12(r30)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// lfs f7,52(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 52);
	ctx.f7.f64 = double(temp.f32);
	// addis r30,r31,2
	var_r30 = (uint32_t)(var_r31 + 131072);
	// addi r30,r30,200
	var_r30 = (uint32_t)(var_r30 + 200);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// stfs f7,4(r30)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r30 + 4,/* phBoundCapsule::flags@+0x4 */ temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f6,56(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 56);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,8(r30)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r30 + 8, temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lfs f5,60(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r29 + 60);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,12(r30)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r30 + 12, temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// lwz r11,48(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 48);
	// cmplwi cr6,r11,0
	// bne cr6,0x8246eaf4
	if (ctx.r11.u32 == 0) {
		// li r10,1
		ctx.r10.s64 = 1;
		// b 0x8246eb28
	} else {
	loc_8246EAF4:
		// clrldi r3,r11,32
		ctx.r3.u64 = ctx.r11.u64 & 0xFFFFFFFF;
		// lfs f4,88(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 88);
		ctx.f4.f64 = double(temp.f32);
		// lfs f13,96(r1)
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
		ctx.f13.f64 = double(temp.f32);
		// addi r11,r1,120
		ctx.r11.s64 = ctx.r1.s64 + 120;
		// std r3,120(r1)
		PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r3.u64);
		// lfd f3,120(r1)
		ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
		// fcfid f2,f3
		ctx.f2.f64 = double(ctx.f3.s64);
		// frsp f1,f2
		ctx.f1.f64 = double(float(ctx.f2.f64));
		// fmuls f0,f1,f4
		ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
		// fmuls f12,f0,f13
		ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
		// fctidz f11,f12
		ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
		// stfiwx f11,0,r11
		PPC_STORE_U32(ctx.r11.u32, ctx.f11.u32);
		// lwz r10,120(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	}
loc_8246EB28:
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addi r11,r11,-20080
	ctx.r11.s64 = ctx.r11.s64 + -20080;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r9,r8
	// bne cr6,0x8246eb50
	if (ctx.r9.u32 == ctx.r8.u32) {
		// lfs f10,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f10.f64 = double(temp.f32);
		// stw r10,8(r11)
		PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
		// stfs f10,4(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	}
loc_8246EB50:
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addi r11,r11,-19024
	ctx.r11.s64 = ctx.r11.s64 + -19024;
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r7,r6
	// bne cr6,0x8246eb78
	if (ctx.r7.u32 == ctx.r6.u32) {
		// lfs f9,80(r1)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		ctx.f9.f64 = double(temp.f32);
		// stw r10,8(r11)
		PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
		// stfs f9,4(r11)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	}
loc_8246EB78:
	// addi r1,r1,432
	ctx.r1.s64 = ctx.r1.s64 + 432;
	// addi r12,r1,-152
	ctx.r12.s64 = ctx.r1.s64 + -152;
	// bl 0x8243662c
	__restfpr_14(ctx, base);
	// b 0x8242f8b0
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__atSingleton_EB88"))) PPC_WEAK_FUNC(atSingleton_EB88);
PPC_FUNC_IMPL(__imp__atSingleton_EB88) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	double var_f29 = 0.0;
	double var_f28 = 0.0;
	double var_f26 = 0.0;
	double var_f25 = 0.0;
	double var_f30 = 0.0;
	double var_f24 = 0.0;
	double var_f23 = 0.0;
	double var_f21 = 0.0;
	double var_f22 = 0.0;
	double var_f20 = 0.0;
	double var_f19 = 0.0;
	double var_f27 = 0.0;
	double var_f31 = 0.0;
	double var_f18 = 0.0;
	double var_f17 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f878
	ctx.lr = 0x8246EB90;
	__savegprlr_20(ctx, base);
	// addi r12,r1,-104
	ctx.r12.s64 = ctx.r1.s64 + -104;
	// bl 0x824365ec
	__savefpr_17(ctx, base);
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r25,-32256
	var_r25 = (uint32_t)(-2113929216);
	// lis r30,-32256
	var_r30 = (uint32_t)(-2113929216);
	// li r10,5
	ctx.r10.s64 = 5;
	// lfs f12,23972(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23972);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r9,r11,22704
	ctx.r9.s64 = ctx.r11.s64 + 22704;
	// lfs f9,23980(r25)
	temp.u32 = PPC_LOAD_U32(var_r25 + 23980);
	ctx.f9.f64 = double(temp.f32);
	// lis r11,-32256
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r8,4
	ctx.r8.s64 = 4;
	// fmr f10,f9
	ctx.f10.f64 = ctx.f9.f64;
	// lis r24,-32256
	var_r24 = (uint32_t)(-2113929216);
	// stw r9,0(r31)
	PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r9.u32);
	// lis r9,-32248
	// lfs f13,23984(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23984);
	ctx.f13.f64 = double(temp.f32);
	// li r11,8
	ctx.r11.s64 = 8;
	// stfs f12,56(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 56, temp.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r10.u32);
	// lfs f11,23988(r24)
	temp.u32 = PPC_LOAD_U32(var_r24 + 23988);
	ctx.f11.f64 = double(temp.f32);
	// stw r10,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
	// stfs f0,60(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 60, temp.u32);
	// stw r7,12(r31)
	PPC_STORE_U32(var_r31 + 12, ctx.r7.u32);
	// lfd f29,-25856(r9)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25856);
	// lis r9,-32248
	// stfs f0,64(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 64, temp.u32);
	// stw r7,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r7.u32);
	// stfs f0,68(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 68, temp.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// stfs f0,72(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 72, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(var_r31 + 32, ctx.r11.u32);
	// stfs f9,76(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 76, temp.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(var_r31 + 36, ctx.r11.u32);
	// lfd f28,-25752(r9)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25752);
	// stw r8,40(r31)
	PPC_STORE_U32(var_r31 + 40, ctx.r8.u32);
	// stfs f13,80(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// stw r11,44(r31)
	PPC_STORE_U32(var_r31 + 44, ctx.r11.u32);
	// stfs f13,84(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 84, temp.u32);
	// stw r8,48(r31)
	PPC_STORE_U32(var_r31 + 48, ctx.r8.u32);
	// stw r10,52(r31)
	PPC_STORE_U32(var_r31 + 52, ctx.r10.u32);
	// fmr f2,f29
	ctx.f2.f64 = var_f29;
	// stfs f11,88(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 88, temp.u32);
	// fmr f1,f28
	ctx.f1.f64 = var_f28;
	// stfs f10,92(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 92, temp.u32);
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// addi r11,r31,104
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 104;
	// frsp f8,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f1.f64));
	// li r29,0
	var_r29 = 0;
	// stfs f8,96(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 96, temp.u32);
	// li r28,40
	var_r28 = 40;
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// lfs f13,23988(r24)
	temp.u32 = PPC_LOAD_U32(var_r24 + 23988);
	ctx.f13.f64 = double(temp.f32);
	// stw r29,100(r31)
	PPC_STORE_U32(var_r31 + 100, var_r29);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r28,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, var_r28);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r28,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r28);
	// stw r28,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r28);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246da18
	atSingleton_DA18_2h(ctx, base);
	// addis r11,r31,1
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 65536;
	// lfs f13,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,23988(r24)
	temp.u32 = PPC_LOAD_U32(var_r24 + 23988);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,136
	ctx.r11.s64 = ctx.r11.s64 + 136;
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r28,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, var_r28);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r28,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r28);
	// stw r28,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r28);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246da18
	atSingleton_DA18_2h(ctx, base);
	// lfs f13,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 88);
	ctx.f13.f64 = double(temp.f32);
	// addis r3,r31,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// lis r26,-32256
	var_r26 = (uint32_t)(-2113929216);
	// addi r3,r3,168
	ctx.r3.s64 = ctx.r3.s64 + 168;
	// lfs f12,27200(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 27200);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f13,f12
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// stfs f13,0(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ temp.u32);
	// stfs f7,4(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stfs f0,8(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f0,12(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f0,24(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f0,28(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// addis r3,r31,2
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f13,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,27200(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 27200);
	ctx.f12.f64 = double(temp.f32);
	// addi r3,r3,200
	ctx.r3.s64 = ctx.r3.s64 + 200;
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f13,f12
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// stfs f13,0(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ temp.u32);
	// stfs f6,4(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stfs f0,8(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f0,12(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f0,24(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f0,28(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// bl 0x8246d8b0
	phBoundCapsule_D8B0_g(ctx, base);
	// lis r11,-32256
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,2
	ctx.r9.s64 = 131072;
	// fmr f5,f0
	ctx.f5.f64 = ctx.f0.f64;
	// addi r28,r11,23992
	var_r28 = (uint32_t)(ctx.r11.s64 + 23992);  // lbl_82005DB8 @ 0x82005db8
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// lis r11,2
	ctx.r11.s64 = 131072;
	// ori r8,r9,236
	ctx.r8.u64 = ctx.r9.u64 | 236;
	// ori r10,r11,232
	ctx.r10.u64 = ctx.r11.u64 | 232;
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// li r20,509
	var_r20 = 509;
	// lfs f26,44(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 44);
	var_f26 = double(temp.f32);
	// addi r11,r11,240
	ctx.r11.s64 = ctx.r11.s64 + 240;
	// lfs f25,48(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 48);
	var_f25 = double(temp.f32);
	// li r21,508
	var_r21 = 508;
	// stfsx f25,r31,r8
	temp.f32 = float(var_f25);
	PPC_STORE_U32(var_r31 + ctx.r8.u32, temp.u32);
	// li r27,1
	var_r27 = 1;
	// stfsx f26,r31,r10
	temp.f32 = float(var_f26);
	PPC_STORE_U32(var_r31 + ctx.r10.u32, temp.u32);
	// addi r3,r11,36
	ctx.r3.s64 = ctx.r11.s64 + 36;
	// lfs f30,0(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 0);
	var_f30 = double(temp.f32);
	// lfs f24,4(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 4);
	var_f24 = double(temp.f32);
	// lfs f23,8(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 8);
	var_f23 = double(temp.f32);
	// stw r20,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r20);
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r21,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r21);
	// stfs f5,8(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r27,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, var_r27);
	// stfs f24,16(r11)
	temp.f32 = float(var_f24);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f4,20(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f23,28(r11)
	temp.f32 = float(var_f23);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// stfs f0,32(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// lis r11,-32256
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r7,83
	ctx.r7.s64 = 83;
	// lfs f21,12(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 12);
	var_f21 = double(temp.f32);
	// lfs f22,27344(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27344);
	var_f22 = double(temp.f32);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// addi r11,r11,2332
	ctx.r11.s64 = ctx.r11.s64 + 2332;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f22,4(r11)
	temp.f32 = float(var_f22);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
	// stfs f21,8(r11)
	temp.f32 = float(var_f21);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dae8
	atSingleton_DAE8_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// lfs f20,20(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 20);
	var_f20 = double(temp.f32);
	// li r6,2039
	ctx.r6.s64 = 2039;
	// addi r11,r11,2868
	ctx.r11.s64 = ctx.r11.s64 + 2868;
	// li r5,2038
	ctx.r5.s64 = 2038;
	// lis r10,-32256
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// li r22,600
	var_r22 = 600;
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// addi r3,r11,36
	ctx.r3.s64 = ctx.r11.s64 + 36;
	// stfs f20,16(r11)
	temp.f32 = float(var_f20);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// stfs f0,32(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stw r5,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r5.u32);
	// lfs f19,27204(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27204);
	var_f19 = double(temp.f32);
	// stfs f19,28(r11)
	temp.f32 = float(var_f19);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// stw r22,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, var_r22);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// lis r11,-32248
	// li r4,211
	ctx.r4.s64 = 211;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// lfs f27,27200(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 27200);
	var_f27 = double(temp.f32);
	// lfs f31,-25608(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25608);
	var_f31 = double(temp.f32);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// addi r11,r11,11104
	ctx.r11.s64 = ctx.r11.s64 + 11104;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r4.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dbb8
	atSingleton_DBB8_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r10,311
	ctx.r10.s64 = 311;
	// addi r11,r11,12152
	ctx.r11.s64 = ctx.r11.s64 + 12152;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r23,1020
	var_r23 = 1020;
	// lfs f18,36(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 36);
	var_f18 = double(temp.f32);
	// addi r11,r11,14224
	ctx.r11.s64 = ctx.r11.s64 + 14224;
	// lfs f17,40(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 40);
	var_f17 = double(temp.f32);
	// fmr f3,f0
	ctx.f3.f64 = ctx.f0.f64;
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// fmr f2,f0
	ctx.f2.f64 = ctx.f0.f64;
	// stfs f18,4(r11)
	temp.f32 = float(var_f18);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r23,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r23);
	// stfs f3,8(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r27,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r27);
	// stfs f17,16(r11)
	temp.f32 = float(var_f17);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f2,20(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dc20
	atSingleton_DC20_2h(ctx, base);
	// lis r9,2
	ctx.r9.s64 = 131072;
	// lfs f1,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f1.f64 = double(temp.f32);
	// lis r7,2
	ctx.r7.s64 = 131072;
	// fmr f13,f1
	ctx.f13.f64 = ctx.f1.f64;
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// fmr f0,f1
	ctx.f0.f64 = ctx.f1.f64;
	// ori r8,r9,18352
	ctx.r8.u64 = ctx.r9.u64 | 18352;
	// ori r6,r7,18356
	ctx.r6.u64 = ctx.r7.u64 | 18356;
	// addi r11,r11,18360
	ctx.r11.s64 = ctx.r11.s64 + 18360;
	// addi r3,r11,36
	ctx.r3.s64 = ctx.r11.s64 + 36;
	// stfsx f26,r31,r8
	temp.f32 = float(var_f26);
	PPC_STORE_U32(var_r31 + ctx.r8.u32, temp.u32);
	// stfsx f25,r31,r6
	temp.f32 = float(var_f25);
	PPC_STORE_U32(var_r31 + ctx.r6.u32, temp.u32);
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r20,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r20);
	// stfs f1,8(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r21,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r21);
	// stfs f24,16(r11)
	temp.f32 = float(var_f24);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stw r27,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, var_r27);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f23,28(r11)
	temp.f32 = float(var_f23);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// stfs f0,32(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r5,97
	ctx.r5.s64 = 97;
	// addi r11,r11,20452
	ctx.r11.s64 = ctx.r11.s64 + 20452;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f22,4(r11)
	temp.f32 = float(var_f22);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
	// stfs f21,8(r11)
	temp.f32 = float(var_f21);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dae8
	atSingleton_DAE8_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r3,1296
	ctx.r3.s64 = 1296;
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// addi r11,r11,20988
	ctx.r11.s64 = ctx.r11.s64 + 20988;
	// li r4,1297
	ctx.r4.s64 = 1297;
	// stw r3,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r3.u32);
	// addi r3,r11,36
	ctx.r3.s64 = ctx.r11.s64 + 36;
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r4.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r22,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, var_r22);
	// stfs f20,16(r11)
	temp.f32 = float(var_f20);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f19,28(r11)
	temp.f32 = float(var_f19);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// stfs f12,32(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 32, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r10,223
	ctx.r10.s64 = 223;
	// addi r11,r11,29224
	ctx.r11.s64 = ctx.r11.s64 + 29224;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dbb8
	atSingleton_DBB8_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r9,293
	ctx.r9.s64 = 293;
	// addi r11,r11,30272
	ctx.r11.s64 = ctx.r11.s64 + 30272;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,2
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 131072;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,32344
	ctx.r11.s64 = ctx.r11.s64 + 32344;
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f18,4(r11)
	temp.f32 = float(var_f18);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r23,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r23);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r27,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r27);
	// stfs f17,16(r11)
	temp.f32 = float(var_f17);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dc20
	atSingleton_DC20_2h(ctx, base);
	// lis r11,-32256
	// lfs f0,76(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r28 + 76);
	ctx.f0.f64 = double(temp.f32);
	// li r8,409
	ctx.r8.s64 = 409;
	// lfs f13,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,27340(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27340);
	var_f30 = double(temp.f32);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addi r11,r11,-29064
	ctx.r11.s64 = ctx.r11.s64 + -29064;
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// li r7,257
	ctx.r7.s64 = 257;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,-26992
	ctx.r11.s64 = ctx.r11.s64 + -26992;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r9,r31,3
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f13,52(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 52);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,-24920
	ctx.r9.s64 = ctx.r9.s64 + -24920;
	// lfs f12,56(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 56);
	ctx.f12.f64 = double(temp.f32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f11,60(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 60);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,-24896
	ctx.r11.s64 = ctx.r11.s64 + -24896;
	// lfs f10,64(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 64);
	ctx.f10.f64 = double(temp.f32);
	// addi r10,r10,-24864
	ctx.r10.s64 = ctx.r10.s64 + -24864;
	// lfs f9,68(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 68);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,72(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 72);
	ctx.f8.f64 = double(temp.f32);
	// fmr f2,f29
	ctx.f2.f64 = var_f29;
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stw r27,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, var_r27);
	// stfs f0,8(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// fmr f1,f28
	ctx.f1.f64 = var_f28;
	// stfs f0,12(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// stfs f0,16(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// stfs f0,20(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stfs f0,28(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// stw r27,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, var_r27);
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f9,8(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f8,12(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f0,16(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// stfs f0,20(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// stfs f0,24(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 24, temp.u32);
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// li r10,3432
	ctx.r10.s64 = 3432;
	// lfs f13,23980(r25)
	temp.u32 = PPC_LOAD_U32(var_r25 + 23980);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-24836
	ctx.r11.s64 = ctx.r11.s64 + -24836;
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lfs f13,84(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 84);
	ctx.f13.f64 = double(temp.f32);
	// stw r10,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f10,16(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dc88
	atSingleton_DC88_2h(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// fmr f13,f20
	ctx.f13.f64 = var_f20;
	// addi r11,r11,-8420
	ctx.r11.s64 = ctx.r11.s64 + -8420;
	// addi r10,r10,-8396
	ctx.r10.s64 = ctx.r10.s64 + -8396;
	// li r6,383
	ctx.r6.s64 = 383;
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// lfs f12,76(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 76);
	ctx.f12.f64 = double(temp.f32);
	// stfs f30,4(r10)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f12,8(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f0,12(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r5,233
	ctx.r5.s64 = 233;
	// fmr f13,f27
	ctx.f13.f64 = var_f27;
	// addi r11,r11,-6324
	ctx.r11.s64 = ctx.r11.s64 + -6324;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,52(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 52);
	ctx.f13.f64 = double(temp.f32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// addi r11,r11,-4252
	ctx.r11.s64 = ctx.r11.s64 + -4252;
	// lfs f12,60(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 60);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r10,-4196
	ctx.r10.s64 = ctx.r10.s64 + -4196;
	// lfs f11,64(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 64);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,68(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 68);
	ctx.f10.f64 = double(temp.f32);
	// fmr f2,f29
	ctx.f2.f64 = var_f29;
	// lfs f9,72(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 72);
	ctx.f9.f64 = double(temp.f32);
	// fmr f1,f28
	ctx.f1.f64 = var_f28;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f13,56(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 56);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-4228
	ctx.r11.s64 = ctx.r11.s64 + -4228;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// stfs f12,8(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f11,12(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stfs f0,28(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 28, temp.u32);
	// stw r27,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, var_r27);
	// stfs f0,4(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f10,8(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f9,12(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stfs f0,16(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// stfs f0,20(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// stfs f0,24(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 24, temp.u32);
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// frsp f9,f1
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// li r10,3820
	ctx.r10.s64 = 3820;
	// lfs f13,23980(r25)
	temp.u32 = PPC_LOAD_U32(var_r25 + 23980);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-4168
	ctx.r11.s64 = ctx.r11.s64 + -4168;
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lfs f13,84(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 84);
	ctx.f13.f64 = double(temp.f32);
	// stw r10,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f8,16(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246dc88
	atSingleton_DC88_2h(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addis r10,r31,3
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// fmr f13,f20
	ctx.f13.f64 = var_f20;
	// addi r11,r11,12248
	ctx.r11.s64 = ctx.r11.s64 + 12248;
	// lfs f27,76(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 76);
	var_f27 = double(temp.f32);
	// addi r10,r10,12272
	ctx.r10.s64 = ctx.r10.s64 + 12272;
	// fmr f7,f0
	ctx.f7.f64 = ctx.f0.f64;
	// li r4,1511
	ctx.r4.s64 = 1511;
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r4,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r4.u32);
	// stfs f30,4(r10)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f27,8(r10)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f7,12(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r10,1061
	ctx.r10.s64 = 1061;
	// addi r11,r11,20488
	ctx.r11.s64 = ctx.r11.s64 + 20488;
	// addi r3,r11,20
	ctx.r3.s64 = ctx.r11.s64 + 20;
	// stfs f27,4(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f30,12(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,3
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 196608;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r9,853
	ctx.r9.s64 = 853;
	// addi r11,r11,28708
	ctx.r11.s64 = ctx.r11.s64 + 28708;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r8,541
	ctx.r8.s64 = 541;
	// lfs f13,27200(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 27200);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-28612
	ctx.r11.s64 = ctx.r11.s64 + -28612;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dc20
	atSingleton_DC20_2h(ctx, base);
	// fmr f2,f29
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = var_f29;
	// fmr f1,f28
	ctx.f1.f64 = var_f28;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// li r10,1510
	ctx.r10.s64 = 1510;
	// lfs f13,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-24492
	ctx.r11.s64 = ctx.r11.s64 + -24492;
	// lfs f0,23980(r25)
	temp.u32 = PPC_LOAD_U32(var_r25 + 23980);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lfs f13,84(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 84);
	ctx.f13.f64 = double(temp.f32);
	// stw r10,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// stfs f5,16(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dc88
	atSingleton_DC88_2h(ctx, base);
	// lis r10,-32256
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// li r7,1657
	ctx.r7.s64 = 1657;
	// addi r11,r11,-8076
	ctx.r11.s64 = ctx.r11.s64 + -8076;
	// lfs f13,27324(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27324);
	ctx.f13.f64 = double(temp.f32);
	// addis r10,r31,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// addi r10,r10,-8048
	ctx.r10.s64 = ctx.r10.s64 + -8048;
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// stfs f30,4(r10)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f27,8(r10)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f0,12(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r6,1103
	ctx.r6.s64 = 1103;
	// addi r11,r11,168
	ctx.r11.s64 = ctx.r11.s64 + 168;
	// addi r3,r11,20
	ctx.r3.s64 = ctx.r11.s64 + 20;
	// stfs f27,4(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f30,12(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r5,887
	ctx.r5.s64 = 887;
	// addi r11,r11,8388
	ctx.r11.s64 = ctx.r11.s64 + 8388;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f30,4(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
	// stfs f27,8(r11)
	temp.f32 = float(var_f27);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246db50
	atSingleton_DB50_2h_DB50_1(ctx, base);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r4,491
	ctx.r4.s64 = 491;
	// lfs f13,27200(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 27200);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,16604
	ctx.r11.s64 = ctx.r11.s64 + 16604;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r4.u32);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246dc20
	atSingleton_DC20_2h(ctx, base);
	// fmr f2,f29
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = var_f29;
	// fmr f1,f28
	ctx.f1.f64 = var_f28;
	// bl 0x82431308
	atSingleton_1308_g(ctx, base);
	// addis r11,r31,4
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 262144;
	// lfs f13,23980(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r25 + 23980);
	ctx.f13.f64 = double(temp.f32);
	// li r10,1438
	ctx.r10.s64 = 1438;
	// lfs f0,23976(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,20724
	ctx.r11.s64 = ctx.r11.s64 + 20724;
	// frsp f4,f1
	ctx.f4.f64 = double(float(ctx.f1.f64));
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lfs f13,84(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 84);
	ctx.f13.f64 = double(temp.f32);
	// stw r10,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// stfs f3,16(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246dc88
	atSingleton_DC88_2h(ctx, base);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// addis r10,r31,5
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,-28396
	ctx.r11.s64 = ctx.r11.s64 + -28396;
	// lfs f13,120(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 120);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r10,-28368
	ctx.r10.s64 = ctx.r10.s64 + -28368;
	// lfs f30,124(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 124);
	var_f30 = double(temp.f32);
	// lis r9,-32256
	// li r8,131
	ctx.r8.s64 = 131;
	// addi r3,r10,16
	ctx.r3.s64 = ctx.r10.s64 + 16;
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, var_r27);
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stfs f0,16(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stfs f0,24(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 24, temp.u32);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lfs f31,27336(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27336);
	var_f31 = double(temp.f32);
	// stfs f31,4(r10)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f30,8(r10)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// stfs f0,12(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// lfs f2,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f2.f64 = double(temp.f32);
	// li r7,113
	ctx.r7.s64 = 113;
	// fmr f0,f2
	ctx.f0.f64 = ctx.f2.f64;
	// addi r11,r11,-26296
	ctx.r11.s64 = ctx.r11.s64 + -26296;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
	// stfs f30,8(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f2,12(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r6,107
	ctx.r6.s64 = 107;
	// addi r11,r11,-24224
	ctx.r11.s64 = ctx.r11.s64 + -24224;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// stfs f30,8(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r5,127
	ctx.r5.s64 = 127;
	// addi r11,r11,-22152
	ctx.r11.s64 = ctx.r11.s64 + -22152;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// stfs f31,4(r11)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
	// stfs f30,8(r11)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// stfs f0,12(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// bl 0x8246da80
	atSingleton_DA80_2h(ctx, base);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// lfs f0,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f0.f64 = double(temp.f32);
	// li r28,20
	var_r28 = 20;
	// lfs f13,23988(r24)
	temp.u32 = PPC_LOAD_U32(var_r24 + 23988);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-20080
	ctx.r11.s64 = ctx.r11.s64 + -20080;
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stw r28,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, var_r28);
	// stfs f0,4(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r28,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r28);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r28,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r28);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246dbb8
	atSingleton_DBB8_2h(ctx, base);
	// addis r11,r31,5
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 327680;
	// lfs f13,23976(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r30 + 23976);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-19024
	ctx.r11.s64 = ctx.r11.s64 + -19024;
	// lfs f0,23988(r24)
	temp.u32 = PPC_LOAD_U32(var_r24 + 23988);
	ctx.f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// stw r28,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, var_r28);
	// stfs f0,20(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// stw r28,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r28);
	// stw r28,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r28);
	// stfs f0,4(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ temp.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ var_r29);
	// bl 0x8246dbb8
	atSingleton_DBB8_2h(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-104
	ctx.r12.s64 = ctx.r1.s64 + -104;
	// bl 0x82436638
	__restfpr_17(ctx, base);
	// b 0x8242f8c8
	__restgprlr_20(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundBVH_F758"))) PPC_WEAK_FUNC(phBoundBVH_F758);
PPC_FUNC_IMPL(__imp__phBoundBVH_F758) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// li r9,4
	ctx.r9.s64 = 4;
	// li r8,6
	ctx.r8.s64 = 6;
	// lwz r11,23968(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23968);
	// stw r11,48(r31)
	PPC_STORE_U32(var_r31 + 48, ctx.r11.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lwz r10,23960(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23960);  /* glob:lbl_82005D98 @ 0x82005d98 */
	// stw r10,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
	// lwz r11,23960(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23960);
	// stw r11,12(r31)
	PPC_STORE_U32(var_r31 + 12, ctx.r11.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lwz r10,23964(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23964);  /* glob:lbl_82005D9C @ 0x82005d9c */
	// stw r10,16(r31)
	PPC_STORE_U32(var_r31 + 16, ctx.r10.u32);
	// lwz r11,23964(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23964);
	// stw r11,20(r31)
	PPC_STORE_U32(var_r31 + 20, ctx.r11.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,23984(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 23984);  /* glob:lbl_82005DB0 @ 0x82005db0 */
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	// stfs f0,80(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 0);
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// lfs f31,18992(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 18992);  /* glob:lbl_82004A30 @ 0x82004a30 */
	var_f31 = double(temp.f32);
	// lis r11,-32256
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f0,80(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfs f30,15788(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);  /* glob:lbl_82003DAC @ 0x82003dac */
	var_f30 = double(temp.f32);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * var_f31));
	// stfs f11,56(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 56, temp.u32);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
	// stw r9,36(r31)
	PPC_STORE_U32(var_r31 + 36, ctx.r9.u32);
	// extsw r4,r5
	ctx.r4.s64 = ctx.r5.s32;
	// stw r8,44(r31)
	PPC_STORE_U32(var_r31 + 44, ctx.r8.u32);
	// std r4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
	// lfd f10,80(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fmuls f7,f8,f31
	ctx.f7.f64 = double(float(ctx.f8.f64 * var_f31));
	// stfs f7,60(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(var_r31 + 60, temp.u32);
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fcmpu cr6,f6,f30
	// blt cr6,0x8246f89c
	if (ctx.f6.f64 >= var_f30) {
		// bso cr6,0x8246f89c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8246F828, "bso");
		// li r3,8
		ctx.r3.s64 = 8;
		// stw r3,40(r31)
		PPC_STORE_U32(var_r31 + 40, ctx.r3.u32);
		// lfs f1,16(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 16);
		ctx.f1.f64 = double(temp.f32);
		// bl 0x82432450
		jumptable_2450(ctx, base);
		// frsp f5,f1
		ctx.fpscr.disableFlushMode();
		ctx.f5.f64 = double(float(ctx.f1.f64));
		// lis r11,-32256
		// addi r10,r1,80
		ctx.r10.s64 = ctx.r1.s64 + 80;
		// lfd f0,27360(r11)
		ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 27360);  /* glob:lbl_82006AE0 @ 0x82006ae0 */
		// fmul f4,f5,f0
		ctx.f4.f64 = ctx.f5.f64 * ctx.f0.f64;
		// fctiwz f3,f4
		ctx.f3.s64 = (ctx.f4.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f4.f64));
		// stfiwx f3,0,r10
		PPC_STORE_U32(ctx.r10.u32, ctx.f3.u32);
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// cmpwi cr6,r11,-8
		// bge cr6,0x8246f870
		if (ctx.r11.s32 < -8) {
			// li r11,-8
			// addi r11,r11,8
			ctx.r11.s64 = ctx.r11.s64 + 8;
			// b 0x8246f884
		} else {
		loc_8246F870:
			// cmpwi cr6,r11,0
			// bge cr6,0x8246f880
			if (ctx.r11.s32 < 0) {
				// addi r11,r11,8
				ctx.r11.s64 = ctx.r11.s64 + 8;
				// b 0x8246f884
			} else {
			loc_8246F880:
				// li r11,8
				ctx.r11.s64 = 8;
			}
		}
	loc_8246F884:
		// stw r11,32(r31)
		PPC_STORE_U32(var_r31 + 32, ctx.r11.u32);
		// lfs f2,12(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 12);
		ctx.f2.f64 = double(temp.f32);
		// lfs f1,16(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 16);
		ctx.f1.f64 = double(temp.f32);
		// fmuls f0,f2,f1
		ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
		// stfs f0,72(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 72, temp.u32);
		// b 0x8246f900
	} else {
	loc_8246F89C:
		// li r9,8
		ctx.r9.s64 = 8;
		// stw r9,32(r31)
		PPC_STORE_U32(var_r31 + 32, ctx.r9.u32);
		// lfs f1,16(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 16);
		ctx.f1.f64 = double(temp.f32);
		// bl 0x82432450
		jumptable_2450(ctx, base);
		// frsp f13,f1
		ctx.fpscr.disableFlushMode();
		ctx.f13.f64 = double(float(ctx.f1.f64));
		// lis r11,-32248
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// lfd f0,-24520(r11)
		ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -24520);
		// fmul f12,f13,f0
		ctx.f12.f64 = ctx.f13.f64 * ctx.f0.f64;
		// fctiwz f11,f12
		ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
		// stfiwx f11,0,r8
		PPC_STORE_U32(ctx.r8.u32, ctx.f11.u32);
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// cmpwi cr6,r11,-8
		// bge cr6,0x8246f8e0
		if (ctx.r11.s32 < -8) {
			// li r11,-8
			// addi r11,r11,8
			ctx.r11.s64 = ctx.r11.s64 + 8;
			// b 0x8246f8f4
		} else {
		loc_8246F8E0:
			// cmpwi cr6,r11,0
			// bge cr6,0x8246f8f0
			if (ctx.r11.s32 < 0) {
				// addi r11,r11,8
				ctx.r11.s64 = ctx.r11.s64 + 8;
				// b 0x8246f8f4
			} else {
			loc_8246F8F0:
				// li r11,8
				ctx.r11.s64 = 8;
			}
		}
	loc_8246F8F4:
		// stw r11,40(r31)
		PPC_STORE_U32(var_r31 + 40, ctx.r11.u32);
		// lfs f10,12(r30)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r30 + 12);
		ctx.f10.f64 = double(temp.f32);
		// stfs f10,72(r31)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r31 + 72, temp.u32);
	}
loc_8246F900:
	// lwz r7,20(r30)
	ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 20);
	// lis r11,-32256
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f9,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// fmuls f6,f7,f31
	ctx.f6.f64 = double(float(ctx.f7.f64 * var_f31));
	// stfs f6,64(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r31 + 64, temp.u32);
	// lwz r5,23952(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23952);
	// lis r11,-32256
	// lfs f5,24(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 24);
	ctx.f5.f64 = double(temp.f32);
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f4,80(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// lfs f12,27352(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27352);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f5,f12
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// frsp f13,f3
	ctx.f13.f64 = double(float(ctx.f3.f64));
	// fcmpu cr6,f0,f13
	// blt cr6,0x8246f958
	if (ctx.f0.f64 >= ctx.f13.f64) {
		// bso cr6,0x8246f958
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8246F950, "bso");
		// fsubs f0,f13,f30
		ctx.f0.f64 = double(float(ctx.f13.f64 - var_f30));
	}
loc_8246F958:
	// fcmpu cr6,f0,f30
	ctx.fpscr.disableFlushMode();
	// bgt cr6,0x8246f968
	if (ctx.f0.f64 <= var_f30) {
		// bso cr6,0x8246f968
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8246F960, "bso");
		// fmr f0,f30
		ctx.f0.f64 = var_f30;
	}
loc_8246F968:
	// fctidz f2,f0
	ctx.fpscr.disableFlushMode();
	ctx.f2.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f2,0,r31
	PPC_STORE_U32(var_r31, ctx.f2.u32);
	// lwz r4,28(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 28);
	// lis r11,-32256
	// extsw r3,r4
	ctx.r3.s64 = ctx.r4.s32;
	// std r3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r3.u64);
	// lfd f1,80(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f1
	ctx.f0.f64 = double(ctx.f1.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// fmuls f11,f13,f31
	ctx.f11.f64 = double(float(ctx.f13.f64 * var_f31));
	// stfs f11,68(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 68, temp.u32);
	// lwz r11,23956(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 23956);
	// lfs f10,32(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 32);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f0,f10,f12
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f9,80(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f13,f8
	ctx.f13.f64 = double(float(ctx.f8.f64));
	// fcmpu cr6,f0,f13
	// blt cr6,0x8246f9c0
	if (ctx.f0.f64 >= ctx.f13.f64) {
		// bso cr6,0x8246f9c0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8246F9B8, "bso");
		// fsubs f0,f13,f30
		ctx.f0.f64 = double(float(ctx.f13.f64 - var_f30));
	}
loc_8246F9C0:
	// addi r10,r31,4
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 4;
	// fctidz f7,f0
	ctx.fpscr.disableFlushMode();
	ctx.f7.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f0.f64));
	// lis r11,-32256
	// stfiwx f7,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f7.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f6,36(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 36);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,27348(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27348);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f5.f64));
	// stfiwx f4,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f4.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r11,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r11.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// lfs f3,40(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 40);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,76(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 76, temp.u32);
	// lfs f2,44(r30)
	temp.u32 = PPC_LOAD_U32(var_r30 + 44);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,52(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r31 + 52, temp.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f30,-40(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_FA28_p45"))) PPC_WEAK_FUNC(phBoundBVH_FA28_p45);
PPC_FUNC_IMPL(__imp__phBoundBVH_FA28_p45) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=192, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r4,12
	// bgt cr6,0x8246fb40
	if (!(ctx.r4.u32 > 12)) {
		// lis r12,-32185
		// addi r12,r12,-1444
		ctx.r12.s64 = ctx.r12.s64 + -1444;
		// rlwinm r0,r4,2,0,29
		ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r0,r12,r0
		ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
		// mtctr r0
		ctx.ctr.u64 = ctx.r0.u64;
		// bctr
		switch (ctx.r4.u64) {
		case 0:
		goto loc_8246FA90;
		case 1:
		goto loc_8246FAB4;
		case 2:
		goto loc_8246FAC0;
		case 3:
		goto loc_8246FACC;
		case 4:
		goto loc_8246FAD8;
		case 5:
		goto loc_8246FAE4;
		case 6:
		goto loc_8246FAF0;
		case 7:
		goto loc_8246FAFC;
		case 8:
		goto loc_8246FB08;
		case 9:
		goto loc_8246FB14;
		case 10:
		goto loc_8246FB20;
		case 11:
		goto loc_8246FB2C;
		case 12:
		goto loc_8246FB38;
		default:
		__builtin_trap(); // Switch case out of range
		}
		loc_8246FA90:
		// addi r11,r31,4
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 4;
		// li r10,12
		ctx.r10.s64 = 12;
		// mtctr r10
		ctx.ctr.u64 = ctx.r10.u64;
		loc_8246FA9C:
		// lwz r10,0(r5)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
		// addi r5,r5,4
		ctx.r5.s64 = ctx.r5.s64 + 4;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// bdnz 0x8246fa9c
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_8246FA9C;
	} else {
		loc_8246FAB4:
		// lwz r9,0(r5)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
		// stw r9,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r9.u32);
		loc_8246FAC0:
		// lwz r8,4(r5)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
		// stw r8,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
		loc_8246FACC:
		// lfs f0,8(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,12(r31)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(var_r31 + 12, temp.u32);
		loc_8246FAD8:
		// lfs f13,12(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 12);
		ctx.f13.f64 = double(temp.f32);
		// stfs f13,16(r31)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(var_r31 + 16, temp.u32);
		loc_8246FAE4:
		// lfs f12,16(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16);
		ctx.f12.f64 = double(temp.f32);
		// stfs f12,20(r31)
		temp.f32 = float(ctx.f12.f64);
		PPC_STORE_U32(var_r31 + 20, temp.u32);
		loc_8246FAF0:
		// lwz r7,20(r5)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 20);
		// stw r7,24(r31)
		PPC_STORE_U32(var_r31 + 24, ctx.r7.u32);
		loc_8246FAFC:
		// lfs f11,24(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 24);
		ctx.f11.f64 = double(temp.f32);
		// stfs f11,28(r31)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(var_r31 + 28, temp.u32);
		loc_8246FB08:
		// lwz r6,28(r5)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 28);
		// stw r6,32(r31)
		PPC_STORE_U32(var_r31 + 32, ctx.r6.u32);
		loc_8246FB14:
		// lfs f10,32(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
		ctx.f10.f64 = double(temp.f32);
		// stfs f10,36(r31)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(var_r31 + 36, temp.u32);
		loc_8246FB20:
		// lfs f9,36(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 36);
		ctx.f9.f64 = double(temp.f32);
		// stfs f9,40(r31)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(var_r31 + 40, temp.u32);
		loc_8246FB2C:
		// lfs f8,40(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 40);
		ctx.f8.f64 = double(temp.f32);
		// stfs f8,44(r31)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(var_r31 + 44, temp.u32);
		loc_8246FB38:
		// lfs f7,44(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 44);
		ctx.f7.f64 = double(temp.f32);
		// stfs f7,48(r31)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(var_r31 + 48, temp.u32);
	}
loc_8246FB40:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8246d830
	atSingleton_D830_p42(ctx, base);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r31,4
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 4;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + var_r31;
	// lfs f1,88(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8246f758
	phBoundBVH_F758(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r3,r11,r31
	ctx.r3.u64 = ctx.r11.u64 + var_r31;
	// bl 0x8246dd10
	phBoundCapsule_DD10_g(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_FB90_2h"))) PPC_WEAK_FUNC(atSingleton_FB90_2h);
PPC_FUNC_IMPL(__imp__atSingleton_FB90_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// li r30,0
	var_r30 = 0;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r4,0
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
	// beq cr6,0x8246fbcc
	if (ctx.r4.u32 != 0) {
		// lis r11,-32256
		// addi r3,r31,52
		ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 52;
		// addi r11,r11,27376
		ctx.r11.s64 = ctx.r11.s64 + 27376;
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0,/* atSingleton::vtable@+0x0 */ ctx.r11.u32);
		// bl 0x8246eb88
		atSingleton_EB88(ctx, base);
	}
loc_8246FBCC:
	// lis r9,-32256
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0)/* atSingleton::vtable@+0x0 */;
	// lis r11,-32256
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r10,r11,22708
	ctx.r10.s64 = ctx.r11.s64 + 22708;
	// li r11,-10000
	// lfs f13,15784(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 15784);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32256
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lfs f12,15788(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 15788);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	// stwx r10,r7,r31
	PPC_STORE_U32(ctx.r7.u32 + var_r31, ctx.r10.u32);
	// stfs f13,12(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 12, temp.u32);
	// stfs f12,16(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 16, temp.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4,/* atSingleton::flags@+0x4 */ ctx.r11.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stw r11,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r11.u32);
	// lfs f11,27200(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27200);
	ctx.f11.f64 = double(temp.f32);
	// lis r9,-32256
	// stfs f11,20(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r31 + 20, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(var_r31 + 32, ctx.r11.u32);
	// lfs f10,27372(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27372);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32256
	// stfs f10,28(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 28, temp.u32);
	// lfs f9,27320(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27320);
	ctx.f9.f64 = double(temp.f32);
	// lis r9,-32256
	// stfs f9,36(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// lfs f0,22092(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 22092);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32256
	// stfs f0,40(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f0,44(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 44, temp.u32);
	// lfs f8,27368(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27368);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,48(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_FC68_h"))) PPC_WEAK_FUNC(ph_FC68_h);
PPC_FUNC_IMPL(__imp__ph_FC68_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r3,r31,12
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 12;
	// bl 0x8246a3a0
	phInst_A3A0_p33(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8246a3a0
	phInst_A3A0_p33(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_FCA0_p45"))) PPC_WEAK_FUNC(phBoundBVH_FCA0_p45);
PPC_FUNC_IMPL(__imp__phBoundBVH_FCA0_p45) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// clrlwi r30,r4,24
	var_r30 = (uint32_t)(ctx.r4.u32 & 0xFF);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmplwi cr6,r30,0
	// beq cr6,0x8246fd48
	if (var_r30 != 0) {
		// lwz r11,0(r5)
  // [ph4a] vtable load collapsed
		// mulli r4,r30,100
		ctx.r4.s64 = static_cast<int64_t>(var_r30 * static_cast<uint64_t>(100));
		// mr r3,r5
		ctx.r3.u64 = ctx.r5.u64;
		// lwz r10,20(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r5.u32, 5, ctx, base);  // pattern-B slot 5 (byte +20)
		// cmplwi cr6,r30,0
		// stw r3,24(r31)
		PPC_STORE_U32(var_r31 + 24, ctx.r3.u32);
		// beq cr6,0x8246fd48
		if (var_r30 == 0) {
			// blr
			return;
		}
		// addi r9,r31,12
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 12;
		// li r10,0
		ctx.r10.s64 = 0;
		// mr r8,r30
		ctx.r8.u64 = var_r30;
	loc_8246FCF4:
		// lwz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// cmplwi cr6,r8,0
		ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
		// stw r11,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
		// lwz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// lwz r7,0(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// stw r7,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
		// lwz r11,8(r9)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
		// lwz r7,24(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 24);
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// addi r10,r10,100
		ctx.r10.s64 = ctx.r10.s64 + 100;
		// add r11,r11,r7
		ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
		// stw r9,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
		// lwz r6,4(r9)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
		// stw r6,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r6.u32);
		// stw r11,4(r9)
		PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
		// lwz r5,4(r11)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r11,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
		// bne cr6,0x8246fcf4
		if (!ctx.cr6.eq) goto loc_8246FCF4;
	}
loc_8246FD48:
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_FD60_2hr"))) PPC_WEAK_FUNC(atSingleton_FD60_2hr);
PPC_FUNC_IMPL(__imp__atSingleton_FD60_2hr) {
	PPC_FUNC_PROLOGUE();
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r3,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0,/* atSingleton::vtable@+0x0 */ ctx.r3.u32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// stw r3,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4,/* atSingleton::flags@+0x4 */ ctx.r3.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_FD88_w"))) PPC_WEAK_FUNC(phBoundBVH_FD88_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_FD88_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r11
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltisw v4,1
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_set1_epi32(int(0x1)));
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lis r10,-32256
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// vspltw v10,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v1,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r28,4
	var_r28 = 4;
	// vor v31,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfd f12,28072(r10)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r10.u32 + 28072);
	// lis r10,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// addi r10,r10,28112
	ctx.r10.s64 = ctx.r10.s64 + 28112;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// li r26,4
	var_r26 = 4;
	// li r7,0
	ctx.r7.s64 = 0;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// lfs f13,22700(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22700);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v30,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v9,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f8.u64);
	// vspltw v12,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// lvlx v7,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v7,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v11,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v8,v12,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vspltw v9,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// vadduwm v12,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v7,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vaddfp v11,v10,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v10,v10,v8
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vsldoi v8,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vsldoi v13,v8,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v0,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v29,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v29.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// vadduwm v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vadduwm v2,v13,v9
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vadduwm v3,v0,v9
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_8246FEF0:
	// li r10,8
	ctx.r10.s64 = 8;
loc_8246FEF4:
	// rldicl r9,r3,32,32
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// cmplwi cr6,r10,0
	// addi r4,r9,2
	ctx.r4.s64 = ctx.r9.s64 + 2;
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v1,v0,2
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 14));
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v31,v13,2
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 14));
	// vor v1,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v31,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// bne cr6,0x8246fef4
	if (ctx.r10.u32 != 0) goto loc_8246FEF4;
	// vupkhsh v9,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
	// vsrw v5,v3,v4
	ctx.v5.u32[0] = ctx.v3.u32[0] >> (ctx.v4.u8[0] & 0x1F);
	ctx.v5.u32[1] = ctx.v3.u32[1] >> (ctx.v4.u8[4] & 0x1F);
	ctx.v5.u32[2] = ctx.v3.u32[2] >> (ctx.v4.u8[8] & 0x1F);
	ctx.v5.u32[3] = ctx.v3.u32[3] >> (ctx.v4.u8[12] & 0x1F);
	// vupklsh v8,v0
	simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.s16)));
	// vsrw v12,v2,v4
	ctx.v12.u32[0] = ctx.v2.u32[0] >> (ctx.v4.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v2.u32[1] >> (ctx.v4.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v2.u32[2] >> (ctx.v4.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v2.u32[3] >> (ctx.v4.u8[12] & 0x1F);
	// vupkhsh v7,v13
	simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
	// rldicl r10,r3,32,32
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vupklsh v6,v13
	simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
	// addi r9,r5,16
	ctx.r9.s64 = ctx.r5.s64 + 16;
	// vcfsx v0,v9,15
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vcfsx v13,v8,15
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// vcfsx v9,v7,15
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// vcfsx v8,v6,15
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfux v7,v5,31
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// xor r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r11.u64;
	// vcfux v12,v12,31
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// rlwinm r4,r7,0,0,24
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// vmulfp128 v6,v0,v10
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v5,v13,v11
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v0,v9,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vsubfp v13,v8,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v9,v7,v11
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v12,v12,v10
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmaddfp v13,v13,v9,v5
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)), simde_mm_load_ps(ctx.v5.f32)));
	// vmaddfp v0,v0,v12,v6
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)), simde_mm_load_ps(ctx.v6.f32)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x8246ffb0
	if (!(ctx.cr6.eq)) {
		// li r10,128
		ctx.r10.s64 = 128;
		// dcbt r10,r11
	}
loc_8246FFB0:
	// addi r6,r6,-8
	ctx.r6.s64 = ctx.r6.s64 + -8;
	// vaddfp v11,v11,v30
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v30.f32)));
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// vaddfp v10,v10,v30
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v30.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vadduwm v3,v3,v29
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), simde_mm_load_si128((simde__m128i*)ctx.v29.u32)));
	// cmpwi cr6,r6,0
	// vadduwm v2,v2,v29
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), simde_mm_load_si128((simde__m128i*)ctx.v29.u32)));
	// bgt cr6,0x8246fef0
	if (ctx.r6.s32 > 0) goto loc_8246FEF0;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lbz r8,13(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 13);
	// subf r7,r9,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rotlwi r6,r8,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// divwu r11,r7,r6
	ctx.r11.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x82470000
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82470000:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82470020
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82470020:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_0048_wrh"))) PPC_WEAK_FUNC(phBoundBVH_0048_wrh);
PPC_FUNC_IMPL(__imp__phBoundBVH_0048_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r10
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltisw v2,1
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_set1_epi32(int(0x1)));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lis r11,-32256
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// vspltw v11,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v28,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r28,4
	var_r28 = 4;
	// vor v30,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfd f12,28072(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);
	// lis r11,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// li r26,4
	var_r26 = 4;
	// vor v27,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v29,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r7,0
	ctx.r7.s64 = 0;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// lfs f13,22700(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22700);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v26,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v8,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f8.u64);
	// vspltw v12,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// lvlx v7,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v7,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v10,v12,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v9,v12,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vspltw v8,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// vadduwm v12,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v7,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vaddfp v10,v11,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v9,v11,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vsldoi v11,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vsldoi v13,v11,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v0,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v25,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v25.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vadduwm v31,v13,v8
	simde_mm_store_si128((simde__m128i*)ctx.v31.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vadduwm v1,v0,v8
	simde_mm_store_si128((simde__m128i*)ctx.v1.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_824701B8:
	// li r9,8
	ctx.r9.s64 = 8;
loc_824701BC:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r4,r11,2
	ctx.r4.s64 = ctx.r11.s64 + 2;
	// addi r30,r11,6
	var_r30 = (uint32_t)(ctx.r11.s64 + 6);  // addr:0x82000006
	// addi r29,r11,4
	var_r29 = (uint32_t)(ctx.r11.s64 + 4);  // addr:0x82000004
	// lvlx v12,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v12,v28,v12,2
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 14));
	// lvlx v0,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v30,v0,2
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 14));
	// lvlx v11,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v29,v13,2
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 14));
	// vsldoi v11,v27,v11,2
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 14));
	// vor v28,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vor v30,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v29,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v27,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// bne cr6,0x824701bc
	if (!ctx.cr6.eq) goto loc_824701BC;
	// vupkhsh v7,v0
	simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
	// vsrw v8,v31,v2
	ctx.v8.u32[0] = ctx.v31.u32[0] >> (ctx.v2.u8[0] & 0x1F);
	ctx.v8.u32[1] = ctx.v31.u32[1] >> (ctx.v2.u8[4] & 0x1F);
	ctx.v8.u32[2] = ctx.v31.u32[2] >> (ctx.v2.u8[8] & 0x1F);
	ctx.v8.u32[3] = ctx.v31.u32[3] >> (ctx.v2.u8[12] & 0x1F);
	// vupkhsh v6,v13
	simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
	// vsrw v3,v1,v2
	ctx.v3.u32[0] = ctx.v1.u32[0] >> (ctx.v2.u8[0] & 0x1F);
	ctx.v3.u32[1] = ctx.v1.u32[1] >> (ctx.v2.u8[4] & 0x1F);
	ctx.v3.u32[2] = ctx.v1.u32[2] >> (ctx.v2.u8[8] & 0x1F);
	ctx.v3.u32[3] = ctx.v1.u32[3] >> (ctx.v2.u8[12] & 0x1F);
	// vupklsh v5,v0
	simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.s16)));
	// addi r9,r5,1024
	ctx.r9.s64 = ctx.r5.s64 + 1024;
	// vupklsh v4,v13
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
	// addi r4,r5,1040
	ctx.r4.s64 = ctx.r5.s64 + 1040;
	// vcfsx v0,v7,15
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vcfsx v7,v6,15
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// vcfux v8,v8,31
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vcfsx v13,v5,15
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v6,v4,15
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// vcfux v5,v3,31
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// addi r11,r5,16
	ctx.r11.s64 = ctx.r5.s64 + 16;
	// vmulfp128 v4,v0,v9
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vsubfp v7,v7,v0
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v8,v8,v9
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v3,v13,v10
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsubfp v13,v6,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v0,v5,v10
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmaddfp v7,v7,v8,v4
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v4.f32)));
	// vmaddfp v6,v13,v0,v3
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// vupkhsh v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
	// vupklsh v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vcfsx v13,v13,15
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v12,v12,15
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// stvx v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v7,v11
	simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
	// vupklsh v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// xor r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// stvx v6,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r7,r9,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// vcfsx v7,v7,15
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v11,v11,15
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// cmplwi cr6,r7,0
	// vmulfp128 v6,v13,v9
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v5,v12,v10
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsubfp v13,v7,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vsubfp v12,v11,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmaddfp v4,v13,v8,v6
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)), simde_mm_load_ps(ctx.v6.f32)));
	// vmaddfp v3,v12,v0,v5
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v5.f32)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x824702e0
	if (ctx.r7.u32 != 0) {
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r10
	}
loc_824702E0:
	// addi r6,r6,-8
	ctx.r6.s64 = ctx.r6.s64 + -8;
	// vaddfp v10,v10,v26
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v26.f32)));
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// vaddfp v9,v9,v26
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v26.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vadduwm v1,v1,v25
	simde_mm_store_si128((simde__m128i*)ctx.v1.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.u32), simde_mm_load_si128((simde__m128i*)ctx.v25.u32)));
	// cmpwi cr6,r6,0
	// vadduwm v31,v31,v25
	simde_mm_store_si128((simde__m128i*)ctx.v31.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v31.u32), simde_mm_load_si128((simde__m128i*)ctx.v25.u32)));
	// bgt cr6,0x824701b8
	if (ctx.r6.s32 > 0) goto loc_824701B8;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lbz r8,13(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 13);
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rotlwi r6,r8,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// divwu r10,r7,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82470330
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82470330:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82470350
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82470350:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_0378_w"))) PPC_WEAK_FUNC(phBoundBVH_0378_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_0378_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_25
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r10
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltisw v27,1
	simde_mm_store_si128((simde__m128i*)ctx.v27.u32, simde_mm_set1_epi32(int(0x1)));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lis r11,-32256
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// vspltw v11,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v18,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r28,4
	var_r28 = 4;
	// vor v20,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfd f12,28072(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);
	// lis r11,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// li r26,4
	var_r26 = 4;
	// vor v22,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v24,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r7,0
	ctx.r7.s64 = 0;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vor v17,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v19,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// vor v21,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// vor v23,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// lfs f13,22700(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22700);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v16,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v16.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v9,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f8.u64);
	// vspltw v12,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// lvlx v7,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v7,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v7,v12,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v8,v12,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vspltw v9,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// vadduwm v10,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vaddfp v12,v11,v7
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vadduwm v7,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vaddfp v11,v11,v8
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vsldoi v8,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vsldoi v13,v8,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r5,2064
	ctx.r9.s64 = ctx.r5.s64 + 2064;
	// vadduwm v0,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v8,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// vadduwm v15,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v15.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vadduwm v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vadduwm v25,v13,v9
	simde_mm_store_si128((simde__m128i*)ctx.v25.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// vadduwm v26,v0,v9
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_824704FC:
	// li r8,8
	ctx.r8.s64 = 8;
loc_82470500:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r4,r3
	ctx.r3.u64 = ctx.r4.u64 + ctx.r3.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// addi r30,r11,6
	var_r30 = (uint32_t)(ctx.r11.s64 + 6);  // addr:0x82000006
	// addi r29,r11,14
	var_r29 = (uint32_t)(ctx.r11.s64 + 14);  // addr:0x8200000e
	// addi r28,r11,4
	var_r28 = (uint32_t)(ctx.r11.s64 + 4);  // addr:0x82000004
	// addi r27,r11,12
	var_r27 = (uint32_t)(ctx.r11.s64 + 12);  // addr:0x8200000c
	// lvlx v6,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r11,2
	var_r26 = (uint32_t)(ctx.r11.s64 + 2);  // addr:0x82000002
	// vsldoi v6,v18,v6,2
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 14));
	// lvlx v0,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r25,r11,10
	var_r25 = (uint32_t)(ctx.r11.s64 + 10);  // addr:0x8200000a
	// addi r30,r11,8
	var_r30 = (uint32_t)(ctx.r11.s64 + 8);  // addr:0x82000008
	// lvlx v13,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v24,v0,2
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 14));
	// lvlx v9,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v23,v13,2
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 14));
	// lvlx v8,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v10,v22,v10,2
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 14));
	// lvlx v7,0,r25
	temp.u32 = var_r25;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v9,v21,v9,2
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 14));
	// lvlx v5,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v8,v20,v8,2
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 14));
	// vsldoi v7,v19,v7,2
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 14));
	// vor v24,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vsldoi v5,v17,v5,2
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 14));
	// vor v23,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v22,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_load_si128((simde__m128i*)ctx.v10.u8));
	// vor v21,v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_load_si128((simde__m128i*)ctx.v9.u8));
	// vor v20,v8,v8
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_load_si128((simde__m128i*)ctx.v8.u8));
	// vor v19,v7,v7
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v7.u8));
	// vor v18,v6,v6
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v6.u8));
	// vor v17,v5,v5
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v5.u8));
	// bne cr6,0x82470500
	if (!ctx.cr6.eq) goto loc_82470500;
	// vupkhsh v3,v0
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
	// vsrw v4,v25,v27
	ctx.v4.u32[0] = ctx.v25.u32[0] >> (ctx.v27.u8[0] & 0x1F);
	ctx.v4.u32[1] = ctx.v25.u32[1] >> (ctx.v27.u8[4] & 0x1F);
	ctx.v4.u32[2] = ctx.v25.u32[2] >> (ctx.v27.u8[8] & 0x1F);
	ctx.v4.u32[3] = ctx.v25.u32[3] >> (ctx.v27.u8[12] & 0x1F);
	// vupkhsh v2,v13
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
	// vsrw v31,v26,v27
	ctx.v31.u32[0] = ctx.v26.u32[0] >> (ctx.v27.u8[0] & 0x1F);
	ctx.v31.u32[1] = ctx.v26.u32[1] >> (ctx.v27.u8[4] & 0x1F);
	ctx.v31.u32[2] = ctx.v26.u32[2] >> (ctx.v27.u8[8] & 0x1F);
	ctx.v31.u32[3] = ctx.v26.u32[3] >> (ctx.v27.u8[12] & 0x1F);
	// vupklsh v1,v13
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
	// rldicl r8,r3,32,32
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vupklsh v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.s16)));
	// addi r30,r9,1008
	var_r30 = (uint32_t)(ctx.r9.s64 + 1008);  // addr:0x820003f0
	// vcfsx v13,v3,15
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// rlwinm r11,r8,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// vcfux v30,v4,31
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v29,v7
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
	// vcfsx v3,v2,15
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// vcfsx v2,v1,15
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// addi r11,r9,1024
	ctx.r11.s64 = ctx.r9.s64 + 1024;
	// vcfsx v4,v0,15
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vupklsh v28,v7
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
	// vcfux v1,v31,31
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// addi r8,r9,-16
	ctx.r8.s64 = ctx.r9.s64 + -16;
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// xor r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// vmulfp128 v31,v13,v11
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v0,v30,v11
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v3,v3,v13
	simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v30,v4,v12
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v4,v2,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vmulfp128 v13,v1,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vupklsh v1,v9
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.s16)));
	// vmaddfp v3,v3,v0,v31
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// vupkhsh v31,v8
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
	// vmaddfp v2,v4,v13,v30
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v30.f32)));
	// vupkhsh v4,v10
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
	// vupklsh v30,v8
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.s16)));
	// vcfsx v8,v31,15
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v7,v30,15
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// stvx v3,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v3,v10
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
	// vcfsx v10,v4,15
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// addi r30,r9,-2048
	var_r30 = (uint32_t)(ctx.r9.s64 + -2048);
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v2,v9
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
	// vcfsx v9,v3,15
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// addi r11,r9,-1040
	ctx.r11.s64 = ctx.r9.s64 + -1040;
	// vcfsx v3,v1,15
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v1,v28,15
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v4,v2,15
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v2,v29,15
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmulfp128 v31,v10,v11
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v30,v9,v12
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v9,v3,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v3,v7,v12
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v10,v4,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v4,v8,v11
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v8,v2,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vsubfp v7,v1,v7
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmaddfp v30,v9,v13,v30
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v30.f32)));
	// vupklsh v9,v6
	simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.s16)));
	// vmaddfp v31,v10,v0,v31
	simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// vupkhsh v10,v6
	simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v6.s16), simde_mm_load_si128((simde__m128i*)ctx.v6.s16))));
	// vupkhsh v6,v5
	simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v5.s16), simde_mm_load_si128((simde__m128i*)ctx.v5.s16))));
	// vupklsh v5,v5
	simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.s16)));
	// vcfsx v9,v9,15
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmaddfp v29,v8,v0,v4
	simde_mm_store_ps(ctx.v29.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v4.f32)));
	// vcfsx v10,v10,15
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v6,v6,15
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v5,v5,15
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmaddfp v28,v7,v13,v3
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// stvx v30,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v31,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r9,-1024
	ctx.r8.s64 = ctx.r9.s64 + -1024;
	// vmulfp128 v7,v9,v12
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v29,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v8,v10,v11
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v10,v6,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsubfp v9,v5,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx v28,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v14,v10,v0,v8
	simde_mm_store_ps(ctx.v14.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// vmaddfp v0,v9,v13,v7
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v7.f32)));
	// stvx v14,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v14.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v0,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r11,r7,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r11,0
	// beq cr6,0x824706f0
	if (ctx.r11.u32 != 0) {
		// li r8,128
		ctx.r8.s64 = 128;
		// dcbt r8,r10
	}
loc_824706F0:
	// addi r6,r6,-8
	ctx.r6.s64 = ctx.r6.s64 + -8;
	// vaddfp v12,v12,v16
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v16.f32)));
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// vaddfp v11,v11,v16
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v16.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vadduwm v26,v26,v15
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v26.u32), simde_mm_load_si128((simde__m128i*)ctx.v15.u32)));
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// vadduwm v25,v25,v15
	simde_mm_store_si128((simde__m128i*)ctx.v25.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v25.u32), simde_mm_load_si128((simde__m128i*)ctx.v15.u32)));
	// cmpwi cr6,r6,0
	// bgt cr6,0x824704fc
	if (ctx.r6.s32 > 0) goto loc_824704FC;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lbz r7,13(r31)
	ctx.r7.u64 = PPC_LOAD_U8(var_r31 + 13);
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rotlwi r4,r7,1
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// divwu r10,r6,r4
	ctx.r10.u32 = ctx.r4.u32 ? ctx.r6.u32 / ctx.r4.u32 : 0;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82470744
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82470744:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82470764
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82470764:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_0788_w"))) PPC_WEAK_FUNC(phBoundBVH_0788_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_0788_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=208, savegprlr_25
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r3
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v21,1
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_set1_epi32(int(0x1)));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lis r11,-32256
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltw v11,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// vor128 v59,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v59.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r28,4
	var_r28 = 4;
	// vor128 v61,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfd f12,28072(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);
	// lis r11,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// li r26,4
	var_r26 = 4;
	// vor128 v63,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v16,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r30,0
	var_r30 = 0;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vor v18,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v20,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// vor128 v58,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v58.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// vor128 v60,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v60.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor128 v62,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// vor v15,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// vor v17,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v19,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// lfs f13,22700(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22700);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw128 v57,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v57.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v9,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.f8.u64);
	// vspltw v12,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// stfd f7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.f7.u64);
	// lvlx v7,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v7,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v7,v12,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v8,v12,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vspltw v9,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// vadduwm v10,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vaddfp v12,v11,v7
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vadduwm v7,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vaddfp v11,v11,v8
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vsldoi v8,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r5,4112
	ctx.r10.s64 = ctx.r5.s64 + 4112;
	// vadduwm v0,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// vsldoi v13,v8,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// vadduwm v10,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// stvx v8,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// vadduwm v0,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vadduwm v8,v13,v9
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vadduwm v14,v0,v9
	simde_mm_store_si128((simde__m128i*)ctx.v14.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_82470928:
	// li r8,8
	ctx.r8.s64 = 8;
loc_8247092C:
	// rldicl r11,r9,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = ctx.r11.u32;
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// addi r7,r11,10
	ctx.r7.s64 = ctx.r11.s64 + 10;
	// addi r29,r11,22
	var_r29 = (uint32_t)(ctx.r11.s64 + 22);  // addr:0x82000016
	// addi r28,r11,8
	var_r28 = (uint32_t)(ctx.r11.s64 + 8);  // addr:0x82000008
	// addi r27,r11,20
	var_r27 = (uint32_t)(ctx.r11.s64 + 20);  // addr:0x82000014
	// lvlx v2,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r11,6
	var_r26 = (uint32_t)(ctx.r11.s64 + 6);  // addr:0x82000006
	// vsldoi128 v2,v59,v2,2
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v59.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8), 14));
	// lvlx v0,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r11,4
	ctx.r7.s64 = ctx.r11.s64 + 4;
	// lvlx v13,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r25,r11,18
	var_r25 = (uint32_t)(ctx.r11.s64 + 18);  // addr:0x82000012
	// lvlx v10,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r11,16
	var_r29 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82000010
	// lvlx v9,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r28,r11,2
	var_r28 = (uint32_t)(ctx.r11.s64 + 2);  // addr:0x82000002
	// addi r27,r11,14
	var_r27 = (uint32_t)(ctx.r11.s64 + 14);  // addr:0x8200000e
	// lvlx v8,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r11,12
	ctx.r7.s64 = ctx.r11.s64 + 12;
	// lvlx v7,0,r25
	temp.u32 = var_r25;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v20,v0,2
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 14));
	// lvlx v5,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v19,v13,2
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 14));
	// lvlx v4,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v10,v18,v10,2
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 14));
	// lvlx v3,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v9,v17,v9,2
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 14));
	// lvlx v1,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v8,v16,v8,2
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 14));
	// vsldoi v7,v15,v7,2
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 14));
	// vor v20,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vsldoi128 v6,v63,v6,2
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 14));
	// vor v19,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi128 v5,v62,v5,2
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 14));
	// vor v18,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v10.u8));
	// vsldoi128 v4,v61,v4,2
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 14));
	// vor v17,v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v9.u8));
	// vsldoi128 v3,v60,v3,2
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v60.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 14));
	// vor v16,v8,v8
	simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_load_si128((simde__m128i*)ctx.v8.u8));
	// vsldoi128 v1,v58,v1,2
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v58.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 14));
	// vor v15,v7,v7
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_load_si128((simde__m128i*)ctx.v7.u8));
	// vor128 v63,v6,v6
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_load_si128((simde__m128i*)ctx.v6.u8));
	// vor128 v62,v5,v5
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_load_si128((simde__m128i*)ctx.v5.u8));
	// vor128 v61,v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_load_si128((simde__m128i*)ctx.v4.u8));
	// vor128 v60,v3,v3
	simde_mm_store_si128((simde__m128i*)ctx.v60.u8, simde_mm_load_si128((simde__m128i*)ctx.v3.u8));
	// vor128 v59,v2,v2
	simde_mm_store_si128((simde__m128i*)ctx.v59.u8, simde_mm_load_si128((simde__m128i*)ctx.v2.u8));
	// vor128 v58,v1,v1
	simde_mm_store_si128((simde__m128i*)ctx.v58.u8, simde_mm_load_si128((simde__m128i*)ctx.v1.u8));
	// bne cr6,0x8247092c
	if (!ctx.cr6.eq) goto loc_8247092C;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vupkhsh v30,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
	// vupkhsh v29,v13
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
	// vsrw v27,v14,v21
	ctx.v27.u32[0] = ctx.v14.u32[0] >> (ctx.v21.u8[0] & 0x1F);
	ctx.v27.u32[1] = ctx.v14.u32[1] >> (ctx.v21.u8[4] & 0x1F);
	ctx.v27.u32[2] = ctx.v14.u32[2] >> (ctx.v21.u8[8] & 0x1F);
	ctx.v27.u32[3] = ctx.v14.u32[3] >> (ctx.v21.u8[12] & 0x1F);
	// vupklsh v28,v13
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
	// addi r7,r10,1008
	ctx.r7.s64 = ctx.r10.s64 + 1008;
	// vupklsh v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v0.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.s16)));
	// rldicl r11,r9,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vcfsx v13,v30,15
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vupklsh v24,v8
	simde_mm_store_si128((simde__m128i*)ctx.v24.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.s16)));
	// lvx128 v31,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v30,v29,15
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vsrw v31,v31,v21
	ctx.v31.u32[0] = ctx.v31.u32[0] >> (ctx.v21.u8[0] & 0x1F);
	ctx.v31.u32[1] = ctx.v31.u32[1] >> (ctx.v21.u8[4] & 0x1F);
	ctx.v31.u32[2] = ctx.v31.u32[2] >> (ctx.v21.u8[8] & 0x1F);
	ctx.v31.u32[3] = ctx.v31.u32[3] >> (ctx.v21.u8[12] & 0x1F);
	// vcfsx v29,v28,15
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfux v28,v27,31
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// rotlwi r11,r11,0
	ctx.r11.u64 = ctx.r11.u32;
	// vupkhsh v25,v8
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vupkhsh v23,v7
	simde_mm_store_si128((simde__m128i*)ctx.v23.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
	// vcfux v26,v31,31
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsh v22,v7
	simde_mm_store_si128((simde__m128i*)ctx.v22.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
	// vcfsx v31,v0,15
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// addi r8,r10,1024
	ctx.r8.s64 = ctx.r10.s64 + 1024;
	// vcfsx v8,v25,15
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v7,v24,15
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v24.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 + ctx.r3.u64;
	// vmulfp128 v27,v13,v11
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r11,r10,-1040
	ctx.r11.s64 = ctx.r10.s64 + -1040;
	// vsubfp v30,v30,v13
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v28,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vupkhsh v28,v9
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
	// vmulfp128 v0,v26,v11
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v26,v31,v12
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v31,v29,v31
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v31.f32)));
	// vupklsh v29,v10
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
	// vmaddfp v30,v30,v0,v27
	simde_mm_store_ps(ctx.v30.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v27.f32)));
	// vupklsh v27,v9
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.s16)));
	// vcfsx v9,v29,15
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmaddfp v26,v31,v13,v26
	simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v26.f32)));
	// vcfsx v31,v28,15
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v29,v23,15
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v23.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// stvx v30,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v30,v10
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
	// addi r7,r10,-16
	ctx.r7.s64 = ctx.r10.s64 + -16;
	// stvx v26,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v26,v22,15
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v22.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// addi r8,r10,-1024
	ctx.r8.s64 = ctx.r10.s64 + -1024;
	// vcfsx v10,v30,15
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v30,v27,15
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmulfp128 v27,v9,v12
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v28,v10,v11
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v10,v31,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsubfp v9,v30,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v31,v8,v11
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v30,v7,v12
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v8,v29,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vsubfp v7,v26,v7
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmaddfp v25,v10,v0,v28
	simde_mm_store_ps(ctx.v25.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v28.f32)));
	// vupkhsh v10,v6
	simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v6.s16), simde_mm_load_si128((simde__m128i*)ctx.v6.s16))));
	// vmaddfp v24,v9,v13,v27
	simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v27.f32)));
	// vupkhsh v9,v5
	simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v5.s16), simde_mm_load_si128((simde__m128i*)ctx.v5.s16))));
	// vupklsh v6,v6
	simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.s16)));
	// vupklsh v5,v5
	simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.s16)));
	// vcfsx v10,v10,15
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v29,v9,15
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v9,v6,15
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmaddfp v23,v8,v0,v31
	simde_mm_store_ps(ctx.v23.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v31.f32)));
	// vmaddfp v22,v7,v13,v30
	simde_mm_store_ps(ctx.v22.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v30.f32)));
	// vcfsx v6,v5,15
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// stvx v25,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r10,-2064
	ctx.r7.s64 = ctx.r10.s64 + -2064;
	// stvx v24,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v8,v10,v11
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v10,v29,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v7,v9,v12
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
	// stvx v23,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v22,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v9,v6,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmaddfp v10,v10,v0,v8
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v8.f32)));
	// addi r11,r10,-2048
	ctx.r11.s64 = ctx.r10.s64 + -2048;
	// vmaddfp v9,v9,v13,v7
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v7.f32)));
	// vupklsh v8,v4
	simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupkhsh v6,v3
	simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// addi r8,r10,-3088
	ctx.r8.s64 = ctx.r10.s64 + -3088;
	// vupklsh v5,v3
	simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vupklsh v3,v2
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vcfsx v6,v6,15
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v5,v5,15
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v7,v3,15
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v10,v4
	simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// stvx v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v9,v8,15
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vupkhsh v4,v2
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// addi r7,r10,-3072
	ctx.r7.s64 = ctx.r10.s64 + -3072;
	// vupkhsh v2,v1
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// addi r11,r10,-4096
	ctx.r11.s64 = ctx.r10.s64 + -4096;
	// vcfsx v10,v10,15
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vupklsh v1,v1
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vcfsx v8,v4,15
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v4,v2,15
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vcfsx v1,v1,15
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
	// vmulfp128 v2,v9,v12
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v9,v5,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v3,v10,v11
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v10,v6,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmaddfp v5,v9,v13,v2
	simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v9,v7,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v7,v1,v7
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmaddfp v6,v10,v0,v3
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v10,v8,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v8,v4,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v8.f32)));
	// stvx v5,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmaddfp v3,v7,v13,v9
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v9.f32)));
	// stvx v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// xor r8,r30,r3
	ctx.r8.u64 = var_r30 ^ ctx.r3.u64;
	// vmaddfp v4,v8,v0,v10
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)), simde_mm_load_ps(ctx.v10.f32)));
	// rlwinm r7,r8,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r7,0
	// stvx v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82470c0c
	if (ctx.r7.u32 != 0) {
		// li r11,128
		ctx.r11.s64 = 128;
		// dcbt r11,r3
	}
loc_82470C0C:
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// vaddfp128 v12,v12,v57
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v57.f32)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vaddfp128 v11,v11,v57
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v57.f32)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// addi r6,r6,-8
	ctx.r6.s64 = ctx.r6.s64 + -8;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// lvx128 v2,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// lvx128 v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// vadduwm v1,v2,v0
	simde_mm_store_si128((simde__m128i*)ctx.v1.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// cmpwi cr6,r6,0
	// vadduwm v14,v14,v0
	simde_mm_store_si128((simde__m128i*)ctx.v14.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v14.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// stvx v1,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bgt cr6,0x82470928
	if (ctx.r6.s32 > 0) goto loc_82470928;
	// lbz r8,13(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 13);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// rotlwi r6,r8,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// subf r7,r10,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r10.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r10,r7,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// blt cr6,0x82470c78
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
	}
loc_82470C78:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82470c98
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82470C98:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__ph_0CC0"))) PPC_WEAK_FUNC(ph_0CC0);
PPC_FUNC_IMPL(__imp__ph_0CC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=128, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// lwz r30,8(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 8));
	// cmplw cr6,r30,r11
	// blt cr6,0x82470cf0
	if (var_r30 < ctx.r11.u32) goto loc_82470CF0;
loc_82470CE8:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82470e68
	// blr
	return;
loc_82470CF0:
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 24);
	// cmplw cr6,r10,r9
	// bge cr6,0x82470ce8
	if (ctx.r10.u32 >= ctx.r9.u32) {
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x82470e68
		// blr
		return;
	}
	// lwz r11,80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 80);
	// cmplwi cr6,r11,0
	// beq cr6,0x82470e50
	if (ctx.r11.u32 == 0) goto loc_82470E50;
	// clrlwi r8,r11,30
	ctx.r8.u64 = ctx.r11.u32 & 0x3;
	// cmplwi cr6,r8,0
	// beq cr6,0x82470e48
	if (ctx.r8.u32 == 0) goto loc_82470E48;
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 32);
	// lwz r6,16(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 16);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// lwz r9,84(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 84);
	// clrlwi r10,r9,31
	ctx.r10.u64 = ctx.r9.u32 & 0x1;
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// std r5,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r5.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f8,f12,f9
	ctx.f8.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// stfs f8,44(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(var_r31 + 44, temp.u32);
	// beq cr6,0x82470db8
	if (!(ctx.cr6.eq)) {
		// rotlwi r4,r6,0
		ctx.r4.u64 = ctx.r6.u32;
		// cmplw cr6,r4,r11
		// bne cr6,0x82470d84
		if (ctx.r4.u32 == ctx.r11.u32) {
			// rlwinm r3,r9,0,24,24
			ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
			// cmplwi cr6,r3,0
			// bne cr6,0x82470d7c
			if (ctx.r3.u32 == 0) {
				// li r7,4
				ctx.r7.s64 = 4;
				// b 0x82470dcc
				goto loc_82470DCC;
			}
		loc_82470D7C:
			// li r7,1
			ctx.r7.s64 = 1;
			// b 0x82470dcc
			goto loc_82470DCC;
		}
	loc_82470D84:
		// cmplwi cr6,r10,0
		// beq cr6,0x82470db8
		if (ctx.r10.u32 == 0) goto loc_82470DB8;
		// lwz r10,16(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 16);
		// rlwinm r8,r10,1,0,30
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// cmplw cr6,r8,r11
		// bne cr6,0x82470db8
		if (ctx.r8.u32 != ctx.r11.u32) goto loc_82470DB8;
		// rlwinm r7,r9,0,24,24
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
		// cmplwi cr6,r7,0
		// bne cr6,0x82470db0
		if (ctx.r7.u32 == 0) {
			// li r7,5
			ctx.r7.s64 = 5;
			// b 0x82470dcc
			goto loc_82470DCC;
		}
	loc_82470DB0:
		// li r7,2
		ctx.r7.s64 = 2;
		// b 0x82470dcc
	} else {
	loc_82470DB8:
		// rlwinm r6,r9,0,24,24
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x80;
		// li r7,3
		ctx.r7.s64 = 3;
		// cmplwi cr6,r6,0
		// beq cr6,0x82470dcc
		if (ctx.r6.u32 == 0) goto loc_82470DCC;
		// li r7,0
		ctx.r7.s64 = 0;
	}
loc_82470DCC:
	// lbz r11,13(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 13);
	// lbz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 12);
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r10,5
	// bgt cr6,0x82470e1c
	if (ctx.r10.u32 > 5) goto loc_82470E1C;
	// lis r12,-32185
	// addi r12,r12,3576
	ctx.r12.s64 = ctx.r12.s64 + 3576;
	// rlwinm r0,r10,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r10.u64) {
	case 0:
		goto loc_82470E10;
	case 1:
		goto loc_82470E10;
	case 2:
		goto loc_82470E1C;
	case 3:
		goto loc_82470E10;
	case 4:
		goto loc_82470E1C;
	case 5:
		goto loc_82470E10;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82470E10:
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// b 0x82470e20
	goto loc_82470E20;
loc_82470E1C:
	// li r8,0
	ctx.r8.s64 = 0;
loc_82470E20:
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-32256
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r10,r10,27384
	ctx.r10.s64 = ctx.r10.s64 + 27384;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r5,r11,r8
	ctx.r5.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r4,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// stw r3,76(r31)
	PPC_STORE_U32(var_r31 + 76, ctx.r3.u32);
loc_82470E48:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,80(r31)
	PPC_STORE_U32(var_r31 + 80, ctx.r11.u32);
loc_82470E50:
	// lwz r10,76(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 76);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
	// subf r3,r30,r9
	ctx.r3.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r30;
loc_82470E68:
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_0E80_w"))) PPC_WEAK_FUNC(phBoundBVH_0E80_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_0E80_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r11
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vspltisw v6,1
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lis r10,-32256
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// lfd f12,28072(r10)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r10.u32 + 28072);
	// lis r10,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// li r28,4
	var_r28 = 4;
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// li r26,4
	var_r26 = 4;
	// li r7,0
	ctx.r7.s64 = 0;
	// lvx128 v12,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// vspltw v7,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lfs f13,22772(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22772);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v5,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f8.u64);
	// vspltw v11,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lvlx v9,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// lvlx v8,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v9,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vspltw v10,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// vsldoi v11,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vadduwm v12,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v8,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vsldoi v13,v11,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// vadduwm v4,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vaddfp v7,v7,v9
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vor v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v8,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vadduwm v9,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_82470FB4:
	// li r10,4
	ctx.r10.s64 = 4;
loc_82470FB8:
	// rldicl r9,r3,32,32
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// cmplwi cr6,r10,0
	// addi r4,r9,4
	ctx.r4.s64 = ctx.r9.s64 + 4;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v11,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v0,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// vor v11,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// bne cr6,0x82470fb8
	if (ctx.r10.u32 != 0) goto loc_82470FB8;
	// vsrw v12,v9,v6
	ctx.v12.u32[0] = ctx.v9.u32[0] >> (ctx.v6.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v9.u32[1] >> (ctx.v6.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v9.u32[2] >> (ctx.v6.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v9.u32[3] >> (ctx.v6.u8[12] & 0x1F);
	// vsubfp v10,v0,v13
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// rldicl r10,r3,32,32
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// vcfux v12,v12,31
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// xor r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 ^ ctx.r11.u64;
	// rlwinm r7,r9,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r7,0
	// vmulfp128 v12,v10,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v13,v12,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v13,v7
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)));
	// stvx v13,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82471030
	if (ctx.r7.u32 != 0) {
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r11
	}
loc_82471030:
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// vaddfp v7,v7,v5
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v5.f32)));
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// vadduwm v9,v9,v4
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v4.u32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// cmpwi cr6,r6,0
	// bgt cr6,0x82470fb4
	if (ctx.r6.s32 > 0) goto loc_82470FB4;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lbz r8,13(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 13);
	// subf r7,r9,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rotlwi r6,r8,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// divwu r11,r7,r6
	ctx.r11.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x82471078
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82471078:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82471098
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82471098:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_10C0_w"))) PPC_WEAK_FUNC(phBoundBVH_10C0_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_10C0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r10
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vspltisw v2,1
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32256
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// lfd f12,28072(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);
	// lis r11,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// li r28,4
	var_r28 = 4;
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// li r26,4
	var_r26 = 4;
	// li r7,0
	ctx.r7.s64 = 0;
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// vspltw v7,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lfs f13,22772(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22772);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v1,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v1.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f8.u64);
	// vspltw v11,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lvlx v9,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// lvlx v8,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v9,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vspltw v10,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// vor v8,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vsldoi v11,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vadduwm v12,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v6,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vsldoi v13,v11,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// vadduwm v31,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v31.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vaddfp v4,v7,v9
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vor v9,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v7,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v6,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vadduwm v3,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_824711FC:
	// li r9,4
	ctx.r9.s64 = 4;
loc_82471200:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r30,r11,12
	var_r30 = (uint32_t)(ctx.r11.s64 + 12);  // addr:0x8200000c
	// addi r4,r11,4
	ctx.r4.s64 = ctx.r11.s64 + 4;
	// addi r29,r11,8
	var_r29 = (uint32_t)(ctx.r11.s64 + 8);  // addr:0x82000008
	// lvlx v11,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v0,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// lvlx v10,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v12,v8,v11,4
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
	// vsldoi v13,v9,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vsldoi v11,v7,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// vor v8,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vor v9,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v7,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// bne cr6,0x82471200
	if (!ctx.cr6.eq) goto loc_82471200;
	// vsrw v10,v3,v2
	ctx.v10.u32[0] = ctx.v3.u32[0] >> (ctx.v2.u8[0] & 0x1F);
	ctx.v10.u32[1] = ctx.v3.u32[1] >> (ctx.v2.u8[4] & 0x1F);
	ctx.v10.u32[2] = ctx.v3.u32[2] >> (ctx.v2.u8[8] & 0x1F);
	ctx.v10.u32[3] = ctx.v3.u32[3] >> (ctx.v2.u8[12] & 0x1F);
	// vsubfp v5,v11,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v6,v0,v13
	simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r9,r5,1024
	ctx.r9.s64 = ctx.r5.s64 + 1024;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// vcfux v11,v10,31
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// xor r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// rlwinm r4,r7,0,0,24
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	// vmulfp128 v10,v6,v11
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v11,v5,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v13,v10,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v12,v11,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v13,v13,v4
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vmulfp128 v12,v12,v4
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x824712b0
	if (ctx.r4.u32 != 0) {
		// li r11,128
		ctx.r11.s64 = 128;
		// dcbt r11,r10
	}
loc_824712B0:
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// vaddfp v4,v4,v1
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v1.f32)));
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// vadduwm v3,v3,v31
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), simde_mm_load_si128((simde__m128i*)ctx.v31.u32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// cmpwi cr6,r6,0
	// bgt cr6,0x824711fc
	if (ctx.r6.s32 > 0) goto loc_824711FC;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lbz r8,13(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 13);
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rotlwi r6,r8,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// divwu r10,r7,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x824712f8
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824712F8:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82471318
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82471318:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_1340_w"))) PPC_WEAK_FUNC(phBoundBVH_1340_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_1340_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r10
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// vspltisw v25,1
	simde_mm_store_si128((simde__m128i*)ctx.v25.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32256
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v30,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v1,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfd f12,28072(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);
	// lis r11,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// li r28,4
	var_r28 = 4;
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// vor v3,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r26,4
	var_r26 = 4;
	// vor v4,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r7,0
	ctx.r7.s64 = 0;
	// vor v29,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v31,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vor v2,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// vspltw v7,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lfs f13,22772(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22772);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v24,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v24.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f8.u64);
	// vspltw v11,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lvlx v9,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// lvlx v8,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v9,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vspltw v10,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// vsldoi v11,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vadduwm v12,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v8,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vsldoi v13,v11,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// vadduwm v23,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vaddfp v28,v7,v9
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v8,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vadduwm v26,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_8247148C:
	// li r9,4
	ctx.r9.s64 = 4;
loc_82471490:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r4,r11,12
	ctx.r4.s64 = ctx.r11.s64 + 12;
	// addi r30,r11,28
	var_r30 = (uint32_t)(ctx.r11.s64 + 28);  // addr:0x8200001c
	// addi r29,r11,8
	var_r29 = (uint32_t)(ctx.r11.s64 + 8);  // addr:0x82000008
	// addi r27,r11,4
	var_r27 = (uint32_t)(ctx.r11.s64 + 4);  // addr:0x82000004
	// lvlx v6,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r28,r11,24
	var_r28 = (uint32_t)(ctx.r11.s64 + 24);  // addr:0x82000018
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r11,20
	var_r26 = (uint32_t)(ctx.r11.s64 + 20);  // addr:0x82000014
	// addi r4,r11,16
	ctx.r4.s64 = ctx.r11.s64 + 16;
	// lvlx v12,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v0,v0,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// lvlx v8,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v12,v3,v11,4
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
	// lvlx v10,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v11,v1,v8,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
	// lvlx v7,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v9,v2,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// lvlx v5,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v8,v31,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// vsldoi v13,v4,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vor v3,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vsldoi v10,v30,v6,4
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
	// vor v1,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// vsldoi v7,v29,v5,4
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
	// vor v2,v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v9.u8));
	// vor v31,v8,v8
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v8.u8));
	// vor v4,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v30,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v10.u8));
	// vor v29,v7,v7
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v7.u8));
	// bne cr6,0x82471490
	if (!ctx.cr6.eq) goto loc_82471490;
	// vsrw v6,v26,v25
	ctx.v6.u32[0] = ctx.v26.u32[0] >> (ctx.v25.u8[0] & 0x1F);
	ctx.v6.u32[1] = ctx.v26.u32[1] >> (ctx.v25.u8[4] & 0x1F);
	ctx.v6.u32[2] = ctx.v26.u32[2] >> (ctx.v25.u8[8] & 0x1F);
	ctx.v6.u32[3] = ctx.v26.u32[3] >> (ctx.v25.u8[12] & 0x1F);
	// vsubfp v27,v9,v12
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v27.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v5,v0,v13
	simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vsubfp v8,v8,v11
	simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r9,r5,3072
	ctx.r9.s64 = ctx.r5.s64 + 3072;
	// vsubfp v7,v7,v10
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v10.f32)));
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// vcfux v9,v6,31
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// addi r4,r5,2048
	ctx.r4.s64 = ctx.r5.s64 + 2048;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r30,r5,1024
	var_r30 = (uint32_t)(ctx.r5.s64 + 1024);
	// xor r11,r7,r10
	ctx.r11.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// vmulfp128 v6,v5,v9
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v5,v27,v9
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v8,v8,v9
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v9,v7,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vaddfp v13,v6,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v12,v5,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v11,v8,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v10,v9,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v13,v13,v28
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v28.f32)));
	// vmulfp128 v12,v12,v28
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v28.f32)));
	// vmulfp128 v11,v11,v28
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v28.f32)));
	// vmulfp128 v10,v10,v28
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v28.f32)));
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r9,r11,0,0,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFF80;
	// stvx v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v11,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmplwi cr6,r9,0
	// stvx v10,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x824715b0
	if (ctx.r9.u32 != 0) {
		// li r7,128
		ctx.r7.s64 = 128;
		// dcbt r7,r10
	}
loc_824715B0:
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// vaddfp v28,v28,v24
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v24.f32)));
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// vadduwm v26,v26,v23
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v26.u32), simde_mm_load_si128((simde__m128i*)ctx.v23.u32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// cmpwi cr6,r6,0
	// bgt cr6,0x8247148c
	if (ctx.r6.s32 > 0) goto loc_8247148C;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lbz r6,13(r31)
	ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 13);
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rotlwi r10,r6,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 2);
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r10,r4,r10
	ctx.r10.u32 = ctx.r10.u32 ? ctx.r4.u32 / ctx.r10.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x824715f8
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824715F8:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r8,r10,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82471618
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82471618:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_1640_w"))) PPC_WEAK_FUNC(phBoundBVH_1640_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_1640_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_26
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// lfs f11,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r3
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vspltisw v18,1
	simde_mm_store_si128((simde__m128i*)ctx.v18.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32256
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r30,r1,80
	var_r30 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v23,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// addi r29,r1,80
	var_r29 = (uint32_t)(ctx.r1.s64 + 80);
	// vor v25,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lfd f12,28072(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);
	// lis r11,-32256
	// fmul f10,f10,f12
	ctx.f10.f64 = ctx.f10.f64 * ctx.f12.f64;
	// li r28,4
	var_r28 = 4;
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// addi r27,r1,88
	var_r27 = (uint32_t)(ctx.r1.s64 + 88);
	// vor v27,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r26,4
	var_r26 = 4;
	// vor v28,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// li r4,0
	ctx.r4.s64 = 0;
	// vor v30,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v22,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// lvx128 v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vor v24,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v26,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vor v6,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// vor v29,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f9.f64));
	// vspltw v7,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lfs f13,22772(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 22772);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f6,f3
	ctx.f0.f64 = double(float(ctx.f6.f64 / ctx.f3.f64));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v17,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v17.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfd f8,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.f8.u64);
	// vspltw v11,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// stfd f7,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f7.u64);
	// lvlx v9,r29,r28
	temp.u32 = var_r29 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// lvlx v8,r27,r26
	temp.u32 = var_r27 + var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v9,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vspltw v10,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// vsldoi v11,v0,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// vadduwm v12,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v8,v13,v12
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vsldoi v13,v11,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// vadduwm v16,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v16.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vor v12,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
	// vaddfp v2,v7,v9
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v13,v13,v8,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
	// vadduwm v19,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v19.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_82471798:
	// li r9,4
	ctx.r9.s64 = 4;
loc_8247179C:
	// rldicl r11,r10,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = ctx.r11.u32;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// addi r8,r11,20
	ctx.r8.s64 = ctx.r11.s64 + 20;
	// addi r29,r11,16
	var_r29 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82000010
	// addi r30,r11,44
	var_r30 = (uint32_t)(ctx.r11.s64 + 44);  // addr:0x8200002c
	// addi r28,r11,40
	var_r28 = (uint32_t)(ctx.r11.s64 + 40);  // addr:0x82000028
	// lvlx v31,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r27,r11,12
	var_r27 = (uint32_t)(ctx.r11.s64 + 12);  // addr:0x8200000c
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r11,8
	ctx.r8.s64 = ctx.r11.s64 + 8;
	// lvlx v10,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r11,4
	var_r29 = (uint32_t)(ctx.r11.s64 + 4);  // addr:0x82000004
	// lvlx v11,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r11,36
	var_r26 = (uint32_t)(ctx.r11.s64 + 36);  // addr:0x82000024
	// lvlx v9,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r11,32
	var_r30 = (uint32_t)(ctx.r11.s64 + 32);  // addr:0x82000020
	// addi r28,r11,28
	var_r28 = (uint32_t)(ctx.r11.s64 + 28);  // addr:0x8200001c
	// lvlx v8,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v5,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r11,24
	ctx.r8.s64 = ctx.r11.s64 + 24;
	// vsldoi v0,v0,v11,4
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
	// lvlx v3,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v11,v30,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// lvlx v7,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v10,v28,v8,4
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
	// lvlx v4,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v12,v12,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// lvlx v1,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v8,v25,v3,4
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
	// lvlx v3,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v29,v9,4
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 12));
	// vor v30,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// vsldoi v9,v27,v5,4
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
	// vor v28,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_load_si128((simde__m128i*)ctx.v10.u8));
	// vsldoi v5,v26,v4,4
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
	// vsldoi v6,v6,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// vor v25,v8,v8
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_load_si128((simde__m128i*)ctx.v8.u8));
	// vsldoi v4,v24,v1,4
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 12));
	// vor v29,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v7,v23,v31,4
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)ctx.v31.u8), 12));
	// vor v27,v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v9.u8));
	// vsldoi v3,v22,v3,4
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
	// vor v26,v5,v5
	simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_load_si128((simde__m128i*)ctx.v5.u8));
	// vor v24,v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v4.u8));
	// vor v23,v7,v7
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_load_si128((simde__m128i*)ctx.v7.u8));
	// vor v22,v3,v3
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_load_si128((simde__m128i*)ctx.v3.u8));
	// bne cr6,0x8247179c
	if (!ctx.cr6.eq) goto loc_8247179C;
	// vsrw v1,v19,v18
	ctx.v1.u32[0] = ctx.v19.u32[0] >> (ctx.v18.u8[0] & 0x1F);
	ctx.v1.u32[1] = ctx.v19.u32[1] >> (ctx.v18.u8[4] & 0x1F);
	ctx.v1.u32[2] = ctx.v19.u32[2] >> (ctx.v18.u8[8] & 0x1F);
	ctx.v1.u32[3] = ctx.v19.u32[3] >> (ctx.v18.u8[12] & 0x1F);
	// vsubfp v21,v13,v11
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v21.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v31,v0,v12
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// rldicl r11,r10,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vsubfp v5,v5,v9
	simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r8,r5,5120
	ctx.r8.s64 = ctx.r5.s64 + 5120;
	// vsubfp v20,v6,v10
	simde_mm_store_ps(ctx.v20.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
	// rotlwi r11,r11,0
	ctx.r11.u64 = ctx.r11.u32;
	// vcfux v13,v1,31
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// addi r30,r5,4096
	var_r30 = (uint32_t)(ctx.r5.s64 + 4096);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r29,r5,3072
	var_r29 = (uint32_t)(ctx.r5.s64 + 3072);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r9,r5,2048
	ctx.r9.s64 = ctx.r5.s64 + 2048;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// add r3,r11,r3
	ctx.r3.u64 = ctx.r11.u64 + ctx.r3.u64;
	// xor r4,r4,r3
	ctx.r4.u64 = ctx.r4.u64 ^ ctx.r3.u64;
	// rlwinm r11,r4,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
	// vmulfp128 v1,v31,v13
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v31,v21,v13
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v21.f32), simde_mm_load_ps(ctx.v13.f32)));
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// vmulfp128 v5,v5,v13
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmaddfp v10,v20,v13,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v20.f32), simde_mm_load_ps(ctx.v13.f32)), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v1,v1,v12
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v11,v31,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v9,v5,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v31,v10,v2
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v10,v3,v7
	simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v5,v1,v2
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v1,v11,v2
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v11,v4,v8
	simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v21,v9,v2
	simde_mm_store_ps(ctx.v21.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v2.f32)));
	// stvx v31,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r5,1024
	ctx.r8.s64 = ctx.r5.s64 + 1024;
	// stvx v1,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v11,v11,v13
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v13,v10,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
	// stvx v21,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v11,v11,v8
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vaddfp v13,v13,v7
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v20,v11,v2
	simde_mm_store_ps(ctx.v20.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v15,v13,v2
	simde_mm_store_ps(ctx.v15.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v2.f32)));
	// stvx v20,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v15,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82471938
	if (!(ctx.cr6.eq)) {
		// li r9,128
		ctx.r9.s64 = 128;
		// dcbt r9,r3
	}
loc_82471938:
	// addi r6,r6,-4
	ctx.r6.s64 = ctx.r6.s64 + -4;
	// vaddfp v2,v2,v17
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v17.f32)));
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// vadduwm v19,v19,v16
	simde_mm_store_si128((simde__m128i*)ctx.v19.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v19.u32), simde_mm_load_si128((simde__m128i*)ctx.v16.u32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// cmpwi cr6,r6,0
	// bgt cr6,0x82471798
	if (ctx.r6.s32 > 0) goto loc_82471798;
	// lbz r8,13(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 13);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// rotlwi r6,r8,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r9,r7,r6
	ctx.r9.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r9,r11
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// blt cr6,0x82471980
	if (ctx.r9.u32 >= ctx.r11.u32) {
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
	}
loc_82471980:
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r5,r9,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r9.s64;
	// stw r8,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
	// rlwinm r9,r5,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	// bge cr6,0x824719a0
	if (ctx.r9.u32 < ctx.r11.u32) {
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	}
loc_824719A0:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,48(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_19C8_w"))) PPC_WEAK_FUNC(phBoundBVH_19C8_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_19C8_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, savegprlr_29
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lfs f12,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r11
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fsubs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltisb v0,7
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x7)));
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lis r10,-32256
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r10,r10,28144
	ctx.r10.s64 = ctx.r10.s64 + 28144;
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vslb v29,v0,v0
	ctx.v29.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v29.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v29.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v29.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v29.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v29.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v29.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v29.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v29.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v29.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v29.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v29.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v29.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v29.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v29.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v29.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// vspltw v0,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// addi r30,r1,88
	var_r30 = (uint32_t)(ctx.r1.s64 + 88);
	// li r29,4
	var_r29 = 4;
	// li r7,0
	ctx.r7.s64 = 0;
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// addi r10,r10,28128
	ctx.r10.s64 = ctx.r10.s64 + 28128;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// addi r10,r10,28112
	ctx.r10.s64 = ctx.r10.s64 + 28112;
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// fdivs f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f9.f64));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f13,28080(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28080);  /* glob:lbl_82006DB0 @ 0x82006db0 */
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f0,f13
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v7,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v20,v7,0
	simde_mm_store_si128((simde__m128i*)ctx.v20.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
	// lvlx v6,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v12,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// vmulfp128 v11,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v10,v12,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v9,v12,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v12,v12,v8
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vaddfp v28,v0,v11
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v27,v0,v10
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v24,v0,v9
	simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vaddfp v22,v0,v12
	simde_mm_store_ps(ctx.v22.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfd f0,28072(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + 28072);  /* glob:lbl_82006DA8 @ 0x82006da8 */
	// fmul f7,f11,f0
	ctx.f7.f64 = ctx.f11.f64 * ctx.f0.f64;
	// fmul f6,f12,f0
	ctx.f6.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f7.f64));
	// stfd f5,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.f5.u64);
	// fctidz f4,f6
	ctx.f4.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfd f4,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f4.u64);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// lvlx v12,r30,r29
	temp.u32 = var_r30 + var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v0,r9,r8
	temp.u32 = ctx.r9.u32 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vadduwm v11,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vsldoi v10,v13,v0,4
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 12));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// vadduwm v8,v0,v11
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vadduwm v0,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vor v18,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v9,v10,v11,4
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v11,v9,v8,4
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
	// vadduwm v10,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vadduwm v21,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v23,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v9,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v9,v0,v0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vadduwm v19,v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v19.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v11,v10,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vadduwm v25,v11,v12
	simde_mm_store_si128((simde__m128i*)ctx.v25.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v10,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), simde_mm_load_si128((simde__m128i*)ctx.v0.u32)));
	// vspltisw v0,1
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x1)));
	// vadduwm v26,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
loc_82471B90:
	// li r10,16
	ctx.r10.s64 = 16;
loc_82471B94:
	// rldicl r9,r3,32,32
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// rotlwi r9,r9,0
	ctx.r9.u64 = ctx.r9.u32;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// cmplwi cr6,r10,0
	// addi r4,r9,1
	ctx.r4.s64 = ctx.r9.s64 + 1;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v12,v18,v12,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 15));
	// lvlx v11,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v13,v11,1
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 15));
	// vor v18,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// bne cr6,0x82471b94
	if (ctx.r10.u32 != 0) goto loc_82471B94;
	// vaddubm v12,v12,v29
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v29.u8)));
	// rldicl r10,r3,32,32
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vaddubm v11,v13,v29
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v29.u8)));
	// addi r9,r5,16
	ctx.r9.s64 = ctx.r5.s64 + 16;
	// vsrw v7,v26,v0
	ctx.v7.u32[0] = ctx.v26.u32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v7.u32[1] = ctx.v26.u32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v7.u32[2] = ctx.v26.u32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v7.u32[3] = ctx.v26.u32[3] >> (ctx.v0.u8[12] & 0x1F);
	// rotlwi r10,r10,0
	ctx.r10.u64 = ctx.r10.u32;
	// vsrw v6,v25,v0
	ctx.v6.u32[0] = ctx.v25.u32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v6.u32[1] = ctx.v25.u32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v6.u32[2] = ctx.v25.u32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v6.u32[3] = ctx.v25.u32[3] >> (ctx.v0.u8[12] & 0x1F);
	// addi r4,r5,32
	ctx.r4.s64 = ctx.r5.s64 + 32;
	// vupkhsb v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
	// vsrw v5,v23,v0
	ctx.v5.u32[0] = ctx.v23.u32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v5.u32[1] = ctx.v23.u32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v5.u32[2] = ctx.v23.u32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v5.u32[3] = ctx.v23.u32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vupkhsb v9,v11
	simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
	// vsrw v8,v21,v0
	ctx.v8.u32[0] = ctx.v21.u32[0] >> (ctx.v0.u8[0] & 0x1F);
	ctx.v8.u32[1] = ctx.v21.u32[1] >> (ctx.v0.u8[4] & 0x1F);
	ctx.v8.u32[2] = ctx.v21.u32[2] >> (ctx.v0.u8[8] & 0x1F);
	ctx.v8.u32[3] = ctx.v21.u32[3] >> (ctx.v0.u8[12] & 0x1F);
	// vupklsb v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vcfux v4,v7,31
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsb v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// vcfux v3,v6,31
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v7,v10
	simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
	// vcfux v2,v5,31
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v1,v9
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
	// vcfux v8,v8,31
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsh v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// vupklsh v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.s16)));
	// addi r10,r5,48
	ctx.r10.s64 = ctx.r5.s64 + 48;
	// vupkhsh v6,v12
	simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// vupklsh v5,v12
	simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vcfsx v12,v7,7
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupkhsh v31,v11
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
	// vcfsx v7,v1,7
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v30,v11
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// vcfsx v11,v10,7
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v1,v9,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v10,v6,7
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v9,v5,7
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v6,v31,7
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v5,v30,7
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v7,v7,v12
	simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v1,v1,v11
	simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v6,v6,v10
	simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsubfp v5,v5,v9
	simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v8,v7,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmaddfp v11,v1,v2,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v2.f32)), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v6,v6,v3
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v5,v5,v4
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v12,v8,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v11,v11,v24
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v24.f32)));
	// vaddfp v10,v6,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v9,v5,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v12,v12,v22
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v22.f32)));
	// stvx v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// xor r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 ^ ctx.r11.u64;
	// rlwinm r7,r9,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// vmulfp128 v10,v10,v27
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v27.f32)));
	// vmulfp128 v9,v9,v28
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v28.f32)));
	// cmplwi cr6,r7,0
	// stvx v12,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82471cc0
	if (ctx.r7.u32 != 0) {
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r11
	}
loc_82471CC0:
	// addi r6,r6,-16
	ctx.r6.s64 = ctx.r6.s64 + -16;
	// vaddfp v28,v28,v20
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v20.f32)));
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// vaddfp v27,v27,v20
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v20.f32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vaddfp v24,v24,v20
	simde_mm_store_ps(ctx.v24.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v24.f32), simde_mm_load_ps(ctx.v20.f32)));
	// cmpwi cr6,r6,0
	// vaddfp v22,v22,v20
	simde_mm_store_ps(ctx.v22.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v22.f32), simde_mm_load_ps(ctx.v20.f32)));
	// vadduwm v26,v26,v19
	simde_mm_store_si128((simde__m128i*)ctx.v26.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v26.u32), simde_mm_load_si128((simde__m128i*)ctx.v19.u32)));
	// vadduwm v25,v25,v19
	simde_mm_store_si128((simde__m128i*)ctx.v25.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v25.u32), simde_mm_load_si128((simde__m128i*)ctx.v19.u32)));
	// vadduwm v23,v23,v19
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v23.u32), simde_mm_load_si128((simde__m128i*)ctx.v19.u32)));
	// vadduwm v21,v21,v19
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v21.u32), simde_mm_load_si128((simde__m128i*)ctx.v19.u32)));
	// bgt cr6,0x82471b90
	if (ctx.r6.s32 > 0) goto loc_82471B90;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lbz r9,13(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 13);
	// divwu r11,r11,r9
	ctx.r11.u32 = ctx.r9.u32 ? ctx.r11.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x82471d1c
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82471D1C:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r8,r10,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82471d3c
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82471D3C:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f3,f0
	ctx.f3.f64 = double(float(ctx.f0.f64));
	// stfs f3,48(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_1D60_w"))) PPC_WEAK_FUNC(phBoundBVH_1D60_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_1D60_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_28
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lfs f12,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r10
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fsubs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltisb v0,7
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x7)));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32256
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,28144
	ctx.r11.s64 = ctx.r11.s64 + 28144;
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,88
	var_r30 = (uint32_t)(ctx.r1.s64 + 88);
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// li r29,4
	var_r29 = 4;
	// li r7,0
	ctx.r7.s64 = 0;
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28128
	ctx.r11.s64 = ctx.r11.s64 + 28128;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// fdivs f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f9.f64));
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,28080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28080);  /* glob:lbl_82006DB0 @ 0x82006db0 */
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f0,f13
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v16,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v16.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v6,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stvx v12,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v11,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// vmulfp128 v10,v11,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v9,v11,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v8,v11,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v11,v11,v7
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v4,v13,v10
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v3,v13,v9
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vaddfp v2,v13,v8
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vaddfp v1,v13,v11
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v12,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28072(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);  /* glob:lbl_82006DA8 @ 0x82006da8 */
	// fmul f7,f11,f0
	ctx.f7.f64 = ctx.f11.f64 * ctx.f0.f64;
	// fmul f6,f12,f0
	ctx.f6.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f7.f64));
	// stfd f5,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.f5.u64);
	// fctidz f4,f6
	ctx.f4.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfd f4,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f4.u64);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v12,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// lvlx v11,r30,r29
	temp.u32 = var_r30 + var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v13,r9,r8
	temp.u32 = ctx.r9.u32 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// vadduwm v10,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vsldoi v9,v12,v13,4
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// vadduwm v7,v13,v10
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vadduwm v13,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vor v17,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vsldoi v8,v9,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// vor v19,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vor v24,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vor v18,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// stvx v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsldoi v10,v8,v7,4
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
	// vadduwm v9,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v20,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v20.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vadduwm v21,v9,v11
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v8,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v8,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vadduwm v15,v8,v8
	simde_mm_store_si128((simde__m128i*)ctx.v15.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v10,v9,v13
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vadduwm v22,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v22.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v9,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// vspltisw v13,1
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r9,r5,1056
	ctx.r9.s64 = ctx.r5.s64 + 1056;
	// vadduwm v23,v9,v11
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v9,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_82471F38:
	// li r8,16
	ctx.r8.s64 = 16;
loc_82471F3C:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r4,r3
	ctx.r3.u64 = ctx.r4.u64 + ctx.r3.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// addi r30,r11,1
	var_r30 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82000001
	// addi r29,r11,3
	var_r29 = (uint32_t)(ctx.r11.s64 + 3);  // addr:0x82000003
	// addi r28,r11,2
	var_r28 = (uint32_t)(ctx.r11.s64 + 2);  // addr:0x82000002
	// lvlx v10,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v25,v17,v10,1
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 15));
	// lvlx v12,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v12,v19,v12,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 15));
	// vsldoi v11,v18,v11,1
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 15));
	// lvlx v9,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v24,v24,v9,1
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 15));
	// vor v17,v25,v25
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v25.u8));
	// vor v19,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vor v18,v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
	// bne cr6,0x82471f3c
	if (!ctx.cr6.eq) goto loc_82471F3C;
	// vaddubm v12,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vaddubm v11,v11,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// addi r8,r9,-32
	ctx.r8.s64 = ctx.r9.s64 + -32;
	// vsrw v8,v20,v13
	ctx.v8.u32[0] = ctx.v20.u32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v8.u32[1] = ctx.v20.u32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v8.u32[2] = ctx.v20.u32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v8.u32[3] = ctx.v20.u32[3] >> (ctx.v13.u8[12] & 0x1F);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vsrw v5,v21,v13
	ctx.v5.u32[0] = ctx.v21.u32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v5.u32[1] = ctx.v21.u32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v5.u32[2] = ctx.v21.u32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v5.u32[3] = ctx.v21.u32[3] >> (ctx.v13.u8[12] & 0x1F);
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// vupkhsb v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
	// vsrw v6,v22,v13
	ctx.v6.u32[0] = ctx.v22.u32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v6.u32[1] = ctx.v22.u32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v6.u32[2] = ctx.v22.u32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v6.u32[3] = ctx.v22.u32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vupkhsb v9,v11
	simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
	// vsrw v7,v23,v13
	ctx.v7.u32[0] = ctx.v23.u32[0] >> (ctx.v13.u8[0] & 0x1F);
	ctx.v7.u32[1] = ctx.v23.u32[1] >> (ctx.v13.u8[4] & 0x1F);
	ctx.v7.u32[2] = ctx.v23.u32[2] >> (ctx.v13.u8[8] & 0x1F);
	ctx.v7.u32[3] = ctx.v23.u32[3] >> (ctx.v13.u8[12] & 0x1F);
	// vupklsb v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vcfux v8,v8,31
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsb v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// vcfux v5,v5,31
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v31,v10
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
	// vcfux v6,v6,31
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v28,v9
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
	// vcfux v7,v7,31
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsh v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// vupklsh v9,v9
	simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.s16)));
	// addi r11,r9,-16
	ctx.r11.s64 = ctx.r9.s64 + -16;
	// vupkhsh v30,v12
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
	// vupklsh v29,v12
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vcfsx v12,v31,7
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v31,v28,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupkhsh v27,v11
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
	// vupklsh v26,v11
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// vcfsx v11,v10,7
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v9,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v10,v30,7
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v9,v29,7
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v31,v31,v12
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v28,v28,v11
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vsubfp v30,v30,v10
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vsubfp v29,v29,v9
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v31,v31,v8
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v28,v28,v5
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v30,v30,v6
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v29,v29,v7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v12,v31,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v11,v28,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v10,v30,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v9,v29,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v12,v12,v1
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v11,v11,v2
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v10,v10,v3
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v9,v9,v4
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r9,16
	ctx.r8.s64 = ctx.r9.s64 + 16;
	// vaddubm v12,v25,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// stvx v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddubm v11,v24,v0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// addi r11,r9,-1040
	ctx.r11.s64 = ctx.r9.s64 + -1040;
	// stvx v10,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsb v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
	// stvx v9,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsb v9,v11
	simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
	// vupklsb v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vupklsb v11,v11
	simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// vupkhsh v31,v10
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
	// vupkhsh v30,v9
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
	// vupkhsh v29,v12
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
	// vupklsh v28,v12
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vcfsx v12,v31,7
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
	// vcfsx v31,v30,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v27,v9
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.s16)));
	// vupkhsh v26,v11
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
	// vupklsh v25,v11
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
	// vcfsx v9,v28,7
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v11,v10,7
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v10,v29,7
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v31,v31,v12
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vsubfp v30,v30,v11
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r8,r9,-1024
	ctx.r8.s64 = ctx.r9.s64 + -1024;
	// vsubfp v29,v29,v10
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v10.f32)));
	// addi r30,r9,-1008
	var_r30 = (uint32_t)(ctx.r9.s64 + -1008);
	// vsubfp v28,v28,v9
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v9.f32)));
	// xor r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r10.u64;
	// vmulfp128 v8,v31,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v8.f32)));
	// rlwinm r7,r7,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// vmulfp128 v5,v30,v5
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v6,v29,v6
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v7,v28,v7
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v12,v8,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v11,v5,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v10,v6,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v9,v7,v9
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v8,v12,v1
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v7,v11,v2
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v6,v10,v3
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v5,v9,v4
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v4.f32)));
	// stvx v8,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v6,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v5,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82472140
	if (!(ctx.cr6.eq)) {
		// li r11,128
		ctx.r11.s64 = 128;
		// dcbt r11,r10
	}
loc_82472140:
	// addi r6,r6,-16
	ctx.r6.s64 = ctx.r6.s64 + -16;
	// vaddfp v4,v4,v16
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v16.f32)));
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// vaddfp v3,v3,v16
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v16.f32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vaddfp v2,v2,v16
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v16.f32)));
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// vaddfp v1,v1,v16
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v16.f32)));
	// cmpwi cr6,r6,0
	// vadduwm v23,v23,v15
	simde_mm_store_si128((simde__m128i*)ctx.v23.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v23.u32), simde_mm_load_si128((simde__m128i*)ctx.v15.u32)));
	// vadduwm v22,v22,v15
	simde_mm_store_si128((simde__m128i*)ctx.v22.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v22.u32), simde_mm_load_si128((simde__m128i*)ctx.v15.u32)));
	// vadduwm v21,v21,v15
	simde_mm_store_si128((simde__m128i*)ctx.v21.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v21.u32), simde_mm_load_si128((simde__m128i*)ctx.v15.u32)));
	// vadduwm v20,v20,v15
	simde_mm_store_si128((simde__m128i*)ctx.v20.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v20.u32), simde_mm_load_si128((simde__m128i*)ctx.v15.u32)));
	// bgt cr6,0x82471f38
	if (ctx.r6.s32 > 0) goto loc_82471F38;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lbz r9,13(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 13);
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r10.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x824721a0
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824721A0:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r8,r10,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x824721c0
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824721C0:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f3,f0
	ctx.f3.f64 = double(float(ctx.f0.f64));
	// stfs f3,48(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_21E8_w"))) PPC_WEAK_FUNC(phBoundBVH_21E8_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_21E8_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_25
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lfs f12,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r9
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fsubs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vspltisb v0,7
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x7)));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32256
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,28144
	ctx.r11.s64 = ctx.r11.s64 + 28144;
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,88
	var_r30 = (uint32_t)(ctx.r1.s64 + 88);
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// li r29,4
	var_r29 = 4;
	// li r7,0
	ctx.r7.s64 = 0;
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28128
	ctx.r11.s64 = ctx.r11.s64 + 28128;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// fdivs f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f9.f64));
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,28080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28080);  /* glob:lbl_82006DB0 @ 0x82006db0 */
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f0,f13
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw128 v61,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v61.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v6,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v11,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// vmulfp128 v6,v11,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v10,v11,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v5,v11,v8
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v11,v11,v7
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v8,v12,v6
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vaddfp v9,v12,v10
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v7,v12,v5
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v5.f32)));
	// vaddfp v6,v12,v11
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28072(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);  /* glob:lbl_82006DA8 @ 0x82006da8 */
	// fmul f7,f11,f0
	ctx.f7.f64 = ctx.f11.f64 * ctx.f0.f64;
	// fmul f6,f12,f0
	ctx.f6.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f7.f64));
	// stfd f5,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.f5.u64);
	// fctidz f4,f6
	ctx.f4.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfd f4,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.f4.u64);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v13,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r8,4
	ctx.r8.s64 = 4;
	// lvlx v11,r30,r29
	temp.u32 = var_r30 + var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v12,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// vadduwm v10,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vsldoi v5,v13,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// vadduwm v3,v12,v10
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vadduwm v12,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// vor v21,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v4,v5,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// vor v23,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor128 v62,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v15,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// stvx v5,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vor v20,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v10,v4,v3,4
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
	// vor v22,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v24,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor128 v63,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vadduwm v5,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v16,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v16.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vadduwm v17,v5,v11
	simde_mm_store_si128((simde__m128i*)ctx.v17.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v4,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v4,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vadduwm v14,v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v14.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.u32), simde_mm_load_si128((simde__m128i*)ctx.v4.u32)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v10,v5,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v12,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v18,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v18.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// vadduwm v19,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v19.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stvx v5,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltisw v5,1
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_set1_epi32(int(0x1)));
	// addi r10,r5,3104
	ctx.r10.s64 = ctx.r5.s64 + 3104;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_824723D0:
	// li r8,16
	ctx.r8.s64 = 16;
loc_824723D4:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r4,r3
	ctx.r3.u64 = ctx.r4.u64 + ctx.r3.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// addi r30,r11,3
	var_r30 = (uint32_t)(ctx.r11.s64 + 3);  // addr:0x82000003
	// addi r29,r11,7
	var_r29 = (uint32_t)(ctx.r11.s64 + 7);  // addr:0x82000007
	// addi r28,r11,2
	var_r28 = (uint32_t)(ctx.r11.s64 + 2);  // addr:0x82000002
	// addi r27,r11,6
	var_r27 = (uint32_t)(ctx.r11.s64 + 6);  // addr:0x82000006
	// lvlx v2,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r11,1
	var_r26 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82000001
	// vsldoi v21,v21,v2,1
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8), 15));
	// lvlx v13,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r25,r11,5
	var_r25 = (uint32_t)(ctx.r11.s64 + 5);  // addr:0x82000005
	// addi r30,r11,4
	var_r30 = (uint32_t)(ctx.r11.s64 + 4);  // addr:0x82000004
	// lvlx v12,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v13,v15,v13,1
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 15));
	// vsldoi128 v12,v63,v12,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 15));
	// lvlx v10,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi128 v25,v62,v11,1
	simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 15));
	// lvlx v4,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v3,0,r25
	temp.u32 = var_r25;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v24,v24,v10,1
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 15));
	// lvlx v1,0,r30
	temp.u32 = var_r30;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v23,v23,v4,1
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 15));
	// vsldoi v22,v22,v3,1
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 15));
	// vor v15,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v20,v20,v1,1
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 15));
	// vor128 v63,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vor128 v62,v25,v25
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_load_si128((simde__m128i*)ctx.v25.u8));
	// bne cr6,0x824723d4
	if (!ctx.cr6.eq) goto loc_824723D4;
	// vaddubm v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vaddubm v12,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// vsrw v11,v16,v5
	ctx.v11.u32[0] = ctx.v16.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v16.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v16.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v16.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vsrw v31,v18,v5
	ctx.v31.u32[0] = ctx.v18.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v31.u32[1] = ctx.v18.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v31.u32[2] = ctx.v18.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v31.u32[3] = ctx.v18.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// vupkhsb v4,v13
	simde_mm_store_si128((simde__m128i*)ctx.v4.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
	// vsrw v10,v19,v5
	ctx.v10.u32[0] = ctx.v19.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v10.u32[1] = ctx.v19.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v10.u32[2] = ctx.v19.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v10.u32[3] = ctx.v19.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// vupklsb v3,v13
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
	// vsrw v30,v17,v5
	ctx.v30.u32[0] = ctx.v17.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v30.u32[1] = ctx.v17.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v30.u32[2] = ctx.v17.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v30.u32[3] = ctx.v17.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// vupkhsb v2,v12
	simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
	// vcfux v13,v11,31
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vcfux v11,v31,31
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsb v1,v12
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// vupkhsh v31,v4
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vcfux v12,v10,31
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v29,v3
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vcfux v10,v30,31
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsh v28,v3
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// vupkhsh v3,v2
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// addi r11,r10,-16
	ctx.r11.s64 = ctx.r10.s64 + -16;
	// vupklsh v30,v4
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v2,v2
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vupkhsh v27,v1
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// vcfsx v31,v3,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v26,v1
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vcfsx v3,v30,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v2,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v2,v29,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v1,v28,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v27,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v26,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v31,v31,v4
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v30,v30,v3
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v31,v31,v13
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v30,v30,v10
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v29,v29,v11
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v4,v31,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v3,v30,v3
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddfp v2,v29,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v4,v4,v6
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v3,v3,v7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v2,v2,v8
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v1,v1,v9
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx v4,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// vaddubm v4,v25,v0
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// stvx v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddubm v3,v24,v0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// addi r11,r10,-1056
	ctx.r11.s64 = ctx.r10.s64 + -1056;
	// stvx v2,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsb v2,v4
	simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s8), simde_mm_load_si128((simde__m128i*)ctx.v4.s8))));
	// stvx v1,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsb v1,v3
	simde_mm_store_si128((simde__m128i*)ctx.v1.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s8), simde_mm_load_si128((simde__m128i*)ctx.v3.s8))));
	// vupklsb v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupklsb v3,v3
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vupkhsh v31,v2
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// vupkhsh v30,v1
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// vupkhsh v29,v4
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vupklsh v28,v4
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v2,v2
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vcfsx v31,v30,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v27,v1
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vupkhsh v26,v3
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vupklsh v25,v3
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vcfsx v1,v28,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v3,v2,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v2,v29,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v27,v31,v4
	simde_mm_store_ps(ctx.v27.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v26,v30,v3
	simde_mm_store_ps(ctx.v26.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddubm v31,v23,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddubm v30,v22,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// addi r8,r10,-1040
	ctx.r8.s64 = ctx.r10.s64 + -1040;
	// vmulfp128 v27,v27,v13
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v13.f32)));
	// addi r30,r10,-1024
	var_r30 = (uint32_t)(ctx.r10.s64 + -1024);
	// addi r29,r10,-1008
	var_r29 = (uint32_t)(ctx.r10.s64 + -1008);
	// vmulfp128 v26,v26,v10
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v25,v29,v11
	simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vupkhsb v29,v31
	simde_mm_store_si128((simde__m128i*)ctx.v29.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s8), simde_mm_load_si128((simde__m128i*)ctx.v31.s8))));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vupklsb v31,v31
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// vaddfp v4,v27,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v3,v26,v3
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddfp v2,v25,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v28,v4,v6
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vupkhsb v4,v30
	simde_mm_store_si128((simde__m128i*)ctx.v4.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v30.s8), simde_mm_load_si128((simde__m128i*)ctx.v30.s8))));
	// vmulfp128 v27,v3,v7
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupklsb v3,v30
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v30.s16)));
	// vmulfp128 v26,v2,v8
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vupkhsh v2,v29
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v29.s16), simde_mm_load_si128((simde__m128i*)ctx.v29.s16))));
	// vmulfp128 v25,v1,v9
	simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vupkhsh v30,v31
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s16), simde_mm_load_si128((simde__m128i*)ctx.v31.s16))));
	// vupklsh v1,v29
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v29.s16)));
	// stvx v28,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v31,v31
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// addi r11,r10,-2080
	ctx.r11.s64 = ctx.r10.s64 + -2080;
	// vupkhsh v29,v4
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vupklsh v28,v4
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vcfsx v4,v2,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v2,v1,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v1,v31,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v31,v29,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx v27,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v27,v3
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// stvx v26,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v26,v3
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vcfsx v3,v30,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx v25,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v30,v28,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r8,r10,-2064
	ctx.r8.s64 = ctx.r10.s64 + -2064;
	// vcfsx v29,v27,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r30,r10,-2048
	var_r30 = (uint32_t)(ctx.r10.s64 + -2048);
	// vcfsx v28,v26,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r29,r10,-2032
	var_r29 = (uint32_t)(ctx.r10.s64 + -2032);
	// vsubfp v31,v31,v4
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v30,v30,v2
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v29,v29,v3
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v31,v31,v13
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v30,v30,v10
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v29,v29,v11
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v27,v31,v4
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddubm v4,v21,v0
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vaddubm v31,v20,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vaddfp v26,v30,v2
	simde_mm_store_ps(ctx.v26.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vupkhsb v2,v4
	simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s8), simde_mm_load_si128((simde__m128i*)ctx.v4.s8))));
	// vaddfp v3,v29,v3
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vupkhsb v30,v31
	simde_mm_store_si128((simde__m128i*)ctx.v30.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s8), simde_mm_load_si128((simde__m128i*)ctx.v31.s8))));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vupklsb v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupklsb v31,v31
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// vupkhsh v29,v2
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vmulfp128 v60,v27,v6
	simde_mm_store_ps(ctx.v60.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vupkhsh v28,v4
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vupklsh v27,v4
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupkhsh v25,v31
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s16), simde_mm_load_si128((simde__m128i*)ctx.v31.s16))));
	// vmulfp128 v59,v26,v7
	simde_mm_store_ps(ctx.v59.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupkhsh v26,v30
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v30.s16), simde_mm_load_si128((simde__m128i*)ctx.v30.s16))));
	// vmulfp128 v58,v3,v8
	simde_mm_store_ps(ctx.v58.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vupklsh v30,v30
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v30.s16)));
	// vmulfp128 v57,v1,v9
	simde_mm_store_ps(ctx.v57.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx128 v60,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v60.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v59,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v59.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v58,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v58.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v57,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v57.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v31,v31
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// vcfsx v3,v2,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v4,v29,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r11,r10,-3088
	ctx.r11.s64 = ctx.r10.s64 + -3088;
	// vcfsx v2,v28,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r8,r10,-3072
	ctx.r8.s64 = ctx.r10.s64 + -3072;
	// vcfsx v1,v27,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r30,r10,-3056
	var_r30 = (uint32_t)(ctx.r10.s64 + -3056);
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// xor r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r9.u64;
	// vcfsx v30,v30,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// rlwinm r7,r7,0,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// vcfsx v31,v31,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// vsubfp v29,v29,v4
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v30,v30,v3
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vsubfp v28,v28,v2
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v31,v31,v1
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v13,v29,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v10,v30,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v11,v28,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v12,v31,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v13,v13,v4
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v10,v10,v3
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddfp v11,v11,v2
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v12,v12,v1
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v56,v13,v6
	simde_mm_store_ps(ctx.v56.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v55,v10,v7
	simde_mm_store_ps(ctx.v55.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v54,v11,v8
	simde_mm_store_ps(ctx.v54.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v53,v12,v9
	simde_mm_store_ps(ctx.v53.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx128 v56,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v56.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v55,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v55.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v54,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v54.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v53,r0,r30
	ea = (var_r30) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v53.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82472778
	if (!(ctx.cr6.eq)) {
		// li r11,128
		ctx.r11.s64 = 128;
		// dcbt r11,r9
	}
loc_82472778:
	// addi r6,r6,-16
	ctx.r6.s64 = ctx.r6.s64 + -16;
	// vaddfp128 v9,v9,v61
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v61.f32)));
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// vaddfp128 v8,v8,v61
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v61.f32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vaddfp128 v7,v7,v61
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v61.f32)));
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// vaddfp128 v6,v6,v61
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v61.f32)));
	// cmpwi cr6,r6,0
	// vadduwm v19,v19,v14
	simde_mm_store_si128((simde__m128i*)ctx.v19.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v19.u32), simde_mm_load_si128((simde__m128i*)ctx.v14.u32)));
	// vadduwm v18,v18,v14
	simde_mm_store_si128((simde__m128i*)ctx.v18.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v18.u32), simde_mm_load_si128((simde__m128i*)ctx.v14.u32)));
	// vadduwm v17,v17,v14
	simde_mm_store_si128((simde__m128i*)ctx.v17.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v17.u32), simde_mm_load_si128((simde__m128i*)ctx.v14.u32)));
	// vadduwm v16,v16,v14
	simde_mm_store_si128((simde__m128i*)ctx.v16.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v16.u32), simde_mm_load_si128((simde__m128i*)ctx.v14.u32)));
	// bgt cr6,0x824723d0
	if (ctx.r6.s32 > 0) goto loc_824723D0;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lbz r9,13(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 13);
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r10.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x824727d8
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824727D8:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r8,r10,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x824727f8
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824727F8:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f3,f0
	ctx.f3.f64 = double(float(ctx.f0.f64));
	// stfs f3,48(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBoundBVH_2820_w"))) PPC_WEAK_FUNC(phBoundBVH_2820_w);
PPC_FUNC_IMPL(__imp__phBoundBVH_2820_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=240, savegprlr_25
	// mr r31,r7
	var_r31 = ctx.r7.u32;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lfs f12,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(var_r31 + 44);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,48(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,36(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 36);
	ctx.f0.f64 = double(temp.f32);
	// dcbt r0,r9
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fsubs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// vspltisw v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_set1_epi32(int(0x0)));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// vspltisb v0,7
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x7)));
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32256
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// addi r11,r11,28144
	ctx.r11.s64 = ctx.r11.s64 + 28144;
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r29,4
	var_r29 = 4;
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// li r30,0
	var_r30 = 0;
	// vslb v0,v0,v0
	ctx.v0.u8[0] = ctx.v0.u8[0] << (ctx.v0.u8[0] & 0x7);
	ctx.v0.u8[1] = ctx.v0.u8[1] << (ctx.v0.u8[1] & 0x7);
	ctx.v0.u8[2] = ctx.v0.u8[2] << (ctx.v0.u8[2] & 0x7);
	ctx.v0.u8[3] = ctx.v0.u8[3] << (ctx.v0.u8[3] & 0x7);
	ctx.v0.u8[4] = ctx.v0.u8[4] << (ctx.v0.u8[4] & 0x7);
	ctx.v0.u8[5] = ctx.v0.u8[5] << (ctx.v0.u8[5] & 0x7);
	ctx.v0.u8[6] = ctx.v0.u8[6] << (ctx.v0.u8[6] & 0x7);
	ctx.v0.u8[7] = ctx.v0.u8[7] << (ctx.v0.u8[7] & 0x7);
	ctx.v0.u8[8] = ctx.v0.u8[8] << (ctx.v0.u8[8] & 0x7);
	ctx.v0.u8[9] = ctx.v0.u8[9] << (ctx.v0.u8[9] & 0x7);
	ctx.v0.u8[10] = ctx.v0.u8[10] << (ctx.v0.u8[10] & 0x7);
	ctx.v0.u8[11] = ctx.v0.u8[11] << (ctx.v0.u8[11] & 0x7);
	ctx.v0.u8[12] = ctx.v0.u8[12] << (ctx.v0.u8[12] & 0x7);
	ctx.v0.u8[13] = ctx.v0.u8[13] << (ctx.v0.u8[13] & 0x7);
	ctx.v0.u8[14] = ctx.v0.u8[14] << (ctx.v0.u8[14] & 0x7);
	ctx.v0.u8[15] = ctx.v0.u8[15] << (ctx.v0.u8[15] & 0x7);
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28128
	ctx.r11.s64 = ctx.r11.s64 + 28128;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// lvx128 v8,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// fdivs f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f9.f64));
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,28080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28080);  /* glob:lbl_82006DB0 @ 0x82006db0 */
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f0,f13
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw128 v60,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v60.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvlx v6,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// stvx v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v11,v6,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vmulfp128 v6,v11,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v10,v11,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v5,v11,v8
	simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v11,v11,v7
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v8,v12,v6
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vaddfp v9,v12,v10
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v7,v12,v5
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v5.f32)));
	// vaddfp v6,v12,v11
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// stvx v13,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// stvx v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28072(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28072);  /* glob:lbl_82006DA8 @ 0x82006da8 */
	// fmul f7,f11,f0
	ctx.f7.f64 = ctx.f11.f64 * ctx.f0.f64;
	// fmul f6,f12,f0
	ctx.f6.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f7.f64));
	// stfd f5,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.f5.u64);
	// fctidz f4,f6
	ctx.f4.s64 = (ctx.f6.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f6.f64));
	// stfd f4,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.f4.u64);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stvx v13,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// lvlx v12,r4,r3
	temp.u32 = ctx.r4.u32 + ctx.r3.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v11,r10,r29
	temp.u32 = ctx.r10.u32 + var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// vadduwm v10,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vsldoi v5,v13,v12,4
	simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vadduwm v3,v12,v10
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vadduwm v12,v10,v10
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// vor v16,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v4,v5,v10,4
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
	// vor v18,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v20,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v22,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// stvx v5,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vor128 v61,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v10,v4,v3,4
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
	// vor128 v63,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v15,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v17,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v19,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vadduwm v5,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vor v21,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor v23,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vor128 v62,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// stvx v5,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v3,v5,v11
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r10,r5,5152
	ctx.r10.s64 = ctx.r5.s64 + 5152;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stvx v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// vadduwm v4,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vadduwm v4,v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.u32), simde_mm_load_si128((simde__m128i*)ctx.v4.u32)));
	// stvx v4,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v3,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v10,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvx v10,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v10,v5,v12
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v5.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vspltisw v5,1
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_set1_epi32(int(0x1)));
	// stvx v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vadduwm v12,v10,v12
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v12.u32)));
	// vadduwm v2,v10,v11
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// stvx v12,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vadduwm v14,v12,v11
	simde_mm_store_si128((simde__m128i*)ctx.v14.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v11.u32)));
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stvx v2,r0,r3
	ea = (ctx.r3.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_82472A38:
	// li r8,16
	ctx.r8.s64 = 16;
loc_82472A3C:
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = ctx.r11.u32;
	// add r3,r4,r3
	ctx.r3.u64 = ctx.r4.u64 + ctx.r3.u64;
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r7,r11,5
	ctx.r7.s64 = ctx.r11.s64 + 5;
	// addi r29,r11,11
	var_r29 = (uint32_t)(ctx.r11.s64 + 11);  // addr:0x8200000b
	// addi r28,r11,4
	var_r28 = (uint32_t)(ctx.r11.s64 + 4);  // addr:0x82000004
	// addi r27,r11,10
	var_r27 = (uint32_t)(ctx.r11.s64 + 10);  // addr:0x8200000a
	// lvlx v29,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r11,3
	var_r26 = (uint32_t)(ctx.r11.s64 + 3);  // addr:0x82000003
	// vsldoi v16,v16,v29,1
	simde_mm_store_si128((simde__m128i*)ctx.v16.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)ctx.v29.u8), 15));
	// lvlx v13,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r11,2
	ctx.r7.s64 = ctx.r11.s64 + 2;
	// lvlx v12,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r25,r11,9
	var_r25 = (uint32_t)(ctx.r11.s64 + 9);  // addr:0x82000009
	// lvlx v11,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r29,r11,8
	var_r29 = (uint32_t)(ctx.r11.s64 + 8);  // addr:0x82000008
	// lvlx v10,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r28,r11,1
	var_r28 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82000001
	// addi r27,r11,7
	var_r27 = (uint32_t)(ctx.r11.s64 + 7);  // addr:0x82000007
	// vsldoi128 v24,v61,v11,1
	simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v61.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 15));
	// lvlx v2,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r11,6
	ctx.r7.s64 = ctx.r11.s64 + 6;
	// vsldoi128 v13,v63,v13,1
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v63.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 15));
	// lvlx v4,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi128 v12,v62,v12,1
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v62.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 15));
	// lvlx v3,0,r25
	temp.u32 = var_r25;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v1,0,r29
	temp.u32 = var_r29;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v23,v23,v10,1
	simde_mm_store_si128((simde__m128i*)ctx.v23.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 15));
	// lvlx v31,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v22,v22,v4,1
	simde_mm_store_si128((simde__m128i*)ctx.v22.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 15));
	// lvlx v30,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v21,v21,v3,1
	simde_mm_store_si128((simde__m128i*)ctx.v21.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 15));
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsldoi v20,v20,v2,1
	simde_mm_store_si128((simde__m128i*)ctx.v20.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8), 15));
	// vsldoi v19,v19,v1,1
	simde_mm_store_si128((simde__m128i*)ctx.v19.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 15));
	// vor128 v63,v13,v13
	simde_mm_store_si128((simde__m128i*)ctx.v63.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
	// vsldoi v18,v18,v31,1
	simde_mm_store_si128((simde__m128i*)ctx.v18.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)ctx.v31.u8), 15));
	// vor128 v62,v12,v12
	simde_mm_store_si128((simde__m128i*)ctx.v62.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
	// vsldoi v17,v17,v30,1
	simde_mm_store_si128((simde__m128i*)ctx.v17.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8), 15));
	// vor128 v61,v24,v24
	simde_mm_store_si128((simde__m128i*)ctx.v61.u8, simde_mm_load_si128((simde__m128i*)ctx.v24.u8));
	// vsldoi v15,v15,v11,1
	simde_mm_store_si128((simde__m128i*)ctx.v15.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 15));
	// bne cr6,0x82472a3c
	if (!ctx.cr6.eq) goto loc_82472A3C;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// vaddubm v13,v13,v0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// rldicl r11,r3,32,32
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u64, 32) & 0xFFFFFFFF;
	// vaddubm v12,v12,v0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsrw v10,v14,v5
	ctx.v10.u32[0] = ctx.v14.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v10.u32[1] = ctx.v14.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v10.u32[2] = ctx.v14.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v10.u32[3] = ctx.v14.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// clrldi r3,r3,32
	ctx.r3.u64 = ctx.r3.u64 & 0xFFFFFFFF;
	// rotlwi r11,r11,0
	ctx.r11.u64 = ctx.r11.u32;
	// vupkhsb v4,v13
	simde_mm_store_si128((simde__m128i*)ctx.v4.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
	// lvx128 v1,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vsrw v11,v1,v5
	ctx.v11.u32[0] = ctx.v1.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v1.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v1.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v1.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// vupklsb v3,v13
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
	// add r7,r11,r8
	ctx.r7.u64 = ctx.r11.u64 + ctx.r8.u64;
	// vupkhsb v2,v12
	simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// vupklsb v1,v12
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vcfux v12,v10,31
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vcfux v13,v11,31
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v29,v3
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vupklsh v28,v3
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vupkhsh v3,v2
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// lvx128 v31,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v27,v2
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vsrw v31,v31,v5
	ctx.v31.u32[0] = ctx.v31.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v31.u32[1] = ctx.v31.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v31.u32[2] = ctx.v31.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v31.u32[3] = ctx.v31.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// lvx128 v30,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsrw v30,v30,v5
	ctx.v30.u32[0] = ctx.v30.u32[0] >> (ctx.v5.u8[0] & 0x1F);
	ctx.v30.u32[1] = ctx.v30.u32[1] >> (ctx.v5.u8[4] & 0x1F);
	ctx.v30.u32[2] = ctx.v30.u32[2] >> (ctx.v5.u8[8] & 0x1F);
	ctx.v30.u32[3] = ctx.v30.u32[3] >> (ctx.v5.u8[12] & 0x1F);
	// vupkhsh v26,v1
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// vupklsh v25,v1
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vcfsx v2,v29,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v1,v28,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// rlwinm r11,r7,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vcfux v11,v31,31
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupkhsh v31,v4
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vcfux v10,v30,31
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
	// vupklsh v30,v4
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r7,r10,-32
	ctx.r7.s64 = ctx.r10.s64 + -32;
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r11,r10,-16
	ctx.r11.s64 = ctx.r10.s64 + -16;
	// vcfsx v31,v3,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// vcfsx v3,v30,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vsubfp v31,v31,v4
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v30,v30,v3
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v29,v29,v11
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v31,v31,v13
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v30,v30,v10
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v2,v29,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vaddfp v4,v31,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v31,v30,v3
	simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddubm v3,v23,v0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vmulfp128 v27,v2,v8
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v26,v1,v9
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vupkhsb v1,v3
	simde_mm_store_si128((simde__m128i*)ctx.v1.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s8), simde_mm_load_si128((simde__m128i*)ctx.v3.s8))));
	// vupklsb v3,v3
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vmulfp128 v29,v4,v6
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vaddubm v4,v24,v0
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vmulfp128 v28,v31,v7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupklsh v25,v3
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vupkhsb v2,v4
	simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s8), simde_mm_load_si128((simde__m128i*)ctx.v4.s8))));
	// vupklsb v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupkhsh v31,v2
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// stvx v27,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v30,v4
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// stvx v26,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v2,v2
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vupklsh v27,v1
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// stvx v29,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v29,v4
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupkhsh v26,v3
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx v28,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v28,v1
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// vcfsx v31,v28,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r7,r10,-1056
	ctx.r7.s64 = ctx.r10.s64 + -1056;
	// vcfsx v3,v2,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r11,r10,-1040
	ctx.r11.s64 = ctx.r10.s64 + -1040;
	// vcfsx v2,v30,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r8,r10,-1024
	ctx.r8.s64 = ctx.r10.s64 + -1024;
	// vcfsx v1,v29,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r29,r10,-1008
	var_r29 = (uint32_t)(ctx.r10.s64 + -1008);
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vsubfp v27,v31,v4
	simde_mm_store_ps(ctx.v27.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddubm v31,v22,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v26,v30,v3
	simde_mm_store_ps(ctx.v26.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddubm v30,v21,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v27,v27,v13
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v26,v26,v10
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v25,v29,v11
	simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vupkhsb v29,v31
	simde_mm_store_si128((simde__m128i*)ctx.v29.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s8), simde_mm_load_si128((simde__m128i*)ctx.v31.s8))));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v4,v27,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v3,v26,v3
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddfp v2,v25,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v25.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v25,v4,v6
	simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vupkhsb v4,v30
	simde_mm_store_si128((simde__m128i*)ctx.v4.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v30.s8), simde_mm_load_si128((simde__m128i*)ctx.v30.s8))));
	// vmulfp128 v24,v3,v7
	simde_mm_store_ps(ctx.v24.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupklsb v3,v31
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// vmulfp128 v59,v2,v8
	simde_mm_store_ps(ctx.v59.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vupklsb v2,v30
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v30.s16)));
	// vupklsh v31,v29
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v29.s16)));
	// vmulfp128 v58,v1,v9
	simde_mm_store_ps(ctx.v58.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vupkhsh v1,v29
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v29.s16), simde_mm_load_si128((simde__m128i*)ctx.v29.s16))));
	// vupkhsh v30,v4
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vupklsh v27,v4
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupkhsh v29,v3
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vcfsx v1,v1,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupklsh v28,v3
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vcfsx v31,v30,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupkhsh v26,v2
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// stvx v25,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v25,v2
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vcfsx v3,v29,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r7,r10,-2080
	ctx.r7.s64 = ctx.r10.s64 + -2080;
	// vcfsx v2,v28,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx v24,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v59,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v59.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v58,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v58.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r10,-2064
	ctx.r11.s64 = ctx.r10.s64 + -2064;
	// addi r8,r10,-2048
	ctx.r8.s64 = ctx.r10.s64 + -2048;
	// addi r29,r10,-2032
	var_r29 = (uint32_t)(ctx.r10.s64 + -2032);
	// vsubfp v31,v31,v1
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vsubfp v30,v30,v4
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v29,v29,v3
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vsubfp v28,v28,v2
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vmulfp128 v31,v31,v13
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v26,v30,v10
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v29,v29,v11
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v27,v31,v1
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vaddubm v1,v20,v0
	simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vaddubm v31,v19,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vupkhsb v30,v1
	simde_mm_store_si128((simde__m128i*)ctx.v30.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s8), simde_mm_load_si128((simde__m128i*)ctx.v1.s8))));
	// vupklsb v1,v1
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vaddfp v29,v29,v3
	simde_mm_store_ps(ctx.v29.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vupklsb v3,v31
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// vaddfp v2,v28,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vupkhsh v28,v1
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// vupklsh v1,v1
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vmulfp128 v57,v27,v6
	simde_mm_store_ps(ctx.v57.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vaddfp v27,v26,v4
	simde_mm_store_ps(ctx.v27.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vupkhsb v4,v31
	simde_mm_store_si128((simde__m128i*)ctx.v4.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s8), simde_mm_load_si128((simde__m128i*)ctx.v31.s8))));
	// vupkhsh v31,v30
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v30.s16), simde_mm_load_si128((simde__m128i*)ctx.v30.s16))));
	// vupklsh v30,v30
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v30.s16)));
	// stvx128 v57,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v57.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupkhsh v26,v4
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vmulfp128 v55,v29,v8
	simde_mm_store_ps(ctx.v55.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vupklsh v25,v4
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupkhsh v29,v3
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vmulfp128 v56,v27,v7
	simde_mm_store_ps(ctx.v56.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupklsh v27,v3
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vmulfp128 v54,v2,v9
	simde_mm_store_ps(ctx.v54.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vcfsx v31,v26,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r7,r10,-3104
	ctx.r7.s64 = ctx.r10.s64 + -3104;
	// vcfsx v2,v28,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v29,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v1,v1,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v27,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v3,v30,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v30,v25,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v55,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v55.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r10,-3072
	ctx.r8.s64 = ctx.r10.s64 + -3072;
	// stvx128 v56,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v56.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r10,-3088
	ctx.r11.s64 = ctx.r10.s64 + -3088;
	// stvx128 v54,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v54.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r29,r10,-3056
	var_r29 = (uint32_t)(ctx.r10.s64 + -3056);
	// vsubfp v31,v31,v4
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vsubfp v30,v30,v3
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v31,v31,v13
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v29,v29,v11
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v30,v30,v10
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v4,v31,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v2,v29,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vaddfp v31,v30,v3
	simde_mm_store_ps(ctx.v31.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddubm v3,v17,v0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vmulfp128 v53,v4,v6
	simde_mm_store_ps(ctx.v53.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vaddubm v4,v18,v0
	simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vmulfp128 v51,v2,v8
	simde_mm_store_ps(ctx.v51.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vupkhsb v2,v4
	simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s8), simde_mm_load_si128((simde__m128i*)ctx.v4.s8))));
	// vmulfp128 v50,v1,v9
	simde_mm_store_ps(ctx.v50.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vupkhsb v1,v3
	simde_mm_store_si128((simde__m128i*)ctx.v1.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s8), simde_mm_load_si128((simde__m128i*)ctx.v3.s8))));
	// vupklsb v4,v4
	simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vmulfp128 v52,v31,v7
	simde_mm_store_ps(ctx.v52.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupklsb v3,v3
	simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vupkhsh v31,v2
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// vupklsh v2,v2
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vupkhsh v30,v4
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vupklsh v29,v4
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// vupkhsh v26,v3
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vupkhsh v28,v1
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// stvx128 v53,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v53.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v27,v1
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// stvx128 v51,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v51.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v25,v3
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vcfsx v3,v2,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v2,v30,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v50,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v50.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v1,v29,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r7,r10,-4128
	ctx.r7.s64 = ctx.r10.s64 + -4128;
	// vcfsx v31,v28,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v52,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v52.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vcfsx v30,v27,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r11,r10,-4112
	ctx.r11.s64 = ctx.r10.s64 + -4112;
	// vcfsx v29,v26,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r8,r10,-4096
	ctx.r8.s64 = ctx.r10.s64 + -4096;
	// vcfsx v28,v25,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r29,r10,-4080
	var_r29 = (uint32_t)(ctx.r10.s64 + -4080);
	// vsubfp v27,v31,v4
	simde_mm_store_ps(ctx.v27.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddubm v31,v16,v0
	simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v16.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v26,v30,v3
	simde_mm_store_ps(ctx.v26.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddubm v30,v15,v0
	simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v15.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v27,v27,v13
	simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v26,v26,v10
	simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v29,v29,v11
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v28,v28,v12
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v4,v27,v4
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v3,v26,v3
	simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v26.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddfp v2,v29,v2
	simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v1,v28,v1
	simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// xor r30,r30,r9
	var_r30 = (uint32_t)(var_r30 ^ ctx.r9.u64);
	// vmulfp128 v49,v4,v6
	simde_mm_store_ps(ctx.v49.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vupkhsb v4,v31
	simde_mm_store_si128((simde__m128i*)ctx.v4.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v31.s8), simde_mm_load_si128((simde__m128i*)ctx.v31.s8))));
	// vmulfp128 v48,v3,v7
	simde_mm_store_ps(ctx.v48.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vupkhsb v3,v30
	simde_mm_store_si128((simde__m128i*)ctx.v3.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v30.s8), simde_mm_load_si128((simde__m128i*)ctx.v30.s8))));
	// vmulfp128 v47,v2,v8
	simde_mm_store_ps(ctx.v47.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vupklsb v2,v31
	simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v31.s16)));
	// rlwinm r30,r30,0,0,24
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0xFFFFFF80);
	// vupkhsh v31,v4
	simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
	// vupklsh v29,v4
	simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(var_r30, 0, ctx.xer);
	// vupkhsh v28,v2
	simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
	// vupklsh v27,v2
	simde_mm_store_si128((simde__m128i*)ctx.v27.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
	// vupklsh v26,v3
	simde_mm_store_si128((simde__m128i*)ctx.v26.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
	// vcfsx v4,v31,7
	simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v2,v28,7
	simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vmulfp128 v46,v1,v9
	simde_mm_store_ps(ctx.v46.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vupklsb v1,v30
	simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v30.s16)));
	// vupkhsh v30,v3
	simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
	// vcfsx v3,v29,7
	simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v49,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v49.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r10,-5136
	ctx.r7.s64 = ctx.r10.s64 + -5136;
	// stvx128 v48,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v48.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r10,-5120
	ctx.r11.s64 = ctx.r10.s64 + -5120;
	// vupkhsh v25,v1
	simde_mm_store_si128((simde__m128i*)ctx.v25.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
	// stvx128 v47,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v47.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vupklsh v24,v1
	simde_mm_store_si128((simde__m128i*)ctx.v24.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
	// vcfsx v31,v30,7
	simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v1,v27,7
	simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v27.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// addi r8,r10,-5104
	ctx.r8.s64 = ctx.r10.s64 + -5104;
	// vcfsx v30,v26,7
	simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v26.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v29,v25,7
	simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v25.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// vcfsx v28,v24,7
	simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v24.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
	// stvx128 v46,r0,r29
	ea = (var_r29) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v46.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vsubfp v31,v31,v4
	simde_mm_store_ps(ctx.v31.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vsubfp v30,v30,v3
	simde_mm_store_ps(ctx.v30.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vsubfp v29,v29,v2
	simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vsubfp v28,v28,v1
	simde_mm_store_ps(ctx.v28.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v13,v31,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v10,v30,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v11,v29,v11
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v12,v28,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v13,v13,v4
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v4.f32)));
	// vaddfp v10,v10,v3
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v3.f32)));
	// vaddfp v11,v11,v2
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v2.f32)));
	// vaddfp v12,v12,v1
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v1.f32)));
	// vmulfp128 v45,v13,v6
	simde_mm_store_ps(ctx.v45.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v44,v10,v7
	simde_mm_store_ps(ctx.v44.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v43,v11,v8
	simde_mm_store_ps(ctx.v43.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v42,v12,v9
	simde_mm_store_ps(ctx.v42.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// stvx128 v45,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v45.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v44,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v44.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v43,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v43.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx128 v42,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v42.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// beq cr6,0x82472fb0
	if (!(ctx.cr6.eq)) {
		// li r7,128
		ctx.r7.s64 = 128;
		// dcbt r7,r9
	}
loc_82472FB0:
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// vaddfp128 v9,v9,v60
	ctx.fpscr.enableFlushMode();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v60.f32)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vaddfp128 v8,v8,v60
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v60.f32)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vaddfp128 v7,v7,v60
	simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v60.f32)));
	// addi r6,r6,-16
	ctx.r6.s64 = ctx.r6.s64 + -16;
	// vaddfp128 v6,v6,v60
	simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v60.f32)));
	// mr r30,r9
	var_r30 = ctx.r9.u32;
	// lvx128 v13,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lvx128 v12,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lvx128 v10,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vadduwm v11,v12,v13
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vadduwm v4,v10,v13
	simde_mm_store_si128((simde__m128i*)ctx.v4.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// lvx128 v3,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// vadduwm v2,v3,v13
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// cmpwi cr6,r6,0
	// vadduwm v14,v14,v13
	simde_mm_store_si128((simde__m128i*)ctx.v14.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v14.u32), simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// stvx v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v4,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// stvx v2,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// bgt cr6,0x82472a38
	if (ctx.r6.s32 > 0) goto loc_82472A38;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lbz r9,13(r31)
	ctx.r9.u64 = PPC_LOAD_U8(var_r31 + 13);
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r10.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82473048
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82473048:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 20);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 24);
	// subf r8,r10,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r9,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82473068
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82473068:
	// stw r11,28(r31)
	PPC_STORE_U32(var_r31 + 28, ctx.r11.u32);
	// bl 0x825693c0
	phBoundBVH_93C0_gen(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfd f0,28064(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 28064);  /* glob:lbl_82006DA0 @ 0x82006da0 */
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f3,f0
	ctx.f3.f64 = double(float(ctx.f0.f64));
	// stfs f3,48(r31)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(var_r31 + 48, temp.u32);
	return;
}

__attribute__((alias("__imp__phBound_0"))) PPC_WEAK_FUNC(phBound_0);
PPC_FUNC_IMPL(__imp__phBound_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r15 = 0;
	uint32_t var_r14 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f860
	ctx.lr = 0x82473098;
	__savegprlr_14(ctx, base);
	// addi r20,r3,8
	var_r20 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r23,r3,4
	var_r23 = (uint32_t)(ctx.r3.s64 + 4);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r24,r3,13
	var_r24 = (uint32_t)(ctx.r3.s64 + 13);
	// addi r17,r3,28
	var_r17 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r18,r3,24
	var_r18 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r19,r3,20
	var_r19 = (uint32_t)(ctx.r3.s64 + 20);
	// lwz r4,0(r23)
	ctx.r4.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r22,r3,48
	var_r22 = (uint32_t)(ctx.r3.s64 + 48);
	// lbz r11,0(r24)
	ctx.r11.u64 = PPC_LOAD_U8(var_r24 + 0);
	// addi r28,r3,52
	var_r28 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r9,0(r17)
	ctx.r9.u64 = PPC_LOAD_U32(var_r17 + 0);
	// subf r25,r10,r4
	var_r25 = (uint32_t)(ctx.r4.s64 - ctx.r10.s64);
	// lwz r31,0(r18)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r5,0(r19)
	ctx.r5.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lfs f12,0(r22)
	temp.u32 = PPC_LOAD_U32(var_r22 + 0);
	ctx.f12.f64 = double(temp.f32);
	// subf r4,r9,r31
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r6,r5
	var_r29 = (uint32_t)(ctx.r6.u64 + ctx.r5.u64);
	// add r6,r10,r7
	ctx.r6.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r4,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r4.u32);
	// dcbt r0,r6
	// addi r10,r1,-192
	ctx.r10.s64 = ctx.r1.s64 + -192;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r9,r3,40
	ctx.r9.s64 = ctx.r3.s64 + 40;
	// rldicr r7,r8,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u64, 32);
	// addi r21,r3,36
	var_r21 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r8,r1,-192
	ctx.r8.s64 = ctx.r1.s64 + -192;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r31,r1,-184
	var_r31 = (uint32_t)(ctx.r1.s64 + -184);
	// std r7,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r7.u64);
	// lfd f0,-184(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r21
	temp.u32 = var_r21;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r30,r1,-192
	var_r30 = (uint32_t)(ctx.r1.s64 + -192);
	// li r9,4
	ctx.r9.s64 = 4;
	// addi r7,r1,-184
	ctx.r7.s64 = ctx.r1.s64 + -184;
	// li r27,4
	var_r27 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v13,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25848);
	// addi r10,r1,-176
	ctx.r10.s64 = ctx.r1.s64 + -176;
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.f10.u64);
	// vmulfp128 v13,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r31)
	PPC_STORE_U64(var_r31 + 0, ctx.f9.u64);
	// ld r26,-184(r1)
	var_r26 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -184));
	// lvlx v11,r30,r9
	temp.u32 = var_r30 + ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r30,-192(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -192));
	// lvlx v9,r7,r27
	temp.u32 = ctx.r7.u32 + var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmpdi cr6,r30,0
	ctx.cr6.compare<int64_t>((int64_t)(int32_t)var_r30, 0, ctx.xer);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v9,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vspltw v8,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// bge cr6,0x8247323c
while ((int64_t)(int32_t)var_r30 < 0) {
	loc_824731AC:
		// cmpwi cr6,r4,0
		// ble cr6,0x8247323c
		if (ctx.r4.s32 <= 0) goto loc_8247323C;
		// cmplwi cr6,r11,0
		// beq cr6,0x82473220
		if (ctx.r11.u32 == 0) goto loc_82473220;
		// vspltisb v13,1
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x1)));
		// mr r10,r6
		ctx.r10.u64 = ctx.r6.u64;
		// vspltisw v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_set1_epi32(int(0x0)));
		// mr r8,r29
		ctx.r8.u64 = var_r29;
		// subf r7,r6,r28
		ctx.r7.s64 = (int64_t)(int32_t)var_r28 - ctx.r6.s64;
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
		// vsr v13,v10,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8)));
		// vsubfp v12,v12,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vcfux v13,v13,31
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v12,v0,v12,4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
		// vmulfp128 v13,v13,v12
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vsldoi v11,v13,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
	loc_824731EC:
		// mr r31,r10
		var_r31 = ctx.r10.u32;
		// lvlx v13,r7,r10
		temp.u32 = ctx.r7.u32 + ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v13,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// mr r27,r8
		var_r27 = ctx.r8.u32;
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// addi r8,r8,1024
		ctx.r8.s64 = ctx.r8.s64 + 1024;
		// lvlx v12,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// cmplwi cr6,r9,0
		// vrlimi128 v13,v12,8,0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v12.f32), 228), 8));
		// vmsum3fp128 v13,v13,v11
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v13,r0,r27
		ea = (var_r27) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// bne cr6,0x824731ec
		if (ctx.r9.u32 != 0) goto loc_824731EC;
	loc_82473220:
		// add r30,r26,r30
		var_r30 = (uint32_t)(var_r26 + var_r30);
		// vaddfp v0,v0,v8
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// vadduwm v10,v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// addi r4,r4,-1
		ctx.r4.s64 = ctx.r4.s64 + -1;
		// cmpdi cr6,r30,0
		// blt cr6,0x824731ac
}
loc_8247323C:
	// sradi r9,r30,63
	ctx.xer.ca = ((int64_t)(int32_t)var_r30 < 0) & ((var_r30 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r9.s64 = (int64_t)(int32_t)var_r30 >> 63;
	// addi r27,r25,-1
	var_r27 = (uint32_t)(var_r25 + -1);
	// sradi r8,r30,32
	ctx.xer.ca = ((int64_t)(int32_t)var_r30 < 0) & ((var_r30 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 >> 32;
	// extsw r31,r27
	var_r31 = (uint32_t)((int32_t)var_r27);
	// subf r10,r9,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r9.s64;
	// cmpd cr6,r10,r31
	// bge cr6,0x82473338
while (ctx.r10.s64 < (int64_t)(int32_t)var_r31) {
	loc_82473258:
		// cmpwi cr6,r4,0
		// beq cr6,0x82473400
		if (ctx.r4.s32 == 0) goto loc_82473400;
		// cmplwi cr6,r11,0
		// beq cr6,0x824732ec
		if (ctx.r11.u32 != 0) {
			// vspltisb v13,1
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x1)));
			// extsw r9,r10
			ctx.r9.s64 = ctx.r10.s32;
			// vspltisw v12,0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_set1_epi32(int(0x0)));
			// mr r7,r29
			ctx.r7.u64 = var_r29;
			// addi r8,r9,1
			ctx.r8.s64 = ctx.r9.s64 + 1;
			// mullw r9,r9,r11
			ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
			// vsr v13,v10,v13
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8)));
			// vsubfp v12,v12,v0
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vcfux v13,v13,31
			simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
			// mullw r8,r8,r11
			ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
			// vsldoi v12,v0,v12,4
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
			// vmulfp128 v13,v13,v12
			simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
			// rlwinm r8,r8,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r9,r9,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
			// mr r10,r11
			ctx.r10.u64 = ctx.r11.u64;
			// add r8,r8,r6
			ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
			// add r9,r9,r6
			ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
			// vsldoi v11,v13,v0,8
			simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		loc_824732B0:
			// mr r16,r9
			var_r16 = ctx.r9.u32;
			// mr r15,r8
			var_r15 = ctx.r8.u32;
			// mr r14,r7
			var_r14 = ctx.r7.u32;
			// addi r10,r10,-1
			ctx.r10.s64 = ctx.r10.s64 + -1;
			// addi r9,r9,4
			ctx.r9.s64 = ctx.r9.s64 + 4;
			// lvlx v13,0,r16
			temp.u32 = var_r16;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
			// addi r8,r8,4
			ctx.r8.s64 = ctx.r8.s64 + 4;
			// vspltw v13,v13,0
			simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
			// lvlx v12,0,r15
			temp.u32 = var_r15;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
			// addi r7,r7,1024
			ctx.r7.s64 = ctx.r7.s64 + 1024;
			// cmplwi cr6,r10,0
			// vrlimi128 v13,v12,8,0
			simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v12.f32), 228), 8));
			// vmsum3fp128 v13,v13,v11
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
			// stvewx v13,r0,r14
			ea = (var_r14) & ~0x3;
			PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
			// bne cr6,0x824732b0
			if (ctx.r10.u32 != 0) goto loc_824732B0;
		}
	loc_824732EC:
		// add r30,r26,r30
		var_r30 = (uint32_t)(var_r26 + var_r30);
		// vaddfp v0,v0,v8
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
		// addi r29,r29,4
		var_r29 = (uint32_t)(var_r29 + 4);
		// vadduwm v10,v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// sradi r10,r30,32
		ctx.xer.ca = ((int64_t)(int32_t)var_r30 < 0) & ((var_r30 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = (int64_t)(int32_t)var_r30 >> 32;
		// addi r4,r4,-1
		ctx.r4.s64 = ctx.r4.s64 + -1;
		// extsw r7,r10
		ctx.r7.s64 = ctx.r10.s32;
		// mullw r9,r7,r11
		ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
		// rlwinm r9,r9,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// add r9,r9,r6
		ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
		// xor r8,r9,r5
		ctx.r8.u64 = ctx.r9.u64 ^ ctx.r5.u64;
		// rlwinm r7,r8,0,0,24
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r7,0
		// beq cr6,0x8247332c
		if (ctx.r7.u32 == 0) goto loc_8247332C;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r9
	loc_8247332C:
		// mr r5,r9
		ctx.r5.u64 = ctx.r9.u64;
		// cmpd cr6,r10,r31
		// blt cr6,0x82473258
}
loc_82473338:
	// cmpwi cr6,r4,0
	// beq cr6,0x82473400
	if (ctx.r4.s32 != 0) {
		// cmpd cr6,r10,r31
		// bne cr6,0x824733e8
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r31) {
			// mullw r4,r27,r11
			ctx.r4.s64 = int64_t((int32_t)var_r27) * int64_t(ctx.r11.s32);
			// rlwinm r9,r4,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// li r5,0
			ctx.r5.s64 = 0;
			// cmpwi cr6,r11,4
			// add r4,r9,r6
			ctx.r4.u64 = ctx.r9.u64 + ctx.r6.u64;
			// blt cr6,0x824733b4
			if (ctx.r11.s32 >= 4) {
				// addi r9,r11,-4
				ctx.r9.s64 = ctx.r11.s64 + -4;
				// addi r8,r4,12
				ctx.r8.s64 = ctx.r4.s64 + 12;
				// rlwinm r7,r9,30,2,31
				ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
				// addi r9,r28,4
				ctx.r9.s64 = (int64_t)(int32_t)var_r28 + 4;
				// addi r7,r7,1
				ctx.r7.s64 = ctx.r7.s64 + 1;
				// subf r31,r28,r4
				var_r31 = (uint32_t)(ctx.r4.s64 - (int64_t)(int32_t)var_r28);
				// rlwinm r5,r7,2,0,29
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			loc_82473380:
				// lfs f8,-12(r8)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -12);
				ctx.f8.f64 = double(temp.f32);
				// addi r7,r7,-1
				ctx.r7.s64 = ctx.r7.s64 + -1;
				// stfs f8,-4(r9)
				temp.f32 = float(ctx.f8.f64);
				PPC_STORE_U32(ctx.r9.u32 + -4, temp.u32);
				// lfsx f7,r31,r9
				temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r9.u32);
				ctx.f7.f64 = double(temp.f32);
				// cmplwi cr6,r7,0
				ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
				// stfs f7,0(r9)
				temp.f32 = float(ctx.f7.f64);
				PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
				// lfs f6,-4(r8)
				temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
				ctx.f6.f64 = double(temp.f32);
				// stfs f6,4(r9)
				temp.f32 = float(ctx.f6.f64);
				PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
				// lfs f5,0(r8)
				temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
				ctx.f5.f64 = double(temp.f32);
				// addi r8,r8,16
				ctx.r8.s64 = ctx.r8.s64 + 16;
				// stfs f5,8(r9)
				temp.f32 = float(ctx.f5.f64);
				PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
				// addi r9,r9,16
				ctx.r9.s64 = ctx.r9.s64 + 16;
				// bne cr6,0x82473380
				if (!ctx.cr6.eq) goto loc_82473380;
			}
		loc_824733B4:
			// cmplw cr6,r5,r11
			// bge cr6,0x82473400
			if (ctx.r5.u32 >= ctx.r11.u32) goto loc_82473400;
			// rlwinm r9,r5,2,0,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			// subf r7,r28,r4
			ctx.r7.s64 = ctx.r4.s64 - (int64_t)(int32_t)var_r28;
			// add r9,r9,r28
			ctx.r9.u64 = ctx.r9.u64 + var_r28;
			// subf r8,r5,r11
			ctx.r8.s64 = ctx.r11.s64 - ctx.r5.s64;
		loc_824733CC:
			// addi r8,r8,-1
			ctx.r8.s64 = ctx.r8.s64 + -1;
			// lfsx f4,r9,r7
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
			ctx.f4.f64 = double(temp.f32);
			// stfs f4,0(r9)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
			// addi r9,r9,4
			ctx.r9.s64 = ctx.r9.s64 + 4;
			// cmplwi cr6,r8,0
			// bne cr6,0x824733cc
			if (ctx.r8.u32 != 0) goto loc_824733CC;
			// b 0x82473400
		} else {
		loc_824733E8:
			// extsw r9,r25
			ctx.r9.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r9
			// ble cr6,0x82473400
			if (ctx.r10.s64 <= ctx.r9.s64) goto loc_82473400;
			// subf r8,r9,r10
			ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
			// rldicr r9,r8,32,31
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
			// add r30,r9,r30
			var_r30 = (uint32_t)(ctx.r9.u64 + var_r30);
		}
	}
loc_82473400:
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// stvewx v0,r0,r21
	ea = (var_r21) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rldicr r4,r10,32,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// lbz r8,0(r24)
	ctx.r8.u64 = PPC_LOAD_U8(var_r24 + 0);
	// mullw r5,r7,r11
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r5,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r6
	ctx.r10.u64 = ctx.r11.u64 + ctx.r6.u64;
	// subf r11,r4,r30
	ctx.r11.s64 = (int64_t)(int32_t)var_r30 - ctx.r4.s64;
	// rotlwi r6,r8,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r10,r7,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// std r11,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r11.u64);
	// lfd f3,-184(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// cmplw cr6,r10,r11
	// fmul f0,f2,f0
	ctx.f0.f64 = ctx.f2.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,0(r22)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r22 + 0, temp.u32);
	// bge cr6,0x8247345c
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247345C:
	// lwz r8,0(r19)
	ctx.r8.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lwz r9,0(r18)
	ctx.r9.u64 = PPC_LOAD_U32(var_r18 + 0);
	// subf r5,r8,r29
	ctx.r5.s64 = (int64_t)(int32_t)var_r29 - ctx.r8.s64;
	// stw r11,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r11.u32);
	// rlwinm r8,r5,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82473480
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r17)
		PPC_STORE_U32(var_r17 + 0, ctx.r8.u32);
		// b 0x8242f8b0
		__restgprlr_14(ctx, base);
		return;
	}
loc_82473480:
	// stw r9,0(r17)
	PPC_STORE_U32(var_r17 + 0, ctx.r9.u32);
	// b 0x8242f8b0
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_1"))) PPC_WEAK_FUNC(phBound_1);
PPC_FUNC_IMPL(__imp__phBound_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f874
	ctx.lr = 0x82473490;
	__savegprlr_19(ctx, base);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// addi r30,r3,13
	var_r30 = (uint32_t)(ctx.r3.s64 + 13);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r29,r3,4
	var_r29 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r28,r3,48
	var_r28 = (uint32_t)(ctx.r3.s64 + 48);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// lbz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U8(var_r30 + 0);
	// addi r31,r3,52
	var_r31 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r4,0(r21)
	ctx.r4.u64 = PPC_LOAD_U32(var_r21 + 0);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 0);
	// lfs f12,0(r28)
	temp.u32 = PPC_LOAD_U32(var_r28 + 0);
	ctx.f12.f64 = double(temp.f32);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r25,r11,r5
	var_r25 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r8,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r8.u32);
	// dcbt r0,r9
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r4,r3,40
	ctx.r4.s64 = ctx.r3.s64 + 40;
	// rldicr r10,r11,32,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u64, 32);
	// addi r27,r3,36
	var_r27 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lis r11,-32248
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r26,r1,-136
	var_r26 = (uint32_t)(ctx.r1.s64 + -136);
	// std r10,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r10.u64);
	// lfd f0,-136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r24,r1,-144
	var_r24 = (uint32_t)(ctx.r1.s64 + -144);
	// addi r10,r1,-136
	ctx.r10.s64 = ctx.r1.s64 + -136;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r6,r1,-128
	ctx.r6.s64 = ctx.r1.s64 + -128;
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r26)
	PPC_STORE_U64(var_r26 + 0, ctx.f9.u64);
	// lvlx v11,r24,r4
	temp.u32 = var_r24 + ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r11,-144(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lvlx v10,r10,r19
	temp.u32 = ctx.r10.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// ld r4,-136(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vspltw v10,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmpdi cr6,r11,0
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v11,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x82473610
	while (ctx.r11.s64 < 0) {
	loc_824735A4:
		// cmpwi cr6,r8,0
		// ble cr6,0x82473610
		if (ctx.r8.s32 <= 0) goto loc_82473610;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vor v6,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vor v5,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vspltisw v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_set1_epi32(int(0x0)));
		// lvlx v8,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v4,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// lvlx v7,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsr v6,v6,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vspltw v12,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vsubfp v9,v9,v5
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v5.f32)));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// vaddfp v0,v0,v11
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vadduwm v13,v13,v10
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// vcfux v8,v6,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// cmpdi cr6,r11,0
		ctx.cr6.compare<int64_t>(ctx.r11.s64, 0, ctx.xer);
		// vsldoi v9,v4,v9,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 12));
		// vmulfp128 v9,v8,v9
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vsldoi v9,v9,v3,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 8));
		// vmsum3fp128 v12,v12,v9
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v12,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// blt cr6,0x824735a4
}
loc_82473610:
	// sradi r10,r11,63
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r6,r11,32
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r11.s64 >> 32;
	// extsw r26,r24
	var_r26 = (uint32_t)((int32_t)var_r24);
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// cmpd cr6,r10,r26
	// bge cr6,0x824736d0
while (ctx.r10.s64 < (int64_t)(int32_t)var_r26) {
	loc_8247362C:
		// cmpwi cr6,r8,0
		// beq cr6,0x8247370c
		if (ctx.r8.s32 == 0) goto loc_8247370C;
		// extsw r10,r10
		ctx.r10.s64 = ctx.r10.s32;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_set1_epi32(int(0x0)));
		// vor v2,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vor v1,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// rlwinm r6,r10,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// vor v31,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vor v30,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// vsr v7,v2,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// rlwinm r10,r10,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// vsubfp v6,v9,v1
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v1.f32)));
		// vadduwm v13,v13,v10
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// lvlx v8,r6,r9
		temp.u32 = ctx.r6.u32 + ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddfp v0,v0,v11
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vspltw v12,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// vcfux v8,v7,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// lvlx v9,r10,r9
		temp.u32 = ctx.r10.u32 + ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r10,r11,32
		ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = ctx.r11.s64 >> 32;
		// vrlimi128 v12,v9,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// rlwinm r6,r10,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// xor r5,r6,r5
		ctx.r5.u64 = ctx.r6.u64 ^ ctx.r5.u64;
		// vsldoi v7,v31,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r5,0
		// vmulfp128 v9,v8,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vsldoi v9,v9,v30,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8), 8));
		// vmsum3fp128 v12,v12,v9
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v12,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// beq cr6,0x824736c4
		if (ctx.r5.u32 == 0) goto loc_824736C4;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r6
	loc_824736C4:
		// mr r5,r6
		ctx.r5.u64 = ctx.r6.u64;
		// cmpd cr6,r10,r26
		// blt cr6,0x8247362c
}
loc_824736D0:
	// cmpwi cr6,r8,0
	// beq cr6,0x8247370c
	if (ctx.r8.s32 != 0) {
		// cmpd cr6,r10,r26
		// bne cr6,0x824736f4
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r26) {
			// rlwinm r8,r24,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 2) & 0xFFFFFFFC;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// lfsx f8,r8,r9
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
			ctx.f8.f64 = double(temp.f32);
			// stfs f8,0(r31)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// b 0x8247370c
		} else {
		loc_824736F4:
			// extsw r8,r25
			ctx.r8.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r8
			// ble cr6,0x8247370c
			if (ctx.r10.s64 <= ctx.r8.s64) goto loc_8247370C;
			// subf r4,r8,r10
			ctx.r4.s64 = ctx.r10.s64 - ctx.r8.s64;
			// rldicr r8,r4,32,31
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u64, 32) & 0xFFFFFFFF00000000;
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
		}
	}
loc_8247370C:
	// rldicr r6,r10,32,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r27
	ea = (var_r27) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U8(var_r30 + 0);
	// subf r11,r6,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r3,r5,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// std r11,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r11.u64);
	// lfd f7,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
	// divwu r10,r4,r3
	ctx.r10.u32 = ctx.r3.u32 ? ctx.r4.u32 / ctx.r3.u32 : 0;
	// cmplw cr6,r10,r11
	// fmul f0,f6,f0
	ctx.f0.f64 = ctx.f6.f64 * ctx.f0.f64;
	// frsp f5,f0
	ctx.f5.f64 = double(float(ctx.f0.f64));
	// stfs f5,0(r28)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(var_r28 + 0, temp.u32);
	// bge cr6,0x82473760
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82473760:
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r8,r10,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82473784
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
		// b 0x8242f8c4
		__restgprlr_19(ctx, base);
		return;
	}
loc_82473784:
	// stw r9,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r9.u32);
	// b 0x8242f8c4
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_2"))) PPC_WEAK_FUNC(phBound_2);
PPC_FUNC_IMPL(__imp__phBound_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f86c
	ctx.lr = 0x82473798;
	__savegprlr_17(ctx, base);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r31,r3,52
	var_r31 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r4,0(r21)
	ctx.r4.u64 = PPC_LOAD_U32(var_r21 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// subf r7,r10,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r25,r11,r5
	var_r25 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r7,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r7.u32);
	// dcbt r0,r9
	// addi r6,r1,-160
	ctx.r6.s64 = ctx.r1.s64 + -160;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r4,r3,40
	ctx.r4.s64 = ctx.r3.s64 + 40;
	// rldicr r10,r11,32,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,-160
	ctx.r6.s64 = ctx.r1.s64 + -160;
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lis r11,-32248
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-152
	var_r30 = (uint32_t)(ctx.r1.s64 + -152);
	// std r10,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r10.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r24,r1,-160
	var_r24 = (uint32_t)(ctx.r1.s64 + -160);
	// addi r10,r1,-152
	ctx.r10.s64 = ctx.r1.s64 + -152;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// ld r11,-160(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// lvlx v11,r24,r4
	temp.u32 = var_r24 + ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r4,-152(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lvlx v10,r10,r19
	temp.u32 = ctx.r10.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v8,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r11,0
	// vspltw v9,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x82473944
	while (ctx.r11.s64 < 0) {
	loc_824738AC:
		// cmpwi cr6,r7,0
		// ble cr6,0x82473944
		if (ctx.r7.s32 <= 0) goto loc_82473944;
		// addi r10,r31,4
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 4;
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r6,r9,4
		ctx.r6.s64 = ctx.r9.s64 + 4;
		// addi r30,r8,1024
		var_r30 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// vsr v6,v13,v11
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// lvlx v12,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v5,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v7,0,r6
		temp.u32 = ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsr v11,v13,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsubfp v10,v10,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// cmpdi cr6,r11,0
		ctx.cr6.compare<int64_t>(ctx.r11.s64, 0, ctx.xer);
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v6,v11,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v11,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vsldoi v10,v0,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// vmulfp128 v11,v7,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vmulfp128 v10,v6,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vsldoi v11,v11,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v11,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v12,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// blt cr6,0x824738ac
}
loc_82473944:
	// sradi r10,r11,63
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r6,r11,32
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r11.s64 >> 32;
	// extsw r30,r24
	var_r30 = (uint32_t)((int32_t)var_r24);
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// cmpd cr6,r10,r30
	// bge cr6,0x82473a3c
while (ctx.r10.s64 < (int64_t)(int32_t)var_r30) {
	loc_82473960:
		// cmpwi cr6,r7,0
		// beq cr6,0x82473a84
		if (ctx.r7.s32 == 0) goto loc_82473A84;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// extsw r10,r10
		ctx.r10.s64 = ctx.r10.s32;
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r19,r8,1024
		var_r19 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// rlwinm r6,r10,3,0,28
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
		// addi r18,r10,1
		var_r18 = (uint32_t)(ctx.r10.s64 + 1);  // addr:0x82080001
		// vsr v10,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// add r10,r6,r9
		ctx.r10.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vsubfp v6,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r6,r18,3,0,28
		ctx.r6.u64 = __builtin_rotateleft64(var_r18 | (var_r18 << 32), 3) & 0xFFFFFFF8;
		// addi r18,r10,4
		var_r18 = (uint32_t)(ctx.r10.s64 + 4);  // lbl_82080004 @ 0x82080004
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// vcfux v10,v10,31
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vor v11,v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_load_si128((simde__m128i*)ctx.v7.u8));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// addi r17,r6,4
		var_r17 = (uint32_t)(ctx.r6.s64 + 4);  // addr:0x82000004
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// lvlx v12,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vcfux v4,v11,31
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v7,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsldoi v11,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vmulfp128 v11,v10,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vmulfp128 v10,v4,v6
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsldoi v11,v11,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// stvewx v12,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r10,r11,32
		ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = ctx.r11.s64 >> 32;
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v11,0,r6
		temp.u32 = ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// rlwinm r6,r10,3,0,28
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// xor r5,r6,r5
		ctx.r5.u64 = ctx.r6.u64 ^ ctx.r5.u64;
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// cmplwi cr6,r5,0
		// stvewx v12,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// beq cr6,0x82473a30
		if (ctx.r5.u32 == 0) goto loc_82473A30;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r6
	loc_82473A30:
		// mr r5,r6
		ctx.r5.u64 = ctx.r6.u64;
		// cmpd cr6,r10,r30
		// blt cr6,0x82473960
}
loc_82473A3C:
	// cmpwi cr6,r7,0
	// beq cr6,0x82473a84
	if (ctx.r7.s32 != 0) {
		// cmpd cr6,r10,r30
		// bne cr6,0x82473a6c
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r7,r24,3,0,28
			ctx.r7.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 3) & 0xFFFFFFF8;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// add r7,r7,r9
			ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
			// lfs f8,4(r7)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
			ctx.f8.f64 = double(temp.f32);
			// stfs f8,4(r31)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(var_r31 + 4, temp.u32);
			// lfs f7,0(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f7.f64 = double(temp.f32);
			// stfs f7,0(r31)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// b 0x82473a84
		} else {
		loc_82473A6C:
			// extsw r7,r25
			ctx.r7.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r7
			// ble cr6,0x82473a84
			if (ctx.r10.s64 <= ctx.r7.s64) goto loc_82473A84;
			// subf r4,r7,r10
			ctx.r4.s64 = ctx.r10.s64 - ctx.r7.s64;
			// rldicr r7,r4,32,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u64, 32) & 0xFFFFFFFF00000000;
			// add r11,r7,r11
			ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
		}
	}
loc_82473A84:
	// rldicr r6,r10,32,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r11,r6,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r3,r5,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// std r11,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r11.u64);
	// lfd f6,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// divwu r10,r4,r3
	ctx.r10.u32 = ctx.r3.u32 ? ctx.r4.u32 / ctx.r3.u32 : 0;
	// cmplw cr6,r10,r11
	// fmul f0,f5,f0
	ctx.f0.f64 = ctx.f5.f64 * ctx.f0.f64;
	// frsp f4,f0
	ctx.f4.f64 = double(float(ctx.f0.f64));
	// stfs f4,0(r27)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82473ad8
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82473AD8:
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r10,r7,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r8,r10,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82473afc
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
		// b 0x8242f8bc
		__restgprlr_17(ctx, base);
		return;
	}
loc_82473AFC:
	// stw r9,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r9.u32);
	// b 0x8242f8bc
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_3"))) PPC_WEAK_FUNC(phBound_3);
PPC_FUNC_IMPL(__imp__phBound_3) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r15 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f864
	ctx.lr = 0x82473B10;
	__savegprlr_15(ctx, base);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// addi r4,r3,52
	ctx.r4.s64 = ctx.r3.s64 + 52;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(var_r20 + 0);
	// lwz r31,0(r21)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r21 + 0));
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r25,r11,r5
	var_r25 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r11,r7,r6
	ctx.r11.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r9,r31
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r6.u32);
	// dcbt r0,r11
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
	// rldicr r7,r7,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r9,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r31,r1,-176
	var_r31 = (uint32_t)(ctx.r1.s64 + -176);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-168
	var_r30 = (uint32_t)(ctx.r1.s64 + -168);
	// std r7,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r7.u64);
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r24,r1,-176
	var_r24 = (uint32_t)(ctx.r1.s64 + -176);
	// li r8,4
	ctx.r8.s64 = 4;
	// addi r7,r1,-168
	ctx.r7.s64 = ctx.r1.s64 + -168;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r9)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25848);
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r31)
	PPC_STORE_U64(var_r31 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// ld r31,-168(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -168));
	// lvlx v11,r24,r8
	temp.u32 = var_r24 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,r7,r19
	temp.u32 = ctx.r7.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r9,-176(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v8,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// cmpdi cr6,r9,0
	// vspltw v9,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x82473d34
	while (ctx.r9.s64 < 0) {
	loc_82473C24:
		// cmpwi cr6,r6,0
		// ble cr6,0x82473d34
		if (ctx.r6.s32 <= 0) goto loc_82473D34;
		// addi r8,r4,12
		ctx.r8.s64 = ctx.r4.s64 + 12;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r7,r11,12
		ctx.r7.s64 = ctx.r11.s64 + 12;
		// addi r30,r10,3072
		var_r30 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82080c00
		// add r9,r31,r9
		ctx.r9.u64 = var_r31 + ctx.r9.u64;
		// vsr v6,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v4,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsr v5,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsubfp v3,v11,v0
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r8,r4,8
		ctx.r8.s64 = ctx.r4.s64 + 8;
		// addi r7,r11,8
		ctx.r7.s64 = ctx.r11.s64 + 8;
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v5,v5,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// cmpdi cr6,r9,0
		ctx.cr6.compare<int64_t>(ctx.r9.s64, 0, ctx.xer);
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsldoi v4,v0,v3,4
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vmulfp128 v6,v5,v4
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v11,v11,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v6,v6,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v10,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r30,r10,1024
		var_r30 = (uint32_t)(ctx.r10.s64 + 1024);  // addr:0x82080400
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r10,2048
		ctx.r8.s64 = ctx.r10.s64 + 2048;
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r7,r4,4
		ctx.r7.s64 = ctx.r4.s64 + 4;
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsr v12,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// vmsum3fp128 v10,v10,v6
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32), 0xEF));
		// vcfux v7,v7,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v6,v12,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v12,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vsldoi v5,v0,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// vmulfp128 v7,v7,v12
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
		// stvewx v10,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r11,4
		ctx.r8.s64 = ctx.r11.s64 + 4;
		// lvlx v10,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vmulfp128 v10,v6,v5
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v5.f32)));
		// lvlx v11,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vsldoi v11,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v11,0,r11
		temp.u32 = ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// blt cr6,0x82473c24
}
loc_82473D34:
	// sradi r7,r9,63
	ctx.xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r8,r9,32
	ctx.xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 32;
	// extsw r30,r24
	var_r30 = (uint32_t)((int32_t)var_r24);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpd cr6,r8,r30
	// bge cr6,0x82473ec0
while (ctx.r8.s64 < (int64_t)(int32_t)var_r30) {
	loc_82473D50:
		// cmpwi cr6,r6,0
		// beq cr6,0x82473f18
		if (ctx.r6.s32 == 0) goto loc_82473F18;
		// extsw r7,r8
		ctx.r7.s64 = ctx.r8.s32;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r19,r10,3072
		var_r19 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82080c00
		// rlwinm r8,r7,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// vsr v10,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r18,r8,3
		var_r18 = (uint32_t)(ctx.r8.s64 + 3);  // addr:0x82000003
		// vsubfp v4,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r7,r7,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r18,r18,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC);
		// vsr v6,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r16,r7,3
		var_r16 = (uint32_t)(ctx.r7.s64 + 3);  // addr:0x82080003
		// vsubfp v3,v11,v0
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vcfux v2,v10,31
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r17,r8,2
		var_r17 = (uint32_t)(ctx.r8.s64 + 2);  // addr:0x82000002
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// vcfux v6,v6,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// rlwinm r17,r17,2,0,29
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 2) & 0xFFFFFFFC);
		// lvlx v7,r18,r11
		temp.u32 = var_r18 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r15,r7,2
		var_r15 = (uint32_t)(ctx.r7.s64 + 2);  // addr:0x82080002
		// vspltw v10,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// rlwinm r8,r8,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// rlwinm r18,r15,2,0,29
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r15 | (var_r15 << 32), 2) & 0xFFFFFFFC);
		// lvlx v5,r16,r11
		temp.u32 = var_r16 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// add r8,r8,r11
		ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
		// rlwinm r7,r7,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// vrlimi128 v10,v5,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v5.f32), 228), 8));
		// add r9,r31,r9
		ctx.r9.u64 = var_r31 + ctx.r9.u64;
		// vsldoi v7,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsr v5,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsr v12,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// add r7,r7,r11
		ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
		// vsldoi v4,v0,v3,4
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// vmulfp128 v7,v2,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v6,v6,v4
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vsubfp v4,v11,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v11,v11,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v6,v6,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v10,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r19,r10,2048
		var_r19 = (uint32_t)(ctx.r10.s64 + 2048);  // addr:0x82080800
		// lvlx v10,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r17,r7,4
		var_r17 = (uint32_t)(ctx.r7.s64 + 4);  // lbl_82080004 @ 0x82080004
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,r18,r11
		temp.u32 = var_r18 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r18,r8,4
		var_r18 = (uint32_t)(ctx.r8.s64 + 4);  // addr:0x82000004
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vcfux v7,v5,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v5,v12,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmsum3fp128 v6,v10,v6
		simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32), 0xEF));
		// vsldoi v10,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vor v4,v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
		// vmulfp128 v12,v7,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stvewx v6,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r19,r10,1024
		var_r19 = (uint32_t)(ctx.r10.s64 + 1024);  // addr:0x82080400
		// lvlx v11,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// lvlx v10,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsldoi v7,v12,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vspltw v12,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vsldoi v10,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmsum3fp128 v12,v12,v7
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vmulfp128 v10,v5,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stvewx v12,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r8,r9,32
		ctx.xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
		ctx.r8.s64 = ctx.r9.s64 >> 32;
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v11,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// rlwinm r7,r8,4,0,27
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
		// add r7,r7,r11
		ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vsldoi v11,v10,v3,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// xor r5,r7,r5
		ctx.r5.u64 = ctx.r7.u64 ^ ctx.r5.u64;
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r5,0
		// beq cr6,0x82473eb4
		if (ctx.r5.u32 == 0) goto loc_82473EB4;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r7
	loc_82473EB4:
		// mr r5,r7
		ctx.r5.u64 = ctx.r7.u64;
		// cmpd cr6,r8,r30
		// blt cr6,0x82473d50
}
loc_82473EC0:
	// cmpwi cr6,r6,0
	// beq cr6,0x82473f18
	if (ctx.r6.s32 != 0) {
		// cmpd cr6,r8,r30
		// bne cr6,0x82473f00
		if (ctx.r8.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r7,r24,4,0,27
			ctx.r7.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 4) & 0xFFFFFFF0;
			// addi r8,r8,1
			ctx.r8.s64 = ctx.r8.s64 + 1;
			// add r7,r7,r11
			ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
			// lfs f8,12(r7)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 12);
			ctx.f8.f64 = double(temp.f32);
			// stfs f8,12(r4)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
			// lfs f7,8(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
			ctx.f7.f64 = double(temp.f32);
			// stfs f7,8(r4)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
			// lfs f6,4(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
			ctx.f6.f64 = double(temp.f32);
			// stfs f6,4(r4)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
			// lfs f5,0(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
			ctx.f5.f64 = double(temp.f32);
			// stfs f5,0(r4)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
			// b 0x82473f18
		} else {
		loc_82473F00:
			// extsw r7,r25
			ctx.r7.s64 = (int32_t)var_r25;
			// cmpd cr6,r8,r7
			// ble cr6,0x82473f18
			if (ctx.r8.s64 <= ctx.r7.s64) goto loc_82473F18;
			// subf r4,r7,r8
			ctx.r4.s64 = ctx.r8.s64 - ctx.r7.s64;
			// rldicr r7,r4,32,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u64, 32) & 0xFFFFFFFF00000000;
			// add r9,r7,r9
			ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
		}
	}
loc_82473F18:
	// rldicr r6,r8,32,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// lbz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// lfd f4,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r3,r5,2
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r5.u32, 2);
	// subf r4,r9,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r9.s64;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// divwu r9,r4,r3
	ctx.r9.u32 = ctx.r3.u32 ? ctx.r4.u32 / ctx.r3.u32 : 0;
	// cmplw cr6,r9,r11
	// fmul f0,f3,f0
	ctx.f0.f64 = ctx.f3.f64 * ctx.f0.f64;
	// frsp f2,f0
	ctx.f2.f64 = double(float(ctx.f0.f64));
	// stfs f2,0(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82473f6c
	if (ctx.r9.u32 < ctx.r11.u32) {
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	}
loc_82473F6C:
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r8
	// bge cr6,0x82473f90
	if (ctx.r10.u32 < ctx.r8.u32) {
		// stw r10,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r10.u32);
		// b 0x8242f8b4
		__restgprlr_15(ctx, base);
		return;
	}
loc_82473F90:
	// stw r8,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
	// b 0x8242f8b4
	__restgprlr_15(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_4"))) PPC_WEAK_FUNC(phBound_4);
PPC_FUNC_IMPL(__imp__phBound_4) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r22 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r15 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f864
	ctx.lr = 0x82473FA0;
	__savegprlr_15(ctx, base);
	// addi r22,r3,8
	var_r22 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// addi r19,r3,28
	var_r19 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r20,r3,24
	var_r20 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r11,0(r22)
	ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r21,r3,20
	var_r21 = (uint32_t)(ctx.r3.s64 + 20);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// addi r4,r3,52
	ctx.r4.s64 = ctx.r3.s64 + 52;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r9,0(r19)
	ctx.r9.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lwz r31,0(r20)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r20 + 0));
	// lwz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r23,r11,r5
	var_r23 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r11,r7,r6
	ctx.r11.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r9,r31
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r6.u32);
	// dcbt r0,r11
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
	// rldicr r5,r7,32,63
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r9,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-168
	var_r30 = (uint32_t)(ctx.r1.s64 + -168);
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r25,r1,-176
	var_r25 = (uint32_t)(ctx.r1.s64 + -176);
	// li r8,4
	ctx.r8.s64 = 4;
	// addi r5,r1,-168
	ctx.r5.s64 = ctx.r1.s64 + -168;
	// li r24,4
	var_r24 = 4;
	// li r31,0
	var_r31 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r9)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25848);
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// ld r7,-176(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// lvlx v11,r25,r8
	temp.u32 = var_r25 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r30,-168(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -168));
	// lvlx v10,r5,r24
	temp.u32 = ctx.r5.u32 + var_r24;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v8,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r7,0
	// vspltw v9,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x82474244
	while (ctx.r7.s64 < 0) {
	loc_824740B4:
		// cmpwi cr6,r6,0
		// ble cr6,0x82474244
		if (ctx.r6.s32 <= 0) goto loc_82474244;
		// addi r8,r4,20
		ctx.r8.s64 = ctx.r4.s64 + 20;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r5,r11,20
		ctx.r5.s64 = ctx.r11.s64 + 20;
		// addi r9,r10,5120
		ctx.r9.s64 = ctx.r10.s64 + 5120;
		// vsr v6,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v4,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v3,v11,v0
		simde_mm_store_ps(ctx.v3.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsr v5,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r8,r4,16
		ctx.r8.s64 = ctx.r4.s64 + 16;
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v5,v5,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsldoi v4,v0,v3,4
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 12));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vmulfp128 v6,v5,v4
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v4,v11,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v6,v6,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v2,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v10,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,4096
		ctx.r9.s64 = ctx.r10.s64 + 4096;
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r4,12
		ctx.r8.s64 = ctx.r4.s64 + 12;
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r11,12
		ctx.r5.s64 = ctx.r11.s64 + 12;
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vmsum3fp128 v6,v10,v6
		simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32), 0xEF));
		// vsr v10,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vcfux v7,v7,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v3,v10,31
		simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v10,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vmulfp128 v7,v7,v10
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stvewx v6,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,3072
		ctx.r9.s64 = ctx.r10.s64 + 3072;
		// lvlx v6,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r4,8
		ctx.r8.s64 = ctx.r4.s64 + 8;
		// vspltw v10,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// lvlx v5,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v4,v3,v3
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v3.u8));
		// addi r5,r11,8
		ctx.r5.s64 = ctx.r11.s64 + 8;
		// vor v3,v2,v2
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v2.u8));
		// vrlimi128 v10,v5,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v5.f32), 228), 8));
		// vsr v5,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vmulfp128 v6,v4,v3
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vsr v12,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsubfp v4,v11,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v11,v11,v0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vcfux v1,v12,31
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vsldoi v6,v6,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v11,v0,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// stvewx v10,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,2048
		ctx.r9.s64 = ctx.r10.s64 + 2048;
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r4,4
		ctx.r8.s64 = ctx.r4.s64 + 4;
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r11,4
		ctx.r5.s64 = ctx.r11.s64 + 4;
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vcfux v7,v5,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmsum3fp128 v6,v10,v6
		simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32), 0xEF));
		// vsldoi v10,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vmulfp128 v10,v7,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vmulfp128 v7,v1,v11
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvewx v6,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,1024
		ctx.r9.s64 = ctx.r10.s64 + 1024;
		// lvlx v12,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// lvlx v11,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// add r7,r30,r7
		ctx.r7.u64 = var_r30 + ctx.r7.u64;
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// cmpdi cr6,r7,0
		ctx.cr6.compare<int64_t>(ctx.r7.s64, 0, ctx.xer);
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// vsldoi v10,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// stvewx v12,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v11,0,r11
		temp.u32 = ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// blt cr6,0x824740b4
}
loc_82474244:
	// sradi r8,r7,63
	ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r7.s64 >> 63;
	// addi r24,r23,-1
	var_r24 = (uint32_t)(var_r23 + -1);
	// sradi r5,r7,32
	ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s64 >> 32;
	// extsw r25,r24
	var_r25 = (uint32_t)((int32_t)var_r24);
	// subf r9,r8,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r8.s64;
	// cmpd cr6,r9,r25
	// bge cr6,0x82474488
while (ctx.r9.s64 < (int64_t)(int32_t)var_r25) {
	loc_82474260:
		// cmpwi cr6,r6,0
		// beq cr6,0x824744f8
		if (ctx.r6.s32 == 0) goto loc_824744F8;
		// extsw r9,r9
		ctx.r9.s64 = ctx.r9.s32;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r18,r10,5120
		var_r18 = (uint32_t)(ctx.r10.s64 + 5120);  // addr:0x82081400
		// rlwinm r5,r9,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// vor v29,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v29.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// addi r8,r9,1
		ctx.r8.s64 = ctx.r9.s64 + 1;
		// add r9,r9,r5
		ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
		// vsr v10,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// rlwinm r5,r8,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vsubfp v5,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r9,r9,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// add r8,r8,r5
		ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
		// vsubfp v4,v11,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r17,r9,5
		var_r17 = (uint32_t)(ctx.r9.s64 + 5);  // addr:0x82080005
		// vcfux v31,v10,31
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// rlwinm r8,r8,1,0,30
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vor v28,v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
		// rlwinm r5,r17,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(var_r17 | (var_r17 << 32), 2) & 0xFFFFFFFC;
		// vcfux v30,v7,31
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r17,r9,4
		var_r17 = (uint32_t)(ctx.r9.s64 + 4);  // lbl_82080004 @ 0x82080004
		// addi r16,r8,4
		var_r16 = (uint32_t)(ctx.r8.s64 + 4);  // addr:0x82000004
		// rlwinm r17,r17,2,0,29
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 2) & 0xFFFFFFFC);
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// lvlx v6,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r8,5
		ctx.r5.s64 = ctx.r8.s64 + 5;
		// vspltw v10,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// addi r15,r8,2
		var_r15 = (uint32_t)(ctx.r8.s64 + 2);  // addr:0x82000002
		// rlwinm r5,r5,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// vsldoi v5,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// rlwinm r15,r15,2,0,29
		var_r15 = (uint32_t)(__builtin_rotateleft64(var_r15 | (var_r15 << 32), 2) & 0xFFFFFFFC);
		// vsldoi v3,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vor v6,v31,v31
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v31.u8));
		// lvlx v7,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v4,v30,v30
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v30.u8));
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// addi r5,r9,3
		ctx.r5.s64 = ctx.r9.s64 + 3;
		// vmulfp128 v7,v6,v5
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vsr v5,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// rlwinm r5,r5,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// vmulfp128 v6,v4,v3
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vsubfp v4,v11,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vcfux v5,v5,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v6,v6,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r18,r10,4096
		var_r18 = (uint32_t)(ctx.r10.s64 + 4096);  // lbl_82081000 @ 0x82081000
		// lvlx v10,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r17,r8,3
		var_r17 = (uint32_t)(ctx.r8.s64 + 3);  // addr:0x82000003
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,r16,r11
		temp.u32 = var_r16 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// rlwinm r17,r17,2,0,29
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 2) & 0xFFFFFFFC);
		// addi r16,r9,2
		var_r16 = (uint32_t)(ctx.r9.s64 + 2);  // addr:0x82080002
		// rlwinm r9,r9,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// rlwinm r16,r16,2,0,29
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 2) & 0xFFFFFFFC);
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// rlwinm r8,r8,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// vmsum3fp128 v10,v10,v6
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32), 0xEF));
		// vcfux v3,v7,31
		simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v7,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsubfp v4,v11,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v7,v5,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r18,r10,3072
		var_r18 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82080c00
		// lvlx v10,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r10,2048
		ctx.r5.s64 = ctx.r10.s64 + 2048;
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v6,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v10,v6,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v6.f32), 228), 8));
		// vsr v6,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v10,r16,r11
		temp.u32 = var_r16 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// lvlx v7,r15,r11
		temp.u32 = var_r15 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsldoi v12,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// add r9,r9,r11
		ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// add r8,r8,r11
		ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
		// vor v27,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r18,r9,4
		var_r18 = (uint32_t)(ctx.r9.s64 + 4);  // lbl_82080004 @ 0x82080004
		// vor v26,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vmulfp128 v12,v3,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v12.f32)));
		// addi r17,r8,4
		var_r17 = (uint32_t)(ctx.r8.s64 + 4);  // addr:0x82000004
		// add r7,r30,r7
		ctx.r7.u64 = var_r30 + ctx.r7.u64;
		// vor v25,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsubfp v11,v11,v27
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v27.f32)));
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsldoi v12,v12,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v11,v26,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// vmsum3fp128 v12,v10,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32), 0xEF));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// stvewx v12,r0,r5
		ea = (ctx.r5.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r5,r10,1024
		ctx.r5.s64 = ctx.r10.s64 + 1024;
		// lvlx v12,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v10,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vsr v10,v29,v28
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)ctx.v28.u8)));
		// vmsum3fp128 v12,v12,v7
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vcfux v10,v10,31
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmulfp128 v11,v10,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvewx v12,r0,r5
		ea = (ctx.r5.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r9,r7,32
		ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
		ctx.r9.s64 = ctx.r7.s64 >> 32;
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// lvlx v7,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// extsw r8,r9
		ctx.r8.s64 = ctx.r9.s32;
		// rlwinm r5,r8,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// add r8,r8,r5
		ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
		// rlwinm r8,r8,3,0,28
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
		// vsldoi v11,v11,v25,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v25.u8), 8));
		// add r8,r8,r11
		ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
		// xor r5,r8,r31
		ctx.r5.u64 = ctx.r8.u64 ^ var_r31;
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r5,0
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// beq cr6,0x8247447c
		if (ctx.r5.u32 == 0) goto loc_8247447C;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r8
	loc_8247447C:
		// mr r31,r8
		var_r31 = ctx.r8.u32;
		// cmpd cr6,r9,r25
		// blt cr6,0x82474260
}
loc_82474488:
	// cmpwi cr6,r6,0
	// beq cr6,0x824744f8
	if (ctx.r6.s32 != 0) {
		// cmpd cr6,r9,r25
		// bne cr6,0x824744e0
		if (ctx.r9.s64 == (int64_t)(int32_t)var_r25) {
			// rlwinm r8,r24,1,0,30
			ctx.r8.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 1) & 0xFFFFFFFE;
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// add r8,r24,r8
			ctx.r8.u64 = var_r24 + ctx.r8.u64;
			// rlwinm r8,r8,3,0,28
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
			// add r8,r8,r11
			ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
			// lfs f8,20(r8)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
			ctx.f8.f64 = double(temp.f32);
			// stfs f8,20(r4)
			temp.f32 = float(ctx.f8.f64);
			PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
			// lfs f7,16(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16);
			ctx.f7.f64 = double(temp.f32);
			// stfs f7,16(r4)
			temp.f32 = float(ctx.f7.f64);
			PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
			// lfs f6,12(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 12);
			ctx.f6.f64 = double(temp.f32);
			// stfs f6,12(r4)
			temp.f32 = float(ctx.f6.f64);
			PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
			// lfs f5,8(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
			ctx.f5.f64 = double(temp.f32);
			// stfs f5,8(r4)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
			// lfs f4,4(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
			ctx.f4.f64 = double(temp.f32);
			// stfs f4,4(r4)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
			// lfs f3,0(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
			ctx.f3.f64 = double(temp.f32);
			// stfs f3,0(r4)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
			// b 0x824744f8
		} else {
		loc_824744E0:
			// extsw r8,r23
			ctx.r8.s64 = (int32_t)var_r23;
			// cmpd cr6,r9,r8
			// ble cr6,0x824744f8
			if (ctx.r9.s64 <= ctx.r8.s64) goto loc_824744F8;
			// subf r6,r8,r9
			ctx.r6.s64 = ctx.r9.s64 - ctx.r8.s64;
			// rldicr r8,r6,32,31
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u64, 32) & 0xFFFFFFFF00000000;
			// add r7,r8,r7
			ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
		}
	}
loc_824744F8:
	// rldicr r5,r9,32,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r8,r7
	ctx.r4.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 0);
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// lfd f2,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r5,r7,2
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// twllei r5,0
	if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
	// subf r6,r9,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r9.s64;
	// divwu r9,r6,r5
	ctx.r9.u32 = ctx.r5.u32 ? ctx.r6.u32 / ctx.r5.u32 : 0;
	// cmplw cr6,r9,r11
	// fmul f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 * ctx.f0.f64;
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// stfs f0,0(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82474558
	if (ctx.r9.u32 < ctx.r11.u32) {
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	}
loc_82474558:
	// lwz r7,0(r21)
	ctx.r7.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lwz r8,0(r20)
	ctx.r8.u64 = PPC_LOAD_U32(var_r20 + 0);
	// subf r4,r7,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r11,0(r22)
	PPC_STORE_U32(var_r22 + 0, ctx.r11.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r8
	// bge cr6,0x8247457c
	if (ctx.r10.u32 < ctx.r8.u32) {
		// stw r10,0(r19)
		PPC_STORE_U32(var_r19 + 0, ctx.r10.u32);
		// b 0x8242f8b4
		__restgprlr_15(ctx, base);
		return;
	}
loc_8247457C:
	// stw r8,0(r19)
	PPC_STORE_U32(var_r19 + 0, ctx.r8.u32);
	// b 0x8242f8b4
	__restgprlr_15(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_5"))) PPC_WEAK_FUNC(phBound_5);
PPC_FUNC_IMPL(__imp__phBound_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r15 = 0;
	uint32_t var_r14 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f860
	ctx.lr = 0x82474590;
	__savegprlr_14(ctx, base);
	// addi r20,r3,8
	var_r20 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r23,r3,4
	var_r23 = (uint32_t)(ctx.r3.s64 + 4);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r24,r3,13
	var_r24 = (uint32_t)(ctx.r3.s64 + 13);
	// addi r17,r3,28
	var_r17 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r18,r3,24
	var_r18 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r19,r3,20
	var_r19 = (uint32_t)(ctx.r3.s64 + 20);
	// lwz r4,0(r23)
	ctx.r4.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r22,r3,48
	var_r22 = (uint32_t)(ctx.r3.s64 + 48);
	// lbz r11,0(r24)
	ctx.r11.u64 = PPC_LOAD_U8(var_r24 + 0);
	// addi r27,r3,52
	var_r27 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r9,0(r17)
	ctx.r9.u64 = PPC_LOAD_U32(var_r17 + 0);
	// subf r25,r10,r4
	var_r25 = (uint32_t)(ctx.r4.s64 - ctx.r10.s64);
	// lwz r31,0(r18)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r5,0(r19)
	ctx.r5.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lfs f12,0(r22)
	temp.u32 = PPC_LOAD_U32(var_r22 + 0);
	ctx.f12.f64 = double(temp.f32);
	// subf r4,r9,r31
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r6,r5
	var_r30 = (uint32_t)(ctx.r6.u64 + ctx.r5.u64);
	// add r6,r10,r7
	ctx.r6.u64 = ctx.r10.u64 + ctx.r7.u64;
	// stw r4,-208(r1)
	PPC_STORE_U32(ctx.r1.u32 + -208, ctx.r4.u32);
	// dcbt r0,r6
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r10,r1,-208
	ctx.r10.s64 = ctx.r1.s64 + -208;
	// addi r9,r3,40
	ctx.r9.s64 = ctx.r3.s64 + 40;
	// rldicr r7,r8,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u64, 32);
	// addi r21,r3,36
	var_r21 = (uint32_t)(ctx.r3.s64 + 36);
	// addi r8,r1,-208
	ctx.r8.s64 = ctx.r1.s64 + -208;
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32248
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vcfsx v11,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// std r7,-200(r1)
	PPC_STORE_U64(ctx.r1.u32 + -200, ctx.r7.u64);
	// lfd f0,-200(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// addi r31,r1,-200
	var_r31 = (uint32_t)(ctx.r1.s64 + -200);
	// addi r29,r1,-208
	var_r29 = (uint32_t)(ctx.r1.s64 + -208);
	// lvlx v0,0,r21
	temp.u32 = var_r21;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,4
	ctx.r9.s64 = 4;
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r7,r1,-200
	ctx.r7.s64 = ctx.r1.s64 + -200;
	// vspltw v13,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// li r28,4
	var_r28 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25848);
	// addi r10,r1,-176
	ctx.r10.s64 = ctx.r1.s64 + -176;
	// fdiv f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.f10.u64);
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r31)
	PPC_STORE_U64(var_r31 + 0, ctx.f9.u64);
	// lvlx v10,r29,r9
	temp.u32 = var_r29 + ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r31,-208(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -208));
	// lvlx v9,r7,r28
	temp.u32 = ctx.r7.u32 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r26,-200(r1)
	var_r26 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -200));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrefp v0,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v11.f32)));
	// vspltw v8,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// cmpdi cr6,r31,0
	ctx.cr6.compare<int64_t>((int64_t)(int32_t)var_r31, 0, ctx.xer);
	// vspltw v9,v9,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
	// vmulfp128 v0,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vspltw v7,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// bge cr6,0x82474740
while ((int64_t)(int32_t)var_r31 < 0) {
	loc_824746A4:
		// cmpwi cr6,r4,0
		// ble cr6,0x82474740
		if (ctx.r4.s32 <= 0) goto loc_82474740;
		// cmplwi cr6,r11,0
		// beq cr6,0x82474724
		if (ctx.r11.u32 == 0) goto loc_82474724;
		// vspltisb v0,1
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x1)));
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
		// vspltisw v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_set1_epi32(int(0x0)));
		// mr r9,r27
		ctx.r9.u64 = var_r27;
		// mr r7,r30
		ctx.r7.u64 = var_r30;
		// mr r10,r11
		ctx.r10.u64 = ctx.r11.u64;
		// vsr v0,v8,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
		// vsubfp v12,v12,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vcfux v0,v0,31
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v12,v13,v12,4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
		// vmulfp128 v0,v0,v12
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vsldoi v10,v0,v13,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 8));
	loc_824746E4:
		// mr r29,r9
		var_r29 = ctx.r9.u32;
		// lvlx v0,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v11,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
		// mr r28,r7
		var_r28 = ctx.r7.u32;
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// addi r8,r8,2
		ctx.r8.s64 = ctx.r8.s64 + 2;
		// lvlx v12,0,r29
		temp.u32 = var_r29;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r7,r7,1024
		ctx.r7.s64 = ctx.r7.s64 + 1024;
		// vspltw v0,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vcfsx v12,v11,15
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// cmplwi cr6,r10,0
		// vrlimi128 v0,v12,8,0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v12.f32), 228), 8));
		// vmsum3fp128 v0,v0,v10
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v0,r0,r28
		ea = (var_r28) & ~0x3;
		PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
		// bne cr6,0x824746e4
		if (ctx.r10.u32 != 0) goto loc_824746E4;
	loc_82474724:
		// add r31,r26,r31
		var_r31 = (uint32_t)(var_r26 + var_r31);
		// vaddfp v13,v13,v7
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)));
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// vadduwm v8,v8,v9
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// addi r4,r4,-1
		ctx.r4.s64 = ctx.r4.s64 + -1;
		// cmpdi cr6,r31,0
		// blt cr6,0x824746a4
}
loc_82474740:
	// sradi r9,r31,63
	ctx.xer.ca = ((int64_t)(int32_t)var_r31 < 0) & ((var_r31 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 >> 63;
	// addi r28,r25,-1
	var_r28 = (uint32_t)(var_r25 + -1);
	// sradi r8,r31,32
	ctx.xer.ca = ((int64_t)(int32_t)var_r31 < 0) & ((var_r31 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = (int64_t)(int32_t)var_r31 >> 32;
	// extsw r29,r28
	var_r29 = (uint32_t)((int32_t)var_r28);
	// subf r10,r9,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r9.s64;
	// cmpd cr6,r10,r29
	// bge cr6,0x8247484c
while (ctx.r10.s64 < (int64_t)(int32_t)var_r29) {
	loc_8247475C:
		// cmpwi cr6,r4,0
		// beq cr6,0x82474998
		if (ctx.r4.s32 == 0) goto loc_82474998;
		// cmplwi cr6,r11,0
		// beq cr6,0x82474800
		if (ctx.r11.u32 != 0) {
			// vspltisb v0,1
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x1)));
			// extsw r9,r10
			ctx.r9.s64 = ctx.r10.s32;
			// vspltisw v12,0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_set1_epi32(int(0x0)));
			// mr r7,r30
			ctx.r7.u64 = var_r30;
			// addi r8,r9,1
			ctx.r8.s64 = ctx.r9.s64 + 1;
			// mullw r9,r9,r11
			ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
			// vsr v0,v8,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
			// vsubfp v12,v12,v13
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
			// vcfux v0,v0,31
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
			// mullw r8,r8,r11
			ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
			// vsldoi v12,v13,v12,4
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
			// vmulfp128 v0,v0,v12
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
			// rlwinm r8,r8,1,0,30
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
			// rlwinm r9,r9,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
			// mr r10,r11
			ctx.r10.u64 = ctx.r11.u64;
			// add r8,r8,r6
			ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
			// add r9,r9,r6
			ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
			// vsldoi v11,v0,v13,8
			simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 8));
		loc_824747B4:
			// mr r16,r9
			var_r16 = ctx.r9.u32;
			// mr r15,r8
			var_r15 = ctx.r8.u32;
			// mr r14,r7
			var_r14 = ctx.r7.u32;
			// addi r10,r10,-1
			ctx.r10.s64 = ctx.r10.s64 + -1;
			// addi r9,r9,2
			ctx.r9.s64 = ctx.r9.s64 + 2;
			// lvlx v0,0,r16
			temp.u32 = var_r16;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
			// addi r8,r8,2
			ctx.r8.s64 = ctx.r8.s64 + 2;
			// vupkhsh v0,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
			// lvlx v12,0,r15
			temp.u32 = var_r15;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
			// vupkhsh v12,v12
			simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
			// addi r7,r7,1024
			ctx.r7.s64 = ctx.r7.s64 + 1024;
			// cmplwi cr6,r10,0
			// vcfsx v0,v0,15
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
			// vcfsx v12,v12,15
			simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
			// vspltw v0,v0,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
			// vrlimi128 v0,v12,8,0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v12.f32), 228), 8));
			// vmsum3fp128 v0,v0,v11
			simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
			// stvewx v0,r0,r14
			ea = (var_r14) & ~0x3;
			PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
			// bne cr6,0x824747b4
			if (ctx.r10.u32 != 0) goto loc_824747B4;
		}
	loc_82474800:
		// add r31,r26,r31
		var_r31 = (uint32_t)(var_r26 + var_r31);
		// vaddfp v13,v13,v7
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)));
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// vadduwm v8,v8,v9
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), simde_mm_load_si128((simde__m128i*)ctx.v9.u32)));
		// sradi r10,r31,32
		ctx.xer.ca = ((int64_t)(int32_t)var_r31 < 0) & ((var_r31 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 >> 32;
		// addi r4,r4,-1
		ctx.r4.s64 = ctx.r4.s64 + -1;
		// extsw r7,r10
		ctx.r7.s64 = ctx.r10.s32;
		// mullw r9,r7,r11
		ctx.r9.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
		// rlwinm r9,r9,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// add r9,r9,r6
		ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
		// xor r8,r9,r5
		ctx.r8.u64 = ctx.r9.u64 ^ ctx.r5.u64;
		// rlwinm r7,r8,0,0,24
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r7,0
		// beq cr6,0x82474840
		if (ctx.r7.u32 == 0) goto loc_82474840;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r9
	loc_82474840:
		// mr r5,r9
		ctx.r5.u64 = ctx.r9.u64;
		// cmpd cr6,r10,r29
		// blt cr6,0x8247475c
}
loc_8247484C:
	// cmpwi cr6,r4,0
	// beq cr6,0x82474998
	if (ctx.r4.s32 != 0) {
		// cmpd cr6,r10,r29
		// bne cr6,0x82474980
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r29) {
			// mullw r4,r28,r11
			ctx.r4.s64 = int64_t((int32_t)var_r28) * int64_t(ctx.r11.s32);
			// rlwinm r9,r4,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// add r4,r9,r6
			ctx.r4.u64 = ctx.r9.u64 + ctx.r6.u64;
			// lis r9,-32256
			// li r7,0
			ctx.r7.s64 = 0;
			// cmpwi cr6,r11,4
			// lfs f0,27864(r9)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27864);
			ctx.f0.f64 = double(temp.f32);
			// blt cr6,0x8247492c
			if (ctx.r11.s32 >= 4) {
				// addi r9,r11,-4
				ctx.r9.s64 = ctx.r11.s64 + -4;
				// addi r8,r27,8
				ctx.r8.s64 = (int64_t)(int32_t)var_r27 + 8;
				// rlwinm r7,r9,30,2,31
				ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
				// addi r9,r4,4
				ctx.r9.s64 = ctx.r4.s64 + 4;
				// addi r5,r7,1
				ctx.r5.s64 = ctx.r7.s64 + 1;
				// rlwinm r7,r5,2,0,29
				ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			loc_82474898:
				// lhz r29,-4(r9)
				var_r29 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + -4));
				// addi r5,r5,-1
				ctx.r5.s64 = ctx.r5.s64 + -1;
				// extsh r29,r29
				var_r29 = (uint32_t)((int16_t)var_r29);
				// cmplwi cr6,r5,0
				ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
				// std r29,-200(r1)
				PPC_STORE_U64(ctx.r1.u32 + -200, var_r29);
				// lfd f8,-200(r1)
				ctx.fpscr.disableFlushMode();
				ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -200);
				// fcfid f7,f8
				ctx.f7.f64 = double(ctx.f8.s64);
				// frsp f6,f7
				ctx.f6.f64 = double(float(ctx.f7.f64));
				// fmuls f5,f6,f0
				ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
				// stfs f5,-8(r8)
				temp.f32 = float(ctx.f5.f64);
				PPC_STORE_U32(ctx.r8.u32 + -8, temp.u32);
				// lhz r29,-2(r9)
				var_r29 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + -2));
				// extsh r29,r29
				var_r29 = (uint32_t)((int16_t)var_r29);
				// std r29,-208(r1)
				PPC_STORE_U64(ctx.r1.u32 + -208, var_r29);
				// lfd f4,-208(r1)
				ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -208);
				// fcfid f3,f4
				ctx.f3.f64 = double(ctx.f4.s64);
				// frsp f2,f3
				ctx.f2.f64 = double(float(ctx.f3.f64));
				// fmuls f1,f2,f0
				ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
				// stfs f1,-4(r8)
				temp.f32 = float(ctx.f1.f64);
				PPC_STORE_U32(ctx.r8.u32 + -4, temp.u32);
				// lhz r29,0(r9)
				var_r29 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 0));
				// extsh r29,r29
				var_r29 = (uint32_t)((int16_t)var_r29);
				// std r29,-192(r1)
				PPC_STORE_U64(ctx.r1.u32 + -192, var_r29);
				// lfd f12,-192(r1)
				ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
				// fcfid f11,f12
				ctx.f11.f64 = double(ctx.f12.s64);
				// frsp f10,f11
				ctx.f10.f64 = double(float(ctx.f11.f64));
				// fmuls f9,f10,f0
				ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
				// stfs f9,0(r8)
				temp.f32 = float(ctx.f9.f64);
				PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
				// lhz r29,2(r9)
				var_r29 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 2));
				// addi r9,r9,8
				ctx.r9.s64 = ctx.r9.s64 + 8;
				// extsh r29,r29
				var_r29 = (uint32_t)((int16_t)var_r29);
				// std r29,-176(r1)
				PPC_STORE_U64(ctx.r1.u32 + -176, var_r29);
				// lfd f8,-176(r1)
				ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
				// fcfid f7,f8
				ctx.f7.f64 = double(ctx.f8.s64);
				// frsp f6,f7
				ctx.f6.f64 = double(float(ctx.f7.f64));
				// fmuls f5,f6,f0
				ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
				// stfs f5,4(r8)
				temp.f32 = float(ctx.f5.f64);
				PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
				// addi r8,r8,16
				ctx.r8.s64 = ctx.r8.s64 + 16;
				// bne cr6,0x82474898
				if (!ctx.cr6.eq) goto loc_82474898;
			}
		loc_8247492C:
			// cmplw cr6,r7,r11
			// bge cr6,0x82474998
			if (ctx.r7.u32 >= ctx.r11.u32) goto loc_82474998;
			// rlwinm r8,r7,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
			// rlwinm r9,r7,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
			// add r5,r8,r27
			ctx.r5.u64 = ctx.r8.u64 + var_r27;
			// add r8,r9,r4
			ctx.r8.u64 = ctx.r9.u64 + ctx.r4.u64;
			// subf r9,r7,r11
			ctx.r9.s64 = ctx.r11.s64 - ctx.r7.s64;
		loc_82474948:
			// lhz r7,0(r8)
			ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
			// addi r9,r9,-1
			ctx.r9.s64 = ctx.r9.s64 + -1;
			// addi r8,r8,2
			ctx.r8.s64 = ctx.r8.s64 + 2;
			// extsh r7,r7
			ctx.r7.s64 = ctx.r7.s16;
			// cmplwi cr6,r9,0
			// std r7,-176(r1)
			PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r7.u64);
			// lfd f4,-176(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
			// fcfid f3,f4
			ctx.f3.f64 = double(ctx.f4.s64);
			// frsp f2,f3
			ctx.f2.f64 = double(float(ctx.f3.f64));
			// fmuls f1,f2,f0
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
			// stfs f1,0(r5)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
			// addi r5,r5,4
			ctx.r5.s64 = ctx.r5.s64 + 4;
			// bne cr6,0x82474948
			if (ctx.r9.u32 != 0) goto loc_82474948;
			// b 0x82474998
		} else {
		loc_82474980:
			// extsw r9,r25
			ctx.r9.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r9
			// ble cr6,0x82474998
			if (ctx.r10.s64 <= ctx.r9.s64) goto loc_82474998;
			// subf r5,r9,r10
			ctx.r5.s64 = ctx.r10.s64 - ctx.r9.s64;
			// rldicr r9,r5,32,31
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r31,r9,r31
			var_r31 = (uint32_t)(ctx.r9.u64 + var_r31);
		}
	}
loc_82474998:
	// extsw r4,r10
	ctx.r4.s64 = ctx.r10.s32;
	// stvewx v13,r0,r21
	ea = (var_r21) & ~0x3;
	PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
	// rldicr r9,r10,32,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// lbz r8,0(r24)
	ctx.r8.u64 = PPC_LOAD_U8(var_r24 + 0);
	// mullw r11,r4,r11
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r11.s32);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r11,r6
	ctx.r10.u64 = ctx.r11.u64 + ctx.r6.u64;
	// subf r11,r9,r31
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r6,r8,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r10,r7,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// std r11,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r11.u64);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// cmplw cr6,r10,r11
	// lfd f0,-176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// fcfid f12,f0
	ctx.f12.f64 = double(ctx.f0.s64);
	// fmul f0,f12,f13
	ctx.f0.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f11,f0
	ctx.f11.f64 = double(float(ctx.f0.f64));
	// stfs f11,0(r22)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r22 + 0, temp.u32);
	// bge cr6,0x824749f4
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824749F4:
	// lwz r8,0(r19)
	ctx.r8.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lwz r9,0(r18)
	ctx.r9.u64 = PPC_LOAD_U32(var_r18 + 0);
	// subf r5,r8,r30
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 - ctx.r8.s64;
	// stw r11,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r11.u32);
	// rlwinm r8,r5,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82474a18
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r17)
		PPC_STORE_U32(var_r17 + 0, ctx.r8.u32);
		// b 0x8242f8b0
		__restgprlr_14(ctx, base);
		return;
	}
loc_82474A18:
	// stw r9,0(r17)
	PPC_STORE_U32(var_r17 + 0, ctx.r9.u32);
	// b 0x8242f8b0
	__restgprlr_14(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_6"))) PPC_WEAK_FUNC(phBound_6);
PPC_FUNC_IMPL(__imp__phBound_6) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f874
	ctx.lr = 0x82474A28;
	__savegprlr_19(ctx, base);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r31,r3,52
	var_r31 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r4,0(r21)
	ctx.r4.u64 = PPC_LOAD_U32(var_r21 + 0);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r25,r11,r5
	var_r25 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r8,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r8.u32);
	// dcbt r0,r9
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r4,r3,40
	ctx.r4.s64 = ctx.r3.s64 + 40;
	// rldicr r10,r11,32,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lis r11,-32248
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-136
	var_r30 = (uint32_t)(ctx.r1.s64 + -136);
	// std r10,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r10.u64);
	// lfd f0,-136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r24,r1,-144
	var_r24 = (uint32_t)(ctx.r1.s64 + -144);
	// addi r10,r1,-136
	ctx.r10.s64 = ctx.r1.s64 + -136;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r6,r1,-128
	ctx.r6.s64 = ctx.r1.s64 + -128;
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// lvlx v11,r24,r4
	temp.u32 = var_r24 + ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r11,-144(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lvlx v10,r10,r19
	temp.u32 = ctx.r10.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// ld r4,-136(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vspltw v10,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// cmpdi cr6,r11,0
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v11,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x82474bb0
	while (ctx.r11.s64 < 0) {
	loc_82474B3C:
		// cmpwi cr6,r8,0
		// ble cr6,0x82474bb0
		if (ctx.r8.s32 <= 0) goto loc_82474BB0;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vor v6,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vspltisw v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_set1_epi32(int(0x0)));
		// vor v5,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// lvlx v8,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v4,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// lvlx v7,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsr v6,v6,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vspltw v12,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// vsubfp v5,v9,v5
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vupkhsh v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// vaddfp v0,v0,v11
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vadduwm v13,v13,v10
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// vcfux v8,v6,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// cmpdi cr6,r11,0
		ctx.cr6.compare<int64_t>(ctx.r11.s64, 0, ctx.xer);
		// vcfsx v7,v9,15
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v9,v4,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vmulfp128 v9,v8,v9
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsldoi v9,v9,v3,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), 8));
		// vmsum3fp128 v12,v12,v9
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v12,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// blt cr6,0x82474b3c
}
loc_82474BB0:
	// sradi r10,r11,63
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r6,r11,32
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r11.s64 >> 32;
	// extsw r30,r24
	var_r30 = (uint32_t)((int32_t)var_r24);
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// cmpd cr6,r10,r30
	// bge cr6,0x82474c80
while (ctx.r10.s64 < (int64_t)(int32_t)var_r30) {
	loc_82474BCC:
		// cmpwi cr6,r8,0
		// beq cr6,0x82474cdc
		if (ctx.r8.s32 == 0) goto loc_82474CDC;
		// vor v2,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vor v1,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vspltisw v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_set1_epi32(int(0x0)));
		// extsw r10,r10
		ctx.r10.s64 = ctx.r10.s32;
		// vor v31,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vor v30,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// rlwinm r6,r10,1,0,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// vsr v12,v2,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsubfp v9,v9,v1
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v1.f32)));
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// vaddfp v0,v0,v11
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vadduwm v13,v13,v10
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// rlwinm r10,r10,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// vcfux v12,v12,31
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// lvlx v8,r6,r9
		temp.u32 = ctx.r6.u32 + ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// lvlx v7,r10,r9
		temp.u32 = ctx.r10.u32 + ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r10,r11,32
		ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = ctx.r11.s64 >> 32;
		// vupkhsh v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
		// vcfsx v8,v8,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// rlwinm r6,r10,1,0,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vsldoi v9,v31,v9,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 12));
		// vcfsx v7,v7,15
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// xor r5,r6,r5
		ctx.r5.u64 = ctx.r6.u64 ^ ctx.r5.u64;
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// vmulfp128 v9,v12,v9
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
		// cmplwi cr6,r5,0
		// vspltw v12,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsldoi v9,v9,v30,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8), 8));
		// vmsum3fp128 v12,v12,v9
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v12,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// beq cr6,0x82474c74
		if (ctx.r5.u32 == 0) goto loc_82474C74;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r6
	loc_82474C74:
		// mr r5,r6
		ctx.r5.u64 = ctx.r6.u64;
		// cmpd cr6,r10,r30
		// blt cr6,0x82474bcc
}
loc_82474C80:
	// cmpwi cr6,r8,0
	// beq cr6,0x82474cdc
	if (ctx.r8.s32 != 0) {
		// cmpd cr6,r10,r30
		// bne cr6,0x82474cc4
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r8,r24,1,0,30
			ctx.r8.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 1) & 0xFFFFFFFE;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// lhzx r4,r8,r9
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r8.u32 + ctx.r9.u32);
			// lis r8,-32256
			// extsh r6,r4
			ctx.r6.s64 = ctx.r4.s16;
			// lfs f13,27864(r8)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27864);
			ctx.f13.f64 = double(temp.f32);
			// std r6,-136(r1)
			PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
			// lfd f8,-136(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// stfs f5,0(r31)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// b 0x82474cdc
		} else {
		loc_82474CC4:
			// extsw r8,r25
			ctx.r8.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r8
			// ble cr6,0x82474cdc
			if (ctx.r10.s64 <= ctx.r8.s64) goto loc_82474CDC;
			// subf r5,r8,r10
			ctx.r5.s64 = ctx.r10.s64 - ctx.r8.s64;
			// rldicr r8,r5,32,31
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
		}
	}
loc_82474CDC:
	// rldicr r8,r10,32,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r11,r8,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r4,r6,1
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// subf r5,r9,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// std r11,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r11.u64);
	// lfd f4,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// divwu r10,r5,r4
	ctx.r10.u32 = ctx.r4.u32 ? ctx.r5.u32 / ctx.r4.u32 : 0;
	// cmplw cr6,r10,r11
	// fmul f0,f3,f0
	ctx.f0.f64 = ctx.f3.f64 * ctx.f0.f64;
	// frsp f2,f0
	ctx.f2.f64 = double(float(ctx.f0.f64));
	// stfs f2,0(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82474d30
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82474D30:
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r3,r8,r7
	ctx.r3.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r8,r3,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82474d54
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
		// b 0x8242f8c4
		__restgprlr_19(ctx, base);
		return;
	}
loc_82474D54:
	// stw r9,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r9.u32);
	// b 0x8242f8c4
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_7"))) PPC_WEAK_FUNC(phBound_7);
PPC_FUNC_IMPL(__imp__phBound_7) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f86c
	ctx.lr = 0x82474D68;
	__savegprlr_17(ctx, base);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r31,r3,52
	var_r31 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r4,0(r21)
	ctx.r4.u64 = PPC_LOAD_U32(var_r21 + 0);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// subf r7,r10,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r25,r11,r5
	var_r25 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// stw r7,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r7.u32);
	// dcbt r0,r9
	// addi r6,r1,-160
	ctx.r6.s64 = ctx.r1.s64 + -160;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r4,r3,40
	ctx.r4.s64 = ctx.r3.s64 + 40;
	// rldicr r10,r11,32,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,-160
	ctx.r6.s64 = ctx.r1.s64 + -160;
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lis r11,-32248
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-152
	var_r30 = (uint32_t)(ctx.r1.s64 + -152);
	// std r10,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r10.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r24,r1,-160
	var_r24 = (uint32_t)(ctx.r1.s64 + -160);
	// addi r10,r1,-152
	ctx.r10.s64 = ctx.r1.s64 + -152;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// ld r11,-160(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// lvlx v11,r24,r4
	temp.u32 = var_r24 + ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r4,-152(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lvlx v10,r10,r19
	temp.u32 = ctx.r10.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v8,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r11,0
	// vspltw v9,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x82474f2c
	while (ctx.r11.s64 < 0) {
	loc_82474E7C:
		// cmpwi cr6,r7,0
		// ble cr6,0x82474f2c
		if (ctx.r7.s32 <= 0) goto loc_82474F2C;
		// addi r10,r9,2
		ctx.r10.s64 = ctx.r9.s64 + 2;
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// addi r6,r31,4
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 4;
		// vspltisw v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r30,r8,1024
		var_r30 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// lvlx v12,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v5,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvlx v7,0,r6
		temp.u32 = ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v6,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// vspltw v12,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// vsr v7,v13,v11
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsr v11,v13,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsubfp v10,v10,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// cmpdi cr6,r11,0
		ctx.cr6.compare<int64_t>(ctx.r11.s64, 0, ctx.xer);
		// vcfsx v6,v6,15
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfux v7,v7,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v4,v11,31
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v11,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vor v5,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsldoi v10,v0,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// vrlimi128 v12,v6,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v6.f32), 228), 8));
		// vmulfp128 v11,v7,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vor v6,v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v10.u8));
		// vmulfp128 v7,v4,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsldoi v11,v11,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// lvlx v11,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vcfsx v10,v12,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v12,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vsldoi v11,v7,v5,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 8));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// blt cr6,0x82474e7c
}
loc_82474F2C:
	// sradi r10,r11,63
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r6,r11,32
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r11.s64 >> 32;
	// extsw r30,r24
	var_r30 = (uint32_t)((int32_t)var_r24);
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// cmpd cr6,r10,r30
	// bge cr6,0x82475050
while (ctx.r10.s64 < (int64_t)(int32_t)var_r30) {
	loc_82474F48:
		// cmpwi cr6,r7,0
		// beq cr6,0x824750d0
		if (ctx.r7.s32 == 0) goto loc_824750D0;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// extsw r10,r10
		ctx.r10.s64 = ctx.r10.s32;
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// rlwinm r6,r10,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// vor v2,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r18,r10,1
		var_r18 = (uint32_t)(ctx.r10.s64 + 1);  // addr:0x82080001
		// vor v1,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsr v4,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// add r10,r6,r9
		ctx.r10.u64 = ctx.r6.u64 + ctx.r9.u64;
		// rlwinm r6,r18,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(var_r18 | (var_r18 << 32), 2) & 0xFFFFFFFC;
		// vsr v6,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsubfp v5,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r18,r10,2
		var_r18 = (uint32_t)(ctx.r10.s64 + 2);  // addr:0x82080002
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vadduwm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v8.u32)));
		// vor v12,v4,v4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_load_si128((simde__m128i*)ctx.v4.u8));
		// addi r19,r8,1024
		var_r19 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// addi r17,r6,2
		var_r17 = (uint32_t)(ctx.r6.s64 + 2);  // addr:0x82000002
		// vcfux v6,v6,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// lvlx v10,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// vcfux v4,v12,31
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v7,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v12,v7
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v7,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vcfsx v5,v12,15
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v7,v4,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vspltw v12,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vrlimi128 v12,v5,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v5.f32), 228), 8));
		// vsldoi v10,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsubfp v7,v11,v3
		simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v12,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v10,0,r6
		temp.u32 = ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// lvlx v12,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v11,v10
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vupkhsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// sradi r10,r11,32
		ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = ctx.r11.s64 >> 32;
		// rlwinm r6,r10,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// vcfsx v10,v11,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v11,v2,v7,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
		// vcfsx v12,v12,15
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// xor r5,r6,r5
		ctx.r5.u64 = ctx.r6.u64 ^ ctx.r5.u64;
		// vmulfp128 v11,v6,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v11.f32)));
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r5,0
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vsldoi v11,v11,v1,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 8));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// beq cr6,0x82475044
		if (ctx.r5.u32 == 0) goto loc_82475044;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r6
	loc_82475044:
		// mr r5,r6
		ctx.r5.u64 = ctx.r6.u64;
		// cmpd cr6,r10,r30
		// blt cr6,0x82474f48
}
loc_82475050:
	// cmpwi cr6,r7,0
	// beq cr6,0x824750d0
	if (ctx.r7.s32 != 0) {
		// cmpd cr6,r10,r30
		// bne cr6,0x824750b8
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r7,r24,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 2) & 0xFFFFFFFC;
			// lis r6,-32256
			// add r7,r7,r9
			ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// lfs f13,27864(r6)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 27864);
			ctx.f13.f64 = double(temp.f32);
			// lhz r4,2(r7)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
			// extsh r5,r4
			ctx.r5.s64 = ctx.r4.s16;
			// std r5,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r5.u64);
			// lfd f8,-152(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// stfs f5,4(r31)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(var_r31 + 4, temp.u32);
			// lhz r4,0(r7)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
			// extsh r6,r4
			ctx.r6.s64 = ctx.r4.s16;
			// std r6,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r6.u64);
			// lfd f4,-152(r1)
			ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f3,f4
			ctx.f3.f64 = double(ctx.f4.s64);
			// frsp f2,f3
			ctx.f2.f64 = double(float(ctx.f3.f64));
			// fmuls f1,f2,f13
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
			// stfs f1,0(r31)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// b 0x824750d0
		} else {
		loc_824750B8:
			// extsw r7,r25
			ctx.r7.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r7
			// ble cr6,0x824750d0
			if (ctx.r10.s64 <= ctx.r7.s64) goto loc_824750D0;
			// subf r5,r7,r10
			ctx.r5.s64 = ctx.r10.s64 - ctx.r7.s64;
			// rldicr r7,r5,32,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r11,r7,r11
			ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
		}
	}
loc_824750D0:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r11,r7,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r7.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r4,r6,1
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// subf r5,r9,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// std r11,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r11.u64);
	// lfd f13,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// divwu r10,r5,r4
	ctx.r10.u32 = ctx.r4.u32 ? ctx.r5.u32 / ctx.r4.u32 : 0;
	// cmplw cr6,r10,r11
	// fmul f0,f12,f0
	ctx.f0.f64 = ctx.f12.f64 * ctx.f0.f64;
	// frsp f11,f0
	ctx.f11.f64 = double(float(ctx.f0.f64));
	// stfs f11,0(r27)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82475124
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82475124:
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r3,r7,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r8,r3,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82475148
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
		// b 0x8242f8bc
		__restgprlr_17(ctx, base);
		return;
	}
loc_82475148:
	// stw r9,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r9.u32);
	// b 0x8242f8bc
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_8"))) PPC_WEAK_FUNC(phBound_8);
PPC_FUNC_IMPL(__imp__phBound_8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r16 = 0;
	uint32_t var_r17 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f868
	ctx.lr = 0x82475158;
	__savegprlr_16(ctx, base);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(var_r28 + 0);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(var_r20 + 0);
	// lwz r31,0(r21)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r21 + 0));
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r25,r11,r4
	var_r25 = (uint32_t)(ctx.r4.s64 - ctx.r11.s64);
	// add r11,r7,r6
	ctx.r11.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r9,r31
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r6.u32);
	// dcbt r0,r11
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
	// rldicr r7,r7,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r9,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r31,r1,-176
	var_r31 = (uint32_t)(ctx.r1.s64 + -176);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-168
	var_r30 = (uint32_t)(ctx.r1.s64 + -168);
	// std r7,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r7.u64);
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r24,r1,-176
	var_r24 = (uint32_t)(ctx.r1.s64 + -176);
	// li r8,4
	ctx.r8.s64 = 4;
	// addi r7,r1,-168
	ctx.r7.s64 = ctx.r1.s64 + -168;
	// li r19,4
	var_r19 = 4;
	// li r4,0
	ctx.r4.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r9)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25848);
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r31)
	PPC_STORE_U64(var_r31 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// ld r31,-168(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -168));
	// lvlx v11,r24,r8
	temp.u32 = var_r24 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,r7,r19
	temp.u32 = ctx.r7.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r9,-176(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v2,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// cmpdi cr6,r9,0
	// vspltw v3,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x824753a8
	while (ctx.r9.s64 < 0) {
	loc_8247526C:
		// cmpwi cr6,r6,0
		// ble cr6,0x824753a8
		if (ctx.r6.s32 <= 0) goto loc_824753A8;
		// addi r8,r11,6
		ctx.r8.s64 = ctx.r11.s64 + 6;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// addi r7,r5,12
		ctx.r7.s64 = ctx.r5.s64 + 12;
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r30,r10,3072
		var_r30 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82080c00
		// add r9,r31,r9
		ctx.r9.u64 = var_r31 + ctx.r9.u64;
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v6,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvlx v9,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v8,v10
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsr v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vcfux v4,v7,31
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r8,r11,4
		ctx.r8.s64 = ctx.r11.s64 + 4;
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r7,r5,8
		ctx.r7.s64 = ctx.r5.s64 + 8;
		// vcfsx v8,v8,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// cmpdi cr6,r9,0
		ctx.cr6.compare<int64_t>(ctx.r9.s64, 0, ctx.xer);
		// vcfux v9,v9,31
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v7,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vrlimi128 v10,v8,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmulfp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v7,v4,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsubfp v6,v11,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v9,v9,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v10,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r30,r10,1024
		var_r30 = (uint32_t)(ctx.r10.s64 + 1024);  // addr:0x82080400
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r10,2048
		ctx.r8.s64 = ctx.r10.s64 + 2048;
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v9,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r7,r11,2
		ctx.r7.s64 = ctx.r11.s64 + 2;
		// vcfsx v8,v10,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsr v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsr v12,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vadduwm v13,v13,v2
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v2.u32)));
		// vcfux v9,v9,31
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vrlimi128 v10,v8,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsldoi v8,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vcfux v7,v12,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v12,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vor v6,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vmsum3fp128 v10,v10,v8
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// vmulfp128 v12,v9,v12
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vsldoi v8,v12,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// stvewx v10,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r5,4
		ctx.r8.s64 = ctx.r5.s64 + 4;
		// lvlx v10,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v9,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v12,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vor v9,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsubfp v11,v11,v9
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v12,v12,v8
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// vor v8,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vaddfp v0,v0,v3
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vsldoi v11,v8,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// vmulfp128 v11,v7,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvewx v12,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v10,0,r11
		temp.u32 = ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v12,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v11,v11,v6,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 8));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// blt cr6,0x8247526c
}
loc_824753A8:
	// sradi r7,r9,63
	ctx.xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r8,r9,32
	ctx.xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 32;
	// extsw r30,r24
	var_r30 = (uint32_t)((int32_t)var_r24);
	// subf r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	// cmpd cr6,r8,r30
	// bge cr6,0x8247557c
while (ctx.r8.s64 < (int64_t)(int32_t)var_r30) {
	loc_824753C4:
		// cmpwi cr6,r6,0
		// beq cr6,0x8247563c
		if (ctx.r6.s32 == 0) goto loc_8247563C;
		// extsw r7,r8
		ctx.r7.s64 = ctx.r8.s32;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r19,r10,3072
		var_r19 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82080c00
		// rlwinm r8,r7,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// vor v1,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// addi r18,r8,3
		var_r18 = (uint32_t)(ctx.r8.s64 + 3);  // addr:0x82000003
		// vsr v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// rlwinm r7,r7,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// vsubfp v6,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r18,r18,1,0,30
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 1) & 0xFFFFFFFE);
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r16,r7,3
		var_r16 = (uint32_t)(ctx.r7.s64 + 3);  // addr:0x82080003
		// vcfux v5,v9,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r17,r8,2
		var_r17 = (uint32_t)(ctx.r8.s64 + 2);  // addr:0x82000002
		// rlwinm r16,r16,1,0,30
		var_r16 = (uint32_t)(__builtin_rotateleft64(var_r16 | (var_r16 << 32), 1) & 0xFFFFFFFE);
		// rlwinm r17,r17,1,0,30
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 1) & 0xFFFFFFFE);
		// vcfux v7,v7,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// lvlx v10,r18,r11
		temp.u32 = var_r18 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// rlwinm r8,r8,1,0,30
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// addi r18,r10,2048
		var_r18 = (uint32_t)(ctx.r10.s64 + 2048);  // addr:0x82080800
		// add r8,r8,r11
		ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
		// lvlx v8,r16,r11
		temp.u32 = var_r16 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v8,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vsubfp v6,v11,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v8,v5,v8
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vrlimi128 v10,v9,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vsldoi v8,v8,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v8
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// vsr v8,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsr v12,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vcfux v8,v8,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vcfux v12,v12,31
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// stvewx v10,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r19,r7,2
		var_r19 = (uint32_t)(ctx.r7.s64 + 2);  // addr:0x82080002
		// lvlx v10,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// rlwinm r7,r7,1,0,30
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
		// rlwinm r19,r19,1,0,30
		var_r19 = (uint32_t)(__builtin_rotateleft64(var_r19 | (var_r19 << 32), 1) & 0xFFFFFFFE);
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// add r7,r7,r11
		ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// lvlx v9,r19,r11
		temp.u32 = var_r19 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r19,r8,2
		var_r19 = (uint32_t)(ctx.r8.s64 + 2);  // addr:0x82000002
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vcfsx v4,v9,15
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v9,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vor v5,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vmulfp128 v9,v7,v9
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmulfp128 v8,v8,v6
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vsubfp v11,v11,v5
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vrlimi128 v10,v4,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v4.f32), 228), 8));
		// vor v4,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsldoi v9,v9,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v8,v8,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v3
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vsldoi v11,v4,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// vmulfp128 v31,v12,v11
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r18,r7,2
		var_r18 = (uint32_t)(ctx.r7.s64 + 2);  // addr:0x82080002
		// lvlx v10,0,r19
		temp.u32 = var_r19;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r19,r10,1024
		var_r19 = (uint32_t)(ctx.r10.s64 + 1024);  // addr:0x82080400
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vrlimi128 v10,v9,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vmsum3fp128 v10,v10,v8
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// stvewx v10,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// lvlx v11,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vor v10,v31,v31
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v31.u8));
		// add r9,r31,r9
		ctx.r9.u64 = var_r31 + ctx.r9.u64;
		// vadduwm v13,v13,v2
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v2.u32)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// vcfsx v12,v12,15
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// sradi r8,r9,32
		ctx.xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
		ctx.r8.s64 = ctx.r9.s64 >> 32;
		// vcfsx v11,v11,15
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vsldoi v10,v10,v1,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 8));
		// rlwinm r7,r8,3,0,28
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
		// add r7,r7,r11
		ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
		// xor r4,r7,r4
		ctx.r4.u64 = ctx.r7.u64 ^ ctx.r4.u64;
		// rlwinm r4,r4,0,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r4,0
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// beq cr6,0x82475570
		if (ctx.r4.u32 == 0) goto loc_82475570;
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r7
	loc_82475570:
		// mr r4,r7
		ctx.r4.u64 = ctx.r7.u64;
		// cmpd cr6,r8,r30
		// blt cr6,0x824753c4
}
loc_8247557C:
	// cmpwi cr6,r6,0
	// beq cr6,0x8247563c
	if (ctx.r6.s32 != 0) {
		// cmpd cr6,r8,r30
		// bne cr6,0x82475624
		if (ctx.r8.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r7,r24,3,0,28
			ctx.r7.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 3) & 0xFFFFFFF8;
			// addi r8,r8,1
			ctx.r8.s64 = ctx.r8.s64 + 1;
			// add r7,r7,r11
			ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
			// lhz r6,6(r7)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
			// extsh r6,r6
			ctx.r6.s64 = ctx.r6.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f8,-168(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// lis r6,-32256
			ctx.r6.s64 = -2113929216;
			// lfs f13,27864(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 27864);  /* glob:lbl_82006CD8 @ 0x82006cd8 */
			ctx.f13.f64 = double(temp.f32);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// stfs f5,12(r5)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
			// lhz r4,4(r7)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
			// extsh r4,r4
			ctx.r4.s64 = ctx.r4.s16;
			// std r4,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r4.u64);
			// lfd f4,-168(r1)
			ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f3,f4
			ctx.f3.f64 = double(ctx.f4.s64);
			// frsp f2,f3
			ctx.f2.f64 = double(float(ctx.f3.f64));
			// fmuls f1,f2,f13
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
			// stfs f1,8(r5)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
			// lhz r6,2(r7)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
			// extsh r6,r6
			ctx.r6.s64 = ctx.r6.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f12,-168(r1)
			ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f11,f12
			ctx.f11.f64 = double(ctx.f12.s64);
			// frsp f10,f11
			ctx.f10.f64 = double(float(ctx.f11.f64));
			// fmuls f9,f10,f13
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
			// stfs f9,4(r5)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
			// lhz r4,0(r7)
			ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
			// extsh r6,r4
			ctx.r6.s64 = ctx.r4.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f8,-168(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// stfs f5,0(r5)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
			// b 0x8247563c
		} else {
		loc_82475624:
			// extsw r7,r25
			ctx.r7.s64 = (int32_t)var_r25;
			// cmpd cr6,r8,r7
			// ble cr6,0x8247563c
			if (ctx.r8.s64 <= ctx.r7.s64) goto loc_8247563C;
			// subf r5,r7,r8
			ctx.r5.s64 = ctx.r8.s64 - ctx.r7.s64;
			// rldicr r7,r5,32,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r9,r7,r9
			ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
		}
	}
loc_8247563C:
	// rldicr r7,r8,32,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lbz r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// rotlwi r4,r6,1
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// lfd f4,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r5,r9,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r9.s64;
	// divwu r9,r5,r4
	ctx.r9.u32 = ctx.r4.u32 ? ctx.r5.u32 / ctx.r4.u32 : 0;
	// cmplw cr6,r9,r11
	// fmul f0,f3,f0
	ctx.f0.f64 = ctx.f3.f64 * ctx.f0.f64;
	// frsp f2,f0
	ctx.f2.f64 = double(float(ctx.f0.f64));
	// stfs f2,0(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82475690
	if (ctx.r9.u32 < ctx.r11.u32) {
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	}
loc_82475690:
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r3,r7,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r10,r3,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r8
	// bge cr6,0x824756b4
	if (ctx.r10.u32 < ctx.r8.u32) {
		// stw r10,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r10.u32);
		// b 0x8242f8b8
		__restgprlr_16(ctx, base);
		return;
	}
loc_824756B4:
	// stw r8,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
	// b 0x8242f8b8
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_9"))) PPC_WEAK_FUNC(phBound_9);
PPC_FUNC_IMPL(__imp__phBound_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r22 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r16 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f868
	ctx.lr = 0x824756C8;
	__savegprlr_16(ctx, base);
	// addi r22,r3,8
	var_r22 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r28,r3,13
	var_r28 = (uint32_t)(ctx.r3.s64 + 13);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r27,r3,4
	var_r27 = (uint32_t)(ctx.r3.s64 + 4);
	// addi r19,r3,28
	var_r19 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r20,r3,24
	var_r20 = (uint32_t)(ctx.r3.s64 + 24);
	// lwz r11,0(r22)
	ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r21,r3,20
	var_r21 = (uint32_t)(ctx.r3.s64 + 20);
	// lbz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U8(var_r28 + 0);
	// addi r26,r3,48
	var_r26 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r5,0(r27)
	ctx.r5.u64 = PPC_LOAD_U32(var_r27 + 0);
	// addi r4,r3,52
	ctx.r4.s64 = ctx.r3.s64 + 52;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r9,0(r19)
	ctx.r9.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lwz r31,0(r20)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r20 + 0));
	// lwz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lfs f12,0(r26)
	temp.u32 = PPC_LOAD_U32(var_r26 + 0);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r23,r11,r5
	var_r23 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// add r11,r7,r6
	ctx.r11.u64 = ctx.r7.u64 + ctx.r6.u64;
	// subf r6,r9,r31
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r6.u32);
	// dcbt r0,r11
	// addi r9,r1,-176
	ctx.r9.s64 = ctx.r1.s64 + -176;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
	// rldicr r5,r7,32,63
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u64, 32);
	// addi r25,r3,36
	var_r25 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r9,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-168
	var_r30 = (uint32_t)(ctx.r1.s64 + -168);
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lvlx v0,0,r25
	temp.u32 = var_r25;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r29,r1,-176
	var_r29 = (uint32_t)(ctx.r1.s64 + -176);
	// li r8,4
	ctx.r8.s64 = 4;
	// addi r5,r1,-168
	ctx.r5.s64 = ctx.r1.s64 + -168;
	// li r24,4
	var_r24 = 4;
	// li r31,0
	var_r31 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r9)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25848);
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// fdiv f13,f13,f0
	ctx.f13.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.f10.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// fctidz f9,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// ld r7,-176(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// lvlx v11,r29,r8
	temp.u32 = var_r29 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r30,-168(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -168));
	// lvlx v10,r5,r24
	temp.u32 = ctx.r5.u32 + var_r24;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v2,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r7,0
	// vspltw v3,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x824759a4
	while (ctx.r7.s64 < 0) {
	loc_824757DC:
		// cmpwi cr6,r6,0
		// ble cr6,0x824759a4
		if (ctx.r6.s32 <= 0) goto loc_824759A4;
		// addi r8,r11,10
		ctx.r8.s64 = ctx.r11.s64 + 10;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// addi r5,r4,20
		ctx.r5.s64 = ctx.r4.s64 + 20;
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r9,r10,5120
		ctx.r9.s64 = ctx.r10.s64 + 5120;
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v6,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvlx v9,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v8,v10
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsr v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vcfux v4,v7,31
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r8,r11,8
		ctx.r8.s64 = ctx.r11.s64 + 8;
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r5,r4,16
		ctx.r5.s64 = ctx.r4.s64 + 16;
		// vcfsx v8,v8,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfux v9,v9,31
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v7,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vrlimi128 v10,v8,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmulfp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v7,v4,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsubfp v6,v11,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v9,v9,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v10,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,4096
		ctx.r9.s64 = ctx.r10.s64 + 4096;
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r11,6
		ctx.r8.s64 = ctx.r11.s64 + 6;
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v9,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r4,12
		ctx.r5.s64 = ctx.r4.s64 + 12;
		// vcfsx v8,v10,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsr v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vcfux v9,v9,31
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vrlimi128 v10,v8,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsr v8,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vmsum3fp128 v7,v10,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vsldoi v10,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vcfux v8,v8,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmulfp128 v10,v9,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stvewx v7,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,3072
		ctx.r9.s64 = ctx.r10.s64 + 3072;
		// lvlx v9,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r8,r4,8
		ctx.r8.s64 = ctx.r4.s64 + 8;
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// lvlx v7,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsldoi v6,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// addi r5,r11,4
		ctx.r5.s64 = ctx.r11.s64 + 4;
		// vspltw v10,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfux v4,v7,31
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vrlimi128 v10,v9,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vmsum3fp128 v10,v10,v6
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v6.f32), 0xEF));
		// vsubfp v6,v11,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v7,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// stvewx v10,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r9,r10,2048
		ctx.r9.s64 = ctx.r10.s64 + 2048;
		// lvlx v9,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r4,4
		ctx.r5.s64 = ctx.r4.s64 + 4;
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// addi r8,r11,2
		ctx.r8.s64 = ctx.r11.s64 + 2;
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vrlimi128 v10,v9,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vmulfp128 v9,v8,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v8,v4,v6
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v4.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsldoi v9,v9,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsldoi v7,v8,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v10,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// lvlx v9,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v8,v10
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vcfsx v9,v8,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vor v6,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// addi r9,r10,1024
		ctx.r9.s64 = ctx.r10.s64 + 1024;
		// vor v5,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r7,r30,r7
		ctx.r7.u64 = var_r30 + ctx.r7.u64;
		// vor v4,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// vor v1,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// cmpdi cr6,r7,0
		ctx.cr6.compare<int64_t>(ctx.r7.s64, 0, ctx.xer);
		// vsr v12,v6,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vaddfp v0,v0,v3
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vsubfp v11,v11,v5
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vadduwm v13,v13,v2
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v2.u32)));
		// vcfux v8,v12,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vrlimi128 v10,v9,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vmsum3fp128 v10,v10,v7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vsldoi v11,v4,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// vmulfp128 v11,v8,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvewx v10,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v10,0,r11
		temp.u32 = ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v9,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsldoi v11,v11,v1,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 8));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// blt cr6,0x824757dc
}
loc_824759A4:
	// sradi r8,r7,63
	ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r7.s64 >> 63;
	// addi r24,r23,-1
	var_r24 = (uint32_t)(var_r23 + -1);
	// sradi r5,r7,32
	ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s64 >> 32;
	// extsw r29,r24
	var_r29 = (uint32_t)((int32_t)var_r24);
	// subf r9,r8,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r8.s64;
	// cmpd cr6,r9,r29
	// bge cr6,0x82475c44
while (ctx.r9.s64 < (int64_t)(int32_t)var_r29) {
	loc_824759C0:
		// cmpwi cr6,r6,0
		// beq cr6,0x82475d4c
		if (ctx.r6.s32 == 0) goto loc_82475D4C;
		// extsw r9,r9
		ctx.r9.s64 = ctx.r9.s32;
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vspltisw v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r18,r10,5120
		var_r18 = (uint32_t)(ctx.r10.s64 + 5120);  // addr:0x82081400
		// rlwinm r5,r9,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// addi r8,r9,1
		ctx.r8.s64 = ctx.r9.s64 + 1;
		// add r9,r9,r5
		ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
		// vsr v10,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// rlwinm r5,r8,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vsubfp v7,v11,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r9,r9,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// vsr v9,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// add r8,r8,r5
		ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
		// vsubfp v5,v11,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r17,r9,5
		var_r17 = (uint32_t)(ctx.r9.s64 + 5);  // addr:0x82080005
		// vcfux v31,v10,31
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// rlwinm r8,r8,1,0,30
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vor v30,v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
		// rlwinm r5,r17,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(var_r17 | (var_r17 << 32), 1) & 0xFFFFFFFE;
		// vcfux v6,v9,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r17,r9,4
		var_r17 = (uint32_t)(ctx.r9.s64 + 4);  // lbl_82080004 @ 0x82080004
		// addi r16,r10,4096
		var_r16 = (uint32_t)(ctx.r10.s64 + 4096);  // lbl_82081000 @ 0x82081000
		// rlwinm r17,r17,1,0,30
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 1) & 0xFFFFFFFE);
		// lvlx v8,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r8,5
		ctx.r5.s64 = ctx.r8.s64 + 5;
		// vupkhsh v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// rlwinm r5,r5,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
		// vsldoi v7,v0,v7,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
		// vsldoi v5,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vor v8,v31,v31
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_load_si128((simde__m128i*)ctx.v31.u8));
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// lvlx v10,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r8,4
		ctx.r5.s64 = ctx.r8.s64 + 4;
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vmulfp128 v8,v8,v7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v7.f32)));
		// rlwinm r5,r5,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
		// vcfsx v7,v10,15
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsldoi v9,v8,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsubfp v8,v11,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vrlimi128 v10,v7,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// vcfux v7,v7,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v8,v0,v8,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r18,r9,3
		var_r18 = (uint32_t)(ctx.r9.s64 + 3);  // addr:0x82080003
		// lvlx v10,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r17,r8,3
		var_r17 = (uint32_t)(ctx.r8.s64 + 3);  // addr:0x82000003
		// vupkhsh v9,v10
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v10,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// rlwinm r18,r18,1,0,30
		var_r18 = (uint32_t)(__builtin_rotateleft64(var_r18 | (var_r18 << 32), 1) & 0xFFFFFFFE);
		// rlwinm r17,r17,1,0,30
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 1) & 0xFFFFFFFE);
		// addi r5,r9,2
		ctx.r5.s64 = ctx.r9.s64 + 2;
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v4,v10,15
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// rlwinm r5,r5,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
		// vspltw v10,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vmulfp128 v9,v6,v8
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vmulfp128 v8,v7,v5
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vsr v7,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vsubfp v6,v11,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vrlimi128 v10,v4,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v4.f32), 228), 8));
		// vsldoi v9,v9,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v10,r0,r16
		ea = (var_r16) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v10,r18,r11
		temp.u32 = var_r18 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r18,r10,3072
		var_r18 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82080c00
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v9,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// addi r17,r8,2
		var_r17 = (uint32_t)(ctx.r8.s64 + 2);  // addr:0x82000002
		// rlwinm r17,r17,1,0,30
		var_r17 = (uint32_t)(__builtin_rotateleft64(var_r17 | (var_r17 << 32), 1) & 0xFFFFFFFE);
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v9,v9,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vrlimi128 v10,v9,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vsldoi v9,v8,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vcfux v8,v7,31
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v7,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// vmulfp128 v9,v8,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vsubfp v29,v11,v0
		simde_mm_store_ps(ctx.v29.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// rlwinm r9,r9,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// rlwinm r8,r8,1,0,30
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vsr v12,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// add r9,r9,r11
		ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
		// vor v28,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v28.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r8,r8,r11
		ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
		// vor v27,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v27.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r7,r30,r7
		ctx.r7.u64 = var_r30 + ctx.r7.u64;
		// vor v26,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vcfux v6,v12,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// vsubfp v11,v11,v28
		simde_mm_store_ps(ctx.v11.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v28.f32)));
		// vsldoi v9,v9,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r18,r10,2048
		var_r18 = (uint32_t)(ctx.r10.s64 + 2048);  // addr:0x82080800
		// lvlx v10,r5,r11
		temp.u32 = ctx.r5.u32 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r5,r10,1024
		ctx.r5.s64 = ctx.r10.s64 + 1024;
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// lvlx v8,r17,r11
		temp.u32 = var_r17 + ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// addi r17,r8,2
		var_r17 = (uint32_t)(ctx.r8.s64 + 2);  // addr:0x82000002
		// vsldoi v11,v27,v11,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), 12));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v8,v8,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v11,v6,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vspltw v10,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vsldoi v11,v11,v26,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v26.u8), 8));
		// vrlimi128 v10,v8,8,0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmsum3fp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
		// stvewx v10,r0,r18
		ea = (var_r18) & ~0x3;
		PPC_STORE_U32(ea, ctx.v10.u32[3 - ((ea & 0xF) >> 2)]);
		// vsr v10,v13,v30
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8)));
		// addi r18,r9,2
		var_r18 = (uint32_t)(ctx.r9.s64 + 2);  // addr:0x82080002
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vadduwm v13,v13,v2
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v2.u32)));
		// vcfux v7,v10,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vor v10,v29,v29
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v29.u8));
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vsldoi v12,v0,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// vcfsx v10,v9,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v9,v8,15
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v8,v7,v12
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vspltw v12,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vrlimi128 v12,v9,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vsldoi v10,v8,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v3
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vmsum3fp128 v12,v12,v10
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v12,r0,r5
		ea = (ctx.r5.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v12,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r9,r7,32
		ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
		ctx.r9.s64 = ctx.r7.s64 >> 32;
		// vupkhsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// lvlx v10,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// extsw r8,r9
		ctx.r8.s64 = ctx.r9.s32;
		// rlwinm r5,r8,1,0,30
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// vcfsx v12,v12,15
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// add r8,r8,r5
		ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
		// rlwinm r8,r8,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// add r8,r8,r11
		ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
		// xor r5,r8,r31
		ctx.r5.u64 = ctx.r8.u64 ^ var_r31;
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r5,0
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// vrlimi128 v12,v10,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// beq cr6,0x82475c38
		if (ctx.r5.u32 == 0) goto loc_82475C38;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r8
	loc_82475C38:
		// mr r31,r8
		var_r31 = ctx.r8.u32;
		// cmpd cr6,r9,r29
		// blt cr6,0x824759c0
}
loc_82475C44:
	// cmpwi cr6,r6,0
	// beq cr6,0x82475d4c
	if (ctx.r6.s32 != 0) {
		// cmpd cr6,r9,r29
		// bne cr6,0x82475d34
		if (ctx.r9.s64 == (int64_t)(int32_t)var_r29) {
			// rlwinm r8,r24,1,0,30
			ctx.r8.u64 = __builtin_rotateleft64(var_r24 | (var_r24 << 32), 1) & 0xFFFFFFFE;
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// add r8,r24,r8
			ctx.r8.u64 = var_r24 + ctx.r8.u64;
			// rlwinm r8,r8,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// add r8,r8,r11
			ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
			// lhz r6,10(r8)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 10);
			// extsh r6,r6
			ctx.r6.s64 = ctx.r6.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f8,-168(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// lis r6,-32256
			ctx.r6.s64 = -2113929216;
			// lfs f0,27864(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 27864);  /* glob:lbl_82006CD8 @ 0x82006cd8 */
			ctx.f0.f64 = double(temp.f32);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f6,f0
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
			// stfs f5,20(r4)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
			// lhz r5,8(r8)
			ctx.r5.u64 = PPC_LOAD_U16(ctx.r8.u32 + 8);
			// extsh r5,r5
			ctx.r5.s64 = ctx.r5.s16;
			// std r5,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
			// lfd f4,-168(r1)
			ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f3,f4
			ctx.f3.f64 = double(ctx.f4.s64);
			// frsp f2,f3
			ctx.f2.f64 = double(float(ctx.f3.f64));
			// fmuls f1,f2,f0
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
			// stfs f1,16(r4)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
			// lhz r6,6(r8)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 6);
			// extsh r6,r6
			ctx.r6.s64 = ctx.r6.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f12,-168(r1)
			ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f11,f12
			ctx.f11.f64 = double(ctx.f12.s64);
			// frsp f10,f11
			ctx.f10.f64 = double(float(ctx.f11.f64));
			// fmuls f9,f10,f0
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
			// stfs f9,12(r4)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
			// lhz r5,4(r8)
			ctx.r5.u64 = PPC_LOAD_U16(ctx.r8.u32 + 4);
			// extsh r5,r5
			ctx.r5.s64 = ctx.r5.s16;
			// std r5,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
			// lfd f8,-168(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fmuls f5,f6,f0
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
			// stfs f5,8(r4)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
			// lhz r6,2(r8)
			ctx.r6.u64 = PPC_LOAD_U16(ctx.r8.u32 + 2);
			// extsh r6,r6
			ctx.r6.s64 = ctx.r6.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f4,-168(r1)
			ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f3,f4
			ctx.f3.f64 = double(ctx.f4.s64);
			// frsp f2,f3
			ctx.f2.f64 = double(float(ctx.f3.f64));
			// fmuls f1,f2,f0
			ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
			// stfs f1,4(r4)
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
			// lhz r5,0(r8)
			ctx.r5.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
			// extsh r6,r5
			ctx.r6.s64 = ctx.r5.s16;
			// std r6,-168(r1)
			PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
			// lfd f12,-168(r1)
			ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
			// fcfid f11,f12
			ctx.f11.f64 = double(ctx.f12.s64);
			// frsp f10,f11
			ctx.f10.f64 = double(float(ctx.f11.f64));
			// fmuls f9,f10,f0
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
			// stfs f9,0(r4)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
			// b 0x82475d4c
		} else {
		loc_82475D34:
			// extsw r8,r23
			ctx.r8.s64 = (int32_t)var_r23;
			// cmpd cr6,r9,r8
			// ble cr6,0x82475d4c
			if (ctx.r9.s64 <= ctx.r8.s64) goto loc_82475D4C;
			// subf r5,r8,r9
			ctx.r5.s64 = ctx.r9.s64 - ctx.r8.s64;
			// rldicr r8,r5,32,31
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r7,r8,r7
			ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
		}
	}
loc_82475D4C:
	// rldicr r4,r9,32,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r25
	ea = (var_r25) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// subf r9,r4,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r4.s64;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r7,0(r28)
	ctx.r7.u64 = PPC_LOAD_U8(var_r28 + 0);
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// lfd f8,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r5,r7,1
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
	// twllei r5,0
	if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
	// subf r6,r9,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r9.s64;
	// divwu r9,r6,r5
	ctx.r9.u32 = ctx.r5.u32 ? ctx.r6.u32 / ctx.r5.u32 : 0;
	// cmplw cr6,r9,r11
	// fmul f0,f7,f13
	ctx.f0.f64 = ctx.f7.f64 * ctx.f13.f64;
	// frsp f6,f0
	ctx.f6.f64 = double(float(ctx.f0.f64));
	// stfs f6,0(r26)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r26 + 0, temp.u32);
	// bge cr6,0x82475dac
	if (ctx.r9.u32 < ctx.r11.u32) {
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	}
loc_82475DAC:
	// lwz r7,0(r21)
	ctx.r7.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lwz r8,0(r20)
	ctx.r8.u64 = PPC_LOAD_U32(var_r20 + 0);
	// subf r4,r7,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r11,0(r22)
	PPC_STORE_U32(var_r22 + 0, ctx.r11.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r8
	// bge cr6,0x82475dd0
	if (ctx.r10.u32 < ctx.r8.u32) {
		// stw r10,0(r19)
		PPC_STORE_U32(var_r19 + 0, ctx.r10.u32);
		// b 0x8242f8b8
		__restgprlr_16(ctx, base);
		return;
	}
loc_82475DD0:
	// stw r8,0(r19)
	PPC_STORE_U32(var_r19 + 0, ctx.r8.u32);
	// b 0x8242f8b8
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_10"))) PPC_WEAK_FUNC(phBound_10);
PPC_FUNC_IMPL(__imp__phBound_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r24 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r16 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f868
	ctx.lr = 0x82475DE0;
	__savegprlr_16(ctx, base);
	// addi r24,r3,13
	var_r24 = (uint32_t)(ctx.r3.s64 + 13);
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r20,r3,8
	var_r20 = (uint32_t)(ctx.r3.s64 + 8);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r17,r3,28
	var_r17 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r18,r3,24
	var_r18 = (uint32_t)(ctx.r3.s64 + 24);
	// addi r23,r3,4
	var_r23 = (uint32_t)(ctx.r3.s64 + 4);
	// lbz r11,0(r24)
	ctx.r11.u64 = PPC_LOAD_U8(var_r24 + 0);
	// addi r19,r3,20
	var_r19 = (uint32_t)(ctx.r3.s64 + 20);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r22,r3,48
	var_r22 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r9,0(r17)
	ctx.r9.u64 = PPC_LOAD_U32(var_r17 + 0);
	// addi r25,r3,52
	var_r25 = (uint32_t)(ctx.r3.s64 + 52);
	// mullw r6,r11,r10
	ctx.r6.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r31,0(r18)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r18 + 0));
	// lwz r4,0(r23)
	ctx.r4.u64 = PPC_LOAD_U32(var_r23 + 0);
	// lwz r7,0(r19)
	ctx.r7.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lfs f12,0(r22)
	temp.u32 = PPC_LOAD_U32(var_r22 + 0);
	ctx.f12.f64 = double(temp.f32);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// subf r5,r9,r31
	ctx.r5.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r26,r10,r4
	var_r26 = (uint32_t)(ctx.r4.s64 - ctx.r10.s64);
	// add r30,r8,r7
	var_r30 = (uint32_t)(ctx.r8.u64 + ctx.r7.u64);
	// stw r5,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r5.u32);
	// dcbt r0,r6
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r10,r1,-192
	ctx.r10.s64 = ctx.r1.s64 + -192;
	// addi r9,r3,40
	ctx.r9.s64 = ctx.r3.s64 + 40;
	// rldicr r7,r8,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u64, 32);
	// addi r21,r3,36
	var_r21 = (uint32_t)(ctx.r3.s64 + 36);
	// addi r8,r1,-192
	ctx.r8.s64 = ctx.r1.s64 + -192;
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32248
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// std r7,-184(r1)
	PPC_STORE_U64(ctx.r1.u32 + -184, ctx.r7.u64);
	// lfd f0,-184(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// addi r31,r1,-184
	var_r31 = (uint32_t)(ctx.r1.s64 + -184);
	// addi r29,r1,-192
	var_r29 = (uint32_t)(ctx.r1.s64 + -192);
	// lvlx v0,0,r21
	temp.u32 = var_r21;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,4
	ctx.r9.s64 = 4;
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r7,r1,-184
	ctx.r7.s64 = ctx.r1.s64 + -184;
	// vspltw v8,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// li r28,4
	var_r28 = 4;
	// li r4,0
	ctx.r4.s64 = 0;
	// fmul f11,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f10,f13,f0
	ctx.f10.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25848);
	// addi r10,r1,-160
	ctx.r10.s64 = ctx.r1.s64 + -160;
	// fdiv f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.f9.u64);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stfd f8,0(r31)
	PPC_STORE_U64(var_r31 + 0, ctx.f8.u64);
	// lvlx v11,r29,r9
	temp.u32 = var_r29 + ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r31,-192(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -192));
	// lvlx v10,r7,r28
	temp.u32 = ctx.r7.u32 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r27,-184(r1)
	var_r27 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -184));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vrefp v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v0.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// vspltw v7,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// cmpdi cr6,r31,0
	ctx.cr6.compare<int64_t>((int64_t)(int32_t)var_r31, 0, ctx.xer);
	// vspltw v5,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vmulfp128 v0,v12,v0
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vspltw v6,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v6.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// bge cr6,0x82475f94
while ((int64_t)(int32_t)var_r31 < 0) {
	loc_82475EF0:
		// cmpwi cr6,r5,0
		// ble cr6,0x82475f94
		if (ctx.r5.s32 <= 0) goto loc_82475F94;
		// li r10,0
		ctx.r10.s64 = 0;
		// cmplwi cr6,r11,0
		// beq cr6,0x82475f78
		if (ctx.r11.u32 == 0) goto loc_82475F78;
		// vspltisb v0,1
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x1)));
		// mr r9,r25
		ctx.r9.u64 = var_r25;
		// vspltisw v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_set1_epi32(int(0x0)));
		// mr r8,r30
		ctx.r8.u64 = var_r30;
		// vspltisb v11,7
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x7)));
		// vsr v0,v7,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
		// vsubfp v13,v13,v8
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vcfux v0,v0,31
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v13,v8,v13,4
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 12));
		// vmulfp128 v0,v0,v13
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vsldoi v10,v0,v8,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 8));
	loc_82475F30:
		// vslb v12,v11,v11
		ctx.v12.u8[0] = ctx.v11.u8[0] << (ctx.v11.u8[0] & 0x7);
		ctx.v12.u8[1] = ctx.v11.u8[1] << (ctx.v11.u8[1] & 0x7);
		ctx.v12.u8[2] = ctx.v11.u8[2] << (ctx.v11.u8[2] & 0x7);
		ctx.v12.u8[3] = ctx.v11.u8[3] << (ctx.v11.u8[3] & 0x7);
		ctx.v12.u8[4] = ctx.v11.u8[4] << (ctx.v11.u8[4] & 0x7);
		ctx.v12.u8[5] = ctx.v11.u8[5] << (ctx.v11.u8[5] & 0x7);
		ctx.v12.u8[6] = ctx.v11.u8[6] << (ctx.v11.u8[6] & 0x7);
		ctx.v12.u8[7] = ctx.v11.u8[7] << (ctx.v11.u8[7] & 0x7);
		ctx.v12.u8[8] = ctx.v11.u8[8] << (ctx.v11.u8[8] & 0x7);
		ctx.v12.u8[9] = ctx.v11.u8[9] << (ctx.v11.u8[9] & 0x7);
		ctx.v12.u8[10] = ctx.v11.u8[10] << (ctx.v11.u8[10] & 0x7);
		ctx.v12.u8[11] = ctx.v11.u8[11] << (ctx.v11.u8[11] & 0x7);
		ctx.v12.u8[12] = ctx.v11.u8[12] << (ctx.v11.u8[12] & 0x7);
		ctx.v12.u8[13] = ctx.v11.u8[13] << (ctx.v11.u8[13] & 0x7);
		ctx.v12.u8[14] = ctx.v11.u8[14] << (ctx.v11.u8[14] & 0x7);
		ctx.v12.u8[15] = ctx.v11.u8[15] << (ctx.v11.u8[15] & 0x7);
		// lvlx v13,r10,r6
		temp.u32 = ctx.r10.u32 + ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// mr r7,r9
		ctx.r7.u64 = ctx.r9.u64;
		// mr r29,r8
		var_r29 = ctx.r8.u32;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// vaddubm v13,v13,v12
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// addi r8,r8,1024
		ctx.r8.s64 = ctx.r8.s64 + 1024;
		// lvlx v0,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// cmplw cr6,r10,r11
		// vspltw v0,v0,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
		// vupkhsb v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
		// vupkhsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vcfsx v13,v13,7
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vrlimi128 v0,v13,8,0
		simde_mm_store_ps(ctx.v0.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v13.f32), 228), 8));
		// vmsum3fp128 v0,v0,v10
		simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v0,r0,r29
		ea = (var_r29) & ~0x3;
		PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
		// blt cr6,0x82475f30
		if (ctx.r10.u32 < ctx.r11.u32) goto loc_82475F30;
	loc_82475F78:
		// add r31,r27,r31
		var_r31 = (uint32_t)(var_r27 + var_r31);
		// vaddfp v8,v8,v6
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v6.f32)));
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// vadduwm v7,v7,v5
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), simde_mm_load_si128((simde__m128i*)ctx.v5.u32)));
		// addi r5,r5,-1
		ctx.r5.s64 = ctx.r5.s64 + -1;
		// cmpdi cr6,r31,0
		// blt cr6,0x82475ef0
}
loc_82475F94:
	// sradi r10,r31,63
	ctx.xer.ca = ((int64_t)(int32_t)var_r31 < 0) & ((var_r31 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 >> 63;
	// addi r28,r26,-1
	var_r28 = (uint32_t)(var_r26 + -1);
	// sradi r9,r31,32
	ctx.xer.ca = ((int64_t)(int32_t)var_r31 < 0) & ((var_r31 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = (int64_t)(int32_t)var_r31 >> 32;
	// extsw r29,r28
	var_r29 = (uint32_t)((int32_t)var_r28);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpd cr6,r10,r29
	// bge cr6,0x824760a0
while (ctx.r10.s64 < (int64_t)(int32_t)var_r29) {
	loc_82475FB0:
		// cmpwi cr6,r5,0
		// beq cr6,0x824761e0
		if (ctx.r5.s32 == 0) goto loc_824761E0;
		// li r9,0
		ctx.r9.s64 = 0;
		// cmplwi cr6,r11,0
		// beq cr6,0x82476058
		if (ctx.r11.u32 != 0) {
			// vspltisb v0,1
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_set1_epi8(char(0x1)));
			// extsw r8,r10
			ctx.r8.s64 = ctx.r10.s32;
			// vspltisw v12,0
			simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_set1_epi32(int(0x0)));
			// mr r10,r30
			ctx.r10.u64 = var_r30;
			// addi r7,r8,1
			ctx.r7.s64 = ctx.r8.s64 + 1;
			// vspltisb v13,7
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
			// mullw r8,r8,r11
			ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
			// vsr v0,v7,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8)));
			// vsubfp v12,v12,v8
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
			// vcfux v0,v0,31
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
			// mullw r7,r7,r11
			ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
			// vsldoi v12,v8,v12,4
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), 12));
			// vmulfp128 v0,v0,v12
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
			// add r8,r8,r6
			ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
			// add r7,r7,r6
			ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
			// vsldoi v9,v0,v8,8
			simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 8));
		loc_82476004:
			// vslb v11,v13,v13
			ctx.v11.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
			ctx.v11.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
			ctx.v11.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
			ctx.v11.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
			ctx.v11.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
			ctx.v11.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
			ctx.v11.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
			ctx.v11.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
			ctx.v11.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
			ctx.v11.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
			ctx.v11.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
			ctx.v11.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
			ctx.v11.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
			ctx.v11.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
			ctx.v11.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
			ctx.v11.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
			// lvlx v0,r8,r9
			temp.u32 = ctx.r8.u32 + ctx.r9.u32;
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
			// vslb v10,v13,v13
			ctx.v10.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
			ctx.v10.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
			ctx.v10.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
			ctx.v10.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
			ctx.v10.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
			ctx.v10.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
			ctx.v10.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
			ctx.v10.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
			ctx.v10.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
			ctx.v10.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
			ctx.v10.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
			ctx.v10.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
			ctx.v10.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
			ctx.v10.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
			ctx.v10.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
			ctx.v10.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
			// lvlx v12,r7,r9
			temp.u32 = ctx.r7.u32 + ctx.r9.u32;
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
			// mr r16,r10
			var_r16 = ctx.r10.u32;
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// vaddubm v0,v0,v11
			simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
			// addi r10,r10,1024
			ctx.r10.s64 = ctx.r10.s64 + 1024;
			// vaddubm v12,v12,v10
			simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)));
			// cmplw cr6,r9,r11
			ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
			// vupkhsb v0,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s8), simde_mm_load_si128((simde__m128i*)ctx.v0.s8))));
			// vupkhsb v12,v12
			simde_mm_store_si128((simde__m128i*)ctx.v12.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
			// vupkhsh v0,v0
			simde_mm_store_si128((simde__m128i*)ctx.v0.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
			// vupkhsh v12,v12
			simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
			// vcfsx v0,v0,7
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vcfsx v12,v12,7
			simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vspltw v0,v0,0
			simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
			// vrlimi128 v0,v12,8,0
			simde_mm_store_ps(ctx.v0.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v12.f32), 228), 8));
			// vmsum3fp128 v0,v0,v9
			simde_mm_store_ps(ctx.v0.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32), 0xEF));
			// stvewx v0,r0,r16
			ea = (var_r16) & ~0x3;
			PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
			// blt cr6,0x82476004
			if (ctx.cr6.lt) goto loc_82476004;
		}
	loc_82476058:
		// add r31,r27,r31
		var_r31 = (uint32_t)(var_r27 + var_r31);
		// vaddfp v8,v8,v6
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v6.f32)));
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// vadduwm v7,v7,v5
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), simde_mm_load_si128((simde__m128i*)ctx.v5.u32)));
		// sradi r10,r31,32
		ctx.xer.ca = ((int64_t)(int32_t)var_r31 < 0) & ((var_r31 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 >> 32;
		// addi r5,r5,-1
		ctx.r5.s64 = ctx.r5.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// mullw r9,r9,r11
		ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
		// add r9,r9,r6
		ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
		// xor r8,r9,r4
		ctx.r8.u64 = ctx.r9.u64 ^ ctx.r4.u64;
		// rlwinm r7,r8,0,0,24
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r7,0
		// beq cr6,0x82476094
		if (ctx.r7.u32 == 0) goto loc_82476094;
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r9
	loc_82476094:
		// mr r4,r9
		ctx.r4.u64 = ctx.r9.u64;
		// cmpd cr6,r10,r29
		// blt cr6,0x82475fb0
}
loc_824760A0:
	// cmpwi cr6,r5,0
	// beq cr6,0x824761e0
	if (ctx.r5.s32 != 0) {
		// cmpd cr6,r10,r29
		// bne cr6,0x824761c8
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r29) {
			// lis r7,-32256
			// lis r8,-32248
			// mullw r9,r28,r11
			ctx.r9.s64 = int64_t((int32_t)var_r28) * int64_t(ctx.r11.s32);
			// lfs f13,27868(r7)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27868);
			ctx.f13.f64 = double(temp.f32);
			// lfs f0,-25388(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25388);
			ctx.f0.f64 = double(temp.f32);
			// add r4,r9,r6
			ctx.r4.u64 = ctx.r9.u64 + ctx.r6.u64;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// li r9,0
			ctx.r9.s64 = 0;
			// cmpwi cr6,r11,4
			// blt cr6,0x82476184
			if (ctx.r11.s32 >= 4) {
				// addi r9,r11,-4
				ctx.r9.s64 = ctx.r11.s64 + -4;
				// addi r8,r4,1
				ctx.r8.s64 = ctx.r4.s64 + 1;
				// rlwinm r9,r9,30,2,31
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
				// addi r7,r25,8
				ctx.r7.s64 = (int64_t)(int32_t)var_r25 + 8;
				// addi r5,r9,1
				ctx.r5.s64 = ctx.r9.s64 + 1;
				// rlwinm r9,r5,2,0,29
				ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
			loc_824760F0:
				// lbz r29,-1(r8)
				var_r29 = (uint32_t)(PPC_LOAD_U8(ctx.r8.u32 + -1));
				// addi r5,r5,-1
				ctx.r5.s64 = ctx.r5.s64 + -1;
				// cmplwi cr6,r5,0
				ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
				// std r29,-184(r1)
				PPC_STORE_U64(ctx.r1.u32 + -184, var_r29);
				// lfd f7,-184(r1)
				ctx.fpscr.disableFlushMode();
				ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -184);
				// fcfid f6,f7
				ctx.f6.f64 = double(ctx.f7.s64);
				// frsp f5,f6
				ctx.f5.f64 = double(float(ctx.f6.f64));
				// fsubs f4,f5,f0
				ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
				// fmuls f3,f4,f13
				ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
				// stfs f3,-8(r7)
				temp.f32 = float(ctx.f3.f64);
				PPC_STORE_U32(ctx.r7.u32 + -8, temp.u32);
				// lbz r29,0(r8)
				var_r29 = (uint32_t)(PPC_LOAD_U8(ctx.r8.u32 + 0));
				// std r29,-192(r1)
				PPC_STORE_U64(ctx.r1.u32 + -192, var_r29);
				// lfd f2,-192(r1)
				ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
				// fcfid f1,f2
				ctx.f1.f64 = double(ctx.f2.s64);
				// frsp f11,f1
				ctx.f11.f64 = double(float(ctx.f1.f64));
				// fsubs f10,f11,f0
				ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
				// fmuls f9,f10,f13
				ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
				// stfs f9,-4(r7)
				temp.f32 = float(ctx.f9.f64);
				PPC_STORE_U32(ctx.r7.u32 + -4, temp.u32);
				// lbz r29,1(r8)
				var_r29 = (uint32_t)(PPC_LOAD_U8(ctx.r8.u32 + 1));
				// std r29,-176(r1)
				PPC_STORE_U64(ctx.r1.u32 + -176, var_r29);
				// lfd f8,-176(r1)
				ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
				// fcfid f7,f8
				ctx.f7.f64 = double(ctx.f8.s64);
				// frsp f6,f7
				ctx.f6.f64 = double(float(ctx.f7.f64));
				// fsubs f5,f6,f0
				ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
				// fmuls f4,f5,f13
				ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
				// stfs f4,0(r7)
				temp.f32 = float(ctx.f4.f64);
				PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
				// lbz r29,2(r8)
				var_r29 = (uint32_t)(PPC_LOAD_U8(ctx.r8.u32 + 2));
				// addi r8,r8,4
				ctx.r8.s64 = ctx.r8.s64 + 4;
				// std r29,-160(r1)
				PPC_STORE_U64(ctx.r1.u32 + -160, var_r29);
				// lfd f3,-160(r1)
				ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
				// fcfid f2,f3
				ctx.f2.f64 = double(ctx.f3.s64);
				// frsp f1,f2
				ctx.f1.f64 = double(float(ctx.f2.f64));
				// fsubs f11,f1,f0
				ctx.f11.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
				// fmuls f10,f11,f13
				ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
				// stfs f10,4(r7)
				temp.f32 = float(ctx.f10.f64);
				PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
				// addi r7,r7,16
				ctx.r7.s64 = ctx.r7.s64 + 16;
				// bne cr6,0x824760f0
				if (!ctx.cr6.eq) goto loc_824760F0;
			}
		loc_82476184:
			// cmplw cr6,r9,r11
			// bge cr6,0x824761e0
			if (ctx.r9.u32 >= ctx.r11.u32) goto loc_824761E0;
			// rlwinm r8,r9,2,0,29
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
			// add r8,r8,r25
			ctx.r8.u64 = ctx.r8.u64 + var_r25;
		loc_82476194:
			// lbzx r5,r9,r4
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// cmplw cr6,r9,r11
			// std r5,-160(r1)
			PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r5.u64);
			// lfd f9,-160(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
			// fcfid f8,f9
			ctx.f8.f64 = double(ctx.f9.s64);
			// frsp f7,f8
			ctx.f7.f64 = double(float(ctx.f8.f64));
			// fsubs f6,f7,f0
			ctx.f6.f64 = double(float(ctx.f7.f64 - ctx.f0.f64));
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// stfs f5,0(r8)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// addi r8,r8,4
			ctx.r8.s64 = ctx.r8.s64 + 4;
			// blt cr6,0x82476194
			if (ctx.r9.u32 < ctx.r11.u32) goto loc_82476194;
			// b 0x824761e0
		} else {
		loc_824761C8:
			// extsw r9,r26
			ctx.r9.s64 = (int32_t)var_r26;
			// cmpd cr6,r10,r9
			// ble cr6,0x824761e0
			if (ctx.r10.s64 <= ctx.r9.s64) goto loc_824761E0;
			// subf r4,r9,r10
			ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
			// rldicr r9,r4,32,31
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u64, 32) & 0xFFFFFFFF00000000;
			// add r31,r9,r31
			var_r31 = (uint32_t)(ctx.r9.u64 + var_r31);
		}
	}
loc_824761E0:
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// stvewx v8,r0,r21
	ea = (var_r21) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// rldicr r8,r10,32,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// add r10,r11,r6
	ctx.r10.u64 = ctx.r11.u64 + ctx.r6.u64;
	// subf r11,r8,r31
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 - ctx.r8.s64;
	// lbz r6,0(r24)
	ctx.r6.u64 = PPC_LOAD_U8(var_r24 + 0);
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r10,r7,r6
	ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// std r11,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r11.u64);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// cmplw cr6,r10,r11
	// lfd f4,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// fmul f0,f3,f12
	ctx.f0.f64 = ctx.f3.f64 * ctx.f12.f64;
	// frsp f2,f0
	ctx.f2.f64 = double(float(ctx.f0.f64));
	// stfs f2,0(r22)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(var_r22 + 0, temp.u32);
	// bge cr6,0x82476234
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82476234:
	// lwz r8,0(r19)
	ctx.r8.u64 = PPC_LOAD_U32(var_r19 + 0);
	// lwz r9,0(r18)
	ctx.r9.u64 = PPC_LOAD_U32(var_r18 + 0);
	// subf r5,r8,r30
	ctx.r5.s64 = (int64_t)(int32_t)var_r30 - ctx.r8.s64;
	// stw r11,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r11.u32);
	// rlwinm r8,r5,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x82476258
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r17)
		PPC_STORE_U32(var_r17 + 0, ctx.r8.u32);
		// b 0x8242f8b8
		__restgprlr_16(ctx, base);
		return;
	}
loc_82476258:
	// stw r9,0(r17)
	PPC_STORE_U32(var_r17 + 0, ctx.r9.u32);
	// b 0x8242f8b8
	__restgprlr_16(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_11"))) PPC_WEAK_FUNC(phBound_11);
PPC_FUNC_IMPL(__imp__phBound_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r19 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f874
	ctx.lr = 0x82476268;
	__savegprlr_19(ctx, base);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,0(r21)
	ctx.r4.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r31,r3,52
	var_r31 = (uint32_t)(ctx.r3.s64 + 52);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stw r8,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r8.u32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// subf r25,r11,r5
	var_r25 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// dcbt r0,r9
	// addi r11,r1,-144
	ctx.r11.s64 = ctx.r1.s64 + -144;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r10,r3,40
	ctx.r10.s64 = ctx.r3.s64 + 40;
	// rldicr r4,r6,32,63
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// lvlx v13,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r11,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// std r4,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r4.u64);
	// lfd f0,-136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v12,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r30,r1,-136
	var_r30 = (uint32_t)(ctx.r1.s64 + -136);
	// addi r4,r1,-136
	ctx.r4.s64 = ctx.r1.s64 + -136;
	// addi r24,r1,-144
	var_r24 = (uint32_t)(ctx.r1.s64 + -144);
	// li r10,4
	ctx.r10.s64 = 4;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v9,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f12,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f11,f13,f0
	ctx.f11.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// addi r11,r1,-128
	ctx.r11.s64 = ctx.r1.s64 + -128;
	// fdiv f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f10,f12
	ctx.f10.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f12.f64));
	// stfd f10,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.f10.u64);
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f9.u64);
	// vmulfp128 v12,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// lvlx v10,r4,r19
	temp.u32 = ctx.r4.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,r24,r10
	temp.u32 = var_r24 + ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r4,-136(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r11,-144(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v13,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// cmpdi cr6,r11,0
	// vspltw v10,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v9,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// bge cr6,0x824763fc
	while (ctx.r11.s64 < 0) {
	loc_82476378:
		// cmpwi cr6,r8,0
		// ble cr6,0x824763fc
		if (ctx.r8.s32 <= 0) goto loc_824763FC;
		// vspltisb v11,7
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x7)));
		// vor v5,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vspltisb v12,1
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x1)));
		// vor v4,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vspltisw v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v7.u32, simde_mm_set1_epi32(int(0x0)));
		// lvlx v8,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// lvlx v6,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vslb v11,v11,v11
		ctx.v11.u8[0] = ctx.v11.u8[0] << (ctx.v11.u8[0] & 0x7);
		ctx.v11.u8[1] = ctx.v11.u8[1] << (ctx.v11.u8[1] & 0x7);
		ctx.v11.u8[2] = ctx.v11.u8[2] << (ctx.v11.u8[2] & 0x7);
		ctx.v11.u8[3] = ctx.v11.u8[3] << (ctx.v11.u8[3] & 0x7);
		ctx.v11.u8[4] = ctx.v11.u8[4] << (ctx.v11.u8[4] & 0x7);
		ctx.v11.u8[5] = ctx.v11.u8[5] << (ctx.v11.u8[5] & 0x7);
		ctx.v11.u8[6] = ctx.v11.u8[6] << (ctx.v11.u8[6] & 0x7);
		ctx.v11.u8[7] = ctx.v11.u8[7] << (ctx.v11.u8[7] & 0x7);
		ctx.v11.u8[8] = ctx.v11.u8[8] << (ctx.v11.u8[8] & 0x7);
		ctx.v11.u8[9] = ctx.v11.u8[9] << (ctx.v11.u8[9] & 0x7);
		ctx.v11.u8[10] = ctx.v11.u8[10] << (ctx.v11.u8[10] & 0x7);
		ctx.v11.u8[11] = ctx.v11.u8[11] << (ctx.v11.u8[11] & 0x7);
		ctx.v11.u8[12] = ctx.v11.u8[12] << (ctx.v11.u8[12] & 0x7);
		ctx.v11.u8[13] = ctx.v11.u8[13] << (ctx.v11.u8[13] & 0x7);
		ctx.v11.u8[14] = ctx.v11.u8[14] << (ctx.v11.u8[14] & 0x7);
		ctx.v11.u8[15] = ctx.v11.u8[15] << (ctx.v11.u8[15] & 0x7);
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// vsr v5,v5,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// vspltw v12,v6,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.u32), 0xFF));
		// vsubfp v7,v7,v4
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v7.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vor v2,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vadduwm v13,v13,v10
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vaddubm v11,v8,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// vcfux v6,v5,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// cmpdi cr6,r11,0
		ctx.cr6.compare<int64_t>(ctx.r11.s64, 0, ctx.xer);
		// vupkhsb v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vsldoi v7,v3,v7,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8), 12));
		// vcfsx v11,v11,7
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v8,v6,v7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vrlimi128 v12,v11,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vsldoi v8,v8,v2,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8), 8));
		// vmsum3fp128 v12,v12,v8
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// stvewx v12,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// blt cr6,0x82476378
}
loc_824763FC:
	// sradi r10,r11,63
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s64 >> 63;
	// addi r24,r25,-1
	var_r24 = (uint32_t)(var_r25 + -1);
	// sradi r6,r11,32
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r11.s64 >> 32;
	// extsw r30,r24
	var_r30 = (uint32_t)((int32_t)var_r24);
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// cmpd cr6,r10,r30
	// bge cr6,0x824764d4
while (ctx.r10.s64 < (int64_t)(int32_t)var_r30) {
	loc_82476418:
		// cmpwi cr6,r8,0
		// beq cr6,0x82476534
		if (ctx.r8.s32 == 0) goto loc_82476534;
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// extsw r10,r10
		ctx.r10.s64 = ctx.r10.s32;
		// vspltisw v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_set1_epi32(int(0x0)));
		// add r11,r4,r11
		ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
		// vspltisb v12,7
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_set1_epi8(char(0x7)));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// vsr v11,v13,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsubfp v8,v8,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvlx v7,r10,r9
		temp.u32 = ctx.r10.u32 + ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// add r10,r10,r9
		ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
		// vslb v5,v12,v12
		ctx.v5.u8[0] = ctx.v12.u8[0] << (ctx.v12.u8[0] & 0x7);
		ctx.v5.u8[1] = ctx.v12.u8[1] << (ctx.v12.u8[1] & 0x7);
		ctx.v5.u8[2] = ctx.v12.u8[2] << (ctx.v12.u8[2] & 0x7);
		ctx.v5.u8[3] = ctx.v12.u8[3] << (ctx.v12.u8[3] & 0x7);
		ctx.v5.u8[4] = ctx.v12.u8[4] << (ctx.v12.u8[4] & 0x7);
		ctx.v5.u8[5] = ctx.v12.u8[5] << (ctx.v12.u8[5] & 0x7);
		ctx.v5.u8[6] = ctx.v12.u8[6] << (ctx.v12.u8[6] & 0x7);
		ctx.v5.u8[7] = ctx.v12.u8[7] << (ctx.v12.u8[7] & 0x7);
		ctx.v5.u8[8] = ctx.v12.u8[8] << (ctx.v12.u8[8] & 0x7);
		ctx.v5.u8[9] = ctx.v12.u8[9] << (ctx.v12.u8[9] & 0x7);
		ctx.v5.u8[10] = ctx.v12.u8[10] << (ctx.v12.u8[10] & 0x7);
		ctx.v5.u8[11] = ctx.v12.u8[11] << (ctx.v12.u8[11] & 0x7);
		ctx.v5.u8[12] = ctx.v12.u8[12] << (ctx.v12.u8[12] & 0x7);
		ctx.v5.u8[13] = ctx.v12.u8[13] << (ctx.v12.u8[13] & 0x7);
		ctx.v5.u8[14] = ctx.v12.u8[14] << (ctx.v12.u8[14] & 0x7);
		ctx.v5.u8[15] = ctx.v12.u8[15] << (ctx.v12.u8[15] & 0x7);
		// vslb v12,v12,v12
		ctx.v12.u8[0] = ctx.v12.u8[0] << (ctx.v12.u8[0] & 0x7);
		ctx.v12.u8[1] = ctx.v12.u8[1] << (ctx.v12.u8[1] & 0x7);
		ctx.v12.u8[2] = ctx.v12.u8[2] << (ctx.v12.u8[2] & 0x7);
		ctx.v12.u8[3] = ctx.v12.u8[3] << (ctx.v12.u8[3] & 0x7);
		ctx.v12.u8[4] = ctx.v12.u8[4] << (ctx.v12.u8[4] & 0x7);
		ctx.v12.u8[5] = ctx.v12.u8[5] << (ctx.v12.u8[5] & 0x7);
		ctx.v12.u8[6] = ctx.v12.u8[6] << (ctx.v12.u8[6] & 0x7);
		ctx.v12.u8[7] = ctx.v12.u8[7] << (ctx.v12.u8[7] & 0x7);
		ctx.v12.u8[8] = ctx.v12.u8[8] << (ctx.v12.u8[8] & 0x7);
		ctx.v12.u8[9] = ctx.v12.u8[9] << (ctx.v12.u8[9] & 0x7);
		ctx.v12.u8[10] = ctx.v12.u8[10] << (ctx.v12.u8[10] & 0x7);
		ctx.v12.u8[11] = ctx.v12.u8[11] << (ctx.v12.u8[11] & 0x7);
		ctx.v12.u8[12] = ctx.v12.u8[12] << (ctx.v12.u8[12] & 0x7);
		ctx.v12.u8[13] = ctx.v12.u8[13] << (ctx.v12.u8[13] & 0x7);
		ctx.v12.u8[14] = ctx.v12.u8[14] << (ctx.v12.u8[14] & 0x7);
		ctx.v12.u8[15] = ctx.v12.u8[15] << (ctx.v12.u8[15] & 0x7);
		// vcfux v11,v11,31
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// vadduwm v13,v13,v10
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vaddubm v7,v7,v5
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// lvlx v6,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r10,r11,32
		ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = ctx.r11.s64 >> 32;
		// vaddubm v12,v6,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8)));
		// extsw r6,r10
		ctx.r6.s64 = ctx.r10.s32;
		// vsldoi v8,v0,v8,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vupkhsb v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s8), simde_mm_load_si128((simde__m128i*)ctx.v12.s8))));
		// xor r5,r6,r5
		ctx.r5.u64 = ctx.r6.u64 ^ ctx.r5.u64;
		// vmulfp128 v11,v11,v8
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vupkhsb v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s8), simde_mm_load_si128((simde__m128i*)ctx.v7.s8))));
		// vupkhsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r5,0
		ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v7,v12,7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v11,v11,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vspltw v12,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vrlimi128 v12,v7,8,0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vmsum3fp128 v12,v12,v11
		simde_mm_store_ps(ctx.v12.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// stvewx v12,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v12.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r7,4
		ctx.r7.s64 = ctx.r7.s64 + 4;
		// beq cr6,0x824764c8
		if (ctx.cr6.eq) goto loc_824764C8;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r6
	loc_824764C8:
		// mr r5,r6
		ctx.r5.u64 = ctx.r6.u64;
		// cmpd cr6,r10,r30
		// blt cr6,0x82476418
}
loc_824764D4:
	// cmpwi cr6,r8,0
	// beq cr6,0x82476534
	if (ctx.r8.s32 != 0) {
		// cmpd cr6,r10,r30
		// bne cr6,0x8247651c
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r30) {
			// lbzx r8,r24,r9
			ctx.r8.u64 = PPC_LOAD_U8(var_r24 + ctx.r9.u32);
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// std r8,-136(r1)
			PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r8.u64);
			// lfd f8,-136(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// lis r8,-32248
			ctx.r8.s64 = -2113404928;
			// lfs f13,-25388(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25388);  /* glob:lbl_82079CD4 @ 0x82079cd4 */
			ctx.f13.f64 = double(temp.f32);
			// lis r8,-32256
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fsubs f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
			// lfs f13,27868(r8)
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27868);  /* glob:0x82086cdc */
			ctx.f13.f64 = double(temp.f32);
			// fmuls f4,f5,f13
			ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
			// stfs f4,0(r31)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// b 0x82476534
		} else {
		loc_8247651C:
			// extsw r8,r25
			ctx.r8.s64 = (int32_t)var_r25;
			// cmpd cr6,r10,r8
			// ble cr6,0x82476534
			if (ctx.r10.s64 <= ctx.r8.s64) goto loc_82476534;
			// subf r6,r8,r10
			ctx.r6.s64 = ctx.r10.s64 - ctx.r8.s64;
			// rldicr r8,r6,32,31
			ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u64, 32) & 0xFFFFFFFF00000000;
			// add r11,r8,r11
			ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
		}
	}
loc_82476534:
	// rldicr r5,r10,32,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// extsw r8,r10
	ctx.r8.s64 = ctx.r10.s32;
	// subf r11,r5,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r5.s64;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// std r11,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r11.u64);
	// lfd f3,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// divwu r10,r4,r3
	ctx.r10.u32 = ctx.r3.u32 ? ctx.r4.u32 / ctx.r3.u32 : 0;
	// cmplw cr6,r10,r11
	// fmul f0,f2,f0
	ctx.f0.f64 = ctx.f2.f64 * ctx.f0.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// stfs f1,0(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82476584
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82476584:
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r8,r10,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x824765a8
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
		// b 0x8242f8c4
		__restgprlr_19(ctx, base);
		return;
	}
loc_824765A8:
	// stw r9,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r9.u32);
	// b 0x8242f8c4
	__restgprlr_19(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_12"))) PPC_WEAK_FUNC(phBound_12);
PPC_FUNC_IMPL(__imp__phBound_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r20 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f870
	ctx.lr = 0x824765B8;
	__savegprlr_18(ctx, base);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,0(r21)
	ctx.r4.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r31,r3,52
	var_r31 = (uint32_t)(ctx.r3.s64 + 52);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r7,r10,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 0);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stw r7,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r7.u32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// subf r24,r11,r5
	var_r24 = (uint32_t)(ctx.r5.s64 - ctx.r11.s64);
	// dcbt r0,r9
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r10,r3,40
	ctx.r10.s64 = ctx.r3.s64 + 40;
	// rldicr r4,r6,32,63
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// addi r6,r1,-160
	ctx.r6.s64 = ctx.r1.s64 + -160;
	// lvlx v13,0,r11
	temp.u32 = ctx.r11.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r11,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v9,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r30,r1,-152
	var_r30 = (uint32_t)(ctx.r1.s64 + -152);
	// li r10,4
	ctx.r10.s64 = 4;
	// addi r25,r1,-160
	var_r25 = (uint32_t)(ctx.r1.s64 + -160);
	// addi r4,r1,-152
	ctx.r4.s64 = ctx.r1.s64 + -152;
	// li r19,4
	var_r19 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// vrefp v13,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f11,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f10,f13,f0
	ctx.f10.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25848);
	// addi r11,r1,-144
	ctx.r11.s64 = ctx.r1.s64 + -144;
	// fdiv f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.f9.u64);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stfd f8,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f8.u64);
	// vmulfp128 v13,v9,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvlx v11,r25,r10
	temp.u32 = var_r25 + ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lvlx v10,r4,r19
	temp.u32 = ctx.r4.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// ld r4,-152(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r10,0
	// vspltw v12,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// vspltw v3,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v2,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// bge cr6,0x8247679c
	while (ctx.r10.s64 < 0) {
	loc_824766C8:
		// cmpwi cr6,r7,0
		// ble cr6,0x8247679c
		if (ctx.r7.s32 <= 0) goto loc_8247679C;
		// vspltisb v13,7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
		// addi r6,r9,1
		ctx.r6.s64 = ctx.r9.s64 + 1;
		// vspltisb v10,1
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_set1_epi8(char(0x1)));
		// addi r11,r31,4
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 4;
		// vspltisw v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r30,r8,1024
		var_r30 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// vor v1,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r10,r4,r10
		ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
		// vslb v7,v13,v13
		ctx.v7.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v7.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v7.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v7.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v7.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v7.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v7.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v7.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v7.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v7.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v7.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v7.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v7.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v7.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v7.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v7.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// vsr v6,v12,v10
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)));
		// lvlx v8,0,r6
		temp.u32 = ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v5,v9,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsr v10,v12,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)));
		// lvlx v11,0,r11
		temp.u32 = ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v4,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vaddubm v8,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vspltw v11,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vor v31,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vcfux v6,v10,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vor v30,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vsubfp v9,v9,v1
		simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v1.f32)));
		// vadduwm v12,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v3.u32)));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// cmpdi cr6,r10,0
		ctx.cr6.compare<int64_t>(ctx.r10.s64, 0, ctx.xer);
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vsldoi v10,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v10,v7,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vrlimi128 v11,v8,8,0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vslb v8,v13,v4
		ctx.v8.u8[0] = ctx.v13.u8[0] << (ctx.v4.u8[0] & 0x7);
		ctx.v8.u8[1] = ctx.v13.u8[1] << (ctx.v4.u8[1] & 0x7);
		ctx.v8.u8[2] = ctx.v13.u8[2] << (ctx.v4.u8[2] & 0x7);
		ctx.v8.u8[3] = ctx.v13.u8[3] << (ctx.v4.u8[3] & 0x7);
		ctx.v8.u8[4] = ctx.v13.u8[4] << (ctx.v4.u8[4] & 0x7);
		ctx.v8.u8[5] = ctx.v13.u8[5] << (ctx.v4.u8[5] & 0x7);
		ctx.v8.u8[6] = ctx.v13.u8[6] << (ctx.v4.u8[6] & 0x7);
		ctx.v8.u8[7] = ctx.v13.u8[7] << (ctx.v4.u8[7] & 0x7);
		ctx.v8.u8[8] = ctx.v13.u8[8] << (ctx.v4.u8[8] & 0x7);
		ctx.v8.u8[9] = ctx.v13.u8[9] << (ctx.v4.u8[9] & 0x7);
		ctx.v8.u8[10] = ctx.v13.u8[10] << (ctx.v4.u8[10] & 0x7);
		ctx.v8.u8[11] = ctx.v13.u8[11] << (ctx.v4.u8[11] & 0x7);
		ctx.v8.u8[12] = ctx.v13.u8[12] << (ctx.v4.u8[12] & 0x7);
		ctx.v8.u8[13] = ctx.v13.u8[13] << (ctx.v4.u8[13] & 0x7);
		ctx.v8.u8[14] = ctx.v13.u8[14] << (ctx.v4.u8[14] & 0x7);
		ctx.v8.u8[15] = ctx.v13.u8[15] << (ctx.v4.u8[15] & 0x7);
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v2
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vmsum3fp128 v11,v11,v10
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v11,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v11,0,r31
		temp.u32 = var_r31;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// lvlx v10,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v13,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vaddubm v11,v10,v8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// vsldoi v10,v31,v9,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), 12));
		// vupkhsb v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
		// vmulfp128 v10,v6,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vcfsx v11,v11,7
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v10,v10,v30,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8), 8));
		// vrlimi128 v13,v11,8,0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v13,v13,v10
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v13,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// blt cr6,0x824766c8
}
loc_8247679C:
	// sradi r6,r10,63
	ctx.xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r10.s64 >> 63;
	// addi r25,r24,-1
	var_r25 = (uint32_t)(var_r24 + -1);
	// sradi r11,r10,32
	ctx.xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0xFFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r10.s64 >> 32;
	// extsw r30,r25
	var_r30 = (uint32_t)((int32_t)var_r25);
	// subf r11,r6,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
	// cmpd cr6,r11,r30
	// bge cr6,0x824768e8
while (ctx.r11.s64 < (int64_t)(int32_t)var_r30) {
	loc_824767B8:
		// cmpwi cr6,r7,0
		// beq cr6,0x82476970
		if (ctx.r7.s32 == 0) goto loc_82476970;
		// extsw r11,r11
		ctx.r11.s64 = ctx.r11.s32;
		// vspltisb v13,7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
		// vspltisb v10,1
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_set1_epi8(char(0x1)));
		// add r10,r4,r10
		ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
		// rlwinm r6,r11,1,0,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
		// vspltisw v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r19,r11,1
		var_r19 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82080001
		// add r11,r6,r9
		ctx.r11.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vslb v8,v13,v13
		ctx.v8.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v8.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v8.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v8.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v8.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v8.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v8.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v8.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v8.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v8.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v8.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v8.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v8.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v8.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v8.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v8.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// rlwinm r6,r19,1,0,30
		ctx.r6.u64 = __builtin_rotateleft64(var_r19 | (var_r19 << 32), 1) & 0xFFFFFFFE;
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r19,r11,1
		var_r19 = (uint32_t)(ctx.r11.s64 + 1);  // addr:0x82080001
		// vsr v5,v12,v10
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)));
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vsubfp v4,v9,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vslb v29,v13,v13
		ctx.v29.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v29.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v29.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v29.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v29.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v29.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v29.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v29.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v29.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v29.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v29.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v29.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v29.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v29.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v29.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v29.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// addi r18,r6,1
		var_r18 = (uint32_t)(ctx.r6.s64 + 1);  // addr:0x82000001
		// lvlx v11,0,r19
		temp.u32 = var_r19;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r19,r8,1024
		var_r19 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// vaddubm v11,v11,v8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// lvlx v7,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v8,v7,v6
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// vcfux v7,v5,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vupkhsb v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v11,v11,7
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v11,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v11,v8,8,0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vslb v8,v13,v13
		ctx.v8.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v8.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v8.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v8.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v8.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v8.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v8.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v8.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v8.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v8.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v8.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v8.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v8.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v8.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v8.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v8.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vmsum3fp128 v7,v11,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vsr v11,v12,v10
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8)));
		// vsubfp v10,v9,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vadduwm v12,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v3.u32)));
		// vcfux v28,v11,31
		simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v9,v0,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// stvewx v7,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v13,0,r11
		temp.u32 = ctx.r11.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// sradi r11,r10,32
		ctx.xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0xFFFFFFFF) != 0);
		ctx.r11.s64 = ctx.r10.s64 >> 32;
		// vaddubm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// lvlx v11,0,r6
		temp.u32 = ctx.r6.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v11,v11,v29
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v29.u8)));
		// rlwinm r6,r11,1,0,30
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
		// vor v10,v28,v28
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v28.u8));
		// add r6,r6,r9
		ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
		// vupkhsb v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
		// vupkhsb v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
		// xor r5,r6,r5
		ctx.r5.u64 = ctx.r6.u64 ^ ctx.r5.u64;
		// vmulfp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)));
		// rlwinm r5,r5,0,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFF80;
		// vupkhsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// cmplwi cr6,r5,0
		// vcfsx v13,v13,7
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v11,v11,7
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v2
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vspltw v13,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// vrlimi128 v13,v11,8,0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v13,v13,v10
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v13,r0,r8
		ea = (ctx.r8.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r8,r8,4
		ctx.r8.s64 = ctx.r8.s64 + 4;
		// beq cr6,0x824768dc
		if (ctx.r5.u32 == 0) goto loc_824768DC;
		// li r5,128
		ctx.r5.s64 = 128;
		// dcbt r5,r6
	loc_824768DC:
		// mr r5,r6
		ctx.r5.u64 = ctx.r6.u64;
		// cmpd cr6,r11,r30
		// blt cr6,0x824767b8
}
loc_824768E8:
	// cmpwi cr6,r7,0
	// beq cr6,0x82476970
	if (ctx.r7.s32 != 0) {
		// cmpd cr6,r11,r30
		// bne cr6,0x82476958
		if (ctx.r11.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r7,r25,1,0,30
			ctx.r7.u64 = __builtin_rotateleft64(var_r25 | (var_r25 << 32), 1) & 0xFFFFFFFE;
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// add r7,r7,r9
			ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
			// lbz r6,1(r7)
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
			// std r6,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r6.u64);
			// lfd f7,-152(r1)
			ctx.fpscr.disableFlushMode();
			ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f6,f7
			ctx.f6.f64 = double(ctx.f7.s64);
			// lis r6,-32248
			ctx.r6.s64 = -2113404928;
			// lfs f0,-25388(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -25388);  /* glob:lbl_82079CD4 @ 0x82079cd4 */
			ctx.f0.f64 = double(temp.f32);
			// lis r6,-32256
			// frsp f5,f6
			ctx.f5.f64 = double(float(ctx.f6.f64));
			// lfs f13,27868(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 27868);  /* glob:0x82086cdc */
			ctx.f13.f64 = double(temp.f32);
			// fsubs f4,f5,f0
			ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
			// fmuls f3,f4,f13
			ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
			// stfs f3,4(r31)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(var_r31 + 4, temp.u32);
			// lbz r4,0(r7)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f2,-152(r1)
			ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f1,f2
			ctx.f1.f64 = double(ctx.f2.s64);
			// frsp f11,f1
			ctx.f11.f64 = double(float(ctx.f1.f64));
			// fsubs f10,f11,f0
			ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
			// fmuls f9,f10,f13
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
			// stfs f9,0(r31)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(var_r31 + 0, temp.u32);
			// b 0x82476970
		} else {
		loc_82476958:
			// extsw r7,r24
			ctx.r7.s64 = (int32_t)var_r24;
			// cmpd cr6,r11,r7
			// ble cr6,0x82476970
			if (ctx.r11.s64 <= ctx.r7.s64) goto loc_82476970;
			// subf r7,r7,r11
			ctx.r7.s64 = ctx.r11.s64 - ctx.r7.s64;
			// rldicr r7,r7,32,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
			// add r10,r7,r10
			ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
		}
	}
loc_82476970:
	// rldicr r5,r11,32,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r5,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r5.s64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U8(var_r29 + 0);
	// subf r4,r9,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r3,0
	if (ctx.r3.s32 == 0 || ctx.r3.u32 < 0u) __builtin_trap();
	// std r11,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r11.u64);
	// lfd f8,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 0);
	// divwu r10,r4,r3
	ctx.r10.u32 = ctx.r3.u32 ? ctx.r4.u32 / ctx.r3.u32 : 0;
	// cmplw cr6,r10,r11
	// fmul f0,f7,f12
	ctx.f0.f64 = ctx.f7.f64 * ctx.f12.f64;
	// frsp f6,f0
	ctx.f6.f64 = double(float(ctx.f0.f64));
	// stfs f6,0(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x824769c0
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824769C0:
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r10,r7,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r11,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r11.u32);
	// rlwinm r8,r10,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r8,r9
	// bge cr6,0x824769e4
	if (ctx.r8.u32 < ctx.r9.u32) {
		// stw r8,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
		// b 0x8242f8c0
		__restgprlr_18(ctx, base);
		return;
	}
loc_824769E4:
	// stw r9,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r9.u32);
	// b 0x8242f8c0
	__restgprlr_18(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_13"))) PPC_WEAK_FUNC(phBound_13);
PPC_FUNC_IMPL(__imp__phBound_13) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r23 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f86c
	ctx.lr = 0x824769F8;
	__savegprlr_17(ctx, base);
	// addi r23,r3,8
	var_r23 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r29,r3,13
	var_r29 = (uint32_t)(ctx.r3.s64 + 13);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r20,r3,28
	var_r20 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r21,r3,24
	var_r21 = (uint32_t)(ctx.r3.s64 + 24);
	// addi r28,r3,4
	var_r28 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r11,0(r23)
	ctx.r11.u64 = PPC_LOAD_U32(var_r23 + 0);
	// addi r22,r3,20
	var_r22 = (uint32_t)(ctx.r3.s64 + 20);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(var_r29 + 0);
	// addi r27,r3,48
	var_r27 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(var_r20 + 0);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r31,0(r21)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r21 + 0));
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(var_r28 + 0);
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lfs f12,0(r27)
	temp.u32 = PPC_LOAD_U32(var_r27 + 0);
	ctx.f12.f64 = double(temp.f32);
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// subf r6,r9,r31
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 - ctx.r9.s64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r24,r11,r4
	var_r24 = (uint32_t)(ctx.r4.s64 - ctx.r11.s64);
	// add r11,r8,r7
	ctx.r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r6,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r6.u32);
	// dcbt r0,r10
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r8,r3,40
	ctx.r8.s64 = ctx.r3.s64 + 40;
	// rldicr r7,r7,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 32);
	// addi r26,r3,36
	var_r26 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r9,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r31,r1,-160
	var_r31 = (uint32_t)(ctx.r1.s64 + -160);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-152
	var_r30 = (uint32_t)(ctx.r1.s64 + -152);
	// std r7,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r7.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lvlx v0,0,r26
	temp.u32 = var_r26;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp v9,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// li r8,4
	ctx.r8.s64 = 4;
	// addi r25,r1,-160
	var_r25 = (uint32_t)(ctx.r1.s64 + -160);
	// addi r7,r1,-152
	ctx.r7.s64 = ctx.r1.s64 + -152;
	// li r19,4
	var_r19 = 4;
	// li r4,0
	ctx.r4.s64 = 0;
	// vrefp v13,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f11,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f10,f13,f0
	ctx.f10.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r9)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25848);
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// fdiv f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r31)
	PPC_STORE_U64(var_r31 + 0, ctx.f9.u64);
	// vmulfp128 v13,v9,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
	// fctidz f8,f10
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// stfd f8,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f8.u64);
	// ld r31,-152(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -152));
	// lvlx v11,r25,r8
	temp.u32 = var_r25 + ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r8,-160(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lvlx v10,r7,r19
	temp.u32 = ctx.r7.u32 + var_r19;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v3,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r8,0
	// vspltw v2,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// bge cr6,0x82476c70
	while (ctx.r8.s64 < 0) {
	loc_82476B08:
		// cmpwi cr6,r6,0
		// ble cr6,0x82476c70
		if (ctx.r6.s32 <= 0) goto loc_82476C70;
		// vspltisb v13,7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
		// addi r7,r10,3
		ctx.r7.s64 = ctx.r10.s64 + 3;
		// vspltisw v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r9,r5,12
		ctx.r9.s64 = ctx.r5.s64 + 12;
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// addi r30,r11,3072
		var_r30 = (uint32_t)(ctx.r11.s64 + 3072);  // addr:0x82080c00
		// vslb v7,v13,v13
		ctx.v7.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v7.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v7.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v7.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v7.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v7.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v7.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v7.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v7.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v7.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v7.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v7.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v7.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v7.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v7.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v7.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vsubfp v4,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvlx v8,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsr v6,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// lvlx v9,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vor v1,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vaddubm v8,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r9,r10,2
		ctx.r9.s64 = ctx.r10.s64 + 2;
		// vsr v5,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// addi r7,r5,8
		ctx.r7.s64 = ctx.r5.s64 + 8;
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vslb v31,v13,v13
		ctx.v31.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v31.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v31.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v31.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v31.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v31.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v31.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v31.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v31.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v31.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v31.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v31.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v31.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v31.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v31.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v31.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vslb v30,v13,v13
		ctx.v30.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v30.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v30.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v30.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v30.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v30.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v30.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v30.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v30.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v30.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v30.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v30.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v30.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v30.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v30.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v30.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vcfux v5,v5,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsr v4,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsr v11,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vadduwm v12,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v3.u32)));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsubfp v6,v10,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vcfux v11,v11,31
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vslb v7,v13,v1
		ctx.v7.u8[0] = ctx.v13.u8[0] << (ctx.v1.u8[0] & 0x7);
		ctx.v7.u8[1] = ctx.v13.u8[1] << (ctx.v1.u8[1] & 0x7);
		ctx.v7.u8[2] = ctx.v13.u8[2] << (ctx.v1.u8[2] & 0x7);
		ctx.v7.u8[3] = ctx.v13.u8[3] << (ctx.v1.u8[3] & 0x7);
		ctx.v7.u8[4] = ctx.v13.u8[4] << (ctx.v1.u8[4] & 0x7);
		ctx.v7.u8[5] = ctx.v13.u8[5] << (ctx.v1.u8[5] & 0x7);
		ctx.v7.u8[6] = ctx.v13.u8[6] << (ctx.v1.u8[6] & 0x7);
		ctx.v7.u8[7] = ctx.v13.u8[7] << (ctx.v1.u8[7] & 0x7);
		ctx.v7.u8[8] = ctx.v13.u8[8] << (ctx.v1.u8[8] & 0x7);
		ctx.v7.u8[9] = ctx.v13.u8[9] << (ctx.v1.u8[9] & 0x7);
		ctx.v7.u8[10] = ctx.v13.u8[10] << (ctx.v1.u8[10] & 0x7);
		ctx.v7.u8[11] = ctx.v13.u8[11] << (ctx.v1.u8[11] & 0x7);
		ctx.v7.u8[12] = ctx.v13.u8[12] << (ctx.v1.u8[12] & 0x7);
		ctx.v7.u8[13] = ctx.v13.u8[13] << (ctx.v1.u8[13] & 0x7);
		ctx.v7.u8[14] = ctx.v13.u8[14] << (ctx.v1.u8[14] & 0x7);
		ctx.v7.u8[15] = ctx.v13.u8[15] << (ctx.v1.u8[15] & 0x7);
		// stvewx v9,r0,r30
		ea = (var_r30) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v8,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r9,r10,1
		ctx.r9.s64 = ctx.r10.s64 + 1;
		// vaddubm v8,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vsldoi v7,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// lvlx v9,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r7,r11,2048
		ctx.r7.s64 = ctx.r11.s64 + 2048;
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vcfux v6,v4,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vmulfp128 v7,v5,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vsubfp v5,v10,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmsum3fp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v9,r0,r7
		ea = (ctx.r7.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r7,r5,4
		ctx.r7.s64 = ctx.r5.s64 + 4;
		// lvlx v9,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r9,r11,1024
		ctx.r9.s64 = ctx.r11.s64 + 1024;
		// vaddubm v9,v9,v31
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v31.u8)));
		// lvlx v8,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vcfsx v7,v9,7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v9,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vsubfp v8,v10,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsldoi v10,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vmulfp128 v10,v6,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vrlimi128 v9,v7,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsldoi v13,v0,v8,4
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
		// vmulfp128 v11,v11,v13
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vsldoi v13,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v13,v9,v13
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32), 0xEF));
		// vsldoi v11,v11,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v2
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// stvewx v13,r0,r9
		ea = (ctx.r9.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v13,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v9,v13,v30
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8)));
		// lvlx v10,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v13,v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
		// vupkhsb v10,v9
		simde_mm_store_si128((simde__m128i*)ctx.v10.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vcfsx v10,v10,7
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vrlimi128 v13,v10,8,0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v10.f32), 228), 8));
		// vmsum3fp128 v13,v13,v11
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32), 0xEF));
		// add r8,r31,r8
		ctx.r8.u64 = var_r31 + ctx.r8.u64;
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// cmpdi cr6,r8,0
		ctx.cr6.compare<int64_t>(ctx.r8.s64, 0, ctx.xer);
		// stvewx v13,r0,r11
		ea = (ctx.r11.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// blt cr6,0x82476b08
}
loc_82476C70:
	// sradi r7,r8,63
	ctx.xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s64 >> 63;
	// addi r25,r24,-1
	var_r25 = (uint32_t)(var_r24 + -1);
	// sradi r9,r8,32
	ctx.xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r8.s64 >> 32;
	// extsw r30,r25
	var_r30 = (uint32_t)((int32_t)var_r25);
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// cmpd cr6,r9,r30
	// bge cr6,0x82476e8c
while (ctx.r9.s64 < (int64_t)(int32_t)var_r30) {
	loc_82476C8C:
		// cmpwi cr6,r6,0
		// beq cr6,0x82476f54
		if (ctx.r6.s32 == 0) goto loc_82476F54;
		// extsw r9,r9
		ctx.r9.s64 = ctx.r9.s32;
		// vspltisb v13,7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// rlwinm r7,r9,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// vspltisw v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r19,r9,1
		var_r19 = (uint32_t)(ctx.r9.s64 + 1);  // addr:0x82080001
		// add r9,r7,r10
		ctx.r9.u64 = ctx.r7.u64 + ctx.r10.u64;
		// vslb v8,v13,v13
		ctx.v8.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v8.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v8.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v8.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v8.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v8.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v8.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v8.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v8.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v8.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v8.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v8.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v8.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v8.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v8.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v8.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// rlwinm r7,r19,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(var_r19 | (var_r19 << 32), 2) & 0xFFFFFFFC;
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r19,r9,3
		var_r19 = (uint32_t)(ctx.r9.s64 + 3);  // addr:0x82080003
		// vsr v5,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// add r7,r7,r10
		ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
		// vsubfp v4,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vslb v29,v13,v13
		ctx.v29.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v29.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v29.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v29.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v29.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v29.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v29.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v29.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v29.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v29.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v29.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v29.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v29.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v29.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v29.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v29.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r18,r7,3
		var_r18 = (uint32_t)(ctx.r7.s64 + 3);  // addr:0x82000003
		// vslb v28,v13,v13
		ctx.v28.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v28.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v28.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v28.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v28.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v28.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v28.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v28.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v28.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v28.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v28.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v28.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v28.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v28.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v28.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v28.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r17,r7,2
		var_r17 = (uint32_t)(ctx.r7.s64 + 2);  // addr:0x82000002
		// vslb v26,v13,v13
		ctx.v26.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v26.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v26.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v26.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v26.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v26.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v26.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v26.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v26.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v26.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v26.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v26.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v26.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v26.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v26.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v26.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// lvlx v9,0,r19
		temp.u32 = var_r19;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r19,r11,3072
		var_r19 = (uint32_t)(ctx.r11.s64 + 3072);  // addr:0x82080c00
		// vaddubm v9,v9,v8
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// vslb v25,v13,v13
		ctx.v25.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v25.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v25.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v25.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v25.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v25.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v25.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v25.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v25.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v25.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v25.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v25.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v25.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v25.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v25.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v25.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// lvlx v7,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r18,r9,2
		var_r18 = (uint32_t)(ctx.r9.s64 + 2);  // addr:0x82080002
		// vaddubm v8,v7,v6
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// vcfux v7,v5,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vsubfp v5,v10,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vsubfp v5,v10,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmsum3fp128 v7,v9,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vsr v9,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vcfux v27,v9,31
		simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// stvewx v7,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r19,r11,2048
		var_r19 = (uint32_t)(ctx.r11.s64 + 2048);  // addr:0x82080800
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r18,r9,1
		var_r18 = (uint32_t)(ctx.r9.s64 + 1);  // addr:0x82080001
		// vaddubm v9,v9,v29
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v29.u8)));
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v8,v8,v28
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v28.u8)));
		// addi r17,r7,1
		var_r17 = (uint32_t)(ctx.r7.s64 + 1);  // addr:0x82000001
		// vor v7,v27,v27
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_load_si128((simde__m128i*)ctx.v27.u8));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmsum3fp128 v7,v9,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vsr v9,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vcfux v24,v9,31
		simde_mm_store_ps(ctx.v24.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// stvewx v7,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v7,v24,v24
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_load_si128((simde__m128i*)ctx.v24.u8));
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v9,v9,v26
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v26.u8)));
		// vaddubm v8,v8,v25
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v25.u8)));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// addi r19,r11,1024
		var_r19 = (uint32_t)(ctx.r11.s64 + 1024);  // addr:0x82080400
		// vslb v23,v13,v13
		ctx.v23.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v23.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v23.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v23.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v23.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v23.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v23.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v23.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v23.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v23.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v23.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v23.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v23.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v23.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v23.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v23.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vsubfp v10,v10,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsr v11,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// add r8,r31,r8
		ctx.r8.u64 = var_r31 + ctx.r8.u64;
		// vadduwm v12,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v3.u32)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vslb v8,v13,v13
		ctx.v8.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v8.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v8.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v8.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v8.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v8.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v8.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v8.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v8.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v8.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v8.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v8.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v8.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v8.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v8.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v8.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vcfux v22,v11,31
		simde_mm_store_ps(ctx.v22.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmsum3fp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v9,r0,r19
		ea = (var_r19) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// vsldoi v9,v0,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// lvlx v13,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v10,v22,v22
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_load_si128((simde__m128i*)ctx.v22.u8));
		// vaddubm v13,v13,v8
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// lvlx v11,0,r7
		temp.u32 = ctx.r7.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v11,v11,v23
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v23.u8)));
		// sradi r9,r8,32
		ctx.xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFFFFFF) != 0);
		ctx.r9.s64 = ctx.r8.s64 >> 32;
		// vmulfp128 v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)));
		// rlwinm r7,r9,2,0,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// vupkhsb v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
		// vupkhsb v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
		// add r7,r7,r10
		ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
		// xor r4,r7,r4
		ctx.r4.u64 = ctx.r7.u64 ^ ctx.r4.u64;
		// vupkhsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// rlwinm r4,r4,0,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r4,0
		// vcfsx v13,v13,7
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v11,v11,7
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v2
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vspltw v13,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// vrlimi128 v13,v11,8,0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v13,v13,v10
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v13,r0,r11
		ea = (ctx.r11.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// beq cr6,0x82476e80
		if (ctx.r4.u32 == 0) goto loc_82476E80;
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r7
	loc_82476E80:
		// mr r4,r7
		ctx.r4.u64 = ctx.r7.u64;
		// cmpd cr6,r9,r30
		// blt cr6,0x82476c8c
}
loc_82476E8C:
	// cmpwi cr6,r6,0
	// beq cr6,0x82476f54
	if (ctx.r6.s32 != 0) {
		// cmpd cr6,r9,r30
		// bne cr6,0x82476f3c
		if (ctx.r9.s64 == (int64_t)(int32_t)var_r30) {
			// rlwinm r7,r25,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(var_r25 | (var_r25 << 32), 2) & 0xFFFFFFFC;
			// lis r6,-32248
			// add r7,r7,r10
			ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
			// addi r9,r9,1
			ctx.r9.s64 = ctx.r9.s64 + 1;
			// lfs f0,-25388(r6)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -25388);
			ctx.f0.f64 = double(temp.f32);
			// lis r6,-32256
			// lbz r4,3(r7)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
			// lfs f13,27868(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 27868);
			ctx.f13.f64 = double(temp.f32);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f7,-152(r1)
			ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f6,f7
			ctx.f6.f64 = double(ctx.f7.s64);
			// frsp f5,f6
			ctx.f5.f64 = double(float(ctx.f6.f64));
			// fsubs f4,f5,f0
			ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
			// fmuls f3,f4,f13
			ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
			// stfs f3,12(r5)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
			// lbz r4,2(r7)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f2,-152(r1)
			ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f1,f2
			ctx.f1.f64 = double(ctx.f2.s64);
			// frsp f11,f1
			ctx.f11.f64 = double(float(ctx.f1.f64));
			// fsubs f10,f11,f0
			ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
			// fmuls f9,f10,f13
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
			// stfs f9,8(r5)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
			// lbz r4,1(r7)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f8,-152(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fsubs f5,f6,f0
			ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
			// fmuls f4,f5,f13
			ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
			// stfs f4,4(r5)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
			// lbz r6,0(r7)
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
			// std r6,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r6.u64);
			// lfd f3,-152(r1)
			ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f2,f3
			ctx.f2.f64 = double(ctx.f3.s64);
			// frsp f1,f2
			ctx.f1.f64 = double(float(ctx.f2.f64));
			// fsubs f0,f1,f0
			ctx.f0.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
			// fmuls f13,f0,f13
			ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
			// stfs f13,0(r5)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
			// b 0x82476f54
		} else {
		loc_82476F3C:
			// extsw r7,r24
			ctx.r7.s64 = (int32_t)var_r24;
			// cmpd cr6,r9,r7
			// ble cr6,0x82476f54
			if (ctx.r9.s64 <= ctx.r7.s64) goto loc_82476F54;
			// subf r5,r7,r9
			ctx.r5.s64 = ctx.r9.s64 - ctx.r7.s64;
			// rldicr r7,r5,32,31
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r8,r7,r8
			ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
		}
	}
loc_82476F54:
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// stvewx v0,r0,r26
	ea = (var_r26) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// rldicr r9,r9,32,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r7,r4,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// add r8,r7,r10
	ctx.r8.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lbz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 0);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 0);
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// std r9,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r9.u64);
	// lfd f11,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r8,r9,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r9.s64;
	// divwu r9,r8,r7
	ctx.r9.u32 = ctx.r7.u32 ? ctx.r8.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r9,r10
	// fmul f0,f10,f12
	ctx.f0.f64 = ctx.f10.f64 * ctx.f12.f64;
	// frsp f9,f0
	ctx.f9.f64 = double(float(ctx.f0.f64));
	// stfs f9,0(r27)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r27 + 0, temp.u32);
	// bge cr6,0x82476fa8
	if (ctx.r9.u32 < ctx.r10.u32) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_82476FA8:
	// lwz r7,0(r22)
	ctx.r7.u64 = PPC_LOAD_U32(var_r22 + 0);
	// lwz r8,0(r21)
	ctx.r8.u64 = PPC_LOAD_U32(var_r21 + 0);
	// subf r6,r7,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r7.s64;
	// stw r10,0(r23)
	PPC_STORE_U32(var_r23 + 0, ctx.r10.u32);
	// rlwinm r11,r6,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	// bge cr6,0x82476fcc
	if (ctx.r11.u32 < ctx.r8.u32) {
		// stw r11,0(r20)
		PPC_STORE_U32(var_r20 + 0, ctx.r11.u32);
		// b 0x8242f8bc
		__restgprlr_17(ctx, base);
		return;
	}
loc_82476FCC:
	// stw r8,0(r20)
	PPC_STORE_U32(var_r20 + 0, ctx.r8.u32);
	// b 0x8242f8bc
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__phBound_14"))) PPC_WEAK_FUNC(phBound_14);
PPC_FUNC_IMPL(__imp__phBound_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r22 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r17 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f86c
	ctx.lr = 0x82476FE0;
	__savegprlr_17(ctx, base);
	// addi r22,r3,8
	var_r22 = (uint32_t)(ctx.r3.s64 + 8);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r27,r3,13
	var_r27 = (uint32_t)(ctx.r3.s64 + 13);
	// lfs f13,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// addi r19,r3,28
	var_r19 = (uint32_t)(ctx.r3.s64 + 28);
	// addi r20,r3,24
	var_r20 = (uint32_t)(ctx.r3.s64 + 24);
	// addi r26,r3,4
	var_r26 = (uint32_t)(ctx.r3.s64 + 4);
	// lwz r11,0(r22)
	ctx.r11.u64 = PPC_LOAD_U32(var_r22 + 0);
	// addi r21,r3,20
	var_r21 = (uint32_t)(ctx.r3.s64 + 20);
	// lbz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 0);
	// addi r25,r3,48
	var_r25 = (uint32_t)(ctx.r3.s64 + 48);
	// lwz r10,0(r19)
	ctx.r10.u64 = PPC_LOAD_U32(var_r19 + 0);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// mullw r8,r9,r11
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// lwz r31,0(r20)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r20 + 0));
	// lwz r4,0(r26)
	ctx.r4.u64 = PPC_LOAD_U32(var_r26 + 0);
	// lwz r7,0(r21)
	ctx.r7.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lfs f12,0(r25)
	temp.u32 = PPC_LOAD_U32(var_r25 + 0);
	ctx.f12.f64 = double(temp.f32);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r6,r10,r31
	ctx.r6.s64 = (int64_t)(int32_t)var_r31 - ctx.r10.s64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r23,r11,r4
	var_r23 = (uint32_t)(ctx.r4.s64 - ctx.r11.s64);
	// add r11,r9,r7
	ctx.r11.u64 = ctx.r9.u64 + ctx.r7.u64;
	// stw r6,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r6.u32);
	// dcbt r0,r8
	// addi r10,r1,-160
	ctx.r10.s64 = ctx.r1.s64 + -160;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r9,r3,40
	ctx.r9.s64 = ctx.r3.s64 + 40;
	// rldicr r4,r7,32,63
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u64, 32);
	// addi r24,r3,36
	var_r24 = (uint32_t)(ctx.r3.s64 + 36);
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32248
	// vcfsx v13,v13,0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)));
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,-152
	var_r30 = (uint32_t)(ctx.r1.s64 + -152);
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lvlx v0,0,r24
	temp.u32 = var_r24;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// vsubfp v9,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
	// addi r29,r1,-160
	var_r29 = (uint32_t)(ctx.r1.s64 + -160);
	// li r9,4
	ctx.r9.s64 = 4;
	// addi r4,r1,-152
	ctx.r4.s64 = ctx.r1.s64 + -152;
	// li r28,4
	var_r28 = 4;
	// li r31,0
	var_r31 = 0;
	// vrefp v13,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_div_ps(simde_mm_set1_ps(1), simde_mm_load_ps(ctx.v13.f32)));
	// fmul f11,f12,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f10,f13,f0
	ctx.f10.f64 = ctx.f13.f64 * ctx.f0.f64;
	// lfd f13,-25848(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25848);
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// fdiv f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 / ctx.f0.f64;
	// fctidz f9,f11
	ctx.f9.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f11.f64));
	// stfd f9,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.f9.u64);
	// vmulfp128 v13,v9,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
	// fctidz f8,f10
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simde_mm_cvttsd_si64(simde_mm_load_sd(&ctx.f10.f64));
	// ld r7,-160(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// stfd f8,0(r30)
	PPC_STORE_U64(var_r30 + 0, ctx.f8.u64);
	// lvlx v11,r29,r9
	temp.u32 = var_r29 + ctx.r9.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r30,-152(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -152));
	// lvlx v10,r4,r28
	temp.u32 = ctx.r4.u32 + var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vspltw v3,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v3.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// cmpdi cr6,r7,0
	// vspltw v2,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v2.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// bge cr6,0x8247730c
	while (ctx.r7.s64 < 0) {
	loc_824770F0:
		// cmpwi cr6,r6,0
		// ble cr6,0x8247730c
		if (ctx.r6.s32 <= 0) goto loc_8247730C;
		// vspltisb v13,7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
		// addi r9,r8,5
		ctx.r9.s64 = ctx.r8.s64 + 5;
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// addi r4,r5,20
		ctx.r4.s64 = ctx.r5.s64 + 20;
		// vspltisw v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r10,r11,5120
		ctx.r10.s64 = ctx.r11.s64 + 5120;
		// vslb v7,v13,v13
		ctx.v7.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v7.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v7.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v7.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v7.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v7.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v7.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v7.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v7.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v7.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v7.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v7.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v7.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v7.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v7.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v7.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vsr v6,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// lvlx v8,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vsubfp v4,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvlx v9,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vor v1,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// vaddubm v8,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r4,r8,4
		ctx.r4.s64 = ctx.r8.s64 + 4;
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsr v5,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// addi r9,r5,16
		ctx.r9.s64 = ctx.r5.s64 + 16;
		// vslb v31,v13,v13
		ctx.v31.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v31.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v31.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v31.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v31.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v31.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v31.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v31.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v31.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v31.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v31.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v31.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v31.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v31.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v31.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v31.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vslb v30,v13,v13
		ctx.v30.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v30.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v30.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v30.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v30.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v30.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v30.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v30.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v30.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v30.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v30.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v30.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v30.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v30.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v30.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v30.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vcfux v5,v5,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vsldoi v6,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsr v4,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vsubfp v6,v10,v0
		simde_mm_store_ps(ctx.v6.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vslb v7,v13,v1
		ctx.v7.u8[0] = ctx.v13.u8[0] << (ctx.v1.u8[0] & 0x7);
		ctx.v7.u8[1] = ctx.v13.u8[1] << (ctx.v1.u8[1] & 0x7);
		ctx.v7.u8[2] = ctx.v13.u8[2] << (ctx.v1.u8[2] & 0x7);
		ctx.v7.u8[3] = ctx.v13.u8[3] << (ctx.v1.u8[3] & 0x7);
		ctx.v7.u8[4] = ctx.v13.u8[4] << (ctx.v1.u8[4] & 0x7);
		ctx.v7.u8[5] = ctx.v13.u8[5] << (ctx.v1.u8[5] & 0x7);
		ctx.v7.u8[6] = ctx.v13.u8[6] << (ctx.v1.u8[6] & 0x7);
		ctx.v7.u8[7] = ctx.v13.u8[7] << (ctx.v1.u8[7] & 0x7);
		ctx.v7.u8[8] = ctx.v13.u8[8] << (ctx.v1.u8[8] & 0x7);
		ctx.v7.u8[9] = ctx.v13.u8[9] << (ctx.v1.u8[9] & 0x7);
		ctx.v7.u8[10] = ctx.v13.u8[10] << (ctx.v1.u8[10] & 0x7);
		ctx.v7.u8[11] = ctx.v13.u8[11] << (ctx.v1.u8[11] & 0x7);
		ctx.v7.u8[12] = ctx.v13.u8[12] << (ctx.v1.u8[12] & 0x7);
		ctx.v7.u8[13] = ctx.v13.u8[13] << (ctx.v1.u8[13] & 0x7);
		ctx.v7.u8[14] = ctx.v13.u8[14] << (ctx.v1.u8[14] & 0x7);
		ctx.v7.u8[15] = ctx.v13.u8[15] << (ctx.v1.u8[15] & 0x7);
		// vsubfp v1,v10,v0
		simde_mm_store_ps(ctx.v1.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvewx v9,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r11,4096
		ctx.r10.s64 = ctx.r11.s64 + 4096;
		// lvlx v8,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r4,r5,12
		ctx.r4.s64 = ctx.r5.s64 + 12;
		// vaddubm v8,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vsldoi v7,v0,v6,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8), 12));
		// lvlx v9,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r9,r8,3
		ctx.r9.s64 = ctx.r8.s64 + 3;
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vcfux v6,v4,31
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsr v4,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vmulfp128 v7,v5,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vsubfp v5,v10,v0
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmsum3fp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v9,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r11,3072
		ctx.r10.s64 = ctx.r11.s64 + 3072;
		// lvlx v9,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r9,r8,2
		ctx.r9.s64 = ctx.r8.s64 + 2;
		// vaddubm v9,v9,v31
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v31.u8)));
		// lvlx v8,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r4,r5,8
		ctx.r4.s64 = ctx.r5.s64 + 8;
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vcfsx v7,v9,7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v9,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vsldoi v8,v0,v1,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), 12));
		// vrlimi128 v9,v7,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vsldoi v7,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vcfux v5,v4,31
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vmulfp128 v7,v6,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v6,v5,v8
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vsldoi v8,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vmsum3fp128 v9,v9,v8
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// vsldoi v8,v6,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// stvewx v9,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v9,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v6,v9,v30
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v30.u8)));
		// lvlx v7,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v9,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// vupkhsb v7,v6
		simde_mm_store_si128((simde__m128i*)ctx.v7.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v6.s8), simde_mm_load_si128((simde__m128i*)ctx.v6.s8))));
		// vupkhsh v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
		// vcfsx v7,v7,7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vrlimi128 v9,v7,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v7.f32), 228), 8));
		// vmsum3fp128 v6,v9,v8
		simde_mm_store_ps(ctx.v6.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// vsr v9,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsubfp v8,v10,v0
		simde_mm_store_ps(ctx.v8.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsr v11,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// addi r10,r11,2048
		ctx.r10.s64 = ctx.r11.s64 + 2048;
		// vslb v29,v13,v13
		ctx.v29.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v29.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v29.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v29.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v29.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v29.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v29.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v29.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v29.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v29.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v29.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v29.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v29.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v29.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v29.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v29.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r9,r8,1
		ctx.r9.s64 = ctx.r8.s64 + 1;
		// vor v26,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v26.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vcfux v28,v9,31
		simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// addi r4,r5,4
		ctx.r4.s64 = ctx.r5.s64 + 4;
		// vcfux v27,v11,31
		simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vslb v13,v13,v13
		ctx.v13.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v13.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v13.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v13.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v13.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v13.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v13.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v13.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v13.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v13.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v13.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v13.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v13.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v13.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v13.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v13.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vor v25,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v25.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// add r7,r30,r7
		ctx.r7.u64 = var_r30 + ctx.r7.u64;
		// vsubfp v10,v10,v26
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v26.f32)));
		// vor v24,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v24.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vadduwm v12,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v3.u32)));
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// cmpdi cr6,r7,0
		ctx.cr6.compare<int64_t>(ctx.r7.s64, 0, ctx.xer);
		// vsldoi v9,v0,v8,4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), 12));
		// stvewx v6,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v6.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r10,r11,1024
		ctx.r10.s64 = ctx.r11.s64 + 1024;
		// lvlx v8,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v11,v28,v28
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_load_si128((simde__m128i*)ctx.v28.u8));
		// lvlx v7,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vor v6,v27,v27
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v27.u8));
		// vsldoi v10,v25,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// vmulfp128 v5,v11,v9
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vaddubm v9,v8,v29
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v29.u8)));
		// vspltw v11,v7,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.u32), 0xFF));
		// vmulfp128 v10,v6,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v8,v5,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v2
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vsldoi v10,v10,v24,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v24.u8), 8));
		// vrlimi128 v11,v9,8,0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v9.f32), 228), 8));
		// vmsum3fp128 v11,v11,v8
		simde_mm_store_ps(ctx.v11.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v8.f32), 0xEF));
		// stvewx v11,r0,r10
		ea = (ctx.r10.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v11.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v9,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v13,v9,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8)));
		// lvlx v11,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v11,v11,0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
		// vupkhsb v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
		// vupkhsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vcfsx v13,v13,7
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vrlimi128 v11,v13,8,0
		simde_mm_store_ps(ctx.v11.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v13.f32), 228), 8));
		// vmsum3fp128 v13,v11,v10
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v13,r0,r11
		ea = (ctx.r11.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// blt cr6,0x824770f0
}
loc_8247730C:
	// sradi r9,r7,63
	ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s64 >> 63;
	// addi r28,r23,-1
	var_r28 = (uint32_t)(var_r23 + -1);
	// sradi r4,r7,32
	ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r4.s64 = ctx.r7.s64 >> 32;
	// extsw r29,r28
	var_r29 = (uint32_t)((int32_t)var_r28);
	// subf r10,r9,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r9.s64;
	// cmpd cr6,r10,r29
	// bge cr6,0x82477600
while (ctx.r10.s64 < (int64_t)(int32_t)var_r29) {
	loc_82477328:
		// cmpwi cr6,r6,0
		// beq cr6,0x82477710
		if (ctx.r6.s32 == 0) goto loc_82477710;
		// extsw r10,r10
		ctx.r10.s64 = ctx.r10.s32;
		// vspltisb v13,7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_set1_epi8(char(0x7)));
		// vspltisb v11,1
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_set1_epi8(char(0x1)));
		// rlwinm r4,r10,1,0,30
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// vspltisw v10,0
		simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_set1_epi32(int(0x0)));
		// addi r9,r10,1
		ctx.r9.s64 = ctx.r10.s64 + 1;
		// add r10,r10,r4
		ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
		// vslb v9,v13,v13
		ctx.v9.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v9.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v9.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v9.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v9.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v9.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v9.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v9.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v9.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v9.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v9.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v9.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v9.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v9.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v9.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v9.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// rlwinm r4,r9,1,0,30
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// vslb v8,v13,v13
		ctx.v8.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v8.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v8.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v8.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v8.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v8.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v8.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v8.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v8.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v8.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v8.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v8.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v8.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v8.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v8.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v8.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// rlwinm r10,r10,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
		// vsr v6,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// add r9,r9,r4
		ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
		// vsubfp v5,v10,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v5.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// add r10,r10,r8
		ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
		// rlwinm r9,r9,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// addi r4,r10,5
		ctx.r4.s64 = ctx.r10.s64 + 5;
		// add r9,r9,r8
		ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
		// addi r18,r10,4
		var_r18 = (uint32_t)(ctx.r10.s64 + 4);  // lbl_82080004 @ 0x82080004
		// addi r17,r9,4
		var_r17 = (uint32_t)(ctx.r9.s64 + 4);  // lbl_82080004 @ 0x82080004
		// lvlx v7,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r4,r9,5
		ctx.r4.s64 = ctx.r9.s64 + 5;
		// vaddubm v9,v7,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8)));
		// vcfux v7,v6,31
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v6,v0,v5,4
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8), 12));
		// vslb v5,v13,v13
		ctx.v5.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v5.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v5.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v5.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v5.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v5.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v5.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v5.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v5.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v5.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v5.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v5.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v5.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v5.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v5.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v5.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// lvlx v4,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vaddubm v8,v4,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// vsubfp v4,v10,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r4,r11,5120
		ctx.r4.s64 = ctx.r11.s64 + 5120;
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vmulfp128 v7,v7,v6
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsr v8,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vmsum3fp128 v7,v9,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vcfux v23,v8,31
		simde_mm_store_ps(ctx.v23.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v8,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsubfp v4,v10,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvewx v7,r0,r4
		ea = (ctx.r4.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// vmulfp128 v7,v23,v8
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v23.f32), simde_mm_load_ps(ctx.v8.f32)));
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r4,r11,4096
		ctx.r4.s64 = ctx.r11.s64 + 4096;
		// vaddubm v9,v9,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v8,v8,v5
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// addi r18,r10,3
		var_r18 = (uint32_t)(ctx.r10.s64 + 3);  // addr:0x82080003
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r17,r9,3
		var_r17 = (uint32_t)(ctx.r9.s64 + 3);  // addr:0x82080003
		// vslb v5,v13,v13
		ctx.v5.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v5.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v5.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v5.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v5.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v5.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v5.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v5.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v5.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v5.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v5.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v5.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v5.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v5.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v5.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v5.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsr v8,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vmsum3fp128 v7,v9,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vcfux v22,v8,31
		simde_mm_store_ps(ctx.v22.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v8,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// stvewx v7,r0,r4
		ea = (ctx.r4.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// vmulfp128 v7,v22,v8
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v22.f32), simde_mm_load_ps(ctx.v8.f32)));
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v9,v9,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v8,v8,v5
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vsubfp v4,v10,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r4,r11,3072
		ctx.r4.s64 = ctx.r11.s64 + 3072;
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r18,r10,2
		var_r18 = (uint32_t)(ctx.r10.s64 + 2);  // addr:0x82080002
		// vslb v5,v13,v13
		ctx.v5.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v5.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v5.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v5.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v5.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v5.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v5.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v5.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v5.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v5.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v5.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v5.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v5.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v5.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v5.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v5.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r17,r9,2
		var_r17 = (uint32_t)(ctx.r9.s64 + 2);  // addr:0x82080002
		// vslb v19,v13,v13
		ctx.v19.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v19.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v19.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v19.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v19.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v19.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v19.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v19.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v19.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v19.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v19.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v19.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v19.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v19.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v19.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v19.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsr v8,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vmsum3fp128 v7,v9,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vcfux v21,v8,31
		simde_mm_store_ps(ctx.v21.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v8,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vsubfp v4,v10,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vsubfp v10,v10,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_sub_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// stvewx v7,r0,r4
		ea = (ctx.r4.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// vmulfp128 v7,v21,v8
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v21.f32), simde_mm_load_ps(ctx.v8.f32)));
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r4,r11,2048
		ctx.r4.s64 = ctx.r11.s64 + 2048;
		// vaddubm v9,v9,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v8,v8,v5
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// addi r18,r10,1
		var_r18 = (uint32_t)(ctx.r10.s64 + 1);  // addr:0x82080001
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// addi r17,r9,1
		var_r17 = (uint32_t)(ctx.r9.s64 + 1);  // addr:0x82080001
		// vslb v5,v13,v13
		ctx.v5.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v5.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v5.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v5.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v5.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v5.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v5.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v5.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v5.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v5.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v5.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v5.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v5.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v5.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v5.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v5.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vsr v8,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vsr v11,v12,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_vsr(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8)));
		// vadduwm v12,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_add_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), simde_mm_load_si128((simde__m128i*)ctx.v3.u32)));
		// vmsum3fp128 v7,v9,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// vcfux v20,v8,31
		simde_mm_store_ps(ctx.v20.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v8,v0,v4,4
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8), 12));
		// vcfux v18,v11,31
		simde_mm_store_ps(ctx.v18.f32, simde_mm_mul_ps(simde_mm_cvtepu32_ps_(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x30000000)))));
		// vsldoi v11,v0,v10,4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), 12));
		// stvewx v7,r0,r4
		ea = (ctx.r4.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
		// vmulfp128 v7,v20,v8
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v20.f32), simde_mm_load_ps(ctx.v8.f32)));
		// lvlx v9,0,r18
		temp.u32 = var_r18;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// addi r4,r11,1024
		ctx.r4.s64 = ctx.r11.s64 + 1024;
		// vaddubm v9,v9,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// lvlx v8,0,r17
		temp.u32 = var_r17;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v8,v8,v5
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// vmulfp128 v10,v18,v11
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v18.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vslb v6,v13,v13
		ctx.v6.u8[0] = ctx.v13.u8[0] << (ctx.v13.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v13.u8[1] << (ctx.v13.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v13.u8[2] << (ctx.v13.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v13.u8[3] << (ctx.v13.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v13.u8[4] << (ctx.v13.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v13.u8[5] << (ctx.v13.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v13.u8[6] << (ctx.v13.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v13.u8[7] << (ctx.v13.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v13.u8[8] << (ctx.v13.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v13.u8[9] << (ctx.v13.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v13.u8[10] << (ctx.v13.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v13.u8[11] << (ctx.v13.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v13.u8[12] << (ctx.v13.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v13.u8[13] << (ctx.v13.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v13.u8[14] << (ctx.v13.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v13.u8[15] << (ctx.v13.u8[15] & 0x7);
		// vupkhsb v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s8), simde_mm_load_si128((simde__m128i*)ctx.v9.s8))));
		// vupkhsb v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s8), simde_mm_load_si128((simde__m128i*)ctx.v8.s8))));
		// vupkhsh v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v9.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v9.s16), simde_mm_load_si128((simde__m128i*)ctx.v9.s16))));
		// vupkhsh v8,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v8.s16), simde_mm_load_si128((simde__m128i*)ctx.v8.s16))));
		// vsldoi v7,v7,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vcfsx v9,v9,7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v9.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v8,v8,7
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vsldoi v10,v10,v0,8
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 8));
		// vaddfp v0,v0,v2
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vspltw v9,v9,0
		simde_mm_store_si128((simde__m128i*)ctx.v9.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v9.u32), 0xFF));
		// vrlimi128 v9,v8,8,0
		simde_mm_store_ps(ctx.v9.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v8.f32), 228), 8));
		// vmsum3fp128 v9,v9,v7
		simde_mm_store_ps(ctx.v9.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v7.f32), 0xEF));
		// stvewx v9,r0,r4
		ea = (ctx.r4.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v9.u32[3 - ((ea & 0xF) >> 2)]);
		// lvlx v13,0,r10
		temp.u32 = ctx.r10.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v13,v13,v6
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// lvlx v11,0,r9
		temp.u32 = ctx.r9.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vaddubm v11,v11,v19
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v19.u8)));
		// vupkhsb v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s8), simde_mm_load_si128((simde__m128i*)ctx.v13.s8))));
		// vupkhsb v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s8), simde_mm_load_si128((simde__m128i*)ctx.v11.s8))));
		// vupkhsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vupkhsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vcfsx v13,v13,7
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v11,v11,7
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vspltw v13,v13,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
		// vrlimi128 v13,v11,8,0
		simde_mm_store_ps(ctx.v13.f32, simde_mm_blend_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_permute_ps(simde_mm_load_ps(ctx.v11.f32), 228), 8));
		// vmsum3fp128 v13,v13,v10
		simde_mm_store_ps(ctx.v13.f32, simde_mm_dp_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32), 0xEF));
		// stvewx v13,r0,r11
		ea = (ctx.r11.u32) & ~0x3;
		PPC_STORE_U32(ea, ctx.v13.u32[3 - ((ea & 0xF) >> 2)]);
		// add r7,r30,r7
		ctx.r7.u64 = var_r30 + ctx.r7.u64;
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// sradi r10,r7,32
		ctx.xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
		ctx.r10.s64 = ctx.r7.s64 >> 32;
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// rlwinm r4,r9,1,0,30
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// add r9,r9,r4
		ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
		// rlwinm r9,r9,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
		// add r9,r9,r8
		ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
		// xor r4,r9,r31
		ctx.r4.u64 = ctx.r9.u64 ^ var_r31;
		// rlwinm r4,r4,0,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
		// cmplwi cr6,r4,0
		// beq cr6,0x824775f4
		if (ctx.r4.u32 == 0) goto loc_824775F4;
		// li r4,128
		ctx.r4.s64 = 128;
		// dcbt r4,r9
	loc_824775F4:
		// mr r31,r9
		var_r31 = ctx.r9.u32;
		// cmpd cr6,r10,r29
		// blt cr6,0x82477328
}
loc_82477600:
	// cmpwi cr6,r6,0
	// beq cr6,0x82477710
	if (ctx.r6.s32 != 0) {
		// cmpd cr6,r10,r29
		// bne cr6,0x824776f8
		if (ctx.r10.s64 == (int64_t)(int32_t)var_r29) {
			// rlwinm r9,r28,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 1) & 0xFFFFFFFE;
			// lis r6,-32248
			// add r9,r28,r9
			ctx.r9.u64 = var_r28 + ctx.r9.u64;
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// rlwinm r9,r9,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
			// add r9,r9,r8
			ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
			// lfs f0,-25388(r6)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -25388);
			ctx.f0.f64 = double(temp.f32);
			// lis r6,-32256
			// lbz r4,5(r9)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
			// lfs f13,27868(r6)
			temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 27868);
			ctx.f13.f64 = double(temp.f32);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f7,-152(r1)
			ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f6,f7
			ctx.f6.f64 = double(ctx.f7.s64);
			// frsp f5,f6
			ctx.f5.f64 = double(float(ctx.f6.f64));
			// fsubs f4,f5,f0
			ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
			// fmuls f3,f4,f13
			ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
			// stfs f3,20(r5)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
			// lbz r4,4(r9)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f2,-152(r1)
			ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f1,f2
			ctx.f1.f64 = double(ctx.f2.s64);
			// frsp f11,f1
			ctx.f11.f64 = double(float(ctx.f1.f64));
			// fsubs f10,f11,f0
			ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
			// fmuls f9,f10,f13
			ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
			// stfs f9,16(r5)
			temp.f32 = float(ctx.f9.f64);
			PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
			// lbz r4,3(r9)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f8,-152(r1)
			ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f7,f8
			ctx.f7.f64 = double(ctx.f8.s64);
			// frsp f6,f7
			ctx.f6.f64 = double(float(ctx.f7.f64));
			// fsubs f5,f6,f0
			ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
			// fmuls f4,f5,f13
			ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
			// stfs f4,12(r5)
			temp.f32 = float(ctx.f4.f64);
			PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
			// lbz r4,2(r9)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f3,-152(r1)
			ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f2,f3
			ctx.f2.f64 = double(ctx.f3.s64);
			// frsp f1,f2
			ctx.f1.f64 = double(float(ctx.f2.f64));
			// fsubs f11,f1,f0
			ctx.f11.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
			// fmuls f10,f11,f13
			ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
			// stfs f10,8(r5)
			temp.f32 = float(ctx.f10.f64);
			PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
			// lbz r4,1(r9)
			ctx.r4.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
			// std r4,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
			// lfd f9,-152(r1)
			ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f8,f9
			ctx.f8.f64 = double(ctx.f9.s64);
			// frsp f7,f8
			ctx.f7.f64 = double(float(ctx.f8.f64));
			// fsubs f6,f7,f0
			ctx.f6.f64 = double(float(ctx.f7.f64 - ctx.f0.f64));
			// fmuls f5,f6,f13
			ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
			// stfs f5,4(r5)
			temp.f32 = float(ctx.f5.f64);
			PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
			// lbz r6,0(r9)
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
			// std r6,-152(r1)
			PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r6.u64);
			// lfd f4,-152(r1)
			ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
			// fcfid f3,f4
			ctx.f3.f64 = double(ctx.f4.s64);
			// frsp f2,f3
			ctx.f2.f64 = double(float(ctx.f3.f64));
			// fsubs f1,f2,f0
			ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f0.f64));
			// fmuls f0,f1,f13
			ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
			// stfs f0,0(r5)
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
			// b 0x82477710
		} else {
		loc_824776F8:
			// extsw r9,r23
			ctx.r9.s64 = (int32_t)var_r23;
			// cmpd cr6,r10,r9
			// ble cr6,0x82477710
			if (ctx.r10.s64 <= ctx.r9.s64) goto loc_82477710;
			// subf r5,r9,r10
			ctx.r5.s64 = ctx.r10.s64 - ctx.r9.s64;
			// rldicr r9,r5,32,31
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u64, 32) & 0xFFFFFFFF00000000;
			// add r7,r9,r7
			ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
		}
	}
loc_82477710:
	// rldicr r4,r10,32,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// stvewx v0,r0,r24
	ea = (var_r24) & ~0x3;
	PPC_STORE_U32(ea, ctx.v0.u32[3 - ((ea & 0xF) >> 2)]);
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// subf r10,r4,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r4.s64;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lbz r7,0(r27)
	ctx.r7.u64 = PPC_LOAD_U8(var_r27 + 0);
	// std r10,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r10.u64);
	// lfd f13,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f13
	ctx.f11.f64 = double(ctx.f13.s64);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(var_r26 + 0);
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// divwu r9,r8,r7
	ctx.r9.u32 = ctx.r7.u32 ? ctx.r8.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r9,r10
	// fmul f0,f11,f12
	ctx.f0.f64 = ctx.f11.f64 * ctx.f12.f64;
	// frsp f10,f0
	ctx.f10.f64 = double(float(ctx.f0.f64));
	// stfs f10,0(r25)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r25 + 0, temp.u32);
	// bge cr6,0x8247776c
	if (ctx.r9.u32 < ctx.r10.u32) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_8247776C:
	// lwz r7,0(r21)
	ctx.r7.u64 = PPC_LOAD_U32(var_r21 + 0);
	// lwz r8,0(r20)
	ctx.r8.u64 = PPC_LOAD_U32(var_r20 + 0);
	// subf r6,r7,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r7.s64;
	// stw r10,0(r22)
	PPC_STORE_U32(var_r22 + 0, ctx.r10.u32);
	// rlwinm r11,r6,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	// bge cr6,0x82477790
	if (ctx.r11.u32 < ctx.r8.u32) {
		// stw r11,0(r19)
		PPC_STORE_U32(var_r19 + 0, ctx.r11.u32);
		// b 0x8242f8bc
		__restgprlr_17(ctx, base);
		return;
	}
loc_82477790:
	// stw r8,0(r19)
	PPC_STORE_U32(var_r19 + 0, ctx.r8.u32);
	// b 0x8242f8bc
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundGeometry_0"))) PPC_WEAK_FUNC(phBoundGeometry_0);
PPC_FUNC_IMPL(__imp__phBoundGeometry_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mullw r11,r7,r11
	ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// cmpw cr6,r8,r9
	// add r6,r11,r6
	ctx.r6.u64 = ctx.r11.u64 + ctx.r6.u64;
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// blt cr6,0x824777e8
	if (ctx.r8.s32 >= ctx.r9.s32) {
		// mr r4,r9
		ctx.r4.u64 = ctx.r9.u64;
	}
loc_824777E8:
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r31,r7,2,0,29
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC);
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_8247780C:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r7,4
	// blt cr6,0x82477874
	if (ctx.r7.s32 >= 4) {
		// addi r9,r7,-4
		ctx.r9.s64 = ctx.r7.s64 + -4;
		// addi r10,r6,2048
		ctx.r10.s64 = ctx.r6.s64 + 2048;
		// rlwinm r9,r9,30,2,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r11,r5,8
		ctx.r11.s64 = ctx.r5.s64 + 8;
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82477830:
		// lfs f8,-8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		ctx.f8.f64 = double(temp.f32);
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// fmuls f7,f8,f0
		ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
		// stfs f7,-2048(r10)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r10.u32 + -2048, temp.u32);
		// lfs f6,-4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		ctx.f6.f64 = double(temp.f32);
		// cmplwi cr6,r9,0
		ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
		// fmuls f5,f6,f0
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
		// stfs f5,-1024(r10)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r10.u32 + -1024, temp.u32);
		// lfs f4,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f4.f64 = double(temp.f32);
		// fmuls f3,f4,f0
		ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// stfs f3,0(r10)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// lfs f2,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f2.f64 = double(temp.f32);
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// fmuls f1,f2,f0
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
		// stfs f1,1024(r10)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
		// addi r10,r10,4096
		ctx.r10.s64 = ctx.r10.s64 + 4096;
		// bne cr6,0x82477830
		if (!ctx.cr6.eq) goto loc_82477830;
	}
loc_82477874:
	// cmplw cr6,r8,r7
	// bge cr6,0x824778b0
	if (ctx.r8.u32 < ctx.r7.u32) {
		// rlwinm r10,r8,10,0,21
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0xFFFFFC00;
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// add r9,r10,r6
		ctx.r9.u64 = ctx.r10.u64 + ctx.r6.u64;
		// add r10,r11,r5
		ctx.r10.u64 = ctx.r11.u64 + ctx.r5.u64;
		// subf r11,r8,r7
		ctx.r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	loc_82477890:
		// lfs f12,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f12.f64 = double(temp.f32);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// fmuls f11,f12,f0
		ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
		// stfs f11,0(r9)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// addi r9,r9,1024
		ctx.r9.s64 = ctx.r9.s64 + 1024;
		// cmplwi cr6,r11,0
		// bne cr6,0x82477890
		if (ctx.r11.u32 != 0) goto loc_82477890;
	}
loc_824778B0:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// fadds f0,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// add r5,r31,r5
	ctx.r5.u64 = var_r31 + ctx.r5.u64;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmpwi cr6,r4,0
	// bne cr6,0x8247780c
	if (ctx.r4.s32 != 0) goto loc_8247780C;
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r5,r8,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r8.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rotlwi r4,r7,2
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r7.u32, 2);
	// divwu r10,r5,r4
	ctx.r10.u32 = ctx.r4.u32 ? ctx.r5.u32 / ctx.r4.u32 : 0;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x824778f4
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824778F4:
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r8,r10,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82477914
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82477914:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_1"))) PPC_WEAK_FUNC(phBoundGeometry_1);
PPC_FUNC_IMPL(__imp__phBoundGeometry_1) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r6
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// blt cr6,0x82477970
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r9,r6
		ctx.r9.u64 = ctx.r6.u64;
	}
loc_82477970:
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8247799c
while (ctx.r10.u32 < ctx.r8.u32) {
	loc_82477988:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r8
		// blt cr6,0x82477988
}
loc_8247799C:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_824779BC:
	// lfs f8,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,0(r7)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// cmpwi cr6,r9,0
	// bne cr6,0x824779bc
	if (ctx.r9.s32 != 0) goto loc_824779BC;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r8,r10,2
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r9,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r9.s64;
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r11,r6,r8
	ctx.r11.u32 = ctx.r8.u32 ? ctx.r6.u32 / ctx.r8.u32 : 0;
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x82477a0c
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82477A0C:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r5.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82477a2c
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82477A2C:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_2"))) PPC_WEAK_FUNC(phBoundGeometry_2);
PPC_FUNC_IMPL(__imp__phBoundGeometry_2) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r6
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// blt cr6,0x82477a80
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r9,r6
		ctx.r9.u64 = ctx.r6.u64;
	}
loc_82477A80:
	// rlwinm r7,r9,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82477aac
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_82477A98:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x82477a98
}
loc_82477AAC:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82477ACC:
	// lfs f8,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,1024(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lfs f6,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// cmpwi cr6,r9,0
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,0(r8)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne cr6,0x82477acc
	if (ctx.r9.s32 != 0) goto loc_82477ACC;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r7,r10,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r9,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r9.s64;
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// divwu r11,r6,r7
	ctx.r11.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x82477b28
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82477B28:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r5.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82477b48
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82477B48:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_3"))) PPC_WEAK_FUNC(phBoundGeometry_3);
PPC_FUNC_IMPL(__imp__phBoundGeometry_3) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r9,r6
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// blt cr6,0x82477ba4
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
	}
loc_82477BA4:
	// rlwinm r7,r8,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82477bd0
while (ctx.r9.u32 < ctx.r7.u32) {
	loc_82477BBC:
		// rlwinm r5,r9,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r7
		// blt cr6,0x82477bbc
}
loc_82477BD0:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82477BF0:
	// lfs f8,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,3072(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,2048(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lfs f4,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,1024(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lfs f2,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// bne cr6,0x82477bf0
	if (!ctx.cr6.eq) goto loc_82477BF0;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r7,r9,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r8,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r8.s64;
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// divwu r11,r6,r7
	ctx.r11.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r11,r9
	// bge cr6,0x82477c60
	if (ctx.r11.u32 < ctx.r9.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82477C60:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r5.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82477c80
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82477C80:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_4"))) PPC_WEAK_FUNC(phBoundGeometry_4);
PPC_FUNC_IMPL(__imp__phBoundGeometry_4) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r8,r6
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x82477cd8
	if (ctx.r8.s32 >= ctx.r6.s32) {
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
	}
loc_82477CD8:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82477d0c
while (ctx.r9.u32 < ctx.r7.u32) {
	loc_82477CF8:
		// rlwinm r4,r9,7,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r4,r11
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r7
		// blt cr6,0x82477cf8
}
loc_82477D0C:
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// std r9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r9.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82477D2C:
	// lfs f8,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,5120(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lfs f6,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,4096(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lfs f4,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,3072(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lfs f2,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,2048(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lfs f12,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,1024(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,0(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// bne cr6,0x82477d2c
	if (!ctx.cr6.eq) goto loc_82477D2C;
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r6,r8,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r7,r11
	ctx.r5.s64 = ctx.r11.s64 - ctx.r7.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r11,r5,r6
	ctx.r11.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r11,r9
	// bge cr6,0x82477db4
	if (ctx.r11.u32 < ctx.r9.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82477DB4:
	// lwz r4,20(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r10,r4,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r4.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82477dd4
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82477DD4:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_5"))) PPC_WEAK_FUNC(phBoundGeometry_5);
PPC_FUNC_IMPL(__imp__phBoundGeometry_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r30);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mullw r11,r7,r11
	ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// cmpw cr6,r8,r9
	// add r6,r11,r6
	ctx.r6.u64 = ctx.r11.u64 + ctx.r6.u64;
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// blt cr6,0x82477e34
	if (ctx.r8.s32 >= ctx.r9.s32) {
		// mr r4,r9
		ctx.r4.u64 = ctx.r9.u64;
	}
loc_82477E34:
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r11,-32256
	// rlwinm r31,r7,1,0,30
	var_r31 = (uint32_t)(__builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE);
	// std r10,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r10.u64);
	// lfs f13,27864(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27864);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82477E60:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r7,4
	// blt cr6,0x82477f28
	if (ctx.r7.s32 >= 4) {
		// addi r9,r7,-4
		ctx.r9.s64 = ctx.r7.s64 + -4;
		// addi r10,r6,2048
		ctx.r10.s64 = ctx.r6.s64 + 2048;
		// rlwinm r9,r9,30,2,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r11,r5,4
		ctx.r11.s64 = ctx.r5.s64 + 4;
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82477E84:
		// lhz r30,-4(r11)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + -4));
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// extsh r30,r30
		var_r30 = (uint32_t)((int16_t)var_r30);
		// cmplwi cr6,r9,0
		ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
		// std r30,-64(r1)
		PPC_STORE_U64(ctx.r1.u32 + -64, var_r30);
		// lfd f8,-64(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
		// fcfid f7,f8
		ctx.f7.f64 = double(ctx.f8.s64);
		// frsp f6,f7
		ctx.f6.f64 = double(float(ctx.f7.f64));
		// fmuls f5,f6,f13
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
		// fmuls f4,f5,f0
		ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
		// stfs f4,-2048(r10)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r10.u32 + -2048, temp.u32);
		// lhz r30,-2(r11)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + -2));
		// extsh r30,r30
		var_r30 = (uint32_t)((int16_t)var_r30);
		// std r30,-56(r1)
		PPC_STORE_U64(ctx.r1.u32 + -56, var_r30);
		// lfd f3,-56(r1)
		ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
		// fcfid f2,f3
		ctx.f2.f64 = double(ctx.f3.s64);
		// frsp f1,f2
		ctx.f1.f64 = double(float(ctx.f2.f64));
		// fmuls f11,f1,f13
		ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
		// fmuls f10,f11,f0
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// stfs f10,-1024(r10)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r10.u32 + -1024, temp.u32);
		// lhz r30,0(r11)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + 0));
		// extsh r30,r30
		var_r30 = (uint32_t)((int16_t)var_r30);
		// std r30,-48(r1)
		PPC_STORE_U64(ctx.r1.u32 + -48, var_r30);
		// lfd f9,-48(r1)
		ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
		// fcfid f8,f9
		ctx.f8.f64 = double(ctx.f9.s64);
		// frsp f7,f8
		ctx.f7.f64 = double(float(ctx.f8.f64));
		// fmuls f6,f7,f13
		ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
		// fmuls f5,f6,f0
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
		// stfs f5,0(r10)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// lhz r30,2(r11)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + 2));
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// extsh r30,r30
		var_r30 = (uint32_t)((int16_t)var_r30);
		// std r30,-40(r1)
		PPC_STORE_U64(ctx.r1.u32 + -40, var_r30);
		// lfd f4,-40(r1)
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// fcfid f3,f4
		ctx.f3.f64 = double(ctx.f4.s64);
		// frsp f2,f3
		ctx.f2.f64 = double(float(ctx.f3.f64));
		// fmuls f1,f2,f13
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
		// fmuls f11,f1,f0
		ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
		// stfs f11,1024(r10)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
		// addi r10,r10,4096
		ctx.r10.s64 = ctx.r10.s64 + 4096;
		// bne cr6,0x82477e84
		if (!ctx.cr6.eq) goto loc_82477E84;
	}
loc_82477F28:
	// cmplw cr6,r8,r7
	// bge cr6,0x82477f7c
	if (ctx.r8.u32 < ctx.r7.u32) {
		// rlwinm r10,r8,10,0,21
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0xFFFFFC00;
		// rlwinm r11,r8,1,0,30
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// add r9,r10,r6
		ctx.r9.u64 = ctx.r10.u64 + ctx.r6.u64;
		// add r10,r11,r5
		ctx.r10.u64 = ctx.r11.u64 + ctx.r5.u64;
		// subf r11,r8,r7
		ctx.r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	loc_82477F44:
		// lhz r8,0(r10)
		ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,2
		ctx.r10.s64 = ctx.r10.s64 + 2;
		// extsh r8,r8
		ctx.r8.s64 = ctx.r8.s16;
		// cmplwi cr6,r11,0
		// std r8,-32(r1)
		PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r8.u64);
		// lfd f10,-32(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f9
		ctx.f8.f64 = double(float(ctx.f9.f64));
		// fmuls f7,f8,f13
		ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
		// fmuls f6,f7,f0
		ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
		// stfs f6,0(r9)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// addi r9,r9,1024
		ctx.r9.s64 = ctx.r9.s64 + 1024;
		// bne cr6,0x82477f44
		if (ctx.r11.u32 != 0) goto loc_82477F44;
	}
loc_82477F7C:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// fadds f0,f12,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// add r5,r31,r5
	ctx.r5.u64 = var_r31 + ctx.r5.u64;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmpwi cr6,r4,0
	// bne cr6,0x82477e60
	if (ctx.r4.s32 != 0) goto loc_82477E60;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r10,r7,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r7.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rotlwi r9,r4,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r10.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82477fc0
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82477FC0:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r7,r8,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82477fe0
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82477FE0:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// ld r30,-16(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_6"))) PPC_WEAK_FUNC(phBoundGeometry_6);
PPC_FUNC_IMPL(__imp__phBoundGeometry_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r6
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// blt cr6,0x82478040
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r9,r6
		ctx.r9.u64 = ctx.r6.u64;
	}
loc_82478040:
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8247806c
while (ctx.r10.u32 < ctx.r8.u32) {
	loc_82478058:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r8
		// blt cr6,0x82478058
}
loc_8247806C:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r10,-32256
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfs f13,27864(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27864);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82478094:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f8,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,0(r7)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// bne cr6,0x82478094
	if (!ctx.cr6.eq) goto loc_82478094;
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r9,r5,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r5.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r8,r4,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r4.s64;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r11,r8,r9
	ctx.r11.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x824780fc
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_824780FC:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r6.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247811c
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247811C:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_7"))) PPC_WEAK_FUNC(phBoundGeometry_7);
PPC_FUNC_IMPL(__imp__phBoundGeometry_7) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r6
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// blt cr6,0x82478170
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r9,r6
		ctx.r9.u64 = ctx.r6.u64;
	}
loc_82478170:
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x8247819c
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_82478188:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x82478188
}
loc_8247819C:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r10,-32256
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfs f13,27864(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27864);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_824781C4:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f8,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,1024(r8)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lhz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// std r10,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r10.u64);
	// lfd f3,-8(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f11,f1,f13
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f10,0(r8)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// bne cr6,0x824781c4
	if (!ctx.cr6.eq) goto loc_824781C4;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r6,r9,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r7,r11
	ctx.r5.s64 = ctx.r11.s64 - ctx.r7.s64;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r11,r5,r6
	ctx.r11.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x82478250
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82478250:
	// lwz r4,20(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r10,r4,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r4.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478270
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478270:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_8"))) PPC_WEAK_FUNC(phBoundGeometry_8);
PPC_FUNC_IMPL(__imp__phBoundGeometry_8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r8,r6
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x824782c8
	if (ctx.r8.s32 >= ctx.r6.s32) {
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
	}
loc_824782C8:
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x824782f4
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_824782E0:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x824782e0
}
loc_824782F4:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r10,-32256
	// std r4,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r4.u64);
	// lfs f13,27864(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27864);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_8247831C:
	// lhz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 6);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lfd f8,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,3072(r9)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r9.u32 + 3072, temp.u32);
	// lhz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 4);
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// std r10,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r10.u64);
	// lfd f3,-24(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f11,f1,f13
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f10,2048(r9)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2048, temp.u32);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f9,-16(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,1024(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// std r7,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r7.u64);
	// lfd f4,-8(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmuls f11,f1,f0
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// bne cr6,0x8247831c
	if (!ctx.cr6.eq) goto loc_8247831C;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r4,r6,1
	ctx.r4.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r11,r5,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r5.s64;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// divwu r11,r11,r4
	ctx.r11.u32 = ctx.r4.u32 ? ctx.r11.u32 / ctx.r4.u32 : 0;
	// cmplw cr6,r11,r10
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// blt cr6,0x824783f0
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
	}
loc_824783F0:
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r9,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478410
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478410:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_9"))) PPC_WEAK_FUNC(phBoundGeometry_9);
PPC_FUNC_IMPL(__imp__phBoundGeometry_9) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mullw r4,r5,r11
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// cmpw cr6,r9,r6
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// blt cr6,0x8247846c
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
	}
loc_8247846C:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x824784a0
while (ctx.r9.u32 < ctx.r7.u32) {
	loc_8247848C:
		// rlwinm r4,r9,7,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r4,r11
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r7
		// blt cr6,0x8247848c
}
loc_824784A0:
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// std r9,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r9.u64);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f13,27864(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27864);  /* glob:lbl_82006CD8 @ 0x82006cd8 */
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_824784C8:
	// lhz r7,10(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 10);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// std r5,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r5.u64);
	// lfd f8,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,5120(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lhz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 8);
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// std r7,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r7.u64);
	// lfd f3,-40(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f11,f1,f13
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f10,4096(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lhz r6,6(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 6);
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r4.u64);
	// lfd f9,-32(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,3072(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 4);
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// std r6,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r6.u64);
	// lfd f4,-24(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmuls f11,f1,f0
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f11,2048(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lhz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// std r9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r9.u64);
	// lfd f10,-16(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f6,1024(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r11,r11,12
	ctx.r11.s64 = ctx.r11.s64 + 12;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r5.u64);
	// lfd f5,-8(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// bne cr6,0x824784c8
	if (!ctx.cr6.eq) goto loc_824784C8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r7,r4,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r4.u32, 1);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r8,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r8.s64;
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// divwu r11,r6,r7
	ctx.r11.u32 = ctx.r7.u32 ? ctx.r6.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r11,r9
	// bge cr6,0x824785e0
	if (ctx.r11.u32 < ctx.r9.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824785E0:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r5.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478600
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478600:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_10"))) PPC_WEAK_FUNC(phBoundGeometry_10);
PPC_FUNC_IMPL(__imp__phBoundGeometry_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r6,r7,r11
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// cmpw cr6,r9,r8
	// add r5,r11,r5
	ctx.r5.u64 = ctx.r11.u64 + ctx.r5.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// blt cr6,0x8247865c
	if (ctx.r9.s32 >= ctx.r8.s32) {
		// mr r4,r8
		ctx.r4.u64 = ctx.r8.u64;
	}
loc_8247865C:
	// extsw r11,r8
	ctx.r11.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r10,-32256
	// std r11,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r11.u64);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lfs f13,-25388(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25388);  /* glob:lbl_82079CD4 @ 0x82079cd4 */
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,27868(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27868);
	ctx.f12.f64 = double(temp.f32);
loc_8247868C:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r7,4
	// blt cr6,0x82478754
	if (ctx.r7.s32 >= 4) {
		// addi r10,r7,-4
		ctx.r10.s64 = ctx.r7.s64 + -4;
		// addi r11,r6,1
		ctx.r11.s64 = ctx.r6.s64 + 1;
		// rlwinm r9,r10,30,2,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r10,r5,2048
		ctx.r10.s64 = ctx.r5.s64 + 2048;
		// addi r8,r9,1
		ctx.r8.s64 = ctx.r9.s64 + 1;
		// rlwinm r9,r8,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	loc_824786B0:
		// lbz r31,-1(r11)
		var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + -1));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// cmplwi cr6,r8,0
		ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
		// std r31,-48(r1)
		PPC_STORE_U64(ctx.r1.u32 + -48, var_r31);
		// lfd f8,-48(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
		// fcfid f7,f8
		ctx.f7.f64 = double(ctx.f8.s64);
		// frsp f6,f7
		ctx.f6.f64 = double(float(ctx.f7.f64));
		// fsubs f5,f6,f13
		ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
		// fmuls f4,f5,f12
		ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
		// fmuls f3,f4,f0
		ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// stfs f3,-2048(r10)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r10.u32 + -2048, temp.u32);
		// lbz r31,0(r11)
		var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 0));
		// std r31,-40(r1)
		PPC_STORE_U64(ctx.r1.u32 + -40, var_r31);
		// lfd f2,-40(r1)
		ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
		// fcfid f1,f2
		ctx.f1.f64 = double(ctx.f2.s64);
		// frsp f10,f1
		ctx.f10.f64 = double(float(ctx.f1.f64));
		// fsubs f9,f10,f13
		ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
		// fmuls f8,f9,f12
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
		// fmuls f7,f8,f0
		ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
		// stfs f7,-1024(r10)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r10.u32 + -1024, temp.u32);
		// lbz r31,1(r11)
		var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 1));
		// std r31,-32(r1)
		PPC_STORE_U64(ctx.r1.u32 + -32, var_r31);
		// lfd f6,-32(r1)
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fsubs f3,f4,f13
		ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
		// fmuls f2,f3,f12
		ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
		// fmuls f1,f2,f0
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
		// stfs f1,0(r10)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// lbz r31,2(r11)
		var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 2));
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// std r31,-24(r1)
		PPC_STORE_U64(ctx.r1.u32 + -24, var_r31);
		// lfd f10,-24(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f9
		ctx.f8.f64 = double(float(ctx.f9.f64));
		// fsubs f7,f8,f13
		ctx.f7.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
		// fmuls f6,f7,f12
		ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
		// fmuls f5,f6,f0
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
		// stfs f5,1024(r10)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
		// addi r10,r10,4096
		ctx.r10.s64 = ctx.r10.s64 + 4096;
		// bne cr6,0x824786b0
		if (!ctx.cr6.eq) goto loc_824786B0;
	}
loc_82478754:
	// cmplw cr6,r9,r7
	// bge cr6,0x82478798
	if (ctx.r9.u32 < ctx.r7.u32) {
		// rlwinm r11,r9,10,0,21
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 10) & 0xFFFFFC00;
		// add r11,r11,r5
		ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	loc_82478764:
		// lbzx r10,r9,r6
		ctx.r10.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r6.u32);
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r7
		ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
		// std r10,-16(r1)
		PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
		// lfd f4,-16(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
		// fcfid f3,f4
		ctx.f3.f64 = double(ctx.f4.s64);
		// frsp f2,f3
		ctx.f2.f64 = double(float(ctx.f3.f64));
		// fsubs f1,f2,f13
		ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
		// fmuls f10,f1,f12
		ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// fmuls f9,f10,f0
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
		// stfs f9,0(r11)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,1024
		ctx.r11.s64 = ctx.r11.s64 + 1024;
		// blt cr6,0x82478764
		if (ctx.cr6.lt) goto loc_82478764;
	}
loc_82478798:
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// fadds f0,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// cmpwi cr6,r4,0
	// bne cr6,0x8247868c
	if (ctx.r4.s32 != 0) goto loc_8247868C;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r9,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r9.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r10,r7,r8
	ctx.r10.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x824787d8
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_824787D8:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x824787f8
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824787F8:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_11"))) PPC_WEAK_FUNC(phBoundGeometry_11);
PPC_FUNC_IMPL(__imp__phBoundGeometry_11) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// mullw r11,r5,r11
	ctx.r11.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r6
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// blt cr6,0x8247884c
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r9,r6
		ctx.r9.u64 = ctx.r6.u64;
	}
loc_8247884C:
	// addi r4,r9,127
	ctx.r4.s64 = ctx.r9.s64 + 127;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r8,r4,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x82478874
while (ctx.r10.u32 < ctx.r8.u32) {
	loc_82478860:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r8
		// blt cr6,0x82478860
}
loc_82478874:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r8,-32256
	// lis r10,-32248
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfs f13,-25388(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25388);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,27868(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27868);
	ctx.f12.f64 = double(temp.f32);
loc_824788A4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f8,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,0(r7)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// bne cr6,0x824788a4
	if (!ctx.cr6.eq) goto loc_824788A4;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r4,r6,r11
	ctx.r4.s64 = ctx.r11.s64 - ctx.r6.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r5,0
	if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
	// divwu r10,r4,r5
	ctx.r10.u32 = ctx.r5.u32 ? ctx.r4.u32 / ctx.r5.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82478908
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82478908:
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r8,r10,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r10.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478928
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478928:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_12"))) PPC_WEAK_FUNC(phBoundGeometry_12);
PPC_FUNC_IMPL(__imp__phBoundGeometry_12) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r11,r5,r11
	ctx.r11.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r6
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// blt cr6,0x8247897c
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r9,r6
		ctx.r9.u64 = ctx.r6.u64;
	}
loc_8247897C:
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r4,r7,127
	ctx.r4.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r4,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x824789a8
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_82478994:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x82478994
}
loc_824789A8:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r7,-32256
	// lis r10,-32248
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfs f13,-25388(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25388);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,27868(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27868);
	ctx.f12.f64 = double(temp.f32);
loc_824789D8:
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// std r7,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r7.u64);
	// lfd f8,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,1024(r8)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// std r5,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r5.u64);
	// lfd f2,-8(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f10,f1
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// fsubs f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fmuls f8,f9,f12
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,0(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// bne cr6,0x824789d8
	if (!ctx.cr6.eq) goto loc_824789D8;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r4,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r4.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r10,r9,r10
	ctx.r10.u32 = ctx.r10.u32 ? ctx.r9.u32 / ctx.r10.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82478a60
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82478A60:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r6,r7,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r6,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478a80
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478A80:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_13"))) PPC_WEAK_FUNC(phBoundGeometry_13);
PPC_FUNC_IMPL(__imp__phBoundGeometry_13) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r11.s64;
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r11,r5,r11
	ctx.r11.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r6
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// blt cr6,0x82478ad4
	if (ctx.r8.s32 >= ctx.r6.s32) {
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
	}
loc_82478AD4:
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r4,r7,127
	ctx.r4.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r4,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82478b00
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_82478AEC:
		// rlwinm r5,r10,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x82478aec
}
loc_82478B00:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r7,-32256
	// lis r10,-32248
	// std r4,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r4.u64);
	// lfs f13,-25388(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25388);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,27868(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27868);
	ctx.f12.f64 = double(temp.f32);
loc_82478B30:
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// std r7,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r7.u64);
	// lfd f8,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,3072(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 3072, temp.u32);
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// std r5,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r5.u64);
	// lfd f2,-24(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f10,f1
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// fsubs f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fmuls f8,f9,f12
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,2048(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2048, temp.u32);
	// lbz r10,1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f6,-16(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
	// fmuls f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,1024(r9)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// std r6,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r6.u64);
	// lfd f10,-8(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,0(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// bne cr6,0x82478b30
	if (!ctx.cr6.eq) goto loc_82478B30;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r10,r5,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r5.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// divwu r10,r10,r4
	ctx.r10.u32 = ctx.r4.u32 ? ctx.r10.u32 / ctx.r4.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// blt cr6,0x82478c00
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
	}
loc_82478C00:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r6,r7,r9
	ctx.r6.s64 = ctx.r9.s64 - ctx.r7.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r6,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478c20
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478C20:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundGeometry_14"))) PPC_WEAK_FUNC(phBoundGeometry_14);
PPC_FUNC_IMPL(__imp__phBoundGeometry_14) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r10,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r10.s64;
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r11,r5,r11
	ctx.r11.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r6
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// blt cr6,0x82478c78
	if (ctx.r9.s32 >= ctx.r6.s32) {
		// mr r8,r6
		ctx.r8.u64 = ctx.r6.u64;
	}
loc_82478C78:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r4,r8,r7
	ctx.r4.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r4,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	// beq cr6,0x82478cac
while (ctx.r9.u32 < ctx.r7.u32) {
	loc_82478C98:
		// rlwinm r5,r9,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r7
		// blt cr6,0x82478c98
}
loc_82478CAC:
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r7,-32256
	// lis r9,-32248
	// std r4,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r4.u64);
	// lfs f13,-25388(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25388);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,27868(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27868);
	ctx.f12.f64 = double(temp.f32);
loc_82478CDC:
	// lbz r7,5(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// std r7,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r7.u64);
	// lfd f8,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,5120(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// std r5,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r5.u64);
	// lfd f2,-40(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f10,f1
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// fsubs f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fmuls f8,f9,f12
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,4096(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// std r9,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r9.u64);
	// lfd f6,-32(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
	// fmuls f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f1,3072(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lbz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// std r6,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r6.u64);
	// lfd f10,-24(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f5,2048(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lbz r4,1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f4,-16(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,1024(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// std r7,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r7.u64);
	// lfd f8,-8(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f3,0(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// bne cr6,0x82478cdc
	if (!ctx.cr6.eq) goto loc_82478CDC;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r4,r6,r11
	ctx.r4.s64 = ctx.r11.s64 - ctx.r6.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r5,0
	if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
	// divwu r9,r4,r5
	ctx.r9.u32 = ctx.r5.u32 ? ctx.r4.u32 / ctx.r5.u32 : 0;
	// cmplw cr6,r9,r11
	// blt cr6,0x82478df0
	if (ctx.r9.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82478DF0:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82478e10
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82478E10:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_0"))) PPC_WEAK_FUNC(phBoundComposite_0);
PPC_FUNC_IMPL(__imp__phBoundComposite_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x82478E28;
	__savegprlr_26(ctx, base);
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r29,r3,52
	var_r29 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r10,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r7,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r5.s32 >> 1;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r5,r4,r10
	ctx.r5.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// addze r10,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r7,r5,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r8
	var_r30 = (uint32_t)(ctx.r11.u64 + ctx.r8.u64);
	// cmpw cr6,r9,r10
	// add r28,r7,r6
	var_r28 = (uint32_t)(ctx.r7.u64 + ctx.r6.u64);
	// mr r26,r9
	var_r26 = ctx.r9.u32;
	// blt cr6,0x82478e80
	if (ctx.r9.s32 >= ctx.r10.s32) {
		// mr r26,r10
		var_r26 = ctx.r10.u32;
	}
loc_82478E80:
	// extsw r11,r10
	ctx.r11.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r4,4
	// std r11,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r11.u64);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,16056(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16056);  /* glob:lbl_82003EB8 @ 0x82003eb8 */
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f9,f12,f9
	ctx.f9.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// blt cr6,0x82478edc
	if (ctx.r4.s32 >= 4) {
		// addi r10,r4,-4
		ctx.r10.s64 = ctx.r4.s64 + -4;
		// addi r9,r29,8
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 8;
		// rlwinm r11,r10,30,2,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82478ECC:
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// cmplwi cr6,r11,0
		// bne cr6,0x82478ecc
		if (ctx.r11.u32 != 0) goto loc_82478ECC;
	}
loc_82478EDC:
	// cmplw cr6,r10,r4
	// bge cr6,0x82478f00
	if (ctx.r10.u32 < ctx.r4.u32) {
		// rlwinm r9,r10,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r11,r10,r4
		ctx.r11.s64 = ctx.r4.s64 - ctx.r10.s64;
		// add r10,r9,r29
		ctx.r10.u64 = ctx.r9.u64 + var_r29;
	loc_82478EF0:
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmplwi cr6,r11,0
		// bne cr6,0x82478ef0
		if (ctx.r11.u32 != 0) goto loc_82478EF0;
	}
loc_82478F00:
	// lis r11,-32256
	// rlwinm r27,r4,2,0,29
	var_r27 = (uint32_t)(__builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC);
	// subf r31,r29,r28
	var_r31 = var_r28 - var_r29;
	// lfs f12,27200(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);
	ctx.f12.f64 = double(temp.f32);
loc_82478F10:
	// li r5,0
	ctx.r5.s64 = 0;
	// cmpwi cr6,r4,4
	// blt cr6,0x82478fec
	if (ctx.r4.s32 >= 4) {
		// addi r8,r4,-4
		ctx.r8.s64 = ctx.r4.s64 + -4;
		// fadds f13,f9,f0
		ctx.fpscr.disableFlushMode();
		ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
		// addi r9,r28,12
		ctx.r9.s64 = (int64_t)(int32_t)var_r28 + 12;
		// rlwinm r10,r8,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r11,r30,1024
		ctx.r11.s64 = (int64_t)(int32_t)var_r30 + 1024;
		// addi r8,r10,1
		ctx.r8.s64 = ctx.r10.s64 + 1;
		// addi r10,r29,4
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 4;
		// rlwinm r5,r8,2,0,29
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82478F3C:
		// lfs f11,-12(r9)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -12);
		ctx.f11.f64 = double(temp.f32);
		// addi r7,r10,4
		ctx.r7.s64 = ctx.r10.s64 + 4;
		// lfs f10,-4(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f7,f13,f11
		ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
		// fadds f6,f11,f10
		ctx.f6.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
		// stfs f11,-4(r10)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
		// addi r6,r10,8
		ctx.r6.s64 = ctx.r10.s64 + 8;
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// cmplwi cr6,r8,0
		ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
		// fmuls f5,f6,f0
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
		// fmuls f4,f5,f12
		ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
		// stfs f4,-1024(r11)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r11.u32 + -1024, temp.u32);
		// stfs f7,-1020(r11)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r11.u32 + -1020, temp.u32);
		// lfsx f11,r10,r31
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f2,f13,f11
		ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
		// fadds f3,f11,f10
		ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
		// stfs f11,0(r10)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// addi r10,r10,16
		ctx.r10.s64 = ctx.r10.s64 + 16;
		// fmuls f1,f3,f0
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// fmuls f11,f1,f12
		ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f11,0(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// stfs f2,4(r11)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// lfs f11,-4(r9)
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -4);
		ctx.f11.f64 = double(temp.f32);
		// lfs f10,0(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f7,f13,f11
		ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
		// fadds f10,f11,f10
		ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
		// stfs f11,0(r7)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// fmuls f6,f10,f0
		ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
		// fmuls f5,f6,f12
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
		// stfs f5,1024(r11)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r11.u32 + 1024, temp.u32);
		// stfs f7,1028(r11)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r11.u32 + 1028, temp.u32);
		// lfs f11,0(r9)
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// lfs f10,0(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		ctx.f10.f64 = double(temp.f32);
		// fmuls f3,f13,f11
		ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
		// fadds f4,f11,f10
		ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
		// stfs f11,0(r6)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// fmuls f2,f4,f0
		ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// fmuls f1,f2,f12
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
		// stfs f1,2048(r11)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r11.u32 + 2048, temp.u32);
		// stfs f3,2052(r11)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r11.u32 + 2052, temp.u32);
		// addi r11,r11,4096
		ctx.r11.s64 = ctx.r11.s64 + 4096;
		// bne cr6,0x82478f3c
		if (!ctx.cr6.eq) goto loc_82478F3C;
	}
loc_82478FEC:
	// cmplw cr6,r5,r4
	// bge cr6,0x82479044
	if (ctx.r5.u32 < ctx.r4.u32) {
		// rlwinm r10,r5,10,0,21
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 10) & 0xFFFFFC00;
		// fadds f10,f9,f0
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
		// rlwinm r11,r5,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// add r10,r10,r30
		ctx.r10.u64 = ctx.r10.u64 + var_r30;
		// add r11,r11,r29
		ctx.r11.u64 = ctx.r11.u64 + var_r29;
		// subf r9,r5,r4
		ctx.r9.s64 = ctx.r4.s64 - ctx.r5.s64;
	loc_8247900C:
		// lfsx f13,r31,r11
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + ctx.r11.u32);
		ctx.f13.f64 = double(temp.f32);
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// lfs f11,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f7,f10,f13
		ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// fadds f11,f13,f11
		ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
		// stfs f13,0(r11)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// cmplwi cr6,r9,0
		// fmuls f6,f11,f0
		ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// fmuls f5,f6,f12
		ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
		// stfs f5,0(r10)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// stfs f7,4(r10)
		temp.f32 = float(ctx.f7.f64);
		PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
		// addi r10,r10,1024
		ctx.r10.s64 = ctx.r10.s64 + 1024;
		// bne cr6,0x8247900c
		if (ctx.r9.u32 != 0) goto loc_8247900C;
	}
loc_82479044:
	// addi r26,r26,-1
	var_r26 = (uint32_t)(var_r26 + -1);
	// fadds f0,f8,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// add r28,r27,r28
	var_r28 = (uint32_t)(var_r27 + var_r28);
	// add r31,r27,r31
	var_r31 = (uint32_t)(var_r27 + var_r31);
	// addi r30,r30,8
	var_r30 = (uint32_t)(var_r30 + 8);
	// cmpwi cr6,r26,0
	// bne cr6,0x82478f10
	if ((int32_t)var_r26 != 0) goto loc_82478F10;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r10,r6,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 2);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r7,r28
	ctx.r5.s64 = (int64_t)(int32_t)var_r28 - ctx.r7.s64;
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r10,r5,r10
	ctx.r10.u32 = ctx.r10.u32 ? ctx.r5.u32 / ctx.r10.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x8247908c
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8247908C:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r7,r8,r30
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x824790ac
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824790AC:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r4,4
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blt cr6,0x824790e4
	if (ctx.r4.s32 >= 4) {
		// addi r6,r4,-4
		ctx.r6.s64 = ctx.r4.s64 + -4;
		// addi r9,r29,8
		ctx.r9.s64 = (int64_t)(int32_t)var_r29 + 8;
		// rlwinm r11,r6,30,2,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// rlwinm r10,r11,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	loc_824790D4:
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r9,r9,16
		ctx.r9.s64 = ctx.r9.s64 + 16;
		// cmplwi cr6,r11,0
		// bne cr6,0x824790d4
		if (ctx.r11.u32 != 0) goto loc_824790D4;
	}
loc_824790E4:
	// cmplw cr6,r10,r4
	// bge cr6,0x82479108
	if (ctx.r10.u32 < ctx.r4.u32) {
		// rlwinm r9,r10,2,0,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r11,r10,r4
		ctx.r11.s64 = ctx.r4.s64 - ctx.r10.s64;
		// add r10,r9,r29
		ctx.r10.u64 = ctx.r9.u64 + var_r29;
	loc_824790F8:
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmplwi cr6,r11,0
		// bne cr6,0x824790f8
		if (ctx.r11.u32 != 0) goto loc_824790F8;
	}
loc_82479108:
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundComposite_1"))) PPC_WEAK_FUNC(phBoundComposite_1);
PPC_FUNC_IMPL(__imp__phBoundComposite_1) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r4,r11,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r10,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r8,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 1;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r6,r6,r10
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r10
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 + ctx.r5.u64;
	// blt cr6,0x82479160
	if (ctx.r9.s32 >= ctx.r10.s32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82479160:
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r6,-32256
	// addi r4,r7,127
	ctx.r4.s64 = ctx.r7.s64 + 127;
	// li r10,0
	ctx.r10.s64 = 0;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// rlwinm r7,r4,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// lfs f13,16056(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// beq cr6,0x824791b8
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_824791A4:
		// rlwinm r6,r10,7,0,24
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r6,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x824791a4
}
loc_824791B8:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f9,27200(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f9.f64 = double(temp.f32);
loc_824791C0:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fadds f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// lfs f12,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f12.f64 = double(temp.f32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// fadds f7,f13,f12
	ctx.f7.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// stfs f13,52(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmpwi cr6,r9,0
	// fmuls f6,f8,f13
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fmuls f4,f5,f9
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// stfs f4,0(r8)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f6,4(r8)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x824791c0
	if (ctx.r9.s32 != 0) goto loc_824791C0;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r10,r5,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r5.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rotlwi r9,r4,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r10.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x8247922c
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8247922C:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r6,r7,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r7.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r6,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247924c
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247924C:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_2"))) PPC_WEAK_FUNC(phBoundComposite_2);
PPC_FUNC_IMPL(__imp__phBoundComposite_2) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r7,r3,52
	ctx.r7.s64 = ctx.r3.s64 + 52;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mullw r5,r5,r10
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r9,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// cmpw cr6,r8,r9
	// add r11,r5,r4
	ctx.r11.u64 = ctx.r5.u64 + ctx.r4.u64;
	// blt cr6,0x824792ac
	if (ctx.r8.s32 >= ctx.r9.s32) {
		// mr r8,r9
		ctx.r8.u64 = ctx.r9.u64;
	}
loc_824792AC:
	// extsw r4,r9
	ctx.r4.s64 = ctx.r9.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r6,r8,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lis r5,-32256
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// li r9,0
	ctx.r9.s64 = 0;
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f9,f12,f9
	ctx.f9.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// beq cr6,0x82479304
while (ctx.r9.u32 < ctx.r6.u32) {
	loc_824792F0:
		// rlwinm r5,r9,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r6
		// blt cr6,0x824792f0
}
loc_82479304:
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,27200(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f11.f64 = double(temp.f32);
loc_8247930C:
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f9,f0
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// lfs f10,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// fadds f7,f13,f10
	ctx.f7.f64 = double(float(ctx.f13.f64 + ctx.f10.f64));
	// stfs f13,4(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// fmuls f6,f12,f13
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,1024(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f6,1028(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// lfs f10,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f2,f12,f13
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fadds f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f10.f64));
	// stfs f13,0(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// fmuls f1,f3,f0
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fadds f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// fmuls f13,f1,f11
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f2,4(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8247930c
	if (!ctx.cr6.eq) goto loc_8247930C;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r7,r9,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// subf r8,r4,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r4.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// divwu r9,r8,r7
	ctx.r9.u32 = ctx.r7.u32 ? ctx.r8.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r9,r11
	// blt cr6,0x82479398
	if (ctx.r9.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82479398:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x824793b8
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_824793B8:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_3"))) PPC_WEAK_FUNC(phBoundComposite_3);
PPC_FUNC_IMPL(__imp__phBoundComposite_3) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r9,r3,52
	ctx.r9.s64 = ctx.r3.s64 + 52;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r11.s64;
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r4,r4,r10
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// cmpw cr6,r7,r8
	// blt cr6,0x8247941c
	if (ctx.r7.s32 >= ctx.r8.s32) {
		// mr r7,r8
		ctx.r7.u64 = ctx.r8.u64;
	}
loc_8247941C:
	// extsw r5,r8
	ctx.r5.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r6,r7,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r4,r6,127
	ctx.r4.s64 = ctx.r6.s64 + 127;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lis r5,-32256
	// rlwinm r6,r4,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f9,f12,f9
	ctx.f9.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// beq cr6,0x82479474
while (ctx.r8.u32 < ctx.r6.u32) {
	loc_82479460:
		// rlwinm r5,r8,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r10
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// cmplw cr6,r8,r6
		// blt cr6,0x82479460
}
loc_82479474:
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f12,27200(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f12.f64 = double(temp.f32);
loc_8247947C:
	// lfs f11,12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f9,f0
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// lfs f10,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// fadds f7,f11,f10
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,12(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// fmuls f6,f13,f11
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f4,3072(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 3072, temp.u32);
	// stfs f6,3076(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + 3076, temp.u32);
	// lfs f11,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f2,f13,f11
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fadds f3,f11,f10
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,8(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// fmuls f1,f3,f0
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f11,f1,f12
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f11,2048(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2048, temp.u32);
	// stfs f2,2052(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2052, temp.u32);
	// lfs f11,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f13,f11
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fadds f10,f11,f10
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// fmuls f6,f10,f0
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f5,f6,f12
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// stfs f5,1024(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 1024, temp.u32);
	// stfs f7,1028(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 1028, temp.u32);
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lfs f10,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f13,f11
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fadds f4,f11,f10
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// fmuls f1,f2,f12
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f3,4(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x8247947c
	if (!ctx.cr6.eq) goto loc_8247947C;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r7,r9,2
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// subf r8,r4,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r4.s64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// divwu r9,r8,r7
	ctx.r9.u32 = ctx.r7.u32 ? ctx.r8.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r9,r10
	// blt cr6,0x82479550
	if (ctx.r9.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82479550:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r11
	ctx.r5.s64 = ctx.r11.s64 - ctx.r6.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r11,r5,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r10
	// blt cr6,0x82479570
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82479570:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_4"))) PPC_WEAK_FUNC(phBoundComposite_4);
PPC_FUNC_IMPL(__imp__phBoundComposite_4) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r3,52
	ctx.r10.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r31,r11,r8
	var_r31 = (uint32_t)(ctx.r8.s64 - ctx.r11.s64);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r4,r4,r9
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r7,r31,1
	ctx.xer.ca = ((int32_t)var_r31 < 0) & ((var_r31 & 0x1) != 0);
	ctx.r7.s64 = (int32_t)var_r31 >> 1;
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// cmpw cr6,r8,r7
	// blt cr6,0x824795d8
	if (ctx.r8.s32 >= ctx.r7.s32) {
		// mr r8,r7
		ctx.r8.u64 = ctx.r7.u64;
	}
loc_824795D8:
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// li r7,0
	ctx.r7.s64 = 0;
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rlwinm r6,r5,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lis r5,-32256
	// addi r4,r6,127
	ctx.r4.s64 = ctx.r6.s64 + 127;
	// rlwinm r6,r4,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f9,f12,f9
	ctx.f9.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// beq cr6,0x82479638
while (ctx.r7.u32 < ctx.r6.u32) {
	loc_82479624:
		// rlwinm r5,r7,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r9
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// cmplw cr6,r7,r6
		// blt cr6,0x82479624
}
loc_82479638:
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f12,27200(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f12.f64 = double(temp.f32);
loc_82479640:
	// lfs f11,20(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	ctx.f11.f64 = double(temp.f32);
	// fadds f13,f9,f0
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// lfs f10,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f10.f64 = double(temp.f32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// fadds f7,f11,f10
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,20(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 20, temp.u32);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// fmuls f6,f13,f11
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f4,5120(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 5120, temp.u32);
	// stfs f6,5124(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + 5124, temp.u32);
	// lfs f11,16(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f2,f13,f11
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fadds f3,f11,f10
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,16(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// fmuls f1,f3,f0
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f11,f1,f12
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f11,4096(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4096, temp.u32);
	// stfs f2,4100(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4100, temp.u32);
	// lfs f11,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f13,f11
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fadds f10,f11,f10
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,12(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 12, temp.u32);
	// fmuls f6,f10,f0
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f5,f6,f12
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// stfs f5,3072(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 3072, temp.u32);
	// stfs f7,3076(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 3076, temp.u32);
	// lfs f11,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f11,f13
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fadds f4,f11,f10
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,8(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f1,f2,f12
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f1,2048(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2048, temp.u32);
	// stfs f3,2052(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 2052, temp.u32);
	// lfs f11,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f11,f13
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fadds f10,f11,f10
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fmuls f6,f10,f0
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f5,f6,f12
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// stfs f5,1024(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 1024, temp.u32);
	// stfs f7,1028(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r11.u32 + 1028, temp.u32);
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f11,f13
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fadds f4,f11,f10
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,0(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// fmuls f1,f2,f12
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f1,0(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// stfs f3,4(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x82479640
	if (!ctx.cr6.eq) goto loc_82479640;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r4,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r4.s64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rotlwi r6,r8,2
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 2);
	// divwu r9,r7,r6
	ctx.r9.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// cmplw cr6,r9,r10
	// blt cr6,0x8247975c
	if (ctx.r9.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_8247975C:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r11
	ctx.r4.s64 = ctx.r11.s64 - ctx.r5.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r11,r4,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r10
	// blt cr6,0x8247977c
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247977C:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_5"))) PPC_WEAK_FUNC(phBoundComposite_5);
PPC_FUNC_IMPL(__imp__phBoundComposite_5) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x82479798;
	__savegprlr_26(ctx, base);
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r28,r3,52
	var_r28 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r31,13(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r10,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r4,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r5.s32 >> 1;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r7,r31,r10
	ctx.r7.s64 = int64_t((int32_t)var_r31) * int64_t(ctx.r10.s32);
	// addze r10,r4
	temp.s64 = ctx.r4.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r11,r8
	ctx.r4.u64 = ctx.r11.u64 + ctx.r8.u64;
	// cmpw cr6,r9,r10
	// add r5,r7,r6
	ctx.r5.u64 = ctx.r7.u64 + ctx.r6.u64;
	// mr r29,r9
	var_r29 = ctx.r9.u32;
	// blt cr6,0x824797f0
	if (ctx.r9.s32 >= ctx.r10.s32) {
		// mr r29,r10
		var_r29 = ctx.r10.u32;
	}
loc_824797F0:
	// extsw r11,r10
	ctx.r11.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// li r6,0
	ctx.r6.s64 = 0;
	// cmpwi cr6,r31,4
	ctx.cr6.compare<int32_t>((int32_t)var_r31, 4, ctx.xer);
	// std r11,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.r11.u64);
	// lfd f11,-96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,16056(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16056);  /* glob:lbl_82003EB8 @ 0x82003eb8 */
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f8,f12,f9
	ctx.f8.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f13,27864(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27864);  /* glob:lbl_82006CD8 @ 0x82006cd8 */
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x824798e0
	if (!(ctx.cr6.lt)) {
		// addi r10,r31,-4
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + -4;
		// addi r11,r28,8
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 8;
		// rlwinm r10,r10,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// rlwinm r6,r10,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82479844:
		// lhz r30,0(r11)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + 0));
		// addi r9,r11,-8
		ctx.r9.s64 = ctx.r11.s64 + -8;
		// addi r8,r11,-4
		ctx.r8.s64 = ctx.r11.s64 + -4;
		// extsh r30,r30
		var_r30 = (uint32_t)((int16_t)var_r30);
		// addi r7,r11,4
		ctx.r7.s64 = ctx.r11.s64 + 4;
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// lhz r27,0(r8)
		var_r27 = (uint32_t)(PPC_LOAD_U16(ctx.r8.u32 + 0));
		// cmplwi cr6,r10,0
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
		// std r30,-96(r1)
		PPC_STORE_U64(ctx.r1.u32 + -96, var_r30);
		// lfd f6,-96(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
		// lhz r30,0(r9)
		var_r30 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 0));
		// extsh r27,r27
		var_r27 = (uint32_t)((int16_t)var_r27);
		// lhz r26,0(r7)
		var_r26 = (uint32_t)(PPC_LOAD_U16(ctx.r7.u32 + 0));
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// extsh r30,r30
		var_r30 = (uint32_t)((int16_t)var_r30);
		// extsh r26,r26
		var_r26 = (uint32_t)((int16_t)var_r26);
		// std r27,-80(r1)
		PPC_STORE_U64(ctx.r1.u32 + -80, var_r27);
		// std r30,-88(r1)
		PPC_STORE_U64(ctx.r1.u32 + -88, var_r30);
		// std r26,-72(r1)
		PPC_STORE_U64(ctx.r1.u32 + -72, var_r26);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fmuls f3,f4,f13
		ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f3,0(r11)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// lfd f1,-80(r1)
		ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
		// fcfid f10,f1
		ctx.f10.f64 = double(ctx.f1.s64);
		// lfd f2,-88(r1)
		ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
		// lfd f11,-72(r1)
		ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
		// fcfid f12,f2
		ctx.f12.f64 = double(ctx.f2.s64);
		// fcfid f9,f11
		ctx.f9.f64 = double(ctx.f11.s64);
		// frsp f5,f10
		ctx.f5.f64 = double(float(ctx.f10.f64));
		// frsp f6,f12
		ctx.f6.f64 = double(float(ctx.f12.f64));
		// frsp f4,f9
		ctx.f4.f64 = double(float(ctx.f9.f64));
		// fmuls f2,f5,f13
		ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
		// stfs f2,0(r8)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// fmuls f3,f6,f13
		ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
		// stfs f3,0(r9)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// fmuls f1,f4,f13
		ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f1,0(r7)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// bne cr6,0x82479844
		if (!ctx.cr6.eq) goto loc_82479844;
	}
loc_824798E0:
	// cmplw cr6,r6,r31
	// bge cr6,0x82479924
	if (ctx.r6.u32 < var_r31) {
		// rlwinm r11,r6,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r10,r6,r31
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 - ctx.r6.s64;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
	loc_824798F4:
		// lhz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// extsh r7,r9
		ctx.r7.s64 = ctx.r9.s16;
		// cmplwi cr6,r10,0
		// std r7,-72(r1)
		PPC_STORE_U64(ctx.r1.u32 + -72, ctx.r7.u64);
		// lfd f12,-72(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// frsp f10,f11
		ctx.f10.f64 = double(float(ctx.f11.f64));
		// fmuls f9,f10,f13
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
		// stfs f9,0(r11)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// bne cr6,0x824798f4
		if (ctx.r10.u32 != 0) goto loc_824798F4;
	}
loc_82479924:
	// lis r11,-32256
	// rlwinm r30,r31,1,0,30
	var_r30 = (uint32_t)(__builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0xFFFFFFFE);
	// lfs f12,27200(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);
	ctx.f12.f64 = double(temp.f32);
loc_82479930:
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r31,4
	// blt cr6,0x82479a68
	if ((int32_t)var_r31 >= 4) {
		// addi r6,r31,-4
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + -4;
		// fadds f11,f8,f0
		ctx.fpscr.disableFlushMode();
		ctx.f11.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
		// addi r10,r28,8
		ctx.r10.s64 = (int64_t)(int32_t)var_r28 + 8;
		// rlwinm r11,r6,30,2,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r9,r5,4
		ctx.r9.s64 = ctx.r5.s64 + 4;
		// addi r7,r11,1
		ctx.r7.s64 = ctx.r11.s64 + 1;
		// addi r11,r4,1024
		ctx.r11.s64 = ctx.r4.s64 + 1024;
		// rlwinm r8,r7,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	loc_8247995C:
		// lhz r27,-4(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + -4));
		// lfs f9,-8(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
		ctx.f9.f64 = double(temp.f32);
		// addi r6,r10,4
		ctx.r6.s64 = ctx.r10.s64 + 4;
		// extsh r27,r27
		var_r27 = (uint32_t)((int16_t)var_r27);
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// cmplwi cr6,r7,0
		ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
		// std r27,-72(r1)
		PPC_STORE_U64(ctx.r1.u32 + -72, var_r27);
		// lfd f6,-72(r1)
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fmuls f10,f4,f13
		ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f10,-8(r10)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
		// fadds f3,f10,f9
		ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// fmuls f2,f10,f11
		ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
		// fmuls f1,f3,f0
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// fmuls f10,f1,f12
		ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f10,-1024(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + -1024, temp.u32);
		// stfs f2,-1020(r11)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r11.u32 + -1020, temp.u32);
		// lhz r27,-2(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + -2));
		// lfs f9,-4(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
		ctx.f9.f64 = double(temp.f32);
		// extsh r27,r27
		var_r27 = (uint32_t)((int16_t)var_r27);
		// std r27,-80(r1)
		PPC_STORE_U64(ctx.r1.u32 + -80, var_r27);
		// lfd f6,-80(r1)
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fmuls f10,f4,f13
		ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f10,-4(r10)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
		// fadds f3,f10,f9
		ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// fmuls f2,f10,f11
		ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
		// fmuls f1,f3,f0
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// fmuls f10,f1,f12
		ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f10,0(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// stfs f2,4(r11)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// lhz r27,0(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 0));
		// lfs f9,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// extsh r27,r27
		var_r27 = (uint32_t)((int16_t)var_r27);
		// std r27,-88(r1)
		PPC_STORE_U64(ctx.r1.u32 + -88, var_r27);
		// lfd f6,-88(r1)
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fmuls f10,f4,f13
		ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f10,0(r10)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// addi r10,r10,16
		ctx.r10.s64 = ctx.r10.s64 + 16;
		// fadds f3,f10,f9
		ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// fmuls f2,f10,f11
		ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
		// fmuls f1,f3,f0
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// fmuls f10,f1,f12
		ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f10,1024(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + 1024, temp.u32);
		// stfs f2,1028(r11)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r11.u32 + 1028, temp.u32);
		// lhz r27,2(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U16(ctx.r9.u32 + 2));
		// lfs f9,0(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// addi r9,r9,8
		ctx.r9.s64 = ctx.r9.s64 + 8;
		// extsh r27,r27
		var_r27 = (uint32_t)((int16_t)var_r27);
		// std r27,-96(r1)
		PPC_STORE_U64(ctx.r1.u32 + -96, var_r27);
		// lfd f6,-96(r1)
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fmuls f10,f4,f13
		ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f10,0(r6)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// fadds f3,f10,f9
		ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// fmuls f2,f10,f11
		ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
		// fmuls f1,f3,f0
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// fmuls f10,f1,f12
		ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f10,2048(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + 2048, temp.u32);
		// stfs f2,2052(r11)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r11.u32 + 2052, temp.u32);
		// addi r11,r11,4096
		ctx.r11.s64 = ctx.r11.s64 + 4096;
		// bne cr6,0x8247995c
		if (!ctx.cr6.eq) goto loc_8247995C;
	}
loc_82479A68:
	// cmplw cr6,r8,r31
	// bge cr6,0x82479ae4
	if (ctx.r8.u32 < var_r31) {
		// rlwinm r9,r8,1,0,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
		// fadds f9,f8,f0
		ctx.fpscr.disableFlushMode();
		ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
		// rlwinm r10,r8,10,0,21
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0xFFFFFC00;
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// add r7,r9,r5
		ctx.r7.u64 = ctx.r9.u64 + ctx.r5.u64;
		// add r10,r10,r4
		ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// subf r9,r8,r31
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 - ctx.r8.s64;
	loc_82479A90:
		// lhz r8,0(r7)
		ctx.r8.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
		// lfs f10,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f10.f64 = double(temp.f32);
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// extsh r8,r8
		ctx.r8.s64 = ctx.r8.s16;
		// addi r7,r7,2
		ctx.r7.s64 = ctx.r7.s64 + 2;
		// cmplwi cr6,r9,0
		ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
		// std r8,-64(r1)
		PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r8.u64);
		// lfd f6,-64(r1)
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
		// fcfid f5,f6
		ctx.f5.f64 = double(ctx.f6.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fmuls f11,f4,f13
		ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
		// stfs f11,0(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// fadds f3,f11,f10
		ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
		// fmuls f2,f11,f9
		ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
		// fmuls f1,f3,f0
		ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// fmuls f11,f1,f12
		ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f11,0(r10)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// stfs f2,4(r10)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
		// addi r10,r10,1024
		ctx.r10.s64 = ctx.r10.s64 + 1024;
		// bne cr6,0x82479a90
		if (!ctx.cr6.eq) goto loc_82479A90;
	}
loc_82479AE4:
	// addi r29,r29,-1
	var_r29 = (uint32_t)(var_r29 + -1);
	// fadds f0,f7,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// add r5,r30,r5
	ctx.r5.u64 = var_r30 + ctx.r5.u64;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// cmpwi cr6,r29,0
	// bne cr6,0x82479930
	if ((int32_t)var_r29 != 0) goto loc_82479930;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r10,r6,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 1);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r7,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r7.s64;
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r10,r5,r10
	ctx.r10.u32 = ctx.r10.u32 ? ctx.r5.u32 / ctx.r10.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82479b28
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82479B28:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r7,r8,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82479b48
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82479B48:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// lis r11,-32248
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r31,4
	// li r6,32767
	ctx.r6.s64 = 32767;
	// li r7,-32738
	// lfs f0,-23892(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23892);
	ctx.f0.f64 = double(temp.f32);
	// blt cr6,0x82479c80
	if ((int32_t)var_r31 >= 4) {
		// addi r5,r31,-4
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + -4;
		// addi r11,r28,8
		ctx.r11.s64 = (int64_t)(int32_t)var_r28 + 8;
		// rlwinm r10,r5,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r9,r10,1
		ctx.r9.s64 = ctx.r10.s64 + 1;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_82479B80:
		// lfs f10,-8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		ctx.f10.f64 = double(temp.f32);
		// addi r4,r1,-96
		ctx.r4.s64 = ctx.r1.s64 + -96;
		// fmuls f9,f10,f0
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
		// fctiwz f8,f9
		ctx.f8.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
		// stfiwx f8,0,r4
		PPC_STORE_U32(ctx.r4.u32, ctx.f8.u32);
		// lwz r10,-96(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		// cmpwi cr6,r10,32767
		// blt cr6,0x82479ba8
		if (ctx.r10.s32 >= 32767) {
			// sth r6,-8(r11)
			PPC_STORE_U16(ctx.r11.u32 + -8, ctx.r6.u16);
			// b 0x82479bbc
		} else {
		loc_82479BA8:
			// cmpwi cr6,r10,-32768
			// bgt cr6,0x82479bb8
			if (ctx.r10.s32 <= -32768) {
				// sth r7,-8(r11)
				PPC_STORE_U16(ctx.r11.u32 + -8, ctx.r7.u16);
				// b 0x82479bbc
			} else {
			loc_82479BB8:
				// sth r10,-8(r11)
				PPC_STORE_U16(ctx.r11.u32 + -8, ctx.r10.u16);
			}
		}
	loc_82479BBC:
		// lfs f7,-4(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		ctx.f7.f64 = double(temp.f32);
		// addi r10,r1,-96
		ctx.r10.s64 = ctx.r1.s64 + -96;
		// fmuls f6,f7,f0
		ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
		// fctiwz f5,f6
		ctx.f5.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f6.f64));
		// stfiwx f5,0,r10
		PPC_STORE_U32(ctx.r10.u32, ctx.f5.u32);
		// lwz r10,-96(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		// cmpwi cr6,r10,32767
		// blt cr6,0x82479be4
		if (ctx.r10.s32 >= 32767) {
			// sth r6,-4(r11)
			PPC_STORE_U16(ctx.r11.u32 + -4, ctx.r6.u16);
			// b 0x82479bf8
		} else {
		loc_82479BE4:
			// cmpwi cr6,r10,-32768
			// bgt cr6,0x82479bf4
			if (ctx.r10.s32 <= -32768) {
				// sth r7,-4(r11)
				PPC_STORE_U16(ctx.r11.u32 + -4, ctx.r7.u16);
				// b 0x82479bf8
			} else {
			loc_82479BF4:
				// sth r10,-4(r11)
				PPC_STORE_U16(ctx.r11.u32 + -4, ctx.r10.u16);
			}
		}
	loc_82479BF8:
		// lfs f4,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f4.f64 = double(temp.f32);
		// addi r4,r1,-96
		ctx.r4.s64 = ctx.r1.s64 + -96;
		// fmuls f3,f4,f0
		ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// fctiwz f2,f3
		ctx.f2.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f3.f64));
		// stfiwx f2,0,r4
		PPC_STORE_U32(ctx.r4.u32, ctx.f2.u32);
		// lwz r10,-96(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		// cmpwi cr6,r10,32767
		// blt cr6,0x82479c20
		if (ctx.r10.s32 >= 32767) {
			// sth r6,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
			// b 0x82479c34
		} else {
		loc_82479C20:
			// cmpwi cr6,r10,-32768
			// bgt cr6,0x82479c30
			if (ctx.r10.s32 <= -32768) {
				// sth r7,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r7.u16);
				// b 0x82479c34
			} else {
			loc_82479C30:
				// sth r10,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
			}
		}
	loc_82479C34:
		// lfs f1,4(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f1.f64 = double(temp.f32);
		// addi r10,r1,-96
		ctx.r10.s64 = ctx.r1.s64 + -96;
		// fmuls f13,f1,f0
		ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
		// fctiwz f12,f13
		ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f13.f64));
		// stfiwx f12,0,r10
		PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
		// lwz r10,-96(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		// cmpwi cr6,r10,32767
		// blt cr6,0x82479c5c
		if (ctx.r10.s32 >= 32767) {
			// sth r6,4(r11)
			PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r6.u16);
			// b 0x82479c70
		} else {
		loc_82479C5C:
			// cmpwi cr6,r10,-32768
			// bgt cr6,0x82479c6c
			if (ctx.r10.s32 <= -32768) {
				// sth r7,4(r11)
				PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r7.u16);
				// b 0x82479c70
			} else {
			loc_82479C6C:
				// sth r10,4(r11)
				PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r10.u16);
			}
		}
	loc_82479C70:
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// cmplwi cr6,r9,0
		// bne cr6,0x82479b80
		if (ctx.r9.u32 != 0) goto loc_82479B80;
	}
loc_82479C80:
	// cmplw cr6,r8,r31
	// bge cr6,0x82479ce0
	if (ctx.r8.u32 < var_r31) {
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r9,r8,r31
		ctx.r9.s64 = (int64_t)(int32_t)var_r31 - ctx.r8.s64;
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
	loc_82479C94:
		// lfs f11,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// addi r4,r1,-96
		ctx.r4.s64 = ctx.r1.s64 + -96;
		// fmuls f10,f11,f0
		ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
		// fctiwz f9,f10
		ctx.f9.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f10.f64));
		// stfiwx f9,0,r4
		PPC_STORE_U32(ctx.r4.u32, ctx.f9.u32);
		// lwz r10,-96(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		// cmpwi cr6,r10,32767
		// blt cr6,0x82479cbc
		if (ctx.r10.s32 >= 32767) {
			// sth r6,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r6.u16);
			// b 0x82479cd0
		} else {
		loc_82479CBC:
			// cmpwi cr6,r10,-32768
			// bgt cr6,0x82479ccc
			if (ctx.r10.s32 <= -32768) {
				// sth r7,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r7.u16);
				// b 0x82479cd0
			} else {
			loc_82479CCC:
				// sth r10,0(r11)
				PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
			}
		}
	loc_82479CD0:
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// cmplwi cr6,r9,0
		// bne cr6,0x82479c94
		if (ctx.r9.u32 != 0) goto loc_82479C94;
	}
loc_82479CE0:
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundComposite_6"))) PPC_WEAK_FUNC(phBoundComposite_6);
PPC_FUNC_IMPL(__imp__phBoundComposite_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r4,r11,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r10,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r8,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 1;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r6,r6,r10
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r9,r10
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 + ctx.r5.u64;
	// blt cr6,0x82479d38
	if (ctx.r9.s32 >= ctx.r10.s32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_82479D38:
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r6,-32256
	// addi r4,r7,127
	ctx.r4.s64 = ctx.r7.s64 + 127;
	// li r10,0
	ctx.r10.s64 = 0;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// rlwinm r7,r4,25,7,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// lfs f13,16056(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// beq cr6,0x82479d90
while (ctx.r10.u32 < ctx.r7.u32) {
	loc_82479D7C:
		// rlwinm r6,r10,7,0,24
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r6,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r7
		// blt cr6,0x82479d7c
}
loc_82479D90:
	// lhz r5,52(r3)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r3.u32 + 52);
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f8,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f8,27864(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27864);  /* glob:lbl_82006CD8 @ 0x82006cd8 */
	ctx.f8.f64 = double(temp.f32);
	// lis r10,-32256
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// lfs f9,27200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f6,f8
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// stfs f5,52(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
loc_82479DC0:
	// lhz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lfs f12,52(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f12.f64 = double(temp.f32);
	// fadds f4,f11,f0
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f3,-16(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f13,f1,f8
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f8.f64));
	// stfs f13,52(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// fadds f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// fmuls f7,f4,f13
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f6,f12,f0
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fmuls f5,f6,f9
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// stfs f5,0(r8)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f7,4(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x82479dc0
	if (!ctx.cr6.eq) goto loc_82479DC0;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r7,r10,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// subf r9,r4,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r4.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r7,0
	if (ctx.r7.s32 == 0 || ctx.r7.u32 < 0u) __builtin_trap();
	// divwu r10,r9,r7
	ctx.r10.u32 = ctx.r7.u32 ? ctx.r9.u32 / ctx.r7.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82479e44
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_82479E44:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x82479e64
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_82479E64:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// lis r11,-32248
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lfs f4,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,-23892(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23892);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fctiwz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f3.f64));
	// stfiwx f2,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f2.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r11,32767
	// blt cr6,0x82479ea0
	if (ctx.r11.s32 >= 32767) {
		// li r11,32767
		ctx.r11.s64 = 32767;
		// sth r11,52(r3)
		PPC_STORE_U16(ctx.r3.u32 + 52, ctx.r11.u16);
		// blr
		return;
	}
loc_82479EA0:
	// cmpwi cr6,r11,-32768
	// bgt cr6,0x82479eb4
	if (ctx.r11.s32 <= -32768) {
		// li r10,-32738
		ctx.r10.s64 = -32738;
		// sth r10,52(r3)
		PPC_STORE_U16(ctx.r3.u32 + 52, ctx.r10.u16);
		// blr
		return;
	}
loc_82479EB4:
	// sth r11,52(r3)
	PPC_STORE_U16(ctx.r3.u32 + 52, ctx.r11.u16);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_7"))) PPC_WEAK_FUNC(phBoundComposite_7);
PPC_FUNC_IMPL(__imp__phBoundComposite_7) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r9,r3,52
	ctx.r9.s64 = ctx.r3.s64 + 52;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r11.s64;
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mullw r5,r5,r10
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// cmpw cr6,r7,r8
	// add r11,r5,r4
	ctx.r11.u64 = ctx.r5.u64 + ctx.r4.u64;
	// blt cr6,0x82479f14
	if (ctx.r7.s32 >= ctx.r8.s32) {
		// mr r7,r8
		ctx.r7.u64 = ctx.r8.u64;
	}
loc_82479F14:
	// extsw r4,r8
	ctx.r4.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r5,-32256
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// li r8,0
	ctx.r8.s64 = 0;
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f8,f12,f9
	ctx.f8.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// beq cr6,0x82479f6c
while (ctx.r8.u32 < ctx.r6.u32) {
	loc_82479F58:
		// rlwinm r5,r8,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// cmplw cr6,r8,r6
		// blt cr6,0x82479f58
}
loc_82479F6C:
	// lhz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lhz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// lis r8,-32256
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f6,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// lfs f11,27864(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27864);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f10,27200(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f10.f64 = double(temp.f32);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f11
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// stfs f3,0(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lfd f2,-8(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f13,f1
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// fmuls f12,f13,f11
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// stfs f12,4(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
loc_82479FBC:
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
	// lfs f9,4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fadds f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// std r4,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r4.u64);
	// lfd f6,-8(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f13,f4,f11
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// fadds f3,f13,f9
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fmuls f2,f12,f13
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f1,f3,f0
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f13,f1,f10
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f13,1024(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f2,1028(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lfs f9,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// extsh r5,r8
	ctx.r5.s64 = ctx.r8.s16;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f6,-16(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f13,f4,f11
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// fadds f3,f13,f9
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fmuls f2,f12,f13
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f1,f3,f0
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fadds f0,f7,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// fmuls f13,f1,f10
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f2,4(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x82479fbc
	if (!ctx.cr6.eq) goto loc_82479FBC;
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r6,r8,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// subf r7,r4,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r4.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r8,r7,r6
	ctx.r8.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r8,r11
	// blt cr6,0x8247a078
	if (ctx.r8.u32 >= ctx.r11.u32) {
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
	}
loc_8247A078:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r5.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247a098
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247A098:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// lis r11,-32248
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f12,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// li r8,-32738
	// lfs f0,-23892(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23892);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fctiwz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f11.f64));
	// stfiwx f10,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f10.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// li r10,32767
	ctx.r10.s64 = 32767;
	// cmpwi cr6,r11,32767
	// blt cr6,0x8247a0d8
	if (ctx.r11.s32 >= 32767) {
		// sth r10,0(r9)
		PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
		// b 0x8247a0ec
	} else {
	loc_8247A0D8:
		// cmpwi cr6,r11,-32768
		// bgt cr6,0x8247a0e8
		if (ctx.r11.s32 <= -32768) {
			// sth r8,0(r9)
			PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r8.u16);
			// b 0x8247a0ec
		} else {
		loc_8247A0E8:
			// sth r11,0(r9)
			PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r11.u16);
		}
	}
loc_8247A0EC:
	// lfs f9,4(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fctiwz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f8.f64));
	// stfiwx f7,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f7.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r11,32767
	// blt cr6,0x8247a114
	if (ctx.r11.s32 >= 32767) {
		// sth r10,4(r9)
		PPC_STORE_U16(ctx.r9.u32 + 4, ctx.r10.u16);
		// blr
		return;
	}
loc_8247A114:
	// cmpwi cr6,r11,-32768
	// bgt cr6,0x8247a124
	if (ctx.r11.s32 <= -32768) {
		// sth r8,4(r9)
		PPC_STORE_U16(ctx.r9.u32 + 4, ctx.r8.u16);
		// blr
		return;
	}
loc_8247A124:
	// sth r11,4(r9)
	PPC_STORE_U16(ctx.r9.u32 + 4, ctx.r11.u16);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_8"))) PPC_WEAK_FUNC(phBoundComposite_8);
PPC_FUNC_IMPL(__imp__phBoundComposite_8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r11,r3,52
	ctx.r11.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r4,r4,r9
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// cmpw cr6,r7,r8
	// blt cr6,0x8247a184
	if (ctx.r7.s32 >= ctx.r8.s32) {
		// mr r7,r8
		ctx.r7.u64 = ctx.r8.u64;
	}
loc_8247A184:
	// extsw r5,r8
	ctx.r5.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r4,r6,127
	ctx.r4.s64 = ctx.r6.s64 + 127;
	// std r5,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r5.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r5,-32256
	// rlwinm r6,r4,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f8,f12,f9
	ctx.f8.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// beq cr6,0x8247a1dc
while (ctx.r8.u32 < ctx.r6.u32) {
	loc_8247A1C8:
		// rlwinm r5,r8,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r9
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// cmplw cr6,r8,r6
		// blt cr6,0x8247a1c8
}
loc_8247A1DC:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lhz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 8);
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// lhz r4,12(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 12);
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lfd f6,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lhz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 4);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// extsh r8,r6
	ctx.r8.s64 = ctx.r6.s16;
	// extsh r6,r5
	ctx.r6.s64 = ctx.r5.s16;
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// std r8,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r8.u64);
	// lis r8,-32256
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lfd f1,-32(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// fcfid f10,f1
	ctx.f10.f64 = double(ctx.f1.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// lfs f12,27864(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27864);
	ctx.f12.f64 = double(temp.f32);
	// lis r8,-32256
	// frsp f5,f10
	ctx.f5.f64 = double(float(ctx.f10.f64));
	// fmuls f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f3,0(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfd f2,-24(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f13,f2
	ctx.f13.f64 = double(ctx.f2.s64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fmuls f2,f5,f12
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fcfid f9,f11
	ctx.f9.f64 = double(ctx.f11.s64);
	// lfs f11,27200(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27200);
	ctx.f11.f64 = double(temp.f32);
	// stfs f2,8(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// frsp f6,f13
	ctx.f6.f64 = double(float(ctx.f13.f64));
	// frsp f4,f9
	ctx.f4.f64 = double(float(ctx.f9.f64));
	// fmuls f3,f6,f12
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// stfs f3,4(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmuls f1,f4,f12
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f1,12(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
loc_8247A26C:
	// lhz r4,6(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lfs f9,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fadds f13,f8,f0
	ctx.f13.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f10,-16(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f6,f10
	ctx.f6.f64 = double(ctx.f10.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f10,f5,f12
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// fadds f4,f10,f9
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f1,f2,f11
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f1,3072(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// stfs f3,3076(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3076, temp.u32);
	// lhz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lfs f9,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// extsh r8,r5
	ctx.r8.s64 = ctx.r5.s16;
	// std r8,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r8.u64);
	// lfd f10,-24(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f6,f10
	ctx.f6.f64 = double(ctx.f10.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f10,f5,f12
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f10,8(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f4,f10,f9
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f1,f2,f11
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f1,2048(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// stfs f3,2052(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2052, temp.u32);
	// lhz r6,2(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lfs f9,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r4.u64);
	// lfd f10,-32(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f6,f10
	ctx.f6.f64 = double(ctx.f10.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f10,f5,f12
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f10,4(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f4,f10,f9
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f1,f2,f11
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f1,1024(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f3,1028(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lhz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// extsh r5,r8
	ctx.r5.s64 = ctx.r8.s16;
	// std r5,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r5.u64);
	// lfd f10,-8(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f6,f10
	ctx.f6.f64 = double(ctx.f10.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f10,f5,f12
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f4,f10,f9
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f0,f7,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// fmuls f1,f2,f11
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f3,4(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8247a26c
	if (!ctx.cr6.eq) goto loc_8247A26C;
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r6,r8,1
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// subf r7,r4,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r4.s64;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r8,r7,r6
	ctx.r8.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r8,r9
	// blt cr6,0x8247a3a0
	if (ctx.r8.u32 >= ctx.r9.u32) {
		// mr r8,r9
		ctx.r8.u64 = ctx.r9.u64;
	}
loc_8247A3A0:
	// lwz r5,20(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r5,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r5.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r9
	// blt cr6,0x8247a3c0
	if (ctx.r10.u32 >= ctx.r9.u32) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_8247A3C0:
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// lis r10,-32248
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r8,-32738
	// lfs f0,-23892(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -23892);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f11.u32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// li r9,32767
	ctx.r9.s64 = 32767;
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a400
	if (ctx.r10.s32 >= 32767) {
		// sth r9,0(r11)
		PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r9.u16);
		// b 0x8247a414
	} else {
	loc_8247A400:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a410
		if (ctx.r10.s32 <= -32768) {
			// sth r8,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r8.u16);
			// b 0x8247a414
		} else {
		loc_8247A410:
			// sth r10,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
		}
	}
loc_8247A414:
	// lfs f10,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r1,-32
	ctx.r6.s64 = ctx.r1.s64 + -32;
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fctiwz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
	// stfiwx f8,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f8.u32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a43c
	if (ctx.r10.s32 >= 32767) {
		// sth r9,4(r11)
		PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r9.u16);
		// b 0x8247a450
	} else {
	loc_8247A43C:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a44c
		if (ctx.r10.s32 <= -32768) {
			// sth r8,4(r11)
			PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r8.u16);
			// b 0x8247a450
		} else {
		loc_8247A44C:
			// sth r10,4(r11)
			PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r10.u16);
		}
	}
loc_8247A450:
	// lfs f7,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// addi r4,r1,-32
	ctx.r4.s64 = ctx.r1.s64 + -32;
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fctiwz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f5.u32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a478
	if (ctx.r10.s32 >= 32767) {
		// sth r9,8(r11)
		PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r9.u16);
		// b 0x8247a48c
	} else {
	loc_8247A478:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a488
		if (ctx.r10.s32 <= -32768) {
			// sth r8,8(r11)
			PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r8.u16);
			// b 0x8247a48c
		} else {
		loc_8247A488:
			// sth r10,8(r11)
			PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r10.u16);
		}
	}
loc_8247A48C:
	// lfs f4,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fctiwz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f3.f64));
	// stfiwx f2,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f2.u32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a4b4
	if (ctx.r10.s32 >= 32767) {
		// sth r9,12(r11)
		PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r9.u16);
		// blr
		return;
	}
loc_8247A4B4:
	// cmpwi cr6,r10,-32768
	// bgt cr6,0x8247a4c4
	if (ctx.r10.s32 <= -32768) {
		// sth r8,12(r11)
		PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r8.u16);
		// blr
		return;
	}
loc_8247A4C4:
	// sth r10,12(r11)
	PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r10.u16);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_9"))) PPC_WEAK_FUNC(phBoundComposite_9);
PPC_FUNC_IMPL(__imp__phBoundComposite_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r11,r3,52
	ctx.r11.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r31,r10,r8
	var_r31 = (uint32_t)(ctx.r8.s64 - ctx.r10.s64);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r4,r4,r9
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r7,r31,1
	ctx.xer.ca = ((int32_t)var_r31 < 0) & ((var_r31 & 0x1) != 0);
	ctx.r7.s64 = (int32_t)var_r31 >> 1;
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// cmpw cr6,r8,r7
	// blt cr6,0x8247a528
	if (ctx.r8.s32 >= ctx.r7.s32) {
		// mr r8,r7
		ctx.r8.u64 = ctx.r7.u64;
	}
loc_8247A528:
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// li r7,0
	ctx.r7.s64 = 0;
	// std r6,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r6.u64);
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rlwinm r6,r5,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r5,-32256
	// addi r4,r6,127
	ctx.r4.s64 = ctx.r6.s64 + 127;
	// rlwinm r6,r4,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// fdivs f8,f12,f9
	ctx.f8.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// beq cr6,0x8247a588
while (ctx.r7.u32 < ctx.r6.u32) {
	loc_8247A574:
		// rlwinm r5,r7,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r9
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// cmplw cr6,r7,r6
		// blt cr6,0x8247a574
}
loc_8247A588:
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lhz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 12);
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// lhz r5,16(r11)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 16);
	// lhz r31,20(r11)
	var_r31 = (uint32_t)(PPC_LOAD_U16(ctx.r11.u32 + 20));
	// extsh r7,r7
	ctx.r7.s64 = ctx.r7.s16;
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// lhz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r11.u32 + 8);
	// extsh r31,r31
	var_r31 = (uint32_t)((int16_t)var_r31);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r6,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r6.u64);
	// lfd f6,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lhz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r11.u32 + 4);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// std r7,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r7.u64);
	// lis r7,-32256
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// std r5,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r5.u64);
	// std r31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, var_r31);
	// std r4,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r4.u64);
	// lfd f1,-64(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f1
	ctx.f10.f64 = double(ctx.f1.s64);
	// lfs f12,27864(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27864);
	ctx.f12.f64 = double(temp.f32);
	// lis r7,-32256
	// std r6,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r6.u64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// frsp f1,f10
	ctx.f1.f64 = double(float(ctx.f10.f64));
	// fmuls f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f3,0(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f9,-40(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f6,f11
	ctx.f6.f64 = double(ctx.f11.s64);
	// lfd f5,-32(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f4,f9
	ctx.f4.f64 = double(ctx.f9.s64);
	// fcfid f3,f5
	ctx.f3.f64 = double(ctx.f5.s64);
	// lfd f2,-56(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f13,f2
	ctx.f13.f64 = double(ctx.f2.s64);
	// frsp f11,f4
	ctx.f11.f64 = double(float(ctx.f4.f64));
	// frsp f10,f3
	ctx.f10.f64 = double(float(ctx.f3.f64));
	// frsp f2,f13
	ctx.f2.f64 = double(float(ctx.f13.f64));
	// frsp f13,f6
	ctx.f13.f64 = double(float(ctx.f6.f64));
	// fmuls f6,f1,f12
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f6,8(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fmuls f4,f11,f12
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f11,27200(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27200);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f3,f10,f12
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// stfs f4,16(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// stfs f3,20(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// fmuls f9,f2,f12
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmuls f5,f13,f12
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// stfs f5,12(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
loc_8247A658:
	// lhz r4,10(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// lfs f9,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fadds f13,f8,f0
	ctx.f13.f64 = double(float(ctx.f8.f64 + ctx.f0.f64));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lfd f2,-32(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f10,f1
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// stfs f10,20(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// fadds f9,f10,f9
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f5,f9,f0
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,5120(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// stfs f6,5124(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5124, temp.u32);
	// lhz r5,8(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// lfs f9,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// extsh r7,r5
	ctx.r7.s64 = ctx.r5.s16;
	// std r7,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r7.u64);
	// lfd f3,-40(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f10,16(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// fadds f9,f10,f9
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f5,f9,f0
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,4096(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// stfs f6,4100(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4100, temp.u32);
	// lhz r6,6(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// lfs f9,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r4.u64);
	// lfd f3,-48(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// fadds f9,f10,f9
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f5,f9,f0
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,3072(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// stfs f6,3076(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3076, temp.u32);
	// lhz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// lfs f9,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r5.u64);
	// lfd f3,-56(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f10,8(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f9,f10,f9
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f5,f9,f0
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,2048(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// stfs f6,2052(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2052, temp.u32);
	// lhz r4,2(r9)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// lfs f9,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// std r6,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r6.u64);
	// lfd f3,-64(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f10,4(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f9,f10,f9
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f5,f13,f10
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f4,f6,f11
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// stfs f4,1024(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f5,1028(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lhz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// lfs f9,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// extsh r7,r5
	ctx.r7.s64 = ctx.r5.s16;
	// std r7,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r7.u64);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// lfd f3,-24(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f9,f10,f9
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f5,f9,f0
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fadds f0,f7,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,0(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f6,4(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8247a658
	if (!ctx.cr6.eq) goto loc_8247A658;
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rotlwi r8,r5,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r5.u32, 1);
	// subf r4,r6,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r6.s64;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r8,r4,r8
	ctx.r8.u32 = ctx.r8.u32 ? ctx.r4.u32 / ctx.r8.u32 : 0;
	// cmplw cr6,r8,r9
	// blt cr6,0x8247a804
	if (ctx.r8.u32 >= ctx.r9.u32) {
		// mr r8,r9
		ctx.r8.u64 = ctx.r9.u64;
	}
loc_8247A804:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r6,r7,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r6,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r9
	// blt cr6,0x8247a824
	if (ctx.r10.u32 >= ctx.r9.u32) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_8247A824:
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// lis r10,-32248
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// lfs f3,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// li r9,32767
	ctx.r9.s64 = 32767;
	// li r8,-32738
	// lfs f0,-23892(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -23892);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fctiwz f1,f2
	ctx.f1.s64 = (ctx.f2.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f2.f64));
	// stfiwx f1,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f1.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a864
	if (ctx.r10.s32 >= 32767) {
		// sth r9,0(r11)
		PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r9.u16);
		// b 0x8247a878
	} else {
	loc_8247A864:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a874
		if (ctx.r10.s32 <= -32768) {
			// sth r8,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r8.u16);
			// b 0x8247a878
		} else {
		loc_8247A874:
			// sth r10,0(r11)
			PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
		}
	}
loc_8247A878:
	// lfs f13,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r3,r1,-64
	ctx.r3.s64 = ctx.r1.s64 + -64;
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r3
	PPC_STORE_U32(ctx.r3.u32, ctx.f11.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a8a0
	if (ctx.r10.s32 >= 32767) {
		// sth r9,4(r11)
		PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r9.u16);
		// b 0x8247a8b4
	} else {
	loc_8247A8A0:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a8b0
		if (ctx.r10.s32 <= -32768) {
			// sth r8,4(r11)
			PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r8.u16);
			// b 0x8247a8b4
		} else {
		loc_8247A8B0:
			// sth r10,4(r11)
			PPC_STORE_U16(ctx.r11.u32 + 4, ctx.r10.u16);
		}
	}
loc_8247A8B4:
	// lfs f10,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// addi r7,r1,-64
	ctx.r7.s64 = ctx.r1.s64 + -64;
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fctiwz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
	// stfiwx f8,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f8.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a8dc
	if (ctx.r10.s32 >= 32767) {
		// sth r9,8(r11)
		PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r9.u16);
		// b 0x8247a8f0
	} else {
	loc_8247A8DC:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a8ec
		if (ctx.r10.s32 <= -32768) {
			// sth r8,8(r11)
			PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r8.u16);
			// b 0x8247a8f0
		} else {
		loc_8247A8EC:
			// sth r10,8(r11)
			PPC_STORE_U16(ctx.r11.u32 + 8, ctx.r10.u16);
		}
	}
loc_8247A8F0:
	// lfs f7,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r5,r1,-64
	ctx.r5.s64 = ctx.r1.s64 + -64;
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fctiwz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f5.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a918
	if (ctx.r10.s32 >= 32767) {
		// sth r9,12(r11)
		PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r9.u16);
		// b 0x8247a92c
	} else {
	loc_8247A918:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a928
		if (ctx.r10.s32 <= -32768) {
			// sth r8,12(r11)
			PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r8.u16);
			// b 0x8247a92c
		} else {
		loc_8247A928:
			// sth r10,12(r11)
			PPC_STORE_U16(ctx.r11.u32 + 12, ctx.r10.u16);
		}
	}
loc_8247A92C:
	// lfs f4,16(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f4.f64 = double(temp.f32);
	// addi r3,r1,-64
	ctx.r3.s64 = ctx.r1.s64 + -64;
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fctiwz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f3.f64));
	// stfiwx f2,0,r3
	PPC_STORE_U32(ctx.r3.u32, ctx.f2.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a954
	if (ctx.r10.s32 >= 32767) {
		// sth r9,16(r11)
		PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
		// b 0x8247a968
	} else {
	loc_8247A954:
		// cmpwi cr6,r10,-32768
		// bgt cr6,0x8247a964
		if (ctx.r10.s32 <= -32768) {
			// sth r8,16(r11)
			PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r8.u16);
			// b 0x8247a968
		} else {
		loc_8247A964:
			// sth r10,16(r11)
			PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r10.u16);
		}
	}
loc_8247A968:
	// lfs f1,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f1.f64 = double(temp.f32);
	// addi r7,r1,-64
	ctx.r7.s64 = ctx.r1.s64 + -64;
	// fmuls f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f13,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f13.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// cmpwi cr6,r10,32767
	// blt cr6,0x8247a994
	if (ctx.r10.s32 >= 32767) {
		// sth r9,20(r11)
		PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r9.u16);
		// ld r31,-8(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
		// blr
		return;
	}
loc_8247A994:
	// cmpwi cr6,r10,-32768
	// bgt cr6,0x8247a9a8
	if (ctx.r10.s32 <= -32768) {
		// sth r8,20(r11)
		PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r8.u16);
		// ld r31,-8(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
		// blr
		return;
	}
loc_8247A9A8:
	// sth r10,20(r11)
	PPC_STORE_U16(ctx.r11.u32 + 20, ctx.r10.u16);
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_10"))) PPC_WEAK_FUNC(phBoundComposite_10);
PPC_FUNC_IMPL(__imp__phBoundComposite_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f88c
	ctx.lr = 0x8247A9C0;
	__savegprlr_25(ctx, base);
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r29,r3,52
	var_r29 = (uint32_t)(ctx.r3.s64 + 52);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r5,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r9,r10,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r10.s64;
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r7,r10,r4
	ctx.r7.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// addze r10,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r10.s64 = temp.s64;
	// add r5,r7,r6
	ctx.r5.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r31,r11,r8
	var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r8.u64);
	// cmpw cr6,r9,r10
	// mr r30,r9
	var_r30 = ctx.r9.u32;
	// blt cr6,0x8247aa14
	if (ctx.r9.s32 >= ctx.r10.s32) {
		// mr r30,r10
		var_r30 = ctx.r10.u32;
	}
loc_8247AA14:
	// extsw r11,r10
	ctx.r11.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lis r10,-32256
	// li r28,0
	var_r28 = 0;
	// cmpwi cr6,r4,4
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 4, ctx.xer);
	// mr r6,r28
	ctx.r6.u64 = var_r28;
	// std r11,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.r11.u64);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,16056(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16056);  /* glob:lbl_82003EB8 @ 0x82003eb8 */
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32248
	// lfd f11,-88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f7,f12,f9
	ctx.f7.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,27868(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27868);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lfs f13,-25388(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25388);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x8247ab10
	if (!(ctx.cr6.lt)) {
		// addi r10,r4,-4
		ctx.r10.s64 = ctx.r4.s64 + -4;
		// addi r11,r29,8
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 8;
		// rlwinm r10,r10,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// rlwinm r6,r10,2,0,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	loc_8247AA74:
		// lbz r27,0(r11)
		var_r27 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 0));
		// addi r9,r11,-8
		ctx.r9.s64 = ctx.r11.s64 + -8;
		// addi r8,r11,-4
		ctx.r8.s64 = ctx.r11.s64 + -4;
		// addi r7,r11,4
		ctx.r7.s64 = ctx.r11.s64 + 4;
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// std r27,-88(r1)
		PPC_STORE_U64(ctx.r1.u32 + -88, var_r27);
		// cmplwi cr6,r10,0
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
		// lbz r27,0(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U8(ctx.r9.u32 + 0));
		// lbz r26,0(r8)
		var_r26 = (uint32_t)(PPC_LOAD_U8(ctx.r8.u32 + 0));
		// lbz r25,0(r7)
		var_r25 = (uint32_t)(PPC_LOAD_U8(ctx.r7.u32 + 0));
		// std r27,-96(r1)
		PPC_STORE_U64(ctx.r1.u32 + -96, var_r27);
		// std r26,-104(r1)
		PPC_STORE_U64(ctx.r1.u32 + -104, var_r26);
		// std r25,-112(r1)
		PPC_STORE_U64(ctx.r1.u32 + -112, var_r25);
		// lfd f8,-88(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
		// fcfid f5,f8
		ctx.f5.f64 = double(ctx.f8.s64);
		// lfd f1,-96(r1)
		ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
		// lfd f11,-104(r1)
		ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
		// fcfid f10,f1
		ctx.f10.f64 = double(ctx.f1.s64);
		// lfd f9,-112(r1)
		ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
		// fcfid f8,f11
		ctx.f8.f64 = double(ctx.f11.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fcfid f5,f9
		ctx.f5.f64 = double(ctx.f9.s64);
		// fsubs f3,f4,f13
		ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
		// frsp f4,f10
		ctx.f4.f64 = double(float(ctx.f10.f64));
		// fmuls f2,f3,f12
		ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
		// stfs f2,0(r11)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// frsp f3,f8
		ctx.f3.f64 = double(float(ctx.f8.f64));
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// frsp f2,f5
		ctx.f2.f64 = double(float(ctx.f5.f64));
		// fsubs f1,f4,f13
		ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
		// fsubs f11,f3,f13
		ctx.f11.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
		// fsubs f10,f2,f13
		ctx.f10.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
		// fmuls f9,f1,f12
		ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f9,0(r9)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
		// fmuls f8,f11,f12
		ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
		// stfs f8,0(r8)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// fmuls f5,f10,f12
		ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
		// stfs f5,0(r7)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
		// bne cr6,0x8247aa74
		if (!ctx.cr6.eq) goto loc_8247AA74;
	}
loc_8247AB10:
	// cmplw cr6,r6,r4
	// bge cr6,0x8247ab54
	if (ctx.r6.u32 < ctx.r4.u32) {
		// rlwinm r11,r6,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r10,r6,r4
		ctx.r10.s64 = ctx.r4.s64 - ctx.r6.s64;
		// add r11,r11,r29
		ctx.r11.u64 = ctx.r11.u64 + var_r29;
	loc_8247AB24:
		// lbz r8,0(r11)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// cmplwi cr6,r10,0
		// std r8,-88(r1)
		PPC_STORE_U64(ctx.r1.u32 + -88, ctx.r8.u64);
		// lfd f4,-88(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
		// fcfid f3,f4
		ctx.f3.f64 = double(ctx.f4.s64);
		// frsp f2,f3
		ctx.f2.f64 = double(float(ctx.f3.f64));
		// fsubs f1,f2,f13
		ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
		// fmuls f11,f1,f12
		ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f11,0(r11)
		temp.f32 = float(ctx.f11.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// bne cr6,0x8247ab24
		if (ctx.r10.u32 != 0) goto loc_8247AB24;
	}
loc_8247AB54:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f11,27200(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f11.f64 = double(temp.f32);
loc_8247AB5C:
	// mr r8,r28
	ctx.r8.u64 = var_r28;
	// cmpwi cr6,r4,4
	// blt cr6,0x8247ac94
	if (ctx.r4.s32 >= 4) {
		// addi r7,r4,-4
		ctx.r7.s64 = ctx.r4.s64 + -4;
		// fadds f10,f7,f0
		ctx.fpscr.disableFlushMode();
		ctx.f10.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
		// addi r9,r5,1
		ctx.r9.s64 = ctx.r5.s64 + 1;
		// rlwinm r11,r7,30,2,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r10,r29,8
		ctx.r10.s64 = (int64_t)(int32_t)var_r29 + 8;
		// addi r7,r11,1
		ctx.r7.s64 = ctx.r11.s64 + 1;
		// addi r11,r31,1024
		ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 1024;
		// rlwinm r8,r7,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	loc_8247AB88:
		// lbz r27,-1(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U8(ctx.r9.u32 + -1));
		// lfs f8,-8(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
		ctx.f8.f64 = double(temp.f32);
		// addi r6,r10,4
		ctx.r6.s64 = ctx.r10.s64 + 4;
		// addi r7,r7,-1
		ctx.r7.s64 = ctx.r7.s64 + -1;
		// cmplwi cr6,r7,0
		ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
		// std r27,-88(r1)
		PPC_STORE_U64(ctx.r1.u32 + -88, var_r27);
		// lfd f9,-88(r1)
		ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
		// fcfid f5,f9
		ctx.f5.f64 = double(ctx.f9.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fsubs f3,f4,f13
		ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
		// fmuls f9,f3,f12
		ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
		// stfs f9,-8(r10)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r10.u32 + -8, temp.u32);
		// fadds f2,f9,f8
		ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
		// fmuls f1,f9,f10
		ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
		// fmuls f9,f2,f0
		ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
		// fmuls f8,f9,f11
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
		// stfs f8,-1024(r11)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r11.u32 + -1024, temp.u32);
		// stfs f1,-1020(r11)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r11.u32 + -1020, temp.u32);
		// lbz r27,0(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U8(ctx.r9.u32 + 0));
		// lfs f8,-4(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -4);
		ctx.f8.f64 = double(temp.f32);
		// std r27,-96(r1)
		PPC_STORE_U64(ctx.r1.u32 + -96, var_r27);
		// lfd f5,-96(r1)
		ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
		// fcfid f4,f5
		ctx.f4.f64 = double(ctx.f5.s64);
		// frsp f3,f4
		ctx.f3.f64 = double(float(ctx.f4.f64));
		// fsubs f2,f3,f13
		ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
		// fmuls f9,f2,f12
		ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
		// stfs f9,-4(r10)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r10.u32 + -4, temp.u32);
		// fadds f1,f9,f8
		ctx.f1.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
		// fmuls f9,f9,f10
		ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
		// fmuls f8,f1,f0
		ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
		// fmuls f5,f8,f11
		ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
		// stfs f5,0(r11)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// stfs f9,4(r11)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
		// lbz r27,1(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U8(ctx.r9.u32 + 1));
		// lfs f8,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// std r27,-104(r1)
		PPC_STORE_U64(ctx.r1.u32 + -104, var_r27);
		// lfd f4,-104(r1)
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
		// fcfid f3,f4
		ctx.f3.f64 = double(ctx.f4.s64);
		// frsp f2,f3
		ctx.f2.f64 = double(float(ctx.f3.f64));
		// fsubs f1,f2,f13
		ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f13.f64));
		// fmuls f9,f1,f12
		ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
		// stfs f9,0(r10)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// addi r10,r10,16
		ctx.r10.s64 = ctx.r10.s64 + 16;
		// fadds f8,f9,f8
		ctx.f8.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
		// fmuls f5,f9,f10
		ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
		// fmuls f4,f8,f0
		ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
		// fmuls f3,f4,f11
		ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
		// stfs f3,1024(r11)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r11.u32 + 1024, temp.u32);
		// stfs f5,1028(r11)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r11.u32 + 1028, temp.u32);
		// lbz r27,2(r9)
		var_r27 = (uint32_t)(PPC_LOAD_U8(ctx.r9.u32 + 2));
		// lfs f8,0(r6)
		temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// addi r9,r9,4
		ctx.r9.s64 = ctx.r9.s64 + 4;
		// std r27,-112(r1)
		PPC_STORE_U64(ctx.r1.u32 + -112, var_r27);
		// lfd f2,-112(r1)
		ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
		// fcfid f1,f2
		ctx.f1.f64 = double(ctx.f2.s64);
		// frsp f9,f1
		ctx.f9.f64 = double(float(ctx.f1.f64));
		// fsubs f5,f9,f13
		ctx.f5.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
		// fmuls f9,f5,f12
		ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
		// stfs f9,0(r6)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
		// fadds f4,f9,f8
		ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
		// fmuls f3,f9,f10
		ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
		// fmuls f2,f4,f0
		ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
		// fmuls f1,f2,f11
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
		// stfs f1,2048(r11)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r11.u32 + 2048, temp.u32);
		// stfs f3,2052(r11)
		temp.f32 = float(ctx.f3.f64);
		PPC_STORE_U32(ctx.r11.u32 + 2052, temp.u32);
		// addi r11,r11,4096
		ctx.r11.s64 = ctx.r11.s64 + 4096;
		// bne cr6,0x8247ab88
		if (!ctx.cr6.eq) goto loc_8247AB88;
	}
loc_8247AC94:
	// cmplw cr6,r8,r4
	// bge cr6,0x8247ad00
	if (ctx.r8.u32 < ctx.r4.u32) {
		// rlwinm r10,r8,10,0,21
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 10) & 0xFFFFFC00;
		// fadds f8,f7,f0
		ctx.fpscr.disableFlushMode();
		ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// add r10,r10,r31
		ctx.r10.u64 = ctx.r10.u64 + var_r31;
		// add r11,r11,r29
		ctx.r11.u64 = ctx.r11.u64 + var_r29;
	loc_8247ACB0:
		// lbzx r7,r8,r5
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r5.u32);
		// lfs f9,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// cmplw cr6,r8,r4
		ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r4.u32, ctx.xer);
		// std r7,-80(r1)
		PPC_STORE_U64(ctx.r1.u32 + -80, ctx.r7.u64);
		// lfd f10,-80(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
		// fcfid f5,f10
		ctx.f5.f64 = double(ctx.f10.s64);
		// frsp f4,f5
		ctx.f4.f64 = double(float(ctx.f5.f64));
		// fsubs f3,f4,f13
		ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
		// fmuls f10,f3,f12
		ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
		// stfs f10,0(r11)
		temp.f32 = float(ctx.f10.f64);
		PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// fadds f2,f10,f9
		ctx.f2.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
		// fmuls f1,f10,f8
		ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
		// fmuls f10,f2,f0
		ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
		// fmuls f9,f10,f11
		ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
		// stfs f9,0(r10)
		temp.f32 = float(ctx.f9.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// stfs f1,4(r10)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
		// addi r10,r10,1024
		ctx.r10.s64 = ctx.r10.s64 + 1024;
		// blt cr6,0x8247acb0
		if (ctx.cr6.lt) goto loc_8247ACB0;
	}
loc_8247AD00:
	// addi r30,r30,-1
	var_r30 = (uint32_t)(var_r30 + -1);
	// fadds f0,f6,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// add r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 + ctx.r4.u64;
	// addi r31,r31,8
	var_r31 = (uint32_t)(var_r31 + 8);
	// cmpwi cr6,r30,0
	// bne cr6,0x8247ab5c
	if ((int32_t)var_r30 != 0) goto loc_8247AB5C;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r6,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r6.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r10,r9,r10
	ctx.r10.u32 = ctx.r10.u32 ? ctx.r9.u32 / ctx.r10.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x8247ad40
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8247AD40:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r7,r8,r31
	ctx.r7.s64 = (int64_t)(int32_t)var_r31 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247ad60
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247AD60:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// lis r11,-32256
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// mr r8,r28
	ctx.r8.u64 = var_r28;
	// cmpwi cr6,r4,4
	// li r7,255
	ctx.r7.s64 = 255;
	// lfs f0,27872(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27872);
	ctx.f0.f64 = double(temp.f32);
	// blt cr6,0x8247aea4
	if (ctx.r4.s32 >= 4) {
		// addi r6,r4,-4
		ctx.r6.s64 = ctx.r4.s64 + -4;
		// addi r11,r29,8
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 8;
		// rlwinm r10,r6,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r9,r10,1
		ctx.r9.s64 = ctx.r10.s64 + 1;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	loc_8247AD94:
		// lfs f8,-8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		ctx.f8.f64 = double(temp.f32);
		// addi r5,r1,-112
		ctx.r5.s64 = ctx.r1.s64 + -112;
		// fmuls f7,f8,f0
		ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
		// fctiwz f6,f7
		ctx.f6.s64 = (ctx.f7.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f7.f64));
		// stfiwx f6,0,r5
		PPC_STORE_U32(ctx.r5.u32, ctx.f6.u32);
		// lwz r3,-112(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -112);
		// subfic r10,r3,128
		ctx.xer.ca = ctx.r3.u32 <= 128;
		ctx.r10.s64 = 128 - ctx.r3.s64;
		// cmpwi cr6,r10,255
		// blt cr6,0x8247adc0
		if (ctx.r10.s32 >= 255) {
			// stb r7,-8(r11)
			PPC_STORE_U8(ctx.r11.u32 + -8, ctx.r7.u8);
			// b 0x8247add4
		} else {
		loc_8247ADC0:
			// cmpwi cr6,r10,0
			// bgt cr6,0x8247add0
			if (ctx.r10.s32 <= 0) {
				// stb r28,-8(r11)
				PPC_STORE_U8(ctx.r11.u32 + -8, (uint8_t)var_r28);
				// b 0x8247add4
			} else {
			loc_8247ADD0:
				// stb r10,-8(r11)
				PPC_STORE_U8(ctx.r11.u32 + -8, ctx.r10.u8);
			}
		}
	loc_8247ADD4:
		// lfs f5,-4(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		ctx.f5.f64 = double(temp.f32);
		// addi r6,r1,-104
		ctx.r6.s64 = ctx.r1.s64 + -104;
		// fmuls f4,f5,f0
		ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
		// fctiwz f3,f4
		ctx.f3.s64 = (ctx.f4.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f4.f64));
		// stfiwx f3,0,r6
		PPC_STORE_U32(ctx.r6.u32, ctx.f3.u32);
		// lwz r5,-104(r1)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -104);
		// subfic r10,r5,128
		ctx.xer.ca = ctx.r5.u32 <= 128;
		ctx.r10.s64 = 128 - ctx.r5.s64;
		// cmpwi cr6,r10,255
		// blt cr6,0x8247ae00
		if (ctx.r10.s32 >= 255) {
			// stb r7,-4(r11)
			PPC_STORE_U8(ctx.r11.u32 + -4, ctx.r7.u8);
			// b 0x8247ae14
		} else {
		loc_8247AE00:
			// cmpwi cr6,r10,0
			// bgt cr6,0x8247ae10
			if (ctx.r10.s32 <= 0) {
				// stb r28,-4(r11)
				PPC_STORE_U8(ctx.r11.u32 + -4, (uint8_t)var_r28);
				// b 0x8247ae14
			} else {
			loc_8247AE10:
				// stb r10,-4(r11)
				PPC_STORE_U8(ctx.r11.u32 + -4, ctx.r10.u8);
			}
		}
	loc_8247AE14:
		// lfs f2,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f2.f64 = double(temp.f32);
		// addi r10,r1,-96
		ctx.r10.s64 = ctx.r1.s64 + -96;
		// fmuls f1,f2,f0
		ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
		// fctiwz f13,f1
		ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f1.f64));
		// stfiwx f13,0,r10
		PPC_STORE_U32(ctx.r10.u32, ctx.f13.u32);
		// lwz r6,-96(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + -96);
		// subfic r10,r6,128
		ctx.xer.ca = ctx.r6.u32 <= 128;
		ctx.r10.s64 = 128 - ctx.r6.s64;
		// cmpwi cr6,r10,255
		// blt cr6,0x8247ae40
		if (ctx.r10.s32 >= 255) {
			// stb r7,0(r11)
			PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r7.u8);
			// b 0x8247ae54
		} else {
		loc_8247AE40:
			// cmpwi cr6,r10,0
			// bgt cr6,0x8247ae50
			if (ctx.r10.s32 <= 0) {
				// stb r28,0(r11)
				PPC_STORE_U8(ctx.r11.u32 + 0, (uint8_t)var_r28);
				// b 0x8247ae54
			} else {
			loc_8247AE50:
				// stb r10,0(r11)
				PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r10.u8);
			}
		}
	loc_8247AE54:
		// lfs f12,4(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f12.f64 = double(temp.f32);
		// addi r3,r1,-88
		ctx.r3.s64 = ctx.r1.s64 + -88;
		// fmuls f11,f12,f0
		ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
		// fctiwz f10,f11
		ctx.f10.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f11.f64));
		// stfiwx f10,0,r3
		PPC_STORE_U32(ctx.r3.u32, ctx.f10.u32);
		// lwz r10,-88(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
		// subfic r10,r10,128
		ctx.xer.ca = ctx.r10.u32 <= 128;
		ctx.r10.s64 = 128 - ctx.r10.s64;
		// cmpwi cr6,r10,255
		// blt cr6,0x8247ae80
		if (ctx.r10.s32 >= 255) {
			// stb r7,4(r11)
			PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r7.u8);
			// b 0x8247ae94
		} else {
		loc_8247AE80:
			// cmpwi cr6,r10,0
			// bgt cr6,0x8247ae90
			if (ctx.r10.s32 <= 0) {
				// stb r28,4(r11)
				PPC_STORE_U8(ctx.r11.u32 + 4, (uint8_t)var_r28);
				// b 0x8247ae94
			} else {
			loc_8247AE90:
				// stb r10,4(r11)
				PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r10.u8);
			}
		}
	loc_8247AE94:
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// cmplwi cr6,r9,0
		// bne cr6,0x8247ad94
		if (ctx.r9.u32 != 0) goto loc_8247AD94;
	}
loc_8247AEA4:
	// cmplw cr6,r8,r4
	// bge cr6,0x8247af08
	if (ctx.r8.u32 < ctx.r4.u32) {
		// rlwinm r11,r8,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
		// subf r9,r8,r4
		ctx.r9.s64 = ctx.r4.s64 - ctx.r8.s64;
		// add r11,r11,r29
		ctx.r11.u64 = ctx.r11.u64 + var_r29;
	loc_8247AEB8:
		// lfs f9,0(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// addi r5,r1,-88
		ctx.r5.s64 = ctx.r1.s64 + -88;
		// fmuls f8,f9,f0
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
		// fctiwz f7,f8
		ctx.f7.s64 = (ctx.f8.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f8.f64));
		// stfiwx f7,0,r5
		PPC_STORE_U32(ctx.r5.u32, ctx.f7.u32);
		// lwz r4,-88(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + -88);
		// subfic r10,r4,128
		ctx.xer.ca = ctx.r4.u32 <= 128;
		ctx.r10.s64 = 128 - ctx.r4.s64;
		// cmpwi cr6,r10,255
		// blt cr6,0x8247aee4
		if (ctx.r10.s32 >= 255) {
			// stb r7,0(r11)
			PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r7.u8);
			// b 0x8247aef8
		} else {
		loc_8247AEE4:
			// cmpwi cr6,r10,0
			// bgt cr6,0x8247aef4
			if (ctx.r10.s32 <= 0) {
				// stb r28,0(r11)
				PPC_STORE_U8(ctx.r11.u32 + 0, (uint8_t)var_r28);
				// b 0x8247aef8
			} else {
			loc_8247AEF4:
				// stb r10,0(r11)
				PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r10.u8);
			}
		}
	loc_8247AEF8:
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// cmplwi cr6,r9,0
		// bne cr6,0x8247aeb8
		if (ctx.r9.u32 != 0) goto loc_8247AEB8;
	}
loc_8247AF08:
	// b 0x8242f8dc
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__phBoundComposite_11"))) PPC_WEAK_FUNC(phBoundComposite_11);
PPC_FUNC_IMPL(__imp__phBoundComposite_11) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r4,r11,r9
	ctx.r4.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 1;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r10,r5,r10
	ctx.r10.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r9
	// blt cr6,0x8247af5c
	if (ctx.r8.s32 >= ctx.r9.s32) {
		// mr r8,r9
		ctx.r8.u64 = ctx.r9.u64;
	}
loc_8247AF5C:
	// extsw r6,r9
	ctx.r6.s64 = ctx.r9.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r5,r8,127
	ctx.r5.s64 = ctx.r8.s64 + 127;
	// li r11,0
	ctx.r11.s64 = 0;
	// rlwinm r9,r5,25,7,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lis r6,-32256
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lfs f13,16056(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f7,f11,f13
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// beq cr6,0x8247afb0
while (ctx.r11.u32 < ctx.r9.u32) {
	loc_8247AF9C:
		// rlwinm r4,r11,7,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r4,r10
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplw cr6,r11,r9
		// blt cr6,0x8247af9c
}
loc_8247AFB0:
	// lbz r9,52(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 52);
	// lis r11,-32248
	// std r9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r9.u64);
	// lfs f10,-25388(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25388);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f9,27868(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27868);  /* glob:lbl_82006CDC @ 0x82006cdc */
	ctx.f9.f64 = double(temp.f32);
	// lis r11,-32256
	// lfd f8,-16(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f6,f8
	ctx.f6.f64 = double(ctx.f8.s64);
	// lfs f8,27200(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f8.f64 = double(temp.f32);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fsubs f4,f5,f10
	ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f10.f64));
	// fmuls f3,f4,f9
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f9.f64));
	// stfs f3,52(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
loc_8247AFE8:
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lfs f12,52(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f12.f64 = double(temp.f32);
	// fadds f2,f11,f0
	ctx.f2.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f1,-16(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f1
	ctx.f13.f64 = double(ctx.f1.s64);
	// frsp f6,f13
	ctx.f6.f64 = double(float(ctx.f13.f64));
	// fsubs f5,f6,f10
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f10.f64));
	// fmuls f13,f5,f9
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// stfs f13,52(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// fadds f4,f13,f12
	ctx.f4.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// fmuls f3,f2,f13
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f0,f7,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// fmuls f1,f2,f8
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// stfs f1,0(r7)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stfs f3,4(r7)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// bne cr6,0x8247afe8
	if (!ctx.cr6.eq) goto loc_8247AFE8;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r8,r4,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r4.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r9.u32 ? ctx.r8.u32 / ctx.r9.u32 : 0;
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x8247b068
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8247B068:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r6.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247b088
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247B088:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// lis r11,-32256
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r4,r1,-16
	ctx.r4.s64 = ctx.r1.s64 + -16;
	// lfs f13,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,27872(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27872);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f11.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// subfic r11,r11,128
	ctx.xer.ca = ctx.r11.u32 <= 128;
	ctx.r11.s64 = 128 - ctx.r11.s64;
	// cmpwi cr6,r11,255
	// blt cr6,0x8247b0c8
	if (ctx.r11.s32 >= 255) {
		// li r10,255
		ctx.r10.s64 = 255;
		// stb r10,52(r3)
		PPC_STORE_U8(ctx.r3.u32 + 52, ctx.r10.u8);
		// blr
		return;
	}
loc_8247B0C8:
	// cmpwi cr6,r11,0
	// bgt cr6,0x8247b0dc
	if (ctx.r11.s32 <= 0) {
		// li r9,0
		ctx.r9.s64 = 0;
		// stb r9,52(r3)
		PPC_STORE_U8(ctx.r3.u32 + 52, ctx.r9.u8);
		// blr
		return;
	}
loc_8247B0DC:
	// stb r11,52(r3)
	PPC_STORE_U8(ctx.r3.u32 + 52, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_12"))) PPC_WEAK_FUNC(phBoundComposite_12);
PPC_FUNC_IMPL(__imp__phBoundComposite_12) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r10,r3,52
	ctx.r10.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r11.s64;
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r5,r5,r9
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r11,r5,r4
	ctx.r11.u64 = ctx.r5.u64 + ctx.r4.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// cmpw cr6,r7,r8
	// blt cr6,0x8247b138
	if (ctx.r7.s32 >= ctx.r8.s32) {
		// mr r7,r8
		ctx.r7.u64 = ctx.r8.u64;
	}
loc_8247B138:
	// extsw r5,r8
	ctx.r5.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r6,r7,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lis r5,-32256
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f7,f12,f9
	ctx.f7.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// beq cr6,0x8247b194
while (ctx.r8.u32 < ctx.r6.u32) {
	loc_8247B180:
		// rlwinm r5,r8,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r11
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// cmplw cr6,r8,r6
		// blt cr6,0x8247b180
}
loc_8247B194:
	// lbz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// lis r8,-32248
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfs f11,-25388(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25388);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f10,27868(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27868);  /* glob:lbl_82006CDC @ 0x82006cdc */
	ctx.f10.f64 = double(temp.f32);
	// lis r8,-32256
	// lfd f1,-8(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// lfd f8,-16(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f13,f1
	ctx.f13.f64 = double(ctx.f1.s64);
	// fcfid f5,f8
	ctx.f5.f64 = double(ctx.f8.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f9,f12,f11
	ctx.f9.f64 = double(float(ctx.f12.f64 - ctx.f11.f64));
	// fsubs f3,f4,f11
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f11.f64));
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f9,27200(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f3,f10
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// stfs f8,4(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// stfs f2,0(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
loc_8247B1EC:
	// lbz r5,1(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// lfs f8,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fadds f12,f7,f0
	ctx.f12.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// std r5,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r5.u64);
	// lfd f5,-8(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fsubs f2,f3,f11
	ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f11.f64));
	// fmuls f13,f2,f10
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// fadds f1,f13,f8
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f8.f64));
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f8,f1,f0
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f5,f8,f9
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f5,1024(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// stfs f13,1028(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1028, temp.u32);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lfs f8,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f4,-16(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f11
	ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f11.f64));
	// fmuls f13,f1,f10
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// fadds f8,f13,f8
	ctx.f8.f64 = double(float(ctx.f13.f64 + ctx.f8.f64));
	// fmuls f5,f12,f13
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f0,f6,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// fmuls f3,f4,f9
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f9.f64));
	// stfs f3,0(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stfs f5,4(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// bne cr6,0x8247b1ec
	if (!ctx.cr6.eq) goto loc_8247B1EC;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r5,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r5.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r8,r7,r8
	ctx.r8.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
	// cmplw cr6,r8,r11
	// blt cr6,0x8247b2a4
	if (ctx.r8.u32 >= ctx.r11.u32) {
		// mr r8,r11
		ctx.r8.u64 = ctx.r11.u64;
	}
loc_8247B2A4:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r6.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r9,r5,30,2,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	// bge cr6,0x8247b2c4
	if (ctx.r9.u32 < ctx.r11.u32) {
		// mr r11,r9
		ctx.r11.u64 = ctx.r9.u64;
	}
loc_8247B2C4:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// lis r11,-32256
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lfs f2,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,27872(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27872);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fctiwz f13,f1
	ctx.f13.s64 = (ctx.f1.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f1.f64));
	// stfiwx f13,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f13.u32);
	// li r9,255
	ctx.r9.s64 = 255;
	// lwz r8,-16(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// subfic r11,r8,128
	ctx.xer.ca = ctx.r8.u32 <= 128;
	ctx.r11.s64 = 128 - ctx.r8.s64;
	// cmpwi cr6,r11,255
	// blt cr6,0x8247b304
	if (ctx.r11.s32 >= 255) {
		// stb r9,0(r10)
		PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
		// b 0x8247b318
	} else {
	loc_8247B304:
		// cmpwi cr6,r11,0
		// bgt cr6,0x8247b314
		if (ctx.r11.s32 <= 0) {
			// stb r4,0(r10)
			PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r4.u8);
			// b 0x8247b318
		} else {
		loc_8247B314:
			// stb r11,0(r10)
			PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r11.u8);
		}
	}
loc_8247B318:
	// lfs f12,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fctiwz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f11.f64));
	// stfiwx f10,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f10.u32);
	// lwz r5,-16(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// subfic r11,r5,128
	ctx.xer.ca = ctx.r5.u32 <= 128;
	ctx.r11.s64 = 128 - ctx.r5.s64;
	// cmpwi cr6,r11,255
	// blt cr6,0x8247b344
	if (ctx.r11.s32 >= 255) {
		// stb r9,4(r10)
		PPC_STORE_U8(ctx.r10.u32 + 4, ctx.r9.u8);
		// blr
		return;
	}
loc_8247B344:
	// cmpwi cr6,r11,0
	// bgt cr6,0x8247b354
	if (ctx.r11.s32 <= 0) {
		// stb r4,4(r10)
		PPC_STORE_U8(ctx.r10.u32 + 4, ctx.r4.u8);
		// blr
		return;
	}
loc_8247B354:
	// stb r11,4(r10)
	PPC_STORE_U8(ctx.r10.u32 + 4, ctx.r11.u8);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_13"))) PPC_WEAK_FUNC(phBoundComposite_13);
PPC_FUNC_IMPL(__imp__phBoundComposite_13) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r11,r3,52
	ctx.r11.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// cmpw cr6,r7,r8
	// blt cr6,0x8247b3b0
	if (ctx.r7.s32 >= ctx.r8.s32) {
		// mr r7,r8
		ctx.r7.u64 = ctx.r8.u64;
	}
loc_8247B3B0:
	// extsw r5,r8
	ctx.r5.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// rlwinm r6,r7,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// std r5,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r5.u64);
	// lis r5,-32256
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f7,f12,f9
	ctx.f7.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// beq cr6,0x8247b40c
while (ctx.r8.u32 < ctx.r6.u32) {
	loc_8247B3F8:
		// rlwinm r5,r8,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r9
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// cmplw cr6,r8,r6
		// blt cr6,0x8247b3f8
}
loc_8247B40C:
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 8);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lbz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 12);
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lis r8,-32248
	// std r5,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r5.u64);
	// lfs f12,-25388(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25388);
	ctx.f12.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f11,27868(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27868);  /* glob:lbl_82006CDC @ 0x82006cdc */
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32256
	// lfd f8,-32(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// fcfid f5,f8
	ctx.f5.f64 = double(ctx.f8.s64);
	// lfd f13,-16(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// lfd f1,-24(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f8,f13
	ctx.f8.f64 = double(ctx.f13.s64);
	// fcfid f10,f1
	ctx.f10.f64 = double(ctx.f1.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// frsp f4,f10
	ctx.f4.f64 = double(float(ctx.f10.f64));
	// fmuls f2,f3,f11
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f2,0(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// frsp f3,f8
	ctx.f3.f64 = double(float(ctx.f8.f64));
	// fsubs f1,f4,f12
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// fsubs f13,f3,f12
	ctx.f13.f64 = double(float(ctx.f3.f64 - ctx.f12.f64));
	// lfd f9,-32(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f5,f9
	ctx.f5.f64 = double(ctx.f9.s64);
	// fmuls f9,f1,f11
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmuls f8,f13,f11
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// stfs f8,8(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// frsp f2,f5
	ctx.f2.f64 = double(float(ctx.f5.f64));
	// fsubs f10,f2,f12
	ctx.f10.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// fmuls f5,f10,f11
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f10,27200(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f10.f64 = double(temp.f32);
	// stfs f5,12(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
loc_8247B4A4:
	// lbz r8,3(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// lfs f8,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// fadds f13,f7,f0
	ctx.f13.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f4,-16(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f12
	ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// fmuls f9,f1,f11
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f9,12(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// fadds f8,f9,f8
	ctx.f8.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f5,f9,f13
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f3,f4,f10
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// stfs f3,3072(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// stfs f5,3076(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3076, temp.u32);
	// lbz r5,2(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// std r5,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r5.u64);
	// lfd f2,-24(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f9,f1
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// fsubs f5,f9,f12
	ctx.f5.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
	// fmuls f9,f5,f11
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f9,8(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f4,f9,f8
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f3,f9,f13
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f1,f2,f10
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f1,2048(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// stfs f3,2052(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2052, temp.u32);
	// lbz r6,1(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lfs f8,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lfd f9,-32(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f5,f9
	ctx.f5.f64 = double(ctx.f9.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// fmuls f9,f3,f11
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f2,f9,f8
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f1,f9,f13
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f9,f2,f0
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// stfs f8,1024(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f1,1028(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lfs f8,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// std r8,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r8.u64);
	// lfd f5,-8(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fsubs f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f12.f64));
	// fmuls f9,f2,f11
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f9,0(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f1,f9,f8
	ctx.f1.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f13,f9,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f9,f1,f0
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fadds f0,f6,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// stfs f8,0(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f13,4(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8247b4a4
	if (!ctx.cr6.eq) goto loc_8247B4A4;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r5,r7,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r6,0
	if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
	// divwu r8,r5,r6
	ctx.r8.u32 = ctx.r6.u32 ? ctx.r5.u32 / ctx.r6.u32 : 0;
	// cmplw cr6,r8,r9
	// blt cr6,0x8247b5d4
	if (ctx.r8.u32 >= ctx.r9.u32) {
		// mr r8,r9
		ctx.r8.u64 = ctx.r9.u64;
	}
loc_8247B5D4:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r6,r7,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r7.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r6,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r9
	// blt cr6,0x8247b5f4
	if (ctx.r10.u32 >= ctx.r9.u32) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_8247B5F4:
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// lis r10,-32256
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r5,r1,-32
	ctx.r5.s64 = ctx.r1.s64 + -32;
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// li r9,255
	ctx.r9.s64 = 255;
	// lfs f0,27872(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27872);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fctiwz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f5.u32);
	// lwz r3,-32(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// subfic r10,r3,128
	ctx.xer.ca = ctx.r3.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r3.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247b634
	if (ctx.r10.s32 >= 255) {
		// stb r9,0(r11)
		PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r9.u8);
		// b 0x8247b648
	} else {
	loc_8247B634:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247b644
		if (ctx.r10.s32 <= 0) {
			// stb r4,0(r11)
			PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r4.u8);
			// b 0x8247b648
		} else {
		loc_8247B644:
			// stb r10,0(r11)
			PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r10.u8);
		}
	}
loc_8247B648:
	// lfs f4,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fctiwz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f3.f64));
	// stfiwx f2,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f2.u32);
	// lwz r7,-32(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// subfic r10,r7,128
	ctx.xer.ca = ctx.r7.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r7.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247b674
	if (ctx.r10.s32 >= 255) {
		// stb r9,4(r11)
		PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r9.u8);
		// b 0x8247b688
	} else {
	loc_8247B674:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247b684
		if (ctx.r10.s32 <= 0) {
			// stb r4,4(r11)
			PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r4.u8);
			// b 0x8247b688
		} else {
		loc_8247B684:
			// stb r10,4(r11)
			PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r10.u8);
		}
	}
loc_8247B688:
	// lfs f1,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// addi r5,r1,-32
	ctx.r5.s64 = ctx.r1.s64 + -32;
	// fmuls f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r5
	PPC_STORE_U32(ctx.r5.u32, ctx.f12.u32);
	// lwz r3,-32(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// subfic r10,r3,128
	ctx.xer.ca = ctx.r3.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r3.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247b6b4
	if (ctx.r10.s32 >= 255) {
		// stb r9,8(r11)
		PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r9.u8);
		// b 0x8247b6c8
	} else {
	loc_8247B6B4:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247b6c4
		if (ctx.r10.s32 <= 0) {
			// stb r4,8(r11)
			PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r4.u8);
			// b 0x8247b6c8
		} else {
		loc_8247B6C4:
			// stb r10,8(r11)
			PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r10.u8);
		}
	}
loc_8247B6C8:
	// lfs f11,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fctiwz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f10.f64));
	// stfiwx f9,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f9.u32);
	// lwz r7,-32(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	// subfic r10,r7,128
	ctx.xer.ca = ctx.r7.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r7.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247b6f4
	if (ctx.r10.s32 >= 255) {
		// stb r9,12(r11)
		PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r9.u8);
		// blr
		return;
	}
loc_8247B6F4:
	// cmpwi cr6,r10,0
	// bgt cr6,0x8247b704
	if (ctx.r10.s32 <= 0) {
		// stb r4,12(r11)
		PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r4.u8);
		// blr
		return;
	}
loc_8247B704:
	// stb r10,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r10.u8);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundComposite_14"))) PPC_WEAK_FUNC(phBoundComposite_14);
PPC_FUNC_IMPL(__imp__phBoundComposite_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r30);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, var_r31);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// addi r11,r3,52
	ctx.r11.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r31,r10,r8
	var_r31 = (uint32_t)(ctx.r8.s64 - ctx.r10.s64);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r9.s64;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r7,r31,1
	ctx.xer.ca = ((int32_t)var_r31 < 0) & ((var_r31 & 0x1) != 0);
	ctx.r7.s64 = (int32_t)var_r31 >> 1;
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// mullw r9,r4,r9
	ctx.r9.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r9.s32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// cmpw cr6,r8,r7
	// blt cr6,0x8247b768
	if (ctx.r8.s32 >= ctx.r7.s32) {
		// mr r8,r7
		ctx.r8.u64 = ctx.r7.u64;
	}
loc_8247B768:
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// std r6,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r6.u64);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r8,r6
	ctx.r5.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rlwinm r6,r5,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r5,-32256
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// lfs f13,16056(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16056);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f7,f12,f9
	ctx.f7.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// beq cr6,0x8247b7cc
while (ctx.r7.u32 < ctx.r6.u32) {
	loc_8247B7B8:
		// rlwinm r5,r7,7,0,24
		ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r5,r9
		// addi r7,r7,1
		ctx.r7.s64 = ctx.r7.s64 + 1;
		// cmplw cr6,r7,r6
		// blt cr6,0x8247b7b8
}
loc_8247B7CC:
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// lbz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 8);
	// lbz r31,16(r11)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 16));
	// lbz r30,20(r11)
	var_r30 = (uint32_t)(PPC_LOAD_U8(ctx.r11.u32 + 20));
	// std r6,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r6.u64);
	// lbz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 12);
	// std r5,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r5.u64);
	// std r7,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r7.u64);
	// lis r7,-32248
	// std r31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, var_r31);
	// std r6,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r6.u64);
	// lfs f12,-25388(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -25388);
	ctx.f12.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f11,27868(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27868);  /* glob:lbl_82006CDC @ 0x82006cdc */
	ctx.f11.f64 = double(temp.f32);
	// lis r7,-32256
	// lfd f8,-64(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// std r30,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, var_r30);
	// fcfid f5,f8
	ctx.f5.f64 = double(ctx.f8.s64);
	// lfd f1,-56(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f13,-48(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f1
	ctx.f10.f64 = double(ctx.f1.s64);
	// fcfid f8,f13
	ctx.f8.f64 = double(ctx.f13.s64);
	// lfd f9,-40(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// lfd f5,-32(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// frsp f13,f10
	ctx.f13.f64 = double(float(ctx.f10.f64));
	// frsp f10,f8
	ctx.f10.f64 = double(float(ctx.f8.f64));
	// fsubs f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// fcfid f4,f9
	ctx.f4.f64 = double(ctx.f9.s64);
	// fmuls f2,f3,f11
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f2,0(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fcfid f2,f5
	ctx.f2.f64 = double(ctx.f5.s64);
	// frsp f9,f4
	ctx.f9.f64 = double(float(ctx.f4.f64));
	// fsubs f4,f13,f12
	ctx.f4.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// frsp f8,f2
	ctx.f8.f64 = double(float(ctx.f2.f64));
	// fsubs f2,f9,f12
	ctx.f2.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
	// lfd f3,-64(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f1,f3
	ctx.f1.f64 = double(ctx.f3.s64);
	// fsubs f3,f10,f12
	ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f12.f64));
	// fmuls f10,f4,f11
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// stfs f10,4(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// lfs f10,27200(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 27200);  /* glob:lbl_82006A40 @ 0x82006a40 */
	ctx.f10.f64 = double(temp.f32);
	// frsp f5,f1
	ctx.f5.f64 = double(float(ctx.f1.f64));
	// fsubs f1,f8,f12
	ctx.f1.f64 = double(float(ctx.f8.f64 - ctx.f12.f64));
	// fmuls f9,f3,f11
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f9,8(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fmuls f8,f2,f11
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f8,12(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// fsubs f13,f5,f12
	ctx.f13.f64 = double(float(ctx.f5.f64 - ctx.f12.f64));
	// fmuls f5,f1,f11
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f5,16(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// stfs f4,20(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
loc_8247B8A4:
	// lbz r7,5(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 5);
	// lfs f8,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fadds f13,f7,f0
	ctx.f13.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// std r7,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r7.u64);
	// lfd f3,-32(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// frsp f1,f2
	ctx.f1.f64 = double(float(ctx.f2.f64));
	// fsubs f9,f1,f12
	ctx.f9.f64 = double(float(ctx.f1.f64 - ctx.f12.f64));
	// fmuls f9,f9,f11
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// stfs f9,20(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 20, temp.u32);
	// fadds f8,f9,f8
	ctx.f8.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f5,f13,f9
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f3,f4,f10
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// stfs f3,5120(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// stfs f5,5124(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5124, temp.u32);
	// lbz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 4);
	// lfs f8,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f8.f64 = double(temp.f32);
	// std r5,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r5.u64);
	// lfd f2,-40(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f9,f1
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// fsubs f5,f9,f12
	ctx.f5.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
	// fmuls f9,f5,f11
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 16, temp.u32);
	// fadds f4,f9,f8
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f3,f13,f9
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f1,f2,f10
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f1,4096(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// stfs f3,4100(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4100, temp.u32);
	// lbz r6,3(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 3);
	// lfs f8,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// std r6,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r6.u64);
	// lfd f9,-48(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f5,f9
	ctx.f5.f64 = double(ctx.f9.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// fmuls f9,f3,f11
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f9,12(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 12, temp.u32);
	// fadds f2,f9,f8
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f1,f13,f9
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f9,f2,f0
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// stfs f8,3072(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// stfs f1,3076(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3076, temp.u32);
	// lbz r7,2(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 2);
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// std r7,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r7.u64);
	// lfd f5,-56(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// fsubs f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f12.f64));
	// fmuls f9,f2,f11
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f9,8(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
	// fadds f1,f9,f8
	ctx.f1.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f9,f13,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f8,f1,f0
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f5,f8,f10
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// stfs f5,2048(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// stfs f9,2052(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2052, temp.u32);
	// lbz r5,1(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 1);
	// lfs f8,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// std r5,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r5.u64);
	// lfd f4,-64(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f12
	ctx.f1.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// fmuls f9,f1,f11
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fadds f8,f9,f8
	ctx.f8.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f4,f13,f9
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f5,f8,f0
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f3,f5,f10
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// stfs f3,1024(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f4,1028(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lfs f8,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// addi r9,r9,6
	ctx.r9.s64 = ctx.r9.s64 + 6;
	// std r6,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r6.u64);
	// lfd f2,-24(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f9,f1
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// fsubs f5,f9,f12
	ctx.f5.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
	// fmuls f9,f5,f11
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f9,0(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fadds f4,f9,f8
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fmuls f3,f13,f9
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fmuls f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f0,f6,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// fmuls f1,f2,f10
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f3,4(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x8247b8a4
	if (!ctx.cr6.eq) goto loc_8247B8A4;
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r5,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r5.s64;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// twllei r8,0
	if (ctx.r8.s32 == 0 || ctx.r8.u32 < 0u) __builtin_trap();
	// divwu r8,r7,r8
	ctx.r8.u32 = ctx.r8.u32 ? ctx.r7.u32 / ctx.r8.u32 : 0;
	// cmplw cr6,r8,r9
	// blt cr6,0x8247ba4c
	if (ctx.r8.u32 >= ctx.r9.u32) {
		// mr r8,r9
		ctx.r8.u64 = ctx.r9.u64;
	}
loc_8247BA4C:
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r10,r5,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r9
	// blt cr6,0x8247ba6c
	if (ctx.r10.u32 >= ctx.r9.u32) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_8247BA6C:
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// lis r10,-32256
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r9,r1,-64
	ctx.r9.s64 = ctx.r1.s64 + -64;
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,27872(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27872);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// stfiwx f11,0,r9
	PPC_STORE_U32(ctx.r9.u32, ctx.f11.u32);
	// li r9,255
	ctx.r9.s64 = 255;
	// lwz r8,-64(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subfic r10,r8,128
	ctx.xer.ca = ctx.r8.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r8.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247baac
	if (ctx.r10.s32 >= 255) {
		// stb r9,0(r11)
		PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r9.u8);
		// b 0x8247bac0
	} else {
	loc_8247BAAC:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247babc
		if (ctx.r10.s32 <= 0) {
			// stb r4,0(r11)
			PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r4.u8);
			// b 0x8247bac0
		} else {
		loc_8247BABC:
			// stb r10,0(r11)
			PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r10.u8);
		}
	}
loc_8247BAC0:
	// lfs f10,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r1,-64
	ctx.r6.s64 = ctx.r1.s64 + -64;
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fctiwz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
	// stfiwx f8,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f8.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subfic r10,r5,128
	ctx.xer.ca = ctx.r5.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r5.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247baec
	if (ctx.r10.s32 >= 255) {
		// stb r9,4(r11)
		PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r9.u8);
		// b 0x8247bb00
	} else {
	loc_8247BAEC:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247bafc
		if (ctx.r10.s32 <= 0) {
			// stb r4,4(r11)
			PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r4.u8);
			// b 0x8247bb00
		} else {
		loc_8247BAFC:
			// stb r10,4(r11)
			PPC_STORE_U8(ctx.r11.u32 + 4, ctx.r10.u8);
		}
	}
loc_8247BB00:
	// lfs f7,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fctiwz f5,f6
	ctx.f5.s64 = (ctx.f6.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f6.f64));
	// stfiwx f5,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f5.u32);
	// lwz r8,-64(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subfic r10,r8,128
	ctx.xer.ca = ctx.r8.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r8.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247bb2c
	if (ctx.r10.s32 >= 255) {
		// stb r9,8(r11)
		PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r9.u8);
		// b 0x8247bb40
	} else {
	loc_8247BB2C:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247bb3c
		if (ctx.r10.s32 <= 0) {
			// stb r4,8(r11)
			PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r4.u8);
			// b 0x8247bb40
		} else {
		loc_8247BB3C:
			// stb r10,8(r11)
			PPC_STORE_U8(ctx.r11.u32 + 8, ctx.r10.u8);
		}
	}
loc_8247BB40:
	// lfs f4,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// addi r6,r1,-64
	ctx.r6.s64 = ctx.r1.s64 + -64;
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fctiwz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f3.f64));
	// stfiwx f2,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f2.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subfic r10,r5,128
	ctx.xer.ca = ctx.r5.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r5.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247bb6c
	if (ctx.r10.s32 >= 255) {
		// stb r9,12(r11)
		PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r9.u8);
		// b 0x8247bb80
	} else {
	loc_8247BB6C:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247bb7c
		if (ctx.r10.s32 <= 0) {
			// stb r4,12(r11)
			PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r4.u8);
			// b 0x8247bb80
		} else {
		loc_8247BB7C:
			// stb r10,12(r11)
			PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r10.u8);
		}
	}
loc_8247BB80:
	// lfs f1,16(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// addi r10,r1,-64
	ctx.r10.s64 = ctx.r1.s64 + -64;
	// fmuls f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fctiwz f12,f13
	ctx.f12.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f13.f64));
	// stfiwx f12,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f12.u32);
	// lwz r8,-64(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subfic r10,r8,128
	ctx.xer.ca = ctx.r8.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r8.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247bbac
	if (ctx.r10.s32 >= 255) {
		// stb r9,16(r11)
		PPC_STORE_U8(ctx.r11.u32 + 16, ctx.r9.u8);
		// b 0x8247bbc0
	} else {
	loc_8247BBAC:
		// cmpwi cr6,r10,0
		// bgt cr6,0x8247bbbc
		if (ctx.r10.s32 <= 0) {
			// stb r4,16(r11)
			PPC_STORE_U8(ctx.r11.u32 + 16, ctx.r4.u8);
			// b 0x8247bbc0
		} else {
		loc_8247BBBC:
			// stb r10,16(r11)
			PPC_STORE_U8(ctx.r11.u32 + 16, ctx.r10.u8);
		}
	}
loc_8247BBC0:
	// lfs f11,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r1,-64
	ctx.r6.s64 = ctx.r1.s64 + -64;
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fctiwz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f10.f64));
	// stfiwx f9,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f9.u32);
	// lwz r5,-64(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// subfic r10,r5,128
	ctx.xer.ca = ctx.r5.u32 <= 128;
	ctx.r10.s64 = 128 - ctx.r5.s64;
	// cmpwi cr6,r10,255
	// blt cr6,0x8247bbf4
	if (ctx.r10.s32 >= 255) {
		// stb r9,20(r11)
		PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r9.u8);
		// ld r30,-16(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// ld r31,-8(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
		// blr
		return;
	}
loc_8247BBF4:
	// cmpwi cr6,r10,0
	// bgt cr6,0x8247bc0c
	if (ctx.r10.s32 <= 0) {
		// stb r4,20(r11)
		PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r4.u8);
		// ld r30,-16(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// ld r31,-8(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
		// blr
		return;
	}
loc_8247BC0C:
	// stb r10,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r10.u8);
	// ld r30,-16(r1)
	var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// ld r31,-8(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -8));
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_1"))) PPC_WEAK_FUNC(phBoundBVH_1);
PPC_FUNC_IMPL(__imp__phBoundBVH_1) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,30
	ctx.r10.u64 = ctx.r6.u32 & 0x3;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247bce4
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247bcc0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247BCB8, "bso");
		// ble cr6,0x8247bce4
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473488
			phBound_1(ctx, base);
			// blr
			return;
		}
	loc_8247BCC0:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247bce4
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473488
			phBound_1(ctx, base);
			// blr
			return;
		}
		// bl 0x82470e80
		phBoundBVH_0E80_w(ctx, base);
		// blr
		return;
	}
loc_8247BCE4:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82473488
	phBound_1(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_2"))) PPC_WEAK_FUNC(phBoundBVH_2);
PPC_FUNC_IMPL(__imp__phBoundBVH_2) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,30
	ctx.r10.u64 = ctx.r6.u32 & 0x3;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247bdc4
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247bda0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247BD98, "bso");
		// ble cr6,0x8247bdc4
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473790
			phBound_2(ctx, base);
			// blr
			return;
		}
	loc_8247BDA0:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247bdc4
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473790
			phBound_2(ctx, base);
			// blr
			return;
		}
		// bl 0x824710c0
		phBoundBVH_10C0_w(ctx, base);
		// blr
		return;
	}
loc_8247BDC4:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82473790
	phBound_2(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_3"))) PPC_WEAK_FUNC(phBoundBVH_3);
PPC_FUNC_IMPL(__imp__phBoundBVH_3) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,30
	ctx.r10.u64 = ctx.r6.u32 & 0x3;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247bea4
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247be80
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247BE78, "bso");
		// ble cr6,0x8247bea4
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473b08
			phBound_3(ctx, base);
			// blr
			return;
		}
	loc_8247BE80:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247bea4
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473b08
			phBound_3(ctx, base);
			// blr
			return;
		}
		// bl 0x82471340
		phBoundBVH_1340_w(ctx, base);
		// blr
		return;
	}
loc_8247BEA4:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82473b08
	phBound_3(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_4"))) PPC_WEAK_FUNC(phBoundBVH_4);
PPC_FUNC_IMPL(__imp__phBoundBVH_4) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,30
	ctx.r10.u64 = ctx.r6.u32 & 0x3;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247bf84
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247bf60
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247BF58, "bso");
		// ble cr6,0x8247bf84
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473f98
			phBound_4(ctx, base);
			// blr
			return;
		}
	loc_8247BF60:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247bf84
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82473f98
			phBound_4(ctx, base);
			// blr
			return;
		}
		// bl 0x82471640
		phBoundBVH_1640_w(ctx, base);
		// blr
		return;
	}
loc_8247BF84:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82473f98
	phBound_4(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_6"))) PPC_WEAK_FUNC(phBoundBVH_6);
PPC_FUNC_IMPL(__imp__phBoundBVH_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,29
	ctx.r10.u64 = ctx.r6.u32 & 0x7;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247c064
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c040
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C038, "bso");
		// ble cr6,0x8247c064
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82474a20
			phBound_6(ctx, base);
			// blr
			return;
		}
	loc_8247C040:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c064
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82474a20
			phBound_6(ctx, base);
			// blr
			return;
		}
		// bl 0x8246fd88
		phBoundBVH_FD88_w(ctx, base);
		// blr
		return;
	}
loc_8247C064:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82474a20
	phBound_6(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_7"))) PPC_WEAK_FUNC(phBoundBVH_7);
PPC_FUNC_IMPL(__imp__phBoundBVH_7) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,29
	ctx.r10.u64 = ctx.r6.u32 & 0x7;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247c144
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c120
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C118, "bso");
		// ble cr6,0x8247c144
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82474d60
			phBound_7(ctx, base);
			// blr
			return;
		}
	loc_8247C120:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c144
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82474d60
			phBound_7(ctx, base);
			// blr
			return;
		}
		// bl 0x82470048
		phBoundBVH_0048_wrh(ctx, base);
		// blr
		return;
	}
loc_8247C144:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82474d60
	phBound_7(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_8"))) PPC_WEAK_FUNC(phBoundBVH_8);
PPC_FUNC_IMPL(__imp__phBoundBVH_8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,29
	ctx.r10.u64 = ctx.r6.u32 & 0x7;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247c224
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c200
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C1F8, "bso");
		// ble cr6,0x8247c224
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82475150
			phBound_8(ctx, base);
			// blr
			return;
		}
	loc_8247C200:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c224
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82475150
			phBound_8(ctx, base);
			// blr
			return;
		}
		// bl 0x82470378
		phBoundBVH_0378_w(ctx, base);
		// blr
		return;
	}
loc_8247C224:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82475150
	phBound_8(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_9"))) PPC_WEAK_FUNC(phBoundBVH_9);
PPC_FUNC_IMPL(__imp__phBoundBVH_9) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// subf r4,r10,r3
	ctx.r4.s64 = ctx.r3.s64 - ctx.r10.s64;
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// clrlwi r10,r6,29
	ctx.r10.u64 = ctx.r6.u32 & 0x7;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// or r9,r10,r5
	ctx.r9.u64 = ctx.r10.u64 | ctx.r5.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// clrlwi r8,r9,28
	ctx.r8.u64 = ctx.r9.u32 & 0xF;
	// cmpwi cr6,r8,0
	// bne cr6,0x8247c304
	if (ctx.r8.s32 == 0) {
		// extsw r11,r6
		ctx.r11.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r10,r4,-1
		ctx.r10.s64 = ctx.r4.s64 + -1;
		// extsw r9,r10
		ctx.r9.s64 = ctx.r10.s32;
		// std r11,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
		// std r9,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c2e0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C2D8, "bso");
		// ble cr6,0x8247c304
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x824756c0
			phBound_9(ctx, base);
			// blr
			return;
		}
	loc_8247C2E0:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c304
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x824756c0
			phBound_9(ctx, base);
			// blr
			return;
		}
		// bl 0x82470788
		phBoundBVH_0788_w(ctx, base);
		// blr
		return;
	}
loc_8247C304:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x824756c0
	phBound_9(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_11"))) PPC_WEAK_FUNC(phBoundBVH_11);
PPC_FUNC_IMPL(__imp__phBoundBVH_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r31,4(r7)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 4));
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// or r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 | ctx.r5.u64;
	// subf r4,r10,r31
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 - ctx.r10.s64;
	// clrlwi r10,r11,28
	ctx.r10.u64 = ctx.r11.u32 & 0xF;
	// cmpwi cr6,r10,0
	// bne cr6,0x8247c3e4
	if (ctx.r10.s32 == 0) {
		// extsw r9,r6
		ctx.r9.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r8,r4,-1
		ctx.r8.s64 = ctx.r4.s64 + -1;
		// extsw r11,r8
		ctx.r11.s64 = ctx.r8.s32;
		// std r9,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
		// std r11,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c3bc
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C3B4, "bso");
		// ble cr6,0x8247c3e4
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82476260
			phBound_11(ctx, base);
			// blr
			return;
		}
	loc_8247C3BC:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c3e4
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82476260
			phBound_11(ctx, base);
			// blr
			return;
		}
		// bl 0x824719c8
		phBoundBVH_19C8_w(ctx, base);
		// blr
		return;
	}
loc_8247C3E4:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82476260
	phBound_11(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_12"))) PPC_WEAK_FUNC(phBoundBVH_12);
PPC_FUNC_IMPL(__imp__phBoundBVH_12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r31,4(r7)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 4));
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// or r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 | ctx.r5.u64;
	// subf r4,r10,r31
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 - ctx.r10.s64;
	// clrlwi r10,r11,28
	ctx.r10.u64 = ctx.r11.u32 & 0xF;
	// cmpwi cr6,r10,0
	// bne cr6,0x8247c4c4
	if (ctx.r10.s32 == 0) {
		// extsw r9,r6
		ctx.r9.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r8,r4,-1
		ctx.r8.s64 = ctx.r4.s64 + -1;
		// extsw r11,r8
		ctx.r11.s64 = ctx.r8.s32;
		// std r9,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
		// std r11,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c49c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C494, "bso");
		// ble cr6,0x8247c4c4
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x824765b0
			phBound_12(ctx, base);
			// blr
			return;
		}
	loc_8247C49C:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c4c4
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x824765b0
			phBound_12(ctx, base);
			// blr
			return;
		}
		// bl 0x82471d60
		phBoundBVH_1D60_w(ctx, base);
		// blr
		return;
	}
loc_8247C4C4:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x824765b0
	phBound_12(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_13"))) PPC_WEAK_FUNC(phBoundBVH_13);
PPC_FUNC_IMPL(__imp__phBoundBVH_13) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r31,4(r7)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 4));
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// or r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 | ctx.r5.u64;
	// subf r4,r10,r31
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 - ctx.r10.s64;
	// clrlwi r10,r11,28
	ctx.r10.u64 = ctx.r11.u32 & 0xF;
	// cmpwi cr6,r10,0
	// bne cr6,0x8247c5a4
	if (ctx.r10.s32 == 0) {
		// extsw r9,r6
		ctx.r9.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r8,r4,-1
		ctx.r8.s64 = ctx.r4.s64 + -1;
		// extsw r11,r8
		ctx.r11.s64 = ctx.r8.s32;
		// std r9,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
		// std r11,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c57c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C574, "bso");
		// ble cr6,0x8247c5a4
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x824769f0
			phBound_13(ctx, base);
			// blr
			return;
		}
	loc_8247C57C:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c5a4
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x824769f0
			phBound_13(ctx, base);
			// blr
			return;
		}
		// bl 0x824721e8
		phBoundBVH_21E8_w(ctx, base);
		// blr
		return;
	}
loc_8247C5A4:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x824769f0
	phBound_13(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phBoundBVH_14"))) PPC_WEAK_FUNC(phBoundBVH_14);
PPC_FUNC_IMPL(__imp__phBoundBVH_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=112, manual
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lwz r11,28(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 28);
	// lfs f13,48(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,24(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r10,8(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r4,13(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 13);
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// lwz r9,20(r7)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mullw r11,r4,r10
	ctx.r11.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r31,4(r7)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r7.u32 + 4));
	// add r5,r5,r9
	ctx.r5.u64 = ctx.r5.u64 + ctx.r9.u64;
	// add r3,r11,r8
	ctx.r3.u64 = ctx.r11.u64 + ctx.r8.u64;
	// or r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 | ctx.r5.u64;
	// subf r4,r10,r31
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 - ctx.r10.s64;
	// clrlwi r10,r11,28
	ctx.r10.u64 = ctx.r11.u32 & 0xF;
	// cmpwi cr6,r10,0
	// bne cr6,0x8247c684
	if (ctx.r10.s32 == 0) {
		// extsw r9,r6
		ctx.r9.s64 = ctx.r6.s32;
		// lfs f0,44(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
		ctx.f0.f64 = double(temp.f32);
		// addi r8,r4,-1
		ctx.r8.s64 = ctx.r4.s64 + -1;
		// extsw r11,r8
		ctx.r11.s64 = ctx.r8.s32;
		// std r9,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
		// std r11,88(r1)
		PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// lfd f10,88(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f6,f0,f8,f13
		ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f13.f64));
		// fcmpu cr6,f7,f6
		// bso cr6,0x8247c65c
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x8247C654, "bso");
		// ble cr6,0x8247c684
		if (ctx.f7.f64 <= ctx.f6.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82476fd8
			phBound_14(ctx, base);
			// blr
			return;
		}
	loc_8247C65C:
		// lis r11,-32256
		ctx.r11.s64 = -2113929216;
		// lfs f0,15784(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// blt cr6,0x8247c684
		if (ctx.f13.f64 < ctx.f0.f64) {
			// mr r3,r7
			ctx.r3.u64 = ctx.r7.u64;
			// bl 0x82476fd8
			phBound_14(ctx, base);
			// blr
			return;
		}
		// bl 0x82472820
		phBoundBVH_2820_w(ctx, base);
		// blr
		return;
	}
loc_8247C684:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82476fd8
	phBound_14(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phWorld_1"))) PPC_WEAK_FUNC(phWorld_1);
PPC_FUNC_IMPL(__imp__phWorld_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r31,13(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// lwz r30,24(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// mullw r11,r31,r11
	ctx.r11.s64 = int64_t((int32_t)var_r31) * int64_t(ctx.r11.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r29,20(r3)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// subf r8,r10,r30
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 - ctx.r10.s64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r8
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r7,r10,r29
	ctx.r7.u64 = ctx.r10.u64 + var_r29;
	// blt cr6,0x8247c6f0
	if (ctx.r9.s32 >= ctx.r8.s32) {
		// mr r9,r8
		ctx.r9.u64 = ctx.r8.u64;
	}
loc_8247C6F0:
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// or r6,r10,r7
	ctx.r6.u64 = ctx.r10.u64 | ctx.r7.u64;
	// or r10,r6,r11
	ctx.r10.u64 = ctx.r6.u64 | ctx.r11.u64;
	// clrlwi r6,r10,28
	ctx.r6.u64 = ctx.r10.u32 & 0xF;
	// cmpwi cr6,r6,0
	// beq cr6,0x8247c714
	if (ctx.r6.s32 != 0) {
		// bl 0x82477928
		phBoundGeometry_1(ctx, base);
		return;
	}
loc_8247C714:
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8247c740
while (ctx.r10.u32 < ctx.r6.u32) {
	loc_8247C72C:
		// rlwinm r28,r10,7,0,24
		var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80);
		// dcbt r28,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r6
		// blt cr6,0x8247c72c
}
loc_8247C740:
	// extsw r10,r8
	ctx.r10.s64 = ctx.r8.s32;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// addi r27,r1,96
	var_r27 = (uint32_t)(ctx.r1.s64 + 96);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r10,-32256
	// stvx v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r28,r1,80
	var_r28 = (uint32_t)(ctx.r1.s64 + 80);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r27,r1,80
	var_r27 = (uint32_t)(ctx.r1.s64 + 80);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,22772(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22772);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32256
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v13,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v13,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v12,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v9,v13,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v12,v12,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v0,v0,v12
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// ble cr6,0x8247c838
	if (ctx.cr6.gt) {
		// addi r6,r9,-1
		ctx.r6.s64 = ctx.r9.s64 + -1;
		// rlwinm r10,r6,29,3,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
	loc_8247C7F8:
		// vaddfp v10,v0,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// vor v8,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r7,16
		ctx.r8.s64 = ctx.r7.s64 + 16;
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// addi r11,r11,32
		ctx.r11.s64 = ctx.r11.s64 + 32;
		// lvx128 v11,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v7,v12,v8
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
		// cmplwi cr6,r10,0
		// vmulfp128 v6,v11,v10
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v10.f32)));
		// stvx v7,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r7,32
		ctx.r7.s64 = ctx.r7.s64 + 32;
		// stvx v6,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247c7f8
		if (ctx.r10.u32 != 0) goto loc_8247C7F8;
	}
loc_8247C838:
	// subf r6,r4,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r4.s64;
	// rlwinm r4,r31,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
	// divwu r11,r6,r4
	ctx.r11.u32 = ctx.r4.u32 ? ctx.r6.u32 / ctx.r4.u32 : 0;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// cmplw cr6,r11,r5
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247c858
	if (ctx.r11.u32 >= ctx.r5.u32) {
		// mr r10,r5
		ctx.r10.u64 = ctx.r5.u64;
	}
loc_8247C858:
	// subf r11,r29,r7
	ctx.r11.s64 = ctx.r7.s64 - (int64_t)(int32_t)var_r29;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r11,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r30
	// blt cr6,0x8247c870
	if (ctx.r11.u32 >= var_r30) {
		// mr r11,r30
		ctx.r11.u64 = var_r30;
	}
loc_8247C870:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phWorld_2"))) PPC_WEAK_FUNC(phWorld_2);
PPC_FUNC_IMPL(__imp__phWorld_2) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r31,13(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// lwz r30,24(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// mullw r11,r31,r11
	ctx.r11.s64 = int64_t((int32_t)var_r31) * int64_t(ctx.r11.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r29,20(r3)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// subf r7,r10,r30
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 - ctx.r10.s64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r7
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r8,r10,r29
	ctx.r8.u64 = ctx.r10.u64 + var_r29;
	// blt cr6,0x8247c8d0
	if (ctx.r9.s32 >= ctx.r7.s32) {
		// mr r9,r7
		ctx.r9.u64 = ctx.r7.u64;
	}
loc_8247C8D0:
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// or r6,r10,r8
	ctx.r6.u64 = ctx.r10.u64 | ctx.r8.u64;
	// or r10,r6,r11
	ctx.r10.u64 = ctx.r6.u64 | ctx.r11.u64;
	// clrlwi r6,r10,28
	ctx.r6.u64 = ctx.r10.u32 & 0xF;
	// cmpwi cr6,r6,0
	// beq cr6,0x8247c8f4
	if (ctx.r6.s32 != 0) {
		// bl 0x82477a38
		phBoundGeometry_2(ctx, base);
		return;
	}
loc_8247C8F4:
	// rlwinm r6,r9,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8247c920
while (ctx.r10.u32 < ctx.r6.u32) {
	loc_8247C90C:
		// rlwinm r28,r10,7,0,24
		var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80);
		// dcbt r28,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r6
		// blt cr6,0x8247c90c
}
loc_8247C920:
	// extsw r10,r7
	ctx.r10.s64 = ctx.r7.s32;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// addi r27,r1,96
	var_r27 = (uint32_t)(ctx.r1.s64 + 96);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r10,-32256
	// stvx v0,r0,r27
	ea = (var_r27) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r28,r1,80
	var_r28 = (uint32_t)(ctx.r1.s64 + 80);
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r27,r1,80
	var_r27 = (uint32_t)(ctx.r1.s64 + 80);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,22772(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22772);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32256
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v13,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v5,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v5.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v12,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v4,v5,v5
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v13,v13,v11
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// ble cr6,0x8247ca6c
	if (ctx.cr6.gt) {
		// addi r6,r9,-1
		ctx.r6.s64 = ctx.r9.s64 + -1;
		// lis r9,-32256
		// lis r7,-32256
		// addi r9,r9,27904
		ctx.r9.s64 = ctx.r9.s64 + 27904;
		// addi r7,r7,27888
		ctx.r7.s64 = ctx.r7.s64 + 27888;
		// rlwinm r10,r6,29,3,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// lvx128 v6,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v7,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	loc_8247C9F0:
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// vaddfp v9,v0,v5
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v5.f32)));
		// addi r7,r11,32
		ctx.r7.s64 = ctx.r11.s64 + 32;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r11,48
		ctx.r6.s64 = ctx.r11.s64 + 48;
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vor v2,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r28,r8,1024
		var_r28 = (uint32_t)(ctx.r8.s64 + 1024);  // addr:0x82000400
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// vaddfp v0,v0,v4
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v4.f32)));
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r9,r8,16
		ctx.r9.s64 = ctx.r8.s64 + 16;
		// lvx128 v11,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v8,v13,v12,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// lvx128 v10,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v13,v13,v12,v6
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// vperm v12,v11,v10,v7
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r7,r8,1040
		ctx.r7.s64 = ctx.r8.s64 + 1040;
		// vperm v11,v11,v10,v6
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// addi r11,r11,64
		ctx.r11.s64 = ctx.r11.s64 + 64;
		// vmulfp128 v1,v8,v3
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v3.f32)));
		// cmplwi cr6,r10,0
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
		// vmulfp128 v31,v13,v2
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vmulfp128 v30,v12,v9
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vmulfp128 v29,v11,v9
		simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
		// stvx v1,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r8,32
		ctx.r8.s64 = ctx.r8.s64 + 32;
		// stvx v31,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v30,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v29,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247c9f0
		if (!ctx.cr6.eq) goto loc_8247C9F0;
	}
loc_8247CA6C:
	// subf r6,r4,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r4.s64;
	// rlwinm r4,r31,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 2) & 0xFFFFFFFC;
	// divwu r11,r6,r4
	ctx.r11.u32 = ctx.r4.u32 ? ctx.r6.u32 / ctx.r4.u32 : 0;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// cmplw cr6,r11,r5
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247ca8c
	if (ctx.r11.u32 >= ctx.r5.u32) {
		// mr r10,r5
		ctx.r10.u64 = ctx.r5.u64;
	}
loc_8247CA8C:
	// subf r11,r29,r8
	ctx.r11.s64 = ctx.r8.s64 - (int64_t)(int32_t)var_r29;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r11,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r30
	// blt cr6,0x8247caa4
	if (ctx.r11.u32 >= var_r30) {
		// mr r11,r30
		ctx.r11.u64 = var_r30;
	}
loc_8247CAA4:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phWorld_4"))) PPC_WEAK_FUNC(phWorld_4);
PPC_FUNC_IMPL(__imp__phWorld_4) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=176, savegprlr_25
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r30,4(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 4));
	// lbz r28,13(r3)
	var_r28 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// subf r8,r11,r30
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 - ctx.r11.s64;
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r11,r28,r11
	ctx.r11.s64 = int64_t((int32_t)var_r28) * int64_t(ctx.r11.s32);
	// lwz r27,24(r3)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// lwz r26,20(r3)
	var_r26 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// lwz r29,0(r3)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r10,r27
	ctx.r6.s64 = (int64_t)(int32_t)var_r27 - ctx.r10.s64;
	// add r7,r11,r26
	ctx.r7.u64 = ctx.r11.u64 + var_r26;
	// add r9,r9,r29
	ctx.r9.u64 = ctx.r9.u64 + var_r29;
	// cmpw cr6,r8,r6
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// blt cr6,0x8247cb04
	if (ctx.r8.s32 >= ctx.r6.s32) {
		// mr r11,r6
		ctx.r11.u64 = ctx.r6.u64;
	}
loc_8247CB04:
	// clrlwi r10,r11,29
	ctx.r10.u64 = ctx.r11.u32 & 0x7;
	// or r8,r10,r7
	ctx.r8.u64 = ctx.r10.u64 | ctx.r7.u64;
	// or r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 | ctx.r9.u64;
	// clrlwi r4,r5,28
	ctx.r4.u64 = ctx.r5.u32 & 0xF;
	// cmpwi cr6,r4,0
	// beq cr6,0x8247cb28
	if (ctx.r4.s32 != 0) {
		// bl 0x82477c90
		phBoundGeometry_4(ctx, base);
		return;
	}
loc_8247CB28:
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// add r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r5,r8,127
	ctx.r5.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r5,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8247cb5c
while (ctx.r10.u32 < ctx.r8.u32) {
	loc_8247CB48:
		// rlwinm r4,r10,7,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r4,r9
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r8
		// blt cr6,0x8247cb48
}
loc_8247CB5C:
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// extsw r5,r8
	ctx.r5.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r10,-32256
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// std r5,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r5.u64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,22772(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22772);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32256
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v29,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v29.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vaddfp v28,v29,v29
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v28.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v29.f32)));
	// vmulfp128 v13,v13,v11
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// ble cr6,0x8247cdb0
	if (ctx.cr6.gt) {
		// addi r6,r11,-1
		ctx.r6.s64 = ctx.r11.s64 + -1;
		// lis r5,-32256
		// rlwinm r8,r6,29,3,31
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
		// lis r6,-32256
		// lis r4,-32256
		// lis r31,-32256
		var_r31 = (uint32_t)(-2113929216);
		// addi r6,r6,27920
		ctx.r6.s64 = ctx.r6.s64 + 27920;
		// addi r5,r5,27904
		ctx.r5.s64 = ctx.r5.s64 + 27904;
		// addi r4,r4,27888
		ctx.r4.s64 = ctx.r4.s64 + 27888;
		// addi r31,r31,27936
		var_r31 = (uint32_t)(var_r31 + 27936);
		// addi r10,r7,2064
		ctx.r10.s64 = ctx.r7.s64 + 2064;
		// lvx128 v4,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r9,128
		ctx.r11.s64 = ctx.r9.s64 + 128;
		// lvx128 v5,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// lvx128 v6,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v7,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	loc_8247CC4C:
		// addi r5,r11,-112
		ctx.r5.s64 = ctx.r11.s64 + -112;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r11,-80
		ctx.r4.s64 = ctx.r11.s64 + -80;
		// addi r6,r11,-64
		ctx.r6.s64 = ctx.r11.s64 + -64;
		// addi r31,r11,-96
		var_r31 = (uint32_t)(ctx.r11.s64 + -96);
		// addi r25,r11,-48
		var_r25 = (uint32_t)(ctx.r11.s64 + -48);
		// lvx128 v12,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r11,-32
		ctx.r5.s64 = ctx.r11.s64 + -32;
		// lvx128 v11,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v3,v13,v12,v7
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// lvx128 v10,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r4,r11,-16
		ctx.r4.s64 = ctx.r11.s64 + -16;
		// lvx128 v9,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v2,v11,v10,v7
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// lvx128 v8,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v13,v13,v9,v4
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
		// vperm v11,v11,v8,v4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
		// addi r6,r11,16
		ctx.r6.s64 = ctx.r11.s64 + 16;
		// vperm v12,v12,v9,v7
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r31,r11,32
		var_r31 = (uint32_t)(ctx.r11.s64 + 32);  // addr:0x82000020
		// vperm v9,v3,v2,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// addi r25,r11,48
		var_r25 = (uint32_t)(ctx.r11.s64 + 48);  // addr:0x82000030
		// vperm v10,v10,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// vperm v1,v13,v11,v6
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// addi r9,r9,192
		ctx.r9.s64 = ctx.r9.s64 + 192;
		// vperm v2,v3,v2,v5
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// cmplwi cr6,r8,0
		ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
		// vperm v13,v13,v11,v5
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// lvx128 v11,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v31,v12,v10,v6
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// addi r5,r10,-1040
		ctx.r5.s64 = ctx.r10.s64 + -1040;
		// vperm v30,v12,v10,v5
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// vmulfp128 v12,v9,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v2,v2,v0
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvx128 v10,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v1,v1,v0
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r4,r10,-16
		ctx.r4.s64 = ctx.r10.s64 + -16;
		// lvx128 v9,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v27,v13,v0
		simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvx128 v8,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v13,v0,v29
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v29.f32)));
		// lvx128 v3,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v26,v31,v0
		simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vmulfp128 v25,v30,v0
		simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r6,r10,1008
		ctx.r6.s64 = ctx.r10.s64 + 1008;
		// addi r31,r10,2032
		var_r31 = (uint32_t)(ctx.r10.s64 + 2032);  // __imp_KfLowerIrql @ 0x820007f0
		// vaddfp v0,v0,v28
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v28.f32)));
		// addi r25,r10,3056
		var_r25 = (uint32_t)(ctx.r10.s64 + 3056);  // addr:0x82000bf0
		// stvx v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r7,32
		ctx.r7.s64 = ctx.r7.s64 + 32;
		// stvx v2,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v2,v11,v10,v7
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// stvx v1,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v1,v9,v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// lvx128 v12,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v9,v9,v3,v4
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
		// vperm v11,v11,v12,v4
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
		// addi r5,r10,-2048
		ctx.r5.s64 = ctx.r10.s64 + -2048;
		// addi r4,r10,-1024
		ctx.r4.s64 = ctx.r10.s64 + -1024;
		// vperm v12,v10,v12,v7
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vperm v31,v2,v1,v6
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// stvx v27,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v2,v2,v1,v5
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// stvx v26,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v1,v11,v9,v6
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// stvx v25,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v10,v8,v3,v7
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v8.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r6,r10,1024
		ctx.r6.s64 = ctx.r10.s64 + 1024;
		// vmulfp128 v23,v31,v13
		simde_mm_store_ps(ctx.v23.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
		// addi r31,r10,2048
		var_r31 = (uint32_t)(ctx.r10.s64 + 2048);  // __imp_KeTryToAcquireSpinLockAtRaisedIrql @ 0x82000800
		// vmulfp128 v22,v2,v13
		simde_mm_store_ps(ctx.v22.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v13.f32)));
		// addi r25,r10,3072
		var_r25 = (uint32_t)(ctx.r10.s64 + 3072);  // addr:0x82000c00
		// vmulfp128 v24,v1,v13
		simde_mm_store_ps(ctx.v24.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vperm v11,v11,v9,v5
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v9.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// addi r11,r11,192
		ctx.r11.s64 = ctx.r11.s64 + 192;
		// vperm v9,v12,v10,v6
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// stvx v23,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v22,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v24,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r10,32
		ctx.r10.s64 = ctx.r10.s64 + 32;
		// vperm v12,v12,v10,v5
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// vmulfp128 v21,v11,v13
		simde_mm_store_ps(ctx.v21.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v20,v9,v13
		simde_mm_store_ps(ctx.v20.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v19,v12,v13
		simde_mm_store_ps(ctx.v19.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v21,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v20,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v19,r0,r25
		ea = (var_r25) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247cc4c
		if (!ctx.cr6.eq) goto loc_8247CC4C;
	}
loc_8247CDB0:
	// rlwinm r4,r28,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r29,r9
	ctx.r5.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r29;
	// twllei r4,0
	if (ctx.r4.s32 == 0 || ctx.r4.u32 < 0u) __builtin_trap();
	// divwu r11,r5,r4
	ctx.r11.u32 = ctx.r4.u32 ? ctx.r5.u32 / ctx.r4.u32 : 0;
	// cmplw cr6,r11,r30
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247cdd0
	if (ctx.r11.u32 >= var_r30) {
		// mr r10,r30
		ctx.r10.u64 = var_r30;
	}
loc_8247CDD0:
	// subf r11,r26,r7
	ctx.r11.s64 = ctx.r7.s64 - (int64_t)(int32_t)var_r26;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r11,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r27
	// blt cr6,0x8247cde8
	if (ctx.r11.u32 >= var_r27) {
		// mr r11,r27
		ctx.r11.u64 = var_r27;
	}
loc_8247CDE8:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phWorld_6"))) PPC_WEAK_FUNC(phWorld_6);
PPC_FUNC_IMPL(__imp__phWorld_6) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r30,13(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r9,r11,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r11.s64;
	// lwz r29,24(r3)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// mullw r11,r30,r11
	ctx.r11.s64 = int64_t((int32_t)var_r30) * int64_t(ctx.r11.s32);
	// lwz r31,0(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
	// lwz r28,20(r3)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// subf r7,r10,r29
	ctx.r7.s64 = (int64_t)(int32_t)var_r29 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r7
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + var_r31;
	// add r8,r10,r28
	ctx.r8.u64 = ctx.r10.u64 + var_r28;
	// blt cr6,0x8247ce48
	if (ctx.r9.s32 >= ctx.r7.s32) {
		// mr r9,r7
		ctx.r9.u64 = ctx.r7.u64;
	}
loc_8247CE48:
	// or r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 | ctx.r8.u64;
	// or r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 | ctx.r11.u64;
	// clrlwi r5,r6,28
	ctx.r5.u64 = ctx.r6.u32 & 0xF;
	// cmpwi cr6,r5,0
	// beq cr6,0x8247ce68
	if (ctx.r5.s32 != 0) {
		// bl 0x82477ff8
		phBoundGeometry_6(ctx, base);
		return;
	}
loc_8247CE68:
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r6,r5,127
	ctx.r6.s64 = ctx.r5.s64 + 127;
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8247ce94
while (ctx.r10.u32 < ctx.r6.u32) {
	loc_8247CE80:
		// rlwinm r27,r10,7,0,24
		var_r27 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80);
		// dcbt r27,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r6
		// blt cr6,0x8247ce80
}
loc_8247CE94:
	// extsw r10,r7
	ctx.r10.s64 = ctx.r7.s32;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// extsw r7,r5
	ctx.r7.s64 = ctx.r5.s32;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r27,r1,80
	var_r27 = (uint32_t)(ctx.r1.s64 + 80);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,22700(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22700);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// addi r10,r10,28112
	ctx.r10.s64 = ctx.r10.s64 + 28112;
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r5
	temp.u32 = ctx.r5.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v11,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v0,v11,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.u32), 0xFF));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// vmulfp128 v11,v0,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// lvx128 v9,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v10,v0,v9
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vaddfp v9,v12,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v0,v13,v11
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v13,v13,v10
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// ble cr6,0x8247cfe8
	if (ctx.cr6.gt) {
		// addi r6,r9,-1
		ctx.r6.s64 = ctx.r9.s64 + -1;
		// rlwinm r10,r6,28,4,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 28) & 0xFFFFFFF;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
	loc_8247CF64:
		// addi r5,r11,16
		ctx.r5.s64 = ctx.r11.s64 + 16;
		// lvx128 v11,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupkhsh v6,v11
		simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vaddfp v8,v0,v12
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vupklsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
		// vaddfp v7,v13,v12
		simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vor v4,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// addi r9,r8,16
		ctx.r9.s64 = ctx.r8.s64 + 16;
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r7,r8,32
		ctx.r7.s64 = ctx.r8.s64 + 32;
		// lvx128 v10,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vcfsx v6,v6,15
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupkhsh v5,v10
		simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// vcfsx v11,v11,15
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupklsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
		// addi r6,r8,48
		ctx.r6.s64 = ctx.r8.s64 + 48;
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// addi r11,r11,32
		ctx.r11.s64 = ctx.r11.s64 + 32;
		// vaddfp v13,v13,v9
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vcfsx v5,v5,15
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// cmplwi cr6,r10,0
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v2,v6,v4
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vmulfp128 v1,v11,v3
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vmulfp128 v31,v5,v7
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v30,v10,v8
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32)));
		// stvx v2,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r8,64
		ctx.r8.s64 = ctx.r8.s64 + 64;
		// stvx v1,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v31,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v30,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247cf64
		if (!ctx.cr6.eq) goto loc_8247CF64;
	}
loc_8247CFE8:
	// subf r5,r31,r11
	ctx.r5.s64 = ctx.r11.s64 - (int64_t)(int32_t)var_r31;
	// rlwinm r11,r30,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 1) & 0xFFFFFFFE;
	// twllei r11,0
	if (ctx.r11.s32 == 0 || ctx.r11.u32 < 0u) __builtin_trap();
	// divwu r11,r5,r11
	ctx.r11.u32 = ctx.r11.u32 ? ctx.r5.u32 / ctx.r11.u32 : 0;
	// cmplw cr6,r11,r4
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247d008
	if (ctx.r11.u32 >= ctx.r4.u32) {
		// mr r10,r4
		ctx.r10.u64 = ctx.r4.u64;
	}
loc_8247D008:
	// subf r9,r28,r8
	ctx.r9.s64 = ctx.r8.s64 - (int64_t)(int32_t)var_r28;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r9,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r29
	// blt cr6,0x8247d020
	if (ctx.r11.u32 >= var_r29) {
		// mr r11,r29
		ctx.r11.u64 = var_r29;
	}
loc_8247D020:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phWorld_7"))) PPC_WEAK_FUNC(phWorld_7);
PPC_FUNC_IMPL(__imp__phWorld_7) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r31,13(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// lwz r30,24(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// mullw r11,r31,r11
	ctx.r11.s64 = int64_t((int32_t)var_r31) * int64_t(ctx.r11.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r29,20(r3)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// subf r7,r10,r30
	ctx.r7.s64 = (int64_t)(int32_t)var_r30 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r9,r7
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r8,r10,r29
	ctx.r8.u64 = ctx.r10.u64 + var_r29;
	// blt cr6,0x8247d080
	if (ctx.r9.s32 >= ctx.r7.s32) {
		// mr r9,r7
		ctx.r9.u64 = ctx.r7.u64;
	}
loc_8247D080:
	// clrlwi r10,r9,29
	ctx.r10.u64 = ctx.r9.u32 & 0x7;
	// or r6,r10,r8
	ctx.r6.u64 = ctx.r10.u64 | ctx.r8.u64;
	// or r10,r6,r11
	ctx.r10.u64 = ctx.r6.u64 | ctx.r11.u64;
	// clrlwi r6,r10,28
	ctx.r6.u64 = ctx.r10.u32 & 0xF;
	// cmpwi cr6,r6,0
	// beq cr6,0x8247d0a4
	if (ctx.r6.s32 != 0) {
		// bl 0x82478128
		phBoundGeometry_7(ctx, base);
		return;
	}
loc_8247D0A4:
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r6,r6,127
	ctx.r6.s64 = ctx.r6.s64 + 127;
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8247d0d0
while (ctx.r10.u32 < ctx.r6.u32) {
	loc_8247D0BC:
		// rlwinm r28,r10,7,0,24
		var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80);
		// dcbt r28,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r6
		// blt cr6,0x8247d0bc
}
loc_8247D0D0:
	// extsw r10,r7
	ctx.r10.s64 = ctx.r7.s32;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r28,r1,80
	var_r28 = (uint32_t)(ctx.r1.s64 + 80);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r27,r1,80
	var_r27 = (uint32_t)(ctx.r1.s64 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stvx v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,22772(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22772);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// addi r10,r10,28112
	ctx.r10.s64 = ctx.r10.s64 + 28112;
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v13,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v8,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v12,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// vaddfp v9,v8,v8
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v8.f32)));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// vmulfp128 v12,v13,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v13,v13,v10
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v10,v0,v12
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// ble cr6,0x8247d238
	if (ctx.cr6.gt) {
		// addi r6,r9,-1
		ctx.r6.s64 = ctx.r9.s64 + -1;
		// lis r9,-32256
		// rlwinm r10,r6,29,3,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
		// addi r9,r9,27952
		ctx.r9.s64 = ctx.r9.s64 + 27952;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// lvx128 v7,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	loc_8247D1B0:
		// addi r9,r11,16
		ctx.r9.s64 = ctx.r11.s64 + 16;
		// lvx128 v13,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v13,v13,v13,v7
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vaddfp v11,v0,v8
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vor v4,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r7,r8,1024
		ctx.r7.s64 = ctx.r8.s64 + 1024;
		// vor v3,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// addi r6,r8,16
		ctx.r6.s64 = ctx.r8.s64 + 16;
		// addi r28,r8,1040
		var_r28 = (uint32_t)(ctx.r8.s64 + 1040);  // addr:0x82000410
		// vaddfp v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)));
		// lvx128 v12,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupkhsh v6,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vperm v12,v12,v12,v7
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// vupklsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
		// addi r11,r11,32
		ctx.r11.s64 = ctx.r11.s64 + 32;
		// vaddfp v0,v0,v9
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
		// cmplwi cr6,r10,0
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
		// vcfsx v6,v6,15
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupkhsh v5,v12
		simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// vupklsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
		// vcfsx v13,v13,15
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v5,v5,15
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v12,v12,15
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v2,v6,v4
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vmulfp128 v1,v13,v3
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vmulfp128 v31,v5,v11
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vmulfp128 v30,v12,v11
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
		// stvx v2,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v2.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r8,32
		ctx.r8.s64 = ctx.r8.s64 + 32;
		// stvx v1,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v31,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v30,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247d1b0
		if (!ctx.cr6.eq) goto loc_8247D1B0;
	}
loc_8247D238:
	// subf r4,r4,r11
	ctx.r4.s64 = ctx.r11.s64 - ctx.r4.s64;
	// rlwinm r11,r31,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(var_r31 | (var_r31 << 32), 1) & 0xFFFFFFFE;
	// twllei r11,0
	if (ctx.r11.s32 == 0 || ctx.r11.u32 < 0u) __builtin_trap();
	// divwu r11,r4,r11
	ctx.r11.u32 = ctx.r11.u32 ? ctx.r4.u32 / ctx.r11.u32 : 0;
	// cmplw cr6,r11,r5
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247d258
	if (ctx.r11.u32 >= ctx.r5.u32) {
		// mr r10,r5
		ctx.r10.u64 = ctx.r5.u64;
	}
loc_8247D258:
	// subf r9,r29,r8
	ctx.r9.s64 = ctx.r8.s64 - (int64_t)(int32_t)var_r29;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r9,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r30
	// blt cr6,0x8247d270
	if (ctx.r11.u32 >= var_r30) {
		// mr r11,r30
		ctx.r11.u64 = var_r30;
	}
loc_8247D270:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phWorld_9"))) PPC_WEAK_FUNC(phWorld_9);
PPC_FUNC_IMPL(__imp__phWorld_9) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=192, savegprlr_24
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r28,4(r3)
	var_r28 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 4));
	// lbz r26,13(r3)
	var_r26 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// subf r8,r11,r28
	ctx.r8.s64 = (int64_t)(int32_t)var_r28 - ctx.r11.s64;
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r11,r26,r11
	ctx.r11.s64 = int64_t((int32_t)var_r26) * int64_t(ctx.r11.s32);
	// lwz r25,24(r3)
	var_r25 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// lwz r24,20(r3)
	var_r24 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// lwz r27,0(r3)
	var_r27 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 0));
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r10,r25
	ctx.r7.s64 = (int64_t)(int32_t)var_r25 - ctx.r10.s64;
	// add r29,r11,r24
	var_r29 = (uint32_t)(ctx.r11.u64 + var_r24);
	// add r9,r9,r27
	ctx.r9.u64 = ctx.r9.u64 + var_r27;
	// cmpw cr6,r8,r7
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// blt cr6,0x8247d2d4
	if (ctx.r8.s32 >= ctx.r7.s32) {
		// mr r11,r7
		ctx.r11.u64 = ctx.r7.u64;
	}
loc_8247D2D4:
	// clrlwi r10,r11,29
	ctx.r10.u64 = ctx.r11.u32 & 0x7;
	// or r8,r10,r29
	ctx.r8.u64 = ctx.r10.u64 | var_r29;
	// or r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 | ctx.r9.u64;
	// clrlwi r5,r6,28
	ctx.r5.u64 = ctx.r6.u32 & 0xF;
	// cmpwi cr6,r5,0
	// beq cr6,0x8247d2f8
	if (ctx.r5.s32 != 0) {
		// bl 0x82478420
		phBoundGeometry_9(ctx, base);
		return;
	}
loc_8247D2F8:
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// add r4,r11,r8
	ctx.r4.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8247d32c
while (ctx.r10.u32 < ctx.r8.u32) {
	loc_8247D318:
		// rlwinm r6,r10,7,0,24
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r6,r9
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r8
		// blt cr6,0x8247d318
}
loc_8247D32C:
	// rlwinm r4,r11,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// extsw r5,r7
	ctx.r5.s64 = ctx.r7.s32;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// extsw r8,r4
	ctx.r8.s64 = ctx.r4.s32;
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stvx v0,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32256
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lfs f0,22772(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22772);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r10,28112
	ctx.r10.s64 = ctx.r10.s64 + 28112;
	// lvlx v13,0,r7
	temp.u32 = ctx.r7.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v28,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v28.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v11,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// vaddfp v8,v28,v28
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v28.f32)));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// vmulfp128 v12,v13,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// lvx128 v10,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v13,v13,v10
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v9,v0,v12
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// ble cr6,0x8247d5b8
	if (ctx.cr6.gt) {
		// lis r7,-32256
		// lis r6,-32256
		// lis r5,-32256
		// lis r4,-32256
		// lis r31,-32256
		var_r31 = (uint32_t)(-2113929216);
		// lis r30,-32256
		var_r30 = (uint32_t)(-2113929216);
		// addi r10,r11,-1
		ctx.r10.s64 = ctx.r11.s64 + -1;
		// addi r7,r7,27968
		ctx.r7.s64 = ctx.r7.s64 + 27968;
		// addi r6,r6,28000
		ctx.r6.s64 = ctx.r6.s64 + 28000;
		// addi r5,r5,27984
		ctx.r5.s64 = ctx.r5.s64 + 27984;
		// addi r4,r4,28016
		ctx.r4.s64 = ctx.r4.s64 + 28016;
		// addi r31,r31,28032
		var_r31 = (uint32_t)(var_r31 + 28032);
		// addi r30,r30,28048
		var_r30 = (uint32_t)(var_r30 + 28048);
		// lvx128 v2,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// rlwinm r8,r10,29,3,31
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFF;
		// lvx128 v3,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvx128 v4,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r29,2064
		ctx.r11.s64 = (int64_t)(int32_t)var_r29 + 2064;
		// lvx128 v5,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r9,80
		ctx.r10.s64 = ctx.r9.s64 + 80;
		// lvx128 v6,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r8,1
		ctx.r8.s64 = ctx.r8.s64 + 1;
		// lvx128 v7,r0,r30
		ea = (var_r30) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	loc_8247D450:
		// addi r7,r10,-64
		ctx.r7.s64 = ctx.r10.s64 + -64;
		// lvx128 v13,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r6,r10,-48
		ctx.r6.s64 = ctx.r10.s64 + -48;
		// vaddfp v9,v9,v8
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v8.f32)));
		// addi r5,r10,-32
		ctx.r5.s64 = ctx.r10.s64 + -32;
		// addi r4,r10,-16
		ctx.r4.s64 = ctx.r10.s64 + -16;
		// addi r31,r11,1008
		var_r31 = (uint32_t)(ctx.r11.s64 + 1008);  // addr:0x820003f0
		// lvx128 v12,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r11,-1040
		ctx.r7.s64 = ctx.r11.s64 + -1040;
		// vperm v10,v13,v12,v7
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// lvx128 v11,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v31,v13,v12,v3
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8)));
		// addi r6,r11,-16
		ctx.r6.s64 = ctx.r11.s64 + -16;
		// vperm v1,v13,v12,v5
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// addi r8,r8,-1
		ctx.r8.s64 = ctx.r8.s64 + -1;
		// addi r9,r9,96
		ctx.r9.s64 = ctx.r9.s64 + 96;
		// vperm v13,v10,v11,v6
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// cmplwi cr6,r8,0
		ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
		// vperm v12,v1,v11,v4
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
		// vperm v11,v31,v11,v2
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8)));
		// vupkhsh v10,v13
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v13.s16), simde_mm_load_si128((simde__m128i*)ctx.v13.s16))));
		// vupklsh v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v13.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.s16)));
		// vupkhsh v1,v12
		simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// vupklsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
		// vcfsx v10,v10,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupkhsh v31,v11
		simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// vupklsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
		// vcfsx v13,v13,15
		simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v13.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v1,v1,15
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v30,v12,15
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v31,v31,15
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v29,v11,15
		simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// lvx128 v11,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r5,r11,2032
		ctx.r5.s64 = ctx.r11.s64 + 2032;
		// vmulfp128 v12,v10,v0
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// lvx128 v10,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v27,v13,v0
		simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r4,r11,3056
		ctx.r4.s64 = ctx.r11.s64 + 3056;
		// vmulfp128 v26,v1,v0
		simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vperm v1,v11,v10,v7
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vmulfp128 v25,v30,v0
		simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vperm v30,v11,v10,v3
		simde_mm_store_si128((simde__m128i*)ctx.v30.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v3.u8)));
		// vmulfp128 v24,v31,v0
		simde_mm_store_ps(ctx.v24.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vperm v31,v11,v10,v5
		simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v10.u8), simde_mm_load_si128((simde__m128i*)ctx.v5.u8)));
		// vaddfp v13,v0,v28
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v28.f32)));
		// vmulfp128 v23,v29,v0
		simde_mm_store_ps(ctx.v23.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vaddfp v0,v0,v8
		simde_mm_store_ps(ctx.v0.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
		// stvx v12,r0,r29
		ea = (var_r29) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r29,r29,32
		var_r29 = (uint32_t)(var_r29 + 32);
		// lvx128 v12,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r10,96
		ctx.r10.s64 = ctx.r10.s64 + 96;
		// vperm v11,v1,v12,v6
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v1.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// stvx v27,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v10,v31,v12,v4
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
		// stvx v26,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vperm v12,v30,v12,v2
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_perm_epi8_(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)ctx.v12.u8), simde_mm_load_si128((simde__m128i*)ctx.v2.u8)));
		// addi r7,r11,-2048
		ctx.r7.s64 = ctx.r11.s64 + -2048;
		// addi r6,r11,-1024
		ctx.r6.s64 = ctx.r11.s64 + -1024;
		// stvx v25,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupkhsh v1,v11
		simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v11.s16), simde_mm_load_si128((simde__m128i*)ctx.v11.s16))));
		// stvx v24,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupklsh v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v11.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v11.s16)));
		// stvx v23,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupkhsh v31,v10
		simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v10.s16), simde_mm_load_si128((simde__m128i*)ctx.v10.s16))));
		// addi r5,r11,1024
		ctx.r5.s64 = ctx.r11.s64 + 1024;
		// vupkhsh v30,v12
		simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v12.s16), simde_mm_load_si128((simde__m128i*)ctx.v12.s16))));
		// addi r4,r11,2048
		ctx.r4.s64 = ctx.r11.s64 + 2048;
		// vcfsx v1,v1,15
		simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupklsh v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v10.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.s16)));
		// vcfsx v11,v11,15
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v11.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vupklsh v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v12.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.s16)));
		// vcfsx v31,v31,15
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// addi r31,r11,3072
		var_r31 = (uint32_t)(ctx.r11.s64 + 3072);  // addr:0x82000c00
		// vcfsx v12,v12,15
		simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v12.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v22,v1,v13
		simde_mm_store_ps(ctx.v22.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmulfp128 v21,v11,v13
		simde_mm_store_ps(ctx.v21.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vcfsx v11,v10,15
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v10.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v20,v31,v13
		simde_mm_store_ps(ctx.v20.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vcfsx v10,v30,15
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// stvx v22,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v21,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v19,v11,v13
		simde_mm_store_ps(ctx.v19.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v20,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v18,v10,v13
		simde_mm_store_ps(ctx.v18.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
		// addi r11,r11,32
		ctx.r11.s64 = ctx.r11.s64 + 32;
		// vmulfp128 v17,v12,v13
		simde_mm_store_ps(ctx.v17.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// stvx v19,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v18,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v18.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v17,r0,r31
		ea = (var_r31) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v17.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247d450
		if (!ctx.cr6.eq) goto loc_8247D450;
	}
loc_8247D5B8:
	// rlwinm r10,r26,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(var_r26 | (var_r26 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r27,r9
	ctx.r11.s64 = ctx.r9.s64 - (int64_t)(int32_t)var_r27;
	// twllei r10,0
	if (ctx.r10.s32 == 0 || ctx.r10.u32 < 0u) __builtin_trap();
	// divwu r11,r11,r10
	ctx.r11.u32 = ctx.r10.u32 ? ctx.r11.u32 / ctx.r10.u32 : 0;
	// cmplw cr6,r11,r28
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247d5d8
	if (ctx.r11.u32 >= var_r28) {
		// mr r10,r28
		ctx.r10.u64 = var_r28;
	}
loc_8247D5D8:
	// subf r9,r24,r29
	ctx.r9.s64 = (int64_t)(int32_t)var_r29 - (int64_t)(int32_t)var_r24;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r9,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r25
	// blt cr6,0x8247d5f0
	if (ctx.r11.u32 >= var_r25) {
		// mr r11,r25
		ctx.r11.u64 = var_r25;
	}
loc_8247D5F0:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phWorld_11"))) PPC_WEAK_FUNC(phWorld_11);
PPC_FUNC_IMPL(__imp__phWorld_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=160, savegprlr_27
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r31,13(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r11.s64;
	// lwz r30,24(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 24));
	// mullw r7,r31,r11
	ctx.r7.s64 = int64_t((int32_t)var_r31) * int64_t(ctx.r11.s32);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r29,20(r3)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 20));
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r10,r30
	ctx.r8.s64 = (int64_t)(int32_t)var_r30 - ctx.r10.s64;
	// add r10,r7,r4
	ctx.r10.u64 = ctx.r7.u64 + ctx.r4.u64;
	// add r7,r11,r29
	ctx.r7.u64 = ctx.r11.u64 + var_r29;
	// cmpw cr6,r9,r8
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// blt cr6,0x8247d650
	if (ctx.r9.s32 >= ctx.r8.s32) {
		// mr r11,r8
		ctx.r11.u64 = ctx.r8.u64;
	}
loc_8247D650:
	// or r9,r7,r10
	ctx.r9.u64 = ctx.r7.u64 | ctx.r10.u64;
	// clrlwi r6,r9,28
	ctx.r6.u64 = ctx.r9.u32 & 0xF;
	// or r9,r6,r11
	ctx.r9.u64 = ctx.r6.u64 | ctx.r11.u64;
	// clrlwi r6,r9,27
	ctx.r6.u64 = ctx.r9.u32 & 0x1F;
	// cmpwi cr6,r6,0
	// beq cr6,0x8247d674
	if (ctx.r6.s32 != 0) {
		// bl 0x82478808
		phBoundGeometry_11(ctx, base);
		return;
	}
loc_8247D674:
	// addi r6,r11,127
	ctx.r6.s64 = ctx.r11.s64 + 127;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r6,r6,25,7,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	// beq cr6,0x8247d69c
while (ctx.r9.u32 < ctx.r6.u32) {
	loc_8247D688:
		// rlwinm r28,r9,7,0,24
		var_r28 = (uint32_t)(__builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80);
		// dcbt r28,r10
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
		// cmplw cr6,r9,r6
		// blt cr6,0x8247d688
}
loc_8247D69C:
	// extsw r9,r8
	ctx.r9.s64 = ctx.r8.s32;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r28,r1,80
	var_r28 = (uint32_t)(ctx.r1.s64 + 80);
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r27,r1,80
	var_r27 = (uint32_t)(ctx.r1.s64 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f0,28080(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 28080);  /* glob:lbl_82006DB0 @ 0x82006db0 */
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32256
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// addi r9,r9,28144
	ctx.r9.s64 = ctx.r9.s64 + 28144;
	// lvx128 v11,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r9,-32256
	// addi r9,r9,28128
	ctx.r9.s64 = ctx.r9.s64 + 28128;
	// lvx128 v9,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r9,-32256
	// addi r9,r9,28112
	ctx.r9.s64 = ctx.r9.s64 + 28112;
	// lvx128 v8,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r9,-32256
	// addi r9,r9,28096
	ctx.r9.s64 = ctx.r9.s64 + 28096;
	// lvx128 v7,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v10,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v13,0,r28
	temp.u32 = var_r28;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v0,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v12,0,r27
	temp.u32 = var_r27;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v12,v13,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v11,v13,v9
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vmulfp128 v8,v13,v8
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v7,v13,v7
	simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v9,v0,v0
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v0.f32)));
	// vaddfp v13,v10,v12
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v12.f32)));
	// vaddfp v12,v10,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v11,v10,v8
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vaddfp v10,v10,v7
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v7.f32)));
	// ble cr6,0x8247d8ac
	if (ctx.cr6.gt) {
		// addi r6,r11,-1
		ctx.r6.s64 = ctx.r11.s64 + -1;
		// vspltisb v8,7
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_set1_epi8(char(0x7)));
		// addi r11,r7,96
		ctx.r11.s64 = ctx.r7.s64 + 96;
		// rlwinm r9,r6,27,5,31
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x7FFFFFF;
		// addi r9,r9,1
		ctx.r9.s64 = ctx.r9.s64 + 1;
	loc_8247D7B0:
		// vslb v6,v8,v8
		ctx.v6.u8[0] = ctx.v8.u8[0] << (ctx.v8.u8[0] & 0x7);
		ctx.v6.u8[1] = ctx.v8.u8[1] << (ctx.v8.u8[1] & 0x7);
		ctx.v6.u8[2] = ctx.v8.u8[2] << (ctx.v8.u8[2] & 0x7);
		ctx.v6.u8[3] = ctx.v8.u8[3] << (ctx.v8.u8[3] & 0x7);
		ctx.v6.u8[4] = ctx.v8.u8[4] << (ctx.v8.u8[4] & 0x7);
		ctx.v6.u8[5] = ctx.v8.u8[5] << (ctx.v8.u8[5] & 0x7);
		ctx.v6.u8[6] = ctx.v8.u8[6] << (ctx.v8.u8[6] & 0x7);
		ctx.v6.u8[7] = ctx.v8.u8[7] << (ctx.v8.u8[7] & 0x7);
		ctx.v6.u8[8] = ctx.v8.u8[8] << (ctx.v8.u8[8] & 0x7);
		ctx.v6.u8[9] = ctx.v8.u8[9] << (ctx.v8.u8[9] & 0x7);
		ctx.v6.u8[10] = ctx.v8.u8[10] << (ctx.v8.u8[10] & 0x7);
		ctx.v6.u8[11] = ctx.v8.u8[11] << (ctx.v8.u8[11] & 0x7);
		ctx.v6.u8[12] = ctx.v8.u8[12] << (ctx.v8.u8[12] & 0x7);
		ctx.v6.u8[13] = ctx.v8.u8[13] << (ctx.v8.u8[13] & 0x7);
		ctx.v6.u8[14] = ctx.v8.u8[14] << (ctx.v8.u8[14] & 0x7);
		ctx.v6.u8[15] = ctx.v8.u8[15] << (ctx.v8.u8[15] & 0x7);
		// lvx128 v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r10,16
		ctx.r8.s64 = ctx.r10.s64 + 16;
		// vaddfp v3,v12,v0
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v3.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
		// vaddfp v4,v13,v0
		simde_mm_store_ps(ctx.v4.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r6,r11,-80
		ctx.r6.s64 = ctx.r11.s64 + -80;
		// vaddfp v2,v11,v0
		simde_mm_store_ps(ctx.v2.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r28,r11,-64
		var_r28 = (uint32_t)(ctx.r11.s64 + -64);
		// vaddubm v7,v7,v6
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v7.u8), simde_mm_load_si128((simde__m128i*)ctx.v6.u8)));
		// vaddfp v1,v10,v0
		simde_mm_store_ps(ctx.v1.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// addi r27,r11,-48
		var_r27 = (uint32_t)(ctx.r11.s64 + -48);
		// lvx128 v5,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r11,-32
		ctx.r8.s64 = ctx.r11.s64 + -32;
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// vupkhsb v6,v7
		simde_mm_store_si128((simde__m128i*)ctx.v6.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s8), simde_mm_load_si128((simde__m128i*)ctx.v7.s8))));
		// addi r10,r10,32
		ctx.r10.s64 = ctx.r10.s64 + 32;
		// vupklsb v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
		// cmplwi cr6,r9,0
		ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
		// vupkhsh v31,v6
		simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v6.s16), simde_mm_load_si128((simde__m128i*)ctx.v6.s16))));
		// vupkhsh v30,v7
		simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
		// vupklsh v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
		// vupklsh v6,v6
		simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.s16)));
		// vcfsx v31,v31,7
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v30,v30,7
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v7,v7,7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v6,v6,7
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v31,v31,v10
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vmulfp128 v28,v30,v12
		simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v12.f32)));
		// vmulfp128 v27,v7,v13
		simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vslb v7,v8,v8
		ctx.v7.u8[0] = ctx.v8.u8[0] << (ctx.v8.u8[0] & 0x7);
		ctx.v7.u8[1] = ctx.v8.u8[1] << (ctx.v8.u8[1] & 0x7);
		ctx.v7.u8[2] = ctx.v8.u8[2] << (ctx.v8.u8[2] & 0x7);
		ctx.v7.u8[3] = ctx.v8.u8[3] << (ctx.v8.u8[3] & 0x7);
		ctx.v7.u8[4] = ctx.v8.u8[4] << (ctx.v8.u8[4] & 0x7);
		ctx.v7.u8[5] = ctx.v8.u8[5] << (ctx.v8.u8[5] & 0x7);
		ctx.v7.u8[6] = ctx.v8.u8[6] << (ctx.v8.u8[6] & 0x7);
		ctx.v7.u8[7] = ctx.v8.u8[7] << (ctx.v8.u8[7] & 0x7);
		ctx.v7.u8[8] = ctx.v8.u8[8] << (ctx.v8.u8[8] & 0x7);
		ctx.v7.u8[9] = ctx.v8.u8[9] << (ctx.v8.u8[9] & 0x7);
		ctx.v7.u8[10] = ctx.v8.u8[10] << (ctx.v8.u8[10] & 0x7);
		ctx.v7.u8[11] = ctx.v8.u8[11] << (ctx.v8.u8[11] & 0x7);
		ctx.v7.u8[12] = ctx.v8.u8[12] << (ctx.v8.u8[12] & 0x7);
		ctx.v7.u8[13] = ctx.v8.u8[13] << (ctx.v8.u8[13] & 0x7);
		ctx.v7.u8[14] = ctx.v8.u8[14] << (ctx.v8.u8[14] & 0x7);
		ctx.v7.u8[15] = ctx.v8.u8[15] << (ctx.v8.u8[15] & 0x7);
		// vmulfp128 v29,v6,v11
		simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vaddfp v13,v13,v9
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vaddfp v12,v12,v9
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vaddubm v7,v5,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)ctx.v7.u8)));
		// vaddfp v11,v11,v9
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vaddfp v10,v10,v9
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vupklsb v6,v7
		simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
		// vupkhsb v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s8), simde_mm_load_si128((simde__m128i*)ctx.v7.s8))));
		// stvx v31,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r7,128
		ctx.r7.s64 = ctx.r7.s64 + 128;
		// stvx v28,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r28,r11,16
		var_r28 = (uint32_t)(ctx.r11.s64 + 16);  // addr:0x82000010
		// vupkhsh v5,v6
		simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v6.s16), simde_mm_load_si128((simde__m128i*)ctx.v6.s16))));
		// stvx v27,r0,r27
		ea = (var_r27) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v27.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupkhsh v31,v7
		simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
		// stvx v29,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vupklsh v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
		// addi r6,r11,-16
		ctx.r6.s64 = ctx.r11.s64 + -16;
		// vupklsh v6,v6
		simde_mm_store_si128((simde__m128i*)ctx.v6.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v6.s16)));
		// vcfsx v5,v5,7
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v31,v31,7
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v7,v7,7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vcfsx v6,v6,7
		simde_mm_store_ps(ctx.v6.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v6.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
		// vmulfp128 v26,v5,v3
		simde_mm_store_ps(ctx.v26.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vmulfp128 v25,v31,v1
		simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v1.f32)));
		// vmulfp128 v24,v7,v2
		simde_mm_store_ps(ctx.v24.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vmulfp128 v23,v6,v4
		simde_mm_store_ps(ctx.v23.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v4.f32)));
		// stvx v26,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v26.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r11,r11,128
		ctx.r11.s64 = ctx.r11.s64 + 128;
		// stvx v25,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v24,r0,r6
		ea = (ctx.r6.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v23,r0,r28
		ea = (var_r28) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247d7b0
		if (!ctx.cr6.eq) goto loc_8247D7B0;
	}
loc_8247D8AC:
	// subf r4,r4,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r4.s64;
	// twllei r31,0
	if ((int32_t)var_r31 == 0 || var_r31 < 0u) __builtin_trap();
	// divwu r11,r4,r31
	ctx.r11.u32 = var_r31 ? ctx.r4.u32 / var_r31 : 0;
	// cmplw cr6,r11,r5
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// blt cr6,0x8247d8c8
	if (ctx.r11.u32 >= ctx.r5.u32) {
		// mr r10,r5
		ctx.r10.u64 = ctx.r5.u64;
	}
loc_8247D8C8:
	// subf r11,r29,r7
	ctx.r11.s64 = ctx.r7.s64 - (int64_t)(int32_t)var_r29;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// rlwinm r11,r11,30,2,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r30
	// blt cr6,0x8247d8e0
	if (ctx.r11.u32 >= var_r30) {
		// mr r11,r30
		ctx.r11.u64 = var_r30;
	}
loc_8247D8E0:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phMaterial_1"))) PPC_WEAK_FUNC(phMaterial_1);
PPC_FUNC_IMPL(__imp__phMaterial_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, manual
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r31,13(r3)
	var_r31 = (uint32_t)(PPC_LOAD_U8(ctx.r3.u32 + 13));
	// subf r9,r10,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r11,r31,r10
	ctx.r11.s64 = int64_t((int32_t)var_r31) * int64_t(ctx.r10.s32);
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// srawi r10,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r5.s32 >> 1;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// cmpw cr6,r9,r10
	// blt cr6,0x8247d94c
	if (ctx.r9.s32 >= ctx.r10.s32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_8247D94C:
	// clrlwi r8,r9,30
	ctx.r8.u64 = ctx.r9.u32 & 0x3;
	// or r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 | ctx.r7.u64;
	// or r4,r6,r11
	ctx.r4.u64 = ctx.r6.u64 | ctx.r11.u64;
	// clrlwi r10,r4,28
	ctx.r10.u64 = ctx.r4.u32 & 0xF;
	// cmpwi cr6,r10,0
	// beq cr6,0x8247d97c
	if (ctx.r10.s32 != 0) {
		// bl 0x82479110
		phBoundComposite_1(ctx, base);
		// blr
		return;
	}
loc_8247D97C:
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	// beq cr6,0x8247d9a8
while (ctx.r10.u32 < ctx.r8.u32) {
	loc_8247D994:
		// rlwinm r6,r10,7,0,24
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r6,r11
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmplw cr6,r10,r8
		// blt cr6,0x8247d994
}
loc_8247D9A8:
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// lis r10,-32256
	// lfs f12,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// extsw r8,r4
	ctx.r8.s64 = ctx.r4.s32;
	// lfs f9,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f9.f64 = double(temp.f32);
	// extsw r5,r5
	ctx.r5.s64 = ctx.r5.s32;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lfs f0,27200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 27200);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// stfs f0,112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f0,120(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lfs f0,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,15788(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15788);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f4,f12,f0
	ctx.f4.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// stfs f13,124(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// frsp f6,f10
	ctx.f6.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f4,f6
	ctx.f13.f64 = double(float(ctx.f4.f64 / ctx.f6.f64));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lfd f8,96(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// stvx v0,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f5,f7
	ctx.f5.f64 = double(float(ctx.f7.f64));
	// stfs f9,124(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fmadds f3,f5,f13,f0
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 + ctx.f0.f64));
	// stfs f3,36(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stvx v0,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32256
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// vspltw v12,v12,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
	// lfs f0,22700(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 22700);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	// fmuls f2,f13,f0
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r10,28112
	ctx.r10.s64 = ctx.r10.s64 + 28112;
	// vmulfp128 v12,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
	// lvlx v10,0,r5
	temp.u32 = ctx.r5.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v0,v0,v13
	simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vspltw v10,v10,0
	simde_mm_store_si128((simde__m128i*)ctx.v10.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v10.u32), 0xFF));
	// lvx128 v11,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r10,-32256
	// vmulfp128 v9,v10,v13
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
	// addi r10,r10,28096
	ctx.r10.s64 = ctx.r10.s64 + 28096;
	// lvx128 v7,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v13,v12,v8
	simde_mm_store_ps(ctx.v13.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v12,v12,v7
	simde_mm_store_ps(ctx.v12.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v13,v0,v13
	simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v13.f32)));
	// vaddfp v12,v0,v12
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v12.f32)));
	// ble cr6,0x8247db0c
	if (ctx.cr6.gt) {
		// addi r6,r9,-1
		ctx.r6.s64 = ctx.r9.s64 + -1;
		// rlwinm r10,r6,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
	loc_8247DABC:
		// lvx128 v0,r0,r11
		ea = (ctx.r11.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vor v7,v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
		// vsldoi v10,v11,v0,12
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v11.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 4));
		// vor v11,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vor v6,v13,v13
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
		// addi r5,r7,16
		ctx.r5.s64 = ctx.r7.s64 + 16;
		// addi r10,r10,-1
		ctx.r10.s64 = ctx.r10.s64 + -1;
		// vaddfp v13,v13,v9
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v13.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
		// addi r11,r11,16
		ctx.r11.s64 = ctx.r11.s64 + 16;
		// vaddfp v12,v12,v9
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
		// vaddfp v10,v10,v0
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
		// cmplwi cr6,r10,0
		// vmrghw v8,v10,v0
		simde_mm_store_si128((simde__m128i*)ctx.v8.u32, simde_mm_unpackhi_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vmrglw v0,v10,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_unpacklo_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), simde_mm_load_si128((simde__m128i*)ctx.v10.u32)));
		// vmulfp128 v5,v8,v7
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vmulfp128 v4,v0,v6
		simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v6.f32)));
		// stvx v5,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v5.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r7,32
		ctx.r7.s64 = ctx.r7.s64 + 32;
		// stvx v4,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247dabc
		if (ctx.r10.u32 != 0) goto loc_8247DABC;
	}
loc_8247DB0C:
	// lfs f0,-4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,52(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rotlwi r9,r4,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 2);
	// divwu r11,r11,r9
	ctx.r11.u32 = ctx.r9.u32 ? ctx.r11.u32 / ctx.r9.u32 : 0;
	// twllei r9,0
	if (ctx.r9.s32 == 0 || ctx.r9.u32 < 0u) __builtin_trap();
	// cmplw cr6,r11,r10
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// blt cr6,0x8247db40
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// mr r9,r10
		ctx.r9.u64 = ctx.r10.u64;
	}
loc_8247DB40:
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r8,r10,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r10.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r8,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247db60
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247DB60:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phMaterial_6"))) PPC_WEAK_FUNC(phMaterial_6);
PPC_FUNC_IMPL(__imp__phMaterial_6) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, manual
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r6,r11,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r11.s64;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r10,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r10,r4,r10
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r9,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r6.s32 >> 1;
	// addze r9,r9
	temp.s64 = ctx.r9.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r9.s64 = temp.s64;
	// cmpw cr6,r11,r9
	// bge cr6,0x8247dbd0
	if (ctx.r11.s32 < ctx.r9.s32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8247DBD0:
	// clrlwi r7,r9,29
	ctx.r7.u64 = ctx.r9.u32 & 0x7;
	// or r5,r7,r8
	ctx.r5.u64 = ctx.r7.u64 | ctx.r8.u64;
	// or r4,r5,r10
	ctx.r4.u64 = ctx.r5.u64 | ctx.r10.u64;
	// clrlwi r11,r4,28
	ctx.r11.u64 = ctx.r4.u32 & 0xF;
	// cmpwi cr6,r11,0
	// beq cr6,0x8247dbfc
	if (ctx.r11.s32 != 0) {
		// bl 0x82479ce8
		phBoundComposite_6(ctx, base);
		// blr
		return;
	}
loc_8247DBFC:
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r5,r5,25,7,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r5,0
	// beq cr6,0x8247dc28
while (ctx.r11.u32 < ctx.r5.u32) {
	loc_8247DC14:
		// rlwinm r4,r11,7,0,24
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0xFFFFFF80;
		// dcbt r4,r10
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// cmplw cr6,r11,r5
		// blt cr6,0x8247dc14
}
loc_8247DC28:
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// vspltisw v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
	// extsw r11,r6
	ctx.r11.s64 = ctx.r6.s32;
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// fsubs f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lhz r11,52(r3)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 52);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// sth r11,110(r1)
	PPC_STORE_U16(ctx.r1.u32 + 110, ctx.r11.u16);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// fdivs f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f5,f7,f13,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stvx v0,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lvx128 v6,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// stvx v0,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r11,28144
	ctx.r11.s64 = ctx.r11.s64 + 28144;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v13,0,r5
	temp.u32 = ctx.r5.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v0,v0,0
	simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
	// vspltw v12,v13,0
	simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v13.u32), 0xFF));
	// lvx128 v11,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// vmulfp128 v11,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
	// addi r11,r11,28128
	ctx.r11.s64 = ctx.r11.s64 + 28128;
	// lvx128 v10,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// vmulfp128 v10,v12,v10
	simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
	// lfs f0,28080(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28080);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r11,r11,28112
	ctx.r11.s64 = ctx.r11.s64 + 28112;
	// lvlx v8,0,r4
	temp.u32 = ctx.r4.u32;
	simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v8,0
	simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
	// lvx128 v9,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// lis r11,-32256
	// vmulfp128 v9,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
	// addi r11,r11,28096
	ctx.r11.s64 = ctx.r11.s64 + 28096;
	// lvx128 v7,r0,r11
	ea = (ctx.r11.u32) & ~0xF;
	simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
	// vmulfp128 v8,v12,v7
	simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32)));
	// vaddfp v12,v0,v11
	simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v11.f32)));
	// vaddfp v11,v0,v10
	simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v10.f32)));
	// vaddfp v10,v0,v9
	simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v9.f32)));
	// vaddfp v9,v0,v8
	simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v8.f32)));
	// ble cr6,0x8247ddd4
	if (ctx.cr6.gt) {
		// addi r6,r9,-1
		ctx.r6.s64 = ctx.r9.s64 + -1;
		// rlwinm r11,r6,29,3,31
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 29) & 0x1FFFFFFF;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
	loc_8247DD3C:
		// lvx128 v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vor v4,v9,v9
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v9.u8));
		// vsldoi v8,v6,v0,14
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v6.u8), simde_mm_load_si128((simde__m128i*)ctx.v0.u8), 2));
		// vor v6,v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_load_si128((simde__m128i*)ctx.v0.u8));
		// vor v3,v10,v10
		simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_load_si128((simde__m128i*)ctx.v10.u8));
		// addi r5,r8,16
		ctx.r5.s64 = ctx.r8.s64 + 16;
		// vor v2,v11,v11
		simde_mm_store_si128((simde__m128i*)ctx.v2.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
		// addi r4,r8,32
		ctx.r4.s64 = ctx.r8.s64 + 32;
		// vor v1,v12,v12
		simde_mm_store_si128((simde__m128i*)ctx.v1.u8, simde_mm_load_si128((simde__m128i*)ctx.v12.u8));
		// addi r9,r8,48
		ctx.r9.s64 = ctx.r8.s64 + 48;
		// vavgsh v8,v0,v8
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_avg_epi16(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)ctx.v8.u8)));
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// addi r10,r10,16
		ctx.r10.s64 = ctx.r10.s64 + 16;
		// vaddfp v12,v12,v13
		ctx.fpscr.enableFlushMode();
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vaddfp v11,v11,v13
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v13.f32)));
		// cmplwi cr6,r11,0
		ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
		// vaddfp v10,v10,v13
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmrghh v7,v8,v0
		simde_mm_store_si128((simde__m128i*)ctx.v7.u16, simde_mm_unpackhi_epi16(simde_mm_load_si128((simde__m128i*)ctx.v0.u16), simde_mm_load_si128((simde__m128i*)ctx.v8.u16)));
		// vaddfp v9,v9,v13
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v13.f32)));
		// vmrglh v0,v8,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u16, simde_mm_unpacklo_epi16(simde_mm_load_si128((simde__m128i*)ctx.v0.u16), simde_mm_load_si128((simde__m128i*)ctx.v8.u16)));
		// vupkhsh v8,v7
		simde_mm_store_si128((simde__m128i*)ctx.v8.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v7.s16), simde_mm_load_si128((simde__m128i*)ctx.v7.s16))));
		// vupkhsh v5,v0
		simde_mm_store_si128((simde__m128i*)ctx.v5.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v0.s16), simde_mm_load_si128((simde__m128i*)ctx.v0.s16))));
		// vupklsh v7,v7
		simde_mm_store_si128((simde__m128i*)ctx.v7.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v7.s16)));
		// vupklsh v0,v0
		simde_mm_store_si128((simde__m128i*)ctx.v0.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.s16)));
		// vcfsx v8,v8,15
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v8.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v5,v5,15
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v5.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v7,v7,15
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v7.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vcfsx v0,v0,15
		simde_mm_store_ps(ctx.v0.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v0.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x38000000)))));
		// vmulfp128 v31,v8,v4
		simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v4.f32)));
		// vmulfp128 v29,v5,v2
		simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v2.f32)));
		// vmulfp128 v30,v7,v3
		simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vmulfp128 v28,v0,v1
		simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v0.f32), simde_mm_load_ps(ctx.v1.f32)));
		// stvx v31,r0,r8
		ea = (ctx.r8.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v31.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r8,r8,64
		ctx.r8.s64 = ctx.r8.s64 + 64;
		// stvx v29,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v29.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v30,r0,r5
		ea = (ctx.r5.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v30.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v28,r0,r9
		ea = (ctx.r9.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v28.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// bne cr6,0x8247dd3c
		if (!ctx.cr6.eq) goto loc_8247DD3C;
	}
loc_8247DDD4:
	// lhz r11,-2(r10)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + -2);
	// sth r11,52(r3)
	PPC_STORE_U16(ctx.r3.u32 + 52, ctx.r11.u16);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rotlwi r5,r7,1
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r7.u32, 1);
	// divwu r10,r6,r5
	ctx.r10.u32 = ctx.r5.u32 ? ctx.r6.u32 / ctx.r5.u32 : 0;
	// twllei r5,0
	if (ctx.r5.s32 == 0 || ctx.r5.u32 < 0u) __builtin_trap();
	// cmplw cr6,r10,r11
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x8247de08
	if (ctx.r10.u32 >= ctx.r11.u32) {
		// mr r9,r11
		ctx.r9.u64 = ctx.r11.u64;
	}
loc_8247DE08:
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// subf r4,r10,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r10.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r4,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	// bge cr6,0x8247de28
	if (ctx.r10.u32 < ctx.r11.u32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247DE28:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phMaterial_11"))) PPC_WEAK_FUNC(phMaterial_11);
PPC_FUNC_IMPL(__imp__phMaterial_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	uint32_t ea{};
	// FRAME: size=144, manual
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r10,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r10,r4,r10
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r10.s32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// srawi r10,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 1;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// cmpw cr6,r11,r10
	// blt cr6,0x8247de9c
	if (ctx.r11.s32 >= ctx.r10.s32) {
		// mr r11,r10
		ctx.r11.u64 = ctx.r10.u64;
	}
loc_8247DE9C:
	// or r7,r11,r9
	ctx.r7.u64 = ctx.r11.u64 | ctx.r9.u64;
	// or r5,r7,r6
	ctx.r5.u64 = ctx.r7.u64 | ctx.r6.u64;
	// clrlwi r4,r5,28
	ctx.r4.u64 = ctx.r5.u32 & 0xF;
	// cmpwi cr6,r4,0
	// beq cr6,0x8247deb8
	if (ctx.r4.s32 != 0) {
		// bl 0x8247af10
		phBoundComposite_11(ctx, base);
		// b 0x8247e1d0
	} else {
	loc_8247DEB8:
		// addi r7,r11,127
		ctx.r7.s64 = ctx.r11.s64 + 127;
		// li r10,0
		ctx.r10.s64 = 0;
		// rlwinm r7,r7,25,7,31
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
		// cmplwi cr6,r7,0
		// beq cr6,0x8247dee0
	while (ctx.r10.u32 < ctx.r7.u32) {
		loc_8247DECC:
			// rlwinm r5,r10,7,0,24
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
			// dcbt r5,r6
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// cmplw cr6,r10,r7
			// blt cr6,0x8247decc
	}
	loc_8247DEE0:
		// rlwinm r10,r11,1,0,30
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
		// vspltisw v0,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_set1_epi32(int(0x0)));
		// extsw r4,r8
		ctx.r4.s64 = ctx.r8.s32;
		// lfs f0,36(r3)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
		ctx.f0.f64 = double(temp.f32);
		// extsw r7,r10
		ctx.r7.s64 = ctx.r10.s32;
		// lfs f13,40(r3)
		temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
		ctx.f13.f64 = double(temp.f32);
		// fsubs f6,f13,f0
		ctx.f6.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
		// addi r8,r1,80
		ctx.r8.s64 = ctx.r1.s64 + 80;
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// vspltisb v4,7
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_set1_epi8(char(0x7)));
		// cmpwi cr6,r11,0
		ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
		// std r4,80(r1)
		PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
		// lfd f12,80(r1)
		ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
		// std r7,96(r1)
		PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
		// fcfid f11,f12
		ctx.f11.f64 = double(ctx.f12.s64);
		// stfs f0,80(r1)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// frsp f8,f11
		ctx.f8.f64 = double(float(ctx.f11.f64));
		// fdivs f13,f6,f8
		ctx.f13.f64 = double(float(ctx.f6.f64 / ctx.f8.f64));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// lfd f10,96(r1)
		ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
		// fcfid f9,f10
		ctx.f9.f64 = double(ctx.f10.s64);
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// frsp f7,f9
		ctx.f7.f64 = double(float(ctx.f9.f64));
		// fmadds f5,f7,f13,f0
		ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f0.f64));
		// stfs f5,36(r3)
		temp.f32 = float(ctx.f5.f64);
		PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r10,-32256
		// addi r10,r10,28208
		ctx.r10.s64 = ctx.r10.s64 + 28208;
		// lvx128 v11,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v11.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r10,-32256
		// addi r10,r10,28192
		ctx.r10.s64 = ctx.r10.s64 + 28192;
		// lvx128 v10,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v10.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r10,-32248
		ctx.r10.s64 = -2113404928;
		// lfs f0,-25872(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25872);  /* glob:lbl_82079AF0 @ 0x82079af0 */
		ctx.f0.f64 = double(temp.f32);
		// lis r10,-32256
		// fmuls f4,f13,f0
		ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// addi r10,r10,28176
		ctx.r10.s64 = ctx.r10.s64 + 28176;
		// lvx128 v9,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v9.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r10,-32256
		// addi r10,r10,28160
		ctx.r10.s64 = ctx.r10.s64 + 28160;
		// lvx128 v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r10,-32256
		// addi r10,r10,28144
		ctx.r10.s64 = ctx.r10.s64 + 28144;
		// lvx128 v6,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lis r10,-32256
		// addi r10,r10,28128
		ctx.r10.s64 = ctx.r10.s64 + 28128;
		// addi r7,r1,96
		ctx.r7.s64 = ctx.r1.s64 + 96;
		// stvx v0,r0,r7
		ea = (ctx.r7.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v0.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// lvlx v0,0,r8
		temp.u32 = ctx.r8.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v0.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// stfs f13,80(r1)
		temp.f32 = float(ctx.f13.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// vspltw v13,v0,0
		simde_mm_store_si128((simde__m128i*)ctx.v13.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v0.u32), 0xFF));
		// lvlx v12,0,r5
		temp.u32 = ctx.r5.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v12.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v12,v12,0
		simde_mm_store_si128((simde__m128i*)ctx.v12.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v12.u32), 0xFF));
		// stfs f4,80(r1)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
		// lvlx v8,0,r4
		temp.u32 = ctx.r4.u32;
		simde_mm_store_si128((simde__m128i*)ctx.v8.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(temp.u32 & ~0xF)), simde_mm_load_si128((simde__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
		// vspltw v0,v8,0
		simde_mm_store_si128((simde__m128i*)ctx.v0.u32, simde_mm_shuffle_epi32(simde_mm_load_si128((simde__m128i*)ctx.v8.u32), 0xFF));
		// vmulfp128 v8,v12,v7
		ctx.fpscr.enableFlushModeUnconditional();
		simde_mm_store_ps(ctx.v8.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32)));
		// lvx128 v7,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v7.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v11,v12,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v11.f32)));
		// lis r10,-32256
		// vmulfp128 v10,v12,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vmulfp128 v9,v12,v9
		simde_mm_store_ps(ctx.v9.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v9.f32)));
		// addi r10,r10,28112
		ctx.r10.s64 = ctx.r10.s64 + 28112;
		// vmulfp128 v3,v12,v6
		simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v6.f32)));
		// lvx128 v5,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v5.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddfp v8,v13,v8
		simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v8.f32)));
		// vaddfp v11,v13,v11
		simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v11.f32)));
		// vaddfp v10,v13,v10
		simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v10.f32)));
		// vaddfp v9,v13,v9
		simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v9.f32)));
		// lbz r5,52(r3)
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 52);
		// lis r10,-32256
		// vmulfp128 v5,v12,v5
		simde_mm_store_ps(ctx.v5.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vslb v26,v4,v4
		ctx.v26.u8[0] = ctx.v4.u8[0] << (ctx.v4.u8[0] & 0x7);
		ctx.v26.u8[1] = ctx.v4.u8[1] << (ctx.v4.u8[1] & 0x7);
		ctx.v26.u8[2] = ctx.v4.u8[2] << (ctx.v4.u8[2] & 0x7);
		ctx.v26.u8[3] = ctx.v4.u8[3] << (ctx.v4.u8[3] & 0x7);
		ctx.v26.u8[4] = ctx.v4.u8[4] << (ctx.v4.u8[4] & 0x7);
		ctx.v26.u8[5] = ctx.v4.u8[5] << (ctx.v4.u8[5] & 0x7);
		ctx.v26.u8[6] = ctx.v4.u8[6] << (ctx.v4.u8[6] & 0x7);
		ctx.v26.u8[7] = ctx.v4.u8[7] << (ctx.v4.u8[7] & 0x7);
		ctx.v26.u8[8] = ctx.v4.u8[8] << (ctx.v4.u8[8] & 0x7);
		ctx.v26.u8[9] = ctx.v4.u8[9] << (ctx.v4.u8[9] & 0x7);
		ctx.v26.u8[10] = ctx.v4.u8[10] << (ctx.v4.u8[10] & 0x7);
		ctx.v26.u8[11] = ctx.v4.u8[11] << (ctx.v4.u8[11] & 0x7);
		ctx.v26.u8[12] = ctx.v4.u8[12] << (ctx.v4.u8[12] & 0x7);
		ctx.v26.u8[13] = ctx.v4.u8[13] << (ctx.v4.u8[13] & 0x7);
		ctx.v26.u8[14] = ctx.v4.u8[14] << (ctx.v4.u8[14] & 0x7);
		ctx.v26.u8[15] = ctx.v4.u8[15] << (ctx.v4.u8[15] & 0x7);
		// addi r10,r10,28096
		ctx.r10.s64 = ctx.r10.s64 + 28096;
		// vmulfp128 v7,v12,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v7.f32)));
		// stb r5,111(r1)
		PPC_STORE_U8(ctx.r1.u32 + 111, ctx.r5.u8);
		// lvx128 v6,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v6.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vmulfp128 v2,v12,v6
		simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v6.f32)));
		// vaddfp v12,v13,v3
		simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v3.f32)));
		// vaddfp v6,v13,v5
		simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v5.f32)));
		// vaddfp v7,v13,v7
		simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v7.f32)));
		// vaddfp v5,v13,v2
		simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v13.f32), simde_mm_load_ps(ctx.v2.f32)));
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// lvx128 v13,r0,r4
		ea = (ctx.r4.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// stvx v13,r0,r10
		ea = (ctx.r10.u32) & ~0xF;
		simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
		// vaddubm v4,v13,v26
		simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v26.u8)));
		// ble cr6,0x8247e17c
		if (ctx.cr6.gt) {
			// addi r8,r11,-1
			ctx.r8.s64 = ctx.r11.s64 + -1;
			// rlwinm r11,r8,28,4,31
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
		loc_8247E07C:
			// lvx128 v13,r0,r6
			ea = (ctx.r6.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)PPC_RAW_ADDR(ea)), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// addi r7,r9,16
			ctx.r7.s64 = ctx.r9.s64 + 16;
			// vaddubm v13,v13,v26
			simde_mm_store_si128((simde__m128i*)ctx.v13.u8, simde_mm_add_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v26.u8)));
			// addi r5,r9,32
			ctx.r5.s64 = ctx.r9.s64 + 32;
			// addi r4,r9,48
			ctx.r4.s64 = ctx.r9.s64 + 48;
			// addi r10,r9,64
			ctx.r10.s64 = ctx.r9.s64 + 64;
			// addi r8,r9,80
			ctx.r8.s64 = ctx.r9.s64 + 80;
			// vsldoi v4,v4,v13,15
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_alignr_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)ctx.v13.u8), 1));
			// addi r31,r9,96
			var_r31 = (uint32_t)(ctx.r9.s64 + 96);  // addr:0x82000060
			// addi r30,r9,112
			var_r30 = (uint32_t)(ctx.r9.s64 + 112);  // addr:0x82000070
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// addi r6,r6,16
			ctx.r6.s64 = ctx.r6.s64 + 16;
			// vavgsb v4,v13,v4
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_avg_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
			// cmplwi cr6,r11,0
			ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
			// vmrghb v3,v4,v13
			simde_mm_store_si128((simde__m128i*)ctx.v3.u8, simde_mm_unpackhi_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
			// vmrglb v4,v4,v13
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_unpacklo_epi8(simde_mm_load_si128((simde__m128i*)ctx.v13.u8), simde_mm_load_si128((simde__m128i*)ctx.v4.u8)));
			// vupkhsb v2,v3
			simde_mm_store_si128((simde__m128i*)ctx.v2.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s8), simde_mm_load_si128((simde__m128i*)ctx.v3.s8))));
			// vupkhsb v1,v4
			simde_mm_store_si128((simde__m128i*)ctx.v1.s16, simde_mm_cvtepi8_epi16(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s8), simde_mm_load_si128((simde__m128i*)ctx.v4.s8))));
			// vupklsb v4,v4
			simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
			// vupklsb v3,v3
			simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi8_epi16(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
			// vupkhsh v31,v2
			simde_mm_store_si128((simde__m128i*)ctx.v31.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v2.s16), simde_mm_load_si128((simde__m128i*)ctx.v2.s16))));
			// vupkhsh v29,v1
			simde_mm_store_si128((simde__m128i*)ctx.v29.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v1.s16), simde_mm_load_si128((simde__m128i*)ctx.v1.s16))));
			// vupkhsh v28,v4
			simde_mm_store_si128((simde__m128i*)ctx.v28.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v4.s16), simde_mm_load_si128((simde__m128i*)ctx.v4.s16))));
			// vupkhsh v30,v3
			simde_mm_store_si128((simde__m128i*)ctx.v30.s32, simde_mm_cvtepi16_epi32(simde_mm_unpackhi_epi64(simde_mm_load_si128((simde__m128i*)ctx.v3.s16), simde_mm_load_si128((simde__m128i*)ctx.v3.s16))));
			// vcfsx v31,v31,7
			ctx.fpscr.enableFlushMode();
			simde_mm_store_ps(ctx.v31.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v31.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vupklsh v4,v4
			simde_mm_store_si128((simde__m128i*)ctx.v4.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v4.s16)));
			// vupklsh v2,v2
			simde_mm_store_si128((simde__m128i*)ctx.v2.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v2.s16)));
			// vcfsx v29,v29,7
			simde_mm_store_ps(ctx.v29.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v29.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vupklsh v3,v3
			simde_mm_store_si128((simde__m128i*)ctx.v3.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v3.s16)));
			// vcfsx v28,v28,7
			simde_mm_store_ps(ctx.v28.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v28.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vupklsh v1,v1
			simde_mm_store_si128((simde__m128i*)ctx.v1.s32, simde_mm_cvtepi16_epi32(simde_mm_load_si128((simde__m128i*)ctx.v1.s16)));
			// vcfsx v30,v30,7
			simde_mm_store_ps(ctx.v30.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v30.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vcfsx v27,v4,7
			simde_mm_store_ps(ctx.v27.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v4.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vcfsx v2,v2,7
			simde_mm_store_ps(ctx.v2.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v2.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vcfsx v3,v3,7
			simde_mm_store_ps(ctx.v3.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v3.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vcfsx v1,v1,7
			simde_mm_store_ps(ctx.v1.f32, simde_mm_mul_ps(simde_mm_cvtepi32_ps(simde_mm_load_si128((simde__m128i*)ctx.v1.u32)), simde_mm_castsi128_ps(simde_mm_set1_epi32(int(0x3C000000)))));
			// vmulfp128 v4,v31,v5
			simde_mm_store_ps(ctx.v4.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v31.f32), simde_mm_load_ps(ctx.v5.f32)));
			// vor v31,v11,v11
			simde_mm_store_si128((simde__m128i*)ctx.v31.u8, simde_mm_load_si128((simde__m128i*)ctx.v11.u8));
			// vmulfp128 v22,v29,v8
			simde_mm_store_ps(ctx.v22.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v29.f32), simde_mm_load_ps(ctx.v8.f32)));
			// vmulfp128 v20,v28,v10
			simde_mm_store_ps(ctx.v20.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v28.f32), simde_mm_load_ps(ctx.v10.f32)));
			// vmulfp128 v24,v30,v7
			simde_mm_store_ps(ctx.v24.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v30.f32), simde_mm_load_ps(ctx.v7.f32)));
			// vmulfp128 v19,v27,v31
			simde_mm_store_ps(ctx.v19.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v27.f32), simde_mm_load_ps(ctx.v31.f32)));
			// vmulfp128 v25,v2,v6
			simde_mm_store_ps(ctx.v25.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v2.f32), simde_mm_load_ps(ctx.v6.f32)));
			// vmulfp128 v23,v3,v12
			simde_mm_store_ps(ctx.v23.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v3.f32), simde_mm_load_ps(ctx.v12.f32)));
			// vmulfp128 v21,v1,v9
			simde_mm_store_ps(ctx.v21.f32, simde_mm_mul_ps(simde_mm_load_ps(ctx.v1.f32), simde_mm_load_ps(ctx.v9.f32)));
			// vaddfp v11,v11,v0
			simde_mm_store_ps(ctx.v11.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v11.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vaddfp v10,v10,v0
			simde_mm_store_ps(ctx.v10.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v10.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vaddfp v9,v9,v0
			simde_mm_store_ps(ctx.v9.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v9.f32), simde_mm_load_ps(ctx.v0.f32)));
			// vaddfp v8,v8,v0
			simde_mm_store_ps(ctx.v8.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v8.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v4,r0,r9
			ea = (ctx.r9.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v4.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vaddfp v12,v12,v0
			simde_mm_store_ps(ctx.v12.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v12.f32), simde_mm_load_ps(ctx.v0.f32)));
			// addi r9,r9,128
			ctx.r9.s64 = ctx.r9.s64 + 128;
			// vor v4,v13,v13
			simde_mm_store_si128((simde__m128i*)ctx.v4.u8, simde_mm_load_si128((simde__m128i*)ctx.v13.u8));
			// vaddfp v7,v7,v0
			simde_mm_store_ps(ctx.v7.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v7.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v22,r0,r10
			ea = (ctx.r10.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v22.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vaddfp v6,v6,v0
			simde_mm_store_ps(ctx.v6.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v6.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v24,r0,r5
			ea = (ctx.r5.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v24.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// vaddfp v5,v5,v0
			simde_mm_store_ps(ctx.v5.f32, simde_mm_add_ps(simde_mm_load_ps(ctx.v5.f32), simde_mm_load_ps(ctx.v0.f32)));
			// stvx v20,r0,r31
			ea = (var_r31) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v20.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v25,r0,r7
			ea = (ctx.r7.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v25.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v23,r0,r4
			ea = (ctx.r4.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v23.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v21,r0,r8
			ea = (ctx.r8.u32) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v21.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// stvx v19,r0,r30
			ea = (var_r30) & ~0xF;
			simde_mm_store_si128((simde__m128i*)PPC_RAW_ADDR(ea), simde_mm_shuffle_epi8(simde_mm_load_si128((simde__m128i*)ctx.v19.u8), simde_mm_load_si128((simde__m128i*)VectorMaskL)));
			// bne cr6,0x8247e07c
			if (!ctx.cr6.eq) goto loc_8247E07C;
		}
	loc_8247E17C:
		// lbz r11,-1(r6)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + -1);
		// stb r11,52(r3)
		PPC_STORE_U8(ctx.r3.u32 + 52, ctx.r11.u8);
		// lwz r10,0(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r11,4(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// subf r7,r10,r6
		ctx.r7.s64 = ctx.r6.s64 - ctx.r10.s64;
		// lbz r6,13(r3)
		ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
		// divwu r10,r7,r6
		ctx.r10.u32 = ctx.r6.u32 ? ctx.r7.u32 / ctx.r6.u32 : 0;
		// twllei r6,0
		if (ctx.r6.s32 == 0 || ctx.r6.u32 < 0u) __builtin_trap();
		// cmplw cr6,r10,r11
		// mr r8,r10
		ctx.r8.u64 = ctx.r10.u64;
		// blt cr6,0x8247e1ac
		if (ctx.r10.u32 >= ctx.r11.u32) {
			// mr r8,r11
			ctx.r8.u64 = ctx.r11.u64;
		}
	loc_8247E1AC:
		// lwz r10,20(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
		// lwz r11,24(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
		// subf r5,r10,r9
		ctx.r5.s64 = ctx.r9.s64 - ctx.r10.s64;
		// stw r8,8(r3)
		PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
		// rlwinm r10,r5,30,2,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
		// cmplw cr6,r10,r11
		// bge cr6,0x8247e1cc
		if (ctx.r10.u32 < ctx.r11.u32) {
			// mr r11,r10
			ctx.r11.u64 = ctx.r10.u64;
		}
	loc_8247E1CC:
		// stw r11,28(r3)
		PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r11.u32);
	}
loc_8247E1D0:
	// blr
	return;
}

__attribute__((alias("__imp__ph_E1E8"))) PPC_WEAK_FUNC(ph_E1E8);
PPC_FUNC_IMPL(__imp__ph_E1E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,28224
	ctx.r11.s64 = ctx.r11.s64 + 28224;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82480160
	ph_vt5D38_15_0160(ctx, base);
	// lwz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247e240
	if (ctx.r11.u32 != 0) {
		// addi r3,r11,4
		ctx.r3.s64 = ctx.r11.s64 + 4;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r10,0(r11)
		// bctrl
		DTOR(ctx.r3.u32, ctx, base);  // vtable slot 0 (destructor)
		// li r9,0
		ctx.r9.s64 = 0;
		// stw r9,68(r31)
		PPC_STORE_U32(var_r31 + 68, ctx.r9.u32);
	}
loc_8247E240:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8247f8e8
	util_F8E8(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_20"))) PPC_WEAK_FUNC(phDemoWorld_vfn_20);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x824806c0
	ph_vt6E94_63_06C0(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247e2a4
	if (ctx.r3.s32 >= 0) {
		// lwz r3,68(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
		// li r5,0
		ctx.r5.s64 = 0;
		// lwz r4,0(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 0);
		// lwz r10,24(r11)
		// bctrl
		VCALL(ctx.r3.u32, 6, ctx, base);  // vtable slot 6 (byte +24)
	}
loc_8247E2A4:
	// blr
	return;
}

__attribute__((alias("__imp__phInst_E2C0_2h"))) PPC_WEAK_FUNC(phInst_E2C0_2h);
PPC_FUNC_IMPL(__imp__phInst_E2C0_2h) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// lwz r3,30860(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);  /* glob:lbl_825E788C @ 0x825e788c */
	// b 0x82469f58
	ke_9F58(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_vfn_48"))) PPC_WEAK_FUNC(phInst_vfn_48);
PPC_FUNC_IMPL(__imp__phInst_vfn_48) {
	PPC_FUNC_PROLOGUE();
	// lis r3,-32768
	// ori r3,r3,16385
	ctx.r3.u64 = ctx.r3.u64 | 16385;
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_E2E0_g"))) PPC_WEAK_FUNC(phDemoWorld_E2E0_g);
PPC_FUNC_IMPL(__imp__phDemoWorld_E2E0_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,52(r11)
	// bctrl
	VCALL(ctx.r3.u32, 13, ctx, base);  // vtable slot 13 (byte +52)
	// lbz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	// blt cr6,0x8247e34c
	if (ctx.r11.u32 >= 1) {
		// beq cr6,0x8247e330
		if (!(ctx.cr6.eq)) {
			// cmplwi cr6,r11,3
			ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x8247e350
			// blr
			return;
		}
	loc_8247E330:
		// lbz r11,76(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 76);
		// lwz r10,64(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 64);
		// rotlwi r9,r11,1
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
		// add r8,r11,r9
		ctx.r8.u64 = ctx.r11.u64 + ctx.r9.u64;
		// rlwinm r11,r8,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
		// add r3,r11,r10
		ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
		// b 0x8247e350
	} else {
	loc_8247E34C:
		// addi r3,r30,40
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 40;
	}
loc_8247E350:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt6E40_0_E368"))) PPC_WEAK_FUNC(ph_vt6E40_0_E368);
PPC_FUNC_IMPL(__imp__ph_vt6E40_0_E368) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8247e1e8
	ph_E1E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_E398_2h"))) PPC_WEAK_FUNC(phInst_E398_2h);
PPC_FUNC_IMPL(__imp__phInst_E398_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=144, manual
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// li r30,0
	var_r30 = 0;
	// lis r10,0
	ctx.r10.s64 = 0;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// ori r9,r10,48000
	ctx.r9.u64 = ctx.r10.u64 | 48000;
	// cmplwi cr6,r4,0
	// std r30,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
	// std r30,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
	// std r30,16(r11)
	PPC_STORE_U64(ctx.r11.u32 + 16, var_r30);
	// li r11,6
	ctx.r11.s64 = 6;
	// stb r30,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, (uint8_t)var_r30);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// stb r11,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r11.u8);
	// beq cr6,0x8247e3fc
	if (ctx.r4.u32 != 0) {
		// lwz r8,0(r4)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		// lwz r7,4(r4)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// lwz r6,8(r4)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
		// stw r8,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
		// stw r7,112(r1)
		PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
		// stw r6,116(r1)
		PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r6.u32);
	}
loc_8247E3FC:
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82480588
	phInst_0588(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247e4a8
	if (ctx.r3.s32 >= 0) {
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// li r5,2
		ctx.r5.s64 = 2;
		// li r4,20
		ctx.r4.s64 = 20;
		// std r30,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
		// stw r30,8(r11)
		PPC_STORE_U32(ctx.r11.u32 + 8, var_r30);
		// stb r5,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r5.u8);
		// stw r31,84(r1)
		PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
		// lwz r10,20(r11)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// cmplwi cr6,r3,0
		// beq cr6,0x8247e458
		if (ctx.r3.u32 != 0) {
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwz r5,8(r31)
			ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
			// bl 0x82461508
			phInst_1508_2hr(ctx, base);
			// b 0x8247e45c
		} else {
		loc_8247E458:
			// mr r3,r30
			ctx.r3.u64 = var_r30;
		}
	loc_8247E45C:
		// cmplwi cr6,r3,0
		// stw r3,68(r31)
		PPC_STORE_U32(var_r31 + 68, ctx.r3.u32);
		// bne cr6,0x8247e474
		if (ctx.r3.u32 == 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			// b 0x8247e4a8
		} else {
		loc_8247E474:
			// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwz r5,8(r31)
			ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
			// lwz r8,32(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 8, ctx, base);  // pattern-B slot 8 (byte +32)
			// cmpwi cr6,r3,0
			// blt cr6,0x8247e4a8
			if (ctx.r3.s32 < 0) {
				// blr
				return;
			}
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,56(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		}
	}
loc_8247E4A8:
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_17"))) PPC_WEAK_FUNC(phDemoWorld_vfn_17);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_17) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// bl 0x8247fba0
	ph_vt5CD8_17_FBA0(ctx, base);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmpwi cr6,r31,0
	// blt cr6,0x8247e4f8
	if ((int32_t)var_r31 >= 0) {
		// li r5,3
		ctx.r5.s64 = 3;
		// lwz r3,32(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 32);
		// li r4,0
		ctx.r4.s64 = 0;
		// bl 0x82466018
		phDemoWorld_6018_g(ctx, base);
	}
loc_8247E4F8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_14"))) PPC_WEAK_FUNC(phDemoWorld_vfn_14);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,61(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 61);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	// beq cr6,0x8247e554
	if (ctx.r10.u32 != 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8247E554:
	// li r5,3
	ctx.r5.s64 = 3;
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 32);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82466018
	phDemoWorld_6018_g(ctx, base);
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r8,8(r9)
	// bctrl
	VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	// cmpwi cr6,r3,0
	// blt cr6,0x8247e5e0
	if (ctx.r3.s32 >= 0) {
	loc_8247E584:
		// lbz r11,84(r1)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 84);
		// addi r6,r11,255
		ctx.r6.s64 = ctx.r11.s64 + 255;
		// cmplwi cr6,r11,0
		// stb r6,84(r1)
		PPC_STORE_U8(ctx.r1.u32 + 84, ctx.r6.u8);
		// beq cr6,0x8247e5d0
		if (ctx.r11.u32 == 0) goto loc_8247E5D0;
		// lwz r3,68(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
		// li r5,0
		ctx.r5.s64 = 0;
		// lwz r4,32(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 32);
		// lwz r10,24(r11)
		// bctrl
		VCALL(ctx.r3.u32, 6, ctx, base);  // vtable slot 6 (byte +24)
		// cmpwi cr6,r3,0
		// bge cr6,0x8247e584
		if (ctx.r3.s32 >= 0) goto loc_8247E584;
		// blr
		return;
	loc_8247E5D0:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247e5e0
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8247ff40
		ph_vt5D38_14_FF40(ctx, base);
	}
loc_8247E5E0:
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_19"))) PPC_WEAK_FUNC(phDemoWorld_vfn_19);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_19) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lwz r3,30860(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// bl 0x8247e2e0
	phDemoWorld_E2E0_g(ctx, base);
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// cmplwi cr6,r27,0
	// beq cr6,0x8247e6e8
	if (var_r27 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// addi r31,r11,30832
		var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8247e650
		if (ctx.r11.s32 != 0) {
			// lwz r8,8(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r8
			// beq cr6,0x8247e668
			if (var_r30 == ctx.r8.u32) goto loc_8247E668;
		}
	loc_8247E650:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r8,r30
		ctx.r8.u64 = var_r30;
		// stw r8,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8247E668:
		// addi r10,r11,1
		ctx.r10.s64 = ctx.r11.s64 + 1;
		// stw r10,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
		// lwz r11,20(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 20);
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r9,r11
		// beq cr6,0x8247e6a8
		if (ctx.r9.u32 != ctx.r11.u32) {
			// lwz r10,4(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// stw r10,4(r9)
			PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
			// lwz r8,0(r11)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r9,4(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// stw r8,0(r9)
			PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
			// stw r11,4(r11)
			PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
			// stw r11,0(r11)
			PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
			// lwz r8,8(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
			// lwz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		}
	loc_8247E6A8:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r10,0
		// beq cr6,0x8247e6e8
		if (ctx.r10.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// cmplw cr6,r11,r8
		// bne cr6,0x8247e6e8
		if (ctx.r11.u32 != ctx.r8.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r10,-1
		ctx.r11.s64 = ctx.r10.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8247e6e8
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8247E6E8:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__phInst_E6F8_2h"))) PPC_WEAK_FUNC(phInst_E6F8_2h);
PPC_FUNC_IMPL(__imp__phInst_E6F8_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=144, manual
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// cmplwi cr6,r3,0
	// std r10,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// std r10,16(r11)
	PPC_STORE_U64(ctx.r11.u32 + 16, ctx.r10.u64);
	// stb r10,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r10.u8);
	// lis r10,0
	ctx.r10.s64 = 0;
	// li r11,6
	ctx.r11.s64 = 6;
	// ori r9,r10,48000
	ctx.r9.u64 = ctx.r10.u64 | 48000;
	// stb r11,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r11.u8);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// beq cr6,0x8247e758
	if (ctx.r3.u32 != 0) {
		// lwz r8,0(r3)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// lwz r7,4(r3)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
		// lwz r6,8(r3)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
		// stw r8,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
		// stw r7,112(r1)
		PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
		// stw r6,116(r1)
		PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r6.u32);
	}
loc_8247E758:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8247f800
	phInst_F800(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247e778
	if (ctx.r3.s32 >= 0) {
		// lwz r5,80(r1)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r4,r5,20
		ctx.r4.s64 = ctx.r5.s64 + 20;
		// stw r4,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r4.u32);
	}
loc_8247E778:
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_18"))) PPC_WEAK_FUNC(phDemoWorld_vfn_18);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,-32162
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r4,r28
	ctx.r4.u64 = var_r28;
	// lwz r3,30860(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// bl 0x8247e2e0
	phDemoWorld_E2E0_g(ctx, base);
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// cmplwi cr6,r27,0
	// beq cr6,0x8247e870
	if (var_r27 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// addi r31,r11,30832
		var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8247e7e8
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8247e7fc
			if (var_r30 == ctx.r10.u32) goto loc_8247E7FC;
		}
	loc_8247E7E8:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8247E7FC:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r27,12
		ctx.r10.s64 = (int64_t)(int32_t)var_r27 + 12;
		// mr r9,r13
		ctx.r9.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lwz r11,8(r10)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// add r11,r11,r28
		ctx.r11.u64 = ctx.r11.u64 + var_r28;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// lwz r8,4(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// stw r8,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
		// stw r11,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r11.u32);
		// lwz r7,4(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r11,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r11.u32);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8247e870
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r9,r10
		// bne cr6,0x8247e870
		if (ctx.r9.u32 != ctx.r10.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8247e870
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8247E870:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__phInst_E880"))) PPC_WEAK_FUNC(phInst_E880);
PPC_FUNC_IMPL(__imp__phInst_E880) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// bl 0x8247e6f8
	phInst_E6F8_2h(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247e8b8
	if (ctx.r3.s32 >= 0) {
		// lwz r10,84(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// addi r4,r10,72
		ctx.r4.s64 = ctx.r10.s64 + 72;
		// b 0x8247e8bc
	} else {
	loc_8247E8B8:
		// lwz r4,84(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	}
loc_8247E8BC:
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r3,0
	// blt cr6,0x8247e960
	if (ctx.r3.s32 >= 0) {
		// lis r3,24962
		ctx.r3.s64 = 1635909632;
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// ori r3,r3,6
		ctx.r3.u64 = ctx.r3.u64 | 6;
		// bl 0x8247ee00
		phInst_EE00(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmpwi cr6,r30,0
		// blt cr6,0x8247e960
		if ((int32_t)var_r30 < 0) goto loc_8247E960;
		// lwz r3,80(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// li r4,72
		ctx.r4.s64 = 72;
		// lwz r8,20(r9)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmplwi cr6,r31,0
		// beq cr6,0x8247e940
		if (var_r31 != 0) {
			// li r5,2
			ctx.r5.s64 = 2;
			// lwz r4,80(r1)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			// bl 0x8247fc60
			phInst_FC60_v12(ctx, base);
			// lis r11,-32256
			// mr r4,r29
			ctx.r4.u64 = var_r29;
			// addi r11,r11,28224
			ctx.r11.s64 = ctx.r11.s64 + 28224;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stw r11,0(r31)
			PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
			// bl 0x8247e398
			phInst_E398_2h(ctx, base);
			// mr r30,r3
			var_r30 = ctx.r3.u32;
			// cmpwi cr6,r30,0
			// blt cr6,0x8247e94c
			if ((int32_t)var_r30 < 0) goto loc_8247E94C;
			// stw r31,0(r28)
			PPC_STORE_U32(var_r28 + 0, var_r31);
			// b 0x8247e960
		} else {
		loc_8247E940:
			// lis r30,-32761
			var_r30 = (uint32_t)(-2147024896);
			// ori r30,r30,14
			var_r30 = (uint32_t)(var_r30 | 14);
			// b 0x8247e960
			goto loc_8247E960;
		loc_8247E94C:
			// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// lwz r6,12(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r31, 3, ctx, base);  // pattern-B slot 3 (byte +12)
		}
	}
loc_8247E960:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	// beq cr6,0x8247e97c
	if (ctx.r3.u32 != 0) {
		// lwz r4,4(r5)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	}
loc_8247E97C:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_33"))) PPC_WEAK_FUNC(phDemoWorld_vfn_33);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_33) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r4,3
	ctx.r11.s64 = ctx.r4.s64 + 3;
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// rlwinm r11,r11,0,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_34"))) PPC_WEAK_FUNC(phDemoWorld_vfn_34);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_34) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 16);
	// cmplwi cr6,r4,0
	// bne cr6,0x8247e9dc
	if (ctx.r4.u32 == 0) {
	loc_8247E9C4:
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8247E9DC:
	// lis r11,-32162
	// stw r5,12(r31)
	PPC_STORE_U32(var_r31 + 12, ctx.r5.u32);
	// addi r3,r11,30856
	ctx.r3.s64 = ctx.r11.s64 + 30856;
	// bl 0x824667c0
	util_67C0(ctx, base);
	// cmplwi cr6,r3,0
	// stw r3,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r3.u32);
	// bne cr6,0x8247e9c4
	if (ctx.r3.u32 != 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_36"))) PPC_WEAK_FUNC(phDemoWorld_vfn_36);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_36) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r10,r4,3
	ctx.r10.s64 = ctx.r4.s64 + 3;
	// rlwinm r30,r10,0,0,29
	var_r30 = (uint32_t)(__builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC);
	// lwz r9,16(r11)
	// bctrl
	VCALL(ctx.r3.u32, 4, ctx, base);  // phDemoWorld::vfn_4 (unnamed)  // vtable slot 4 (byte +16)
	// cmplw cr6,r30,r3
	// ble cr6,0x8247ea58
	if (var_r30 > ctx.r3.u32) {
		// li r3,0
		ctx.r3.s64 = 0;
		// b 0x8247ea6c
	} else {
	loc_8247EA58:
		// lwz r11,20(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 20);
		// lwz r10,24(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 24);
		// add r8,r11,r30
		ctx.r8.u64 = ctx.r11.u64 + var_r30;
		// add r3,r10,r11
		ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r8,20(r31)
		PPC_STORE_U32(var_r31 + 20, ctx.r8.u32);
	}
loc_8247EA6C:
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_EA88_fw"))) PPC_WEAK_FUNC(phDemoWorld_EA88_fw);
PPC_FUNC_IMPL(__imp__phDemoWorld_EA88_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r11,-32256
	// lis r10,-32256
	// addi r11,r11,28348
	ctx.r11.s64 = ctx.r11.s64 + 28348;
	// addi r10,r10,28308
	ctx.r10.s64 = ctx.r10.s64 + 28308;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// cmplwi cr6,r4,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// beq cr6,0x8247ead8
	if (ctx.r4.u32 != 0) {
		// lis r11,-32162
		// lwz r5,12(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 12);
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,24(r31)
		PPC_STORE_U32(var_r31 + 24, ctx.r11.u32);
	}
loc_8247EAD8:
	// lis r11,-32256
	// addi r11,r11,15792
	ctx.r11.s64 = ctx.r11.s64 + 15792;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_21"))) PPC_WEAK_FUNC(phDemoWorld_vfn_21);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_21) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x8247eb88
	phDemoWorld_EB88(ctx, base);
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_35"))) PPC_WEAK_FUNC(phDemoWorld_vfn_35);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_35) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// subf r3,r10,r11
	ctx.r3.s64 = ctx.r11.s64 - ctx.r10.s64;
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_26"))) PPC_WEAK_FUNC(phDemoWorld_vfn_26);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_26) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x8247ebe8
	phDemoWorld_EBE8(ctx, base);
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_37"))) PPC_WEAK_FUNC(phDemoWorld_vfn_37);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_37) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x8247ec60
	phDemoWorld_EC60_fw(ctx, base);
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_40"))) PPC_WEAK_FUNC(phDemoWorld_vfn_40);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_40) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lwz r11,0(r3)
  // [ph4a] vtable load collapsed
	// addi r31,r3,-4
	var_r31 = (uint32_t)(ctx.r3.s64 + -4);
	// lwz r30,8(r3)
	var_r30 = (uint32_t)(PPC_LOAD_U32(ctx.r3.u32 + 8));
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
  // [ph4a] slot load collapsed
	// bctrl
	phDemoWorld_vfn_0(ctx, base);  // pattern-B slot 0 (byte +0)  // phDemoWorld::vfn_0
	// cmplwi cr6,r31,0
	// beq cr6,0x8247eb6c
	if (var_r31 != 0) {
		// lis r11,-32162
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_8247EB6C:
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_EB88"))) PPC_WEAK_FUNC(phDemoWorld_EB88);
PPC_FUNC_IMPL(__imp__phDemoWorld_EB88) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8247ea88
	phDemoWorld_EA88_fw(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// cmplwi cr6,r11,0
	// beq cr6,0x8247ebc8
	if (ctx.r11.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_8247EBC8:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_EBE8"))) PPC_WEAK_FUNC(phDemoWorld_EBE8);
PPC_FUNC_IMPL(__imp__phDemoWorld_EBE8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32256
	// lis r10,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,28348
	ctx.r11.s64 = ctx.r11.s64 + 28348;
	// addi r10,r10,28328
	ctx.r10.s64 = ctx.r10.s64 + 28328;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// bl 0x8247ea88
	phDemoWorld_EA88_fw(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// cmplwi cr6,r11,0
	// beq cr6,0x8247ec40
	if (ctx.r11.u32 != 0) {
		// lis r11,-32162
		// li r5,1
		ctx.r5.s64 = 1;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_8247EC40:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phDemoWorld_EC60_fw"))) PPC_WEAK_FUNC(phDemoWorld_EC60_fw);
PPC_FUNC_IMPL(__imp__phDemoWorld_EC60_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32256
	// lis r10,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,28348
	ctx.r11.s64 = ctx.r11.s64 + 28348;
	// addi r10,r10,28372
	ctx.r10.s64 = ctx.r10.s64 + 28372;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// stw r9,24(r31)
	PPC_STORE_U32(var_r31 + 24, ctx.r9.u32);
	// bl 0x8247ea88
	phDemoWorld_EA88_fw(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_ECB0_wrh"))) PPC_WEAK_FUNC(phInst_ECB0_wrh);
PPC_FUNC_IMPL(__imp__phInst_ECB0_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// lis r11,-32162
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r3,r11,30856
	ctx.r3.s64 = ctx.r11.s64 + 30856;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,28
	ctx.r4.s64 = 28;
	// bl 0x824667c0
	util_67C0(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	// beq cr6,0x8247ed40
	if (ctx.r11.u32 != 0) {
		// lis r10,-32256
		// li r6,1
		ctx.r6.s64 = 1;
		// addi r9,r10,15792
		ctx.r9.s64 = ctx.r10.s64 + 15792;
		// lis r10,-32256
		ctx.r10.s64 = -2113929216;
		// addi r8,r10,28348
		ctx.r8.s64 = ctx.r10.s64 + 28348;
		// lis r10,-32256
		// stw r6,8(r11)
		PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
		// stw r9,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
		// addi r7,r10,28328
		ctx.r7.s64 = ctx.r10.s64 + 28328;
		// li r10,0
		ctx.r10.s64 = 0;
		// stw r8,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
		// mr r3,r10
		ctx.r3.u64 = ctx.r10.u64;
		// stw r7,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
		// stw r10,12(r11)
		PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r10.u32);
		// stw r10,16(r11)
		PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
		// stw r10,20(r11)
		PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
		// stw r10,24(r11)
		PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
		// stw r11,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
		// blr
		return;
	}
loc_8247ED40:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_ED60_w"))) PPC_WEAK_FUNC(phInst_ED60_w);
PPC_FUNC_IMPL(__imp__phInst_ED60_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lis r11,-32162
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// addi r3,r11,30856
	ctx.r3.s64 = ctx.r11.s64 + 30856;
	// mr r5,r31
	ctx.r5.u64 = var_r31;
	// addi r4,r30,28
	ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 28;
	// bl 0x824667c0
	util_67C0(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	// bne cr6,0x8247eda8
	if (ctx.r11.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
loc_8247EDA8:
	// lis r10,-32256
	// lis r9,-32256
	// lis r8,-32256
	// addi r10,r10,15792
	ctx.r10.s64 = ctx.r10.s64 + 15792;
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r9,r9,28348
	ctx.r9.s64 = ctx.r9.s64 + 28348;
	// addi r8,r8,28372
	ctx.r8.s64 = ctx.r8.s64 + 28372;
	// addi r6,r11,28
	ctx.r6.s64 = ctx.r11.s64 + 28;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// stw r7,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r7.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
	// stw r31,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, var_r31);
	// stw r5,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r5.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r30);
	// stw r6,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r6.u32);
	// stw r11,0(r29)
	PPC_STORE_U32(var_r29 + 0, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phInst_EE00"))) PPC_WEAK_FUNC(phInst_EE00);
PPC_FUNC_IMPL(__imp__phInst_EE00) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r31,0(r28)
	var_r31 = (uint32_t)(PPC_LOAD_U32(var_r28 + 0));
	// cmplwi cr6,r31,0
	// bne cr6,0x8247ee60
	if (var_r31 == 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// cmplwi cr6,r30,0
		// stw r11,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
		// beq cr6,0x8247ee40
		if (var_r30 != 0) {
			// addi r5,r1,80
			ctx.r5.s64 = ctx.r1.s64 + 80;
			// bl 0x8247ed60
			phInst_ED60_w(ctx, base);
			// b 0x8247ee48
		} else {
		loc_8247EE40:
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// bl 0x8247ecb0
			phInst_ECB0_wrh(ctx, base);
		}
	loc_8247EE48:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247eea4
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// stw r10,0(r28)
		PPC_STORE_U32(var_r28 + 0, ctx.r10.u32);
		return;
	}
loc_8247EE60:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,0(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 0, ctx, base);  // pattern-B slot 0 (byte +0)
	// lwz r7,0(r31)
  // [ph4a] vtable load collapsed
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r6,8(r7)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	// lwz r5,0(r31)
  // [ph4a] vtable load collapsed
	// mr r4,r29
	ctx.r4.u64 = var_r29;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r11,12(r5)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 3, ctx, base);  // pattern-B slot 3 (byte +12)
loc_8247EEA4:
	return;
}

__attribute__((alias("__imp__ke_EEB0"))) PPC_WEAK_FUNC(ke_EEB0);
PPC_FUNC_IMPL(__imp__ke_EEB0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8247fc60
	phInst_FC60_v12(ctx, base);
	// lis r11,-32256
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,28392
	ctx.r11.s64 = ctx.r11.s64 + 28392;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_21_EEF0"))) PPC_WEAK_FUNC(ph_vt5CD8_21_EEF0);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_21_EEF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// bl 0x8247f7e8
	ph_F7E8_h(ctx, base);
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,36(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 9, ctx, base);  // pattern-B slot 9 (byte +36)
	// blr
	return;
}

__attribute__((alias("__imp__phInst_EF40"))) PPC_WEAK_FUNC(phInst_EF40);
PPC_FUNC_IMPL(__imp__phInst_EF40) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=144, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8247f800
	phInst_F800(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247efdc
	if (ctx.r3.s32 >= 0) {
		// li r11,0
		ctx.r11.s64 = 0;
		// addi r10,r1,96
		ctx.r10.s64 = ctx.r1.s64 + 96;
		// li r9,1
		ctx.r9.s64 = 1;
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// std r11,0(r10)
		PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r11.u64);
		// std r11,8(r10)
		PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r11.u64);
		// stw r11,100(r1)
		PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
		// stb r11,105(r1)
		PPC_STORE_U8(ctx.r1.u32 + 105, ctx.r11.u8);
		// lis r11,-32162
		// lbz r10,25(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 25);
		// stb r9,96(r1)
		PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r9.u8);
		// lwz r11,30860(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
		// stb r10,104(r1)
		PPC_STORE_U8(ctx.r1.u32 + 104, ctx.r10.u8);
		// lwz r3,20(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
		// bl 0x8246bb20
		phInst_BB20_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8247efdc
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// lbz r11,24(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 24);
		// lwz r8,80(r1)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// rotlwi r9,r11,1
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
		// lwz r6,84(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// mullw r10,r11,r8
		ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r8.s32);
		// add r7,r11,r9
		ctx.r7.u64 = ctx.r11.u64 + ctx.r9.u64;
		// add r11,r10,r6
		ctx.r11.u64 = ctx.r10.u64 + ctx.r6.u64;
		// rlwinm r10,r7,2,0,29
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,0(r30)
		PPC_STORE_U32(var_r30 + 0, ctx.r5.u32);
	}
loc_8247EFDC:
	// blr
	return;
}

__attribute__((alias("__imp__phInst_EFF8_2hr"))) PPC_WEAK_FUNC(phInst_EFF8_2hr);
PPC_FUNC_IMPL(__imp__phInst_EFF8_2hr) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=144, savegprlr_28
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lbz r11,24(r28)
	ctx.r11.u64 = PPC_LOAD_U8(var_r28 + 24);
	// stb r11,68(r31)
	PPC_STORE_U8(var_r31 + 68, ctx.r11.u8);
	// bl 0x82480588
	phInst_0588(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247f0f4
	if (ctx.r3.s32 >= 0) {
		// lbz r11,68(r31)
		ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 68);
		// li r30,0
		var_r30 = 0;
		// cmplwi cr6,r11,0
		// beq cr6,0x8247f060
		if (ctx.r11.u32 != 0) {
			// lwz r3,8(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
			// rlwinm r10,r11,1,0,30
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// add r10,r11,r10
			ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
			// rlwinm r4,r10,2,0,29
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
			// lwz r8,20(r9)
			// bctrl
			VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
			// cmplwi cr6,r3,0
			// stw r3,72(r31)
			PPC_STORE_U32(var_r31 + 72, ctx.r3.u32);
			// beq cr6,0x8247f0fc
			if (ctx.r3.u32 == 0) {
				// lis r3,-32761
				// ori r3,r3,14
				ctx.r3.u64 = ctx.r3.u64 | 14;
				return;
			}
			// mr r3,r30
			ctx.r3.u64 = var_r30;
		}
	loc_8247F060:
		// lbz r10,68(r31)
		ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 68);
		// cmplwi cr6,r10,0
		// beq cr6,0x8247f0d4
		if (ctx.r10.u32 != 0) {
			// addi r11,r1,80
			ctx.r11.s64 = ctx.r1.s64 + 80;
			// lbz r7,25(r28)
			ctx.r7.u64 = PPC_LOAD_U8(var_r28 + 25);
			// li r6,1
			ctx.r6.s64 = 1;
			// cmplwi cr6,r10,0
			// std r30,0(r11)
			PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
			// std r30,8(r11)
			PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
			// stb r7,88(r1)
			PPC_STORE_U8(ctx.r1.u32 + 88, ctx.r7.u8);
			// stb r6,80(r1)
			PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r6.u8);
			// stw r31,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
			// stb r30,89(r1)
			PPC_STORE_U8(ctx.r1.u32 + 89, (uint8_t)var_r30);
			// beq cr6,0x8247f0d4
			if (ctx.r10.u32 == 0) goto loc_8247F0D4;
			// mr r29,r30
			var_r29 = (uint32_t)(var_r30);
		loc_8247F09C:
			// cmpwi cr6,r3,0
			// blt cr6,0x8247f0f4
			if (ctx.r3.s32 < 0) {
				return;
			}
			// lwz r11,72(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 72);
			// li r6,255
			ctx.r6.s64 = 255;
			// addi r5,r1,80
			ctx.r5.s64 = ctx.r1.s64 + 80;
			// add r11,r11,r29
			ctx.r11.u64 = ctx.r11.u64 + var_r29;
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// addi r4,r11,4
			ctx.r4.s64 = ctx.r11.s64 + 4;
			// bl 0x8247f9b0
			phInst_F9B0_2h(ctx, base);
			// lbz r5,68(r31)
			ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 68);
			// addi r30,r30,1
			var_r30 = (uint32_t)(var_r30 + 1);
			// addi r29,r29,12
			var_r29 = (uint32_t)(var_r29 + 12);
			// cmplw cr6,r30,r5
			// blt cr6,0x8247f09c
			if (var_r30 < ctx.r5.u32) goto loc_8247F09C;
		}
	loc_8247F0D4:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f0f4
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r4,28(r28)
		ctx.r4.u64 = PPC_LOAD_U32(var_r28 + 28);
		// lwz r10,36(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 9, ctx, base);  // pattern-B slot 9 (byte +36)
	}
loc_8247F0F4:
	return;
}

__attribute__((alias("__imp__phDemoWorld_F110_p42"))) PPC_WEAK_FUNC(phDemoWorld_F110_p42);
PPC_FUNC_IMPL(__imp__phDemoWorld_F110_p42) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_29
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// cmplwi cr6,r30,0
	// bne cr6,0x8247f138
	if (var_r30 == 0) {
		// lis r11,-32250
		ctx.r11.s64 = -2113536000;
		// addi r30,r11,24144
		var_r30 = (uint32_t)(ctx.r11.s64 + 24144);  // lbl_82065E50 @ 0x82065e50
	}
loc_8247F138:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
	// cmplwi cr6,r11,0
	// bne cr6,0x8247f150
	if (ctx.r11.u32 == 0) {
		// lis r11,-32162
		ctx.r11.s64 = -2107768832;
		// lwz r11,30860(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
		// lwz r11,24(r11)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	}
loc_8247F150:
	// rotlwi r3,r11,0
	ctx.r3.u64 = ctx.r11.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lwz r9,4(r10)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 4);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f17c
	if (ctx.r11.u32 != 0) {
		// mr r4,r11
		ctx.r4.u64 = ctx.r11.u64;
		// b 0x8247f1b0
	} else {
	loc_8247F17C:
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lwz r7,64(r8)
		// bctrl
		VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f1c8
		if (ctx.r3.s32 < 0) {
			return;
		}
		// addi r5,r1,88
		ctx.r5.s64 = ctx.r1.s64 + 88;
		// lbz r4,81(r1)
		ctx.r4.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
		// lbz r3,53(r29)
		ctx.r3.u64 = PPC_LOAD_U8(var_r29 + 53);
		// bl 0x82465a58
		phInst_5A58_p39(ctx, base);
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
	}
loc_8247F1B0:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// lwz r5,40(r6)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
loc_8247F1C8:
	return;
}

__attribute__((alias("__imp__phInst_F1D0_p33"))) PPC_WEAK_FUNC(phInst_F1D0_p33);
PPC_FUNC_IMPL(__imp__phInst_F1D0_p33) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r28,0
	var_r28 = 0;
	// mr r30,r28
	var_r30 = (uint32_t)(var_r28);
	// lbz r11,69(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 69);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f224
	if (ctx.r11.u32 != 0) {
		// mr r31,r28
		var_r31 = (uint32_t)(var_r28);
	loc_8247F1F8:
		// lwz r10,72(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 72);
		// lwzx r3,r10,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r31);
		// lwz r8,8(r9)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// lbz r7,69(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 69);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,12
		var_r31 = (uint32_t)(var_r31 + 12);
		// cmplw cr6,r30,r7
		// blt cr6,0x8247f1f8
		if (var_r30 < ctx.r7.u32) goto loc_8247F1F8;
	}
loc_8247F224:
	// stb r28,69(r29)
	PPC_STORE_U8(var_r29 + 69, (uint8_t)var_r28);
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_10_F230"))) PPC_WEAK_FUNC(ph_vt5CD8_10_F230);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_10_F230) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// li r31,0
	var_r31 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 0);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f2b0
	if (ctx.r11.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_8247F25C:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f2b0
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r11,4(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 4);
		// lwz r9,72(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 72);
		// add r11,r11,r30
		ctx.r11.u64 = ctx.r11.u64 + var_r30;
		// lbz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// lwz r4,4(r11)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// rotlwi r11,r10,1
		ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
		// add r10,r10,r11
		ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
		// rlwinm r11,r10,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		// add r9,r11,r9
		ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
		// lwz r3,4(r9)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
		// lwz r7,48(r8)
		// bctrl
		VCALL(ctx.r3.u32, 12, ctx, base);  // vtable slot 12 (byte +48)
		// lbz r6,0(r29)
		ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 0);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r30,r30,8
		var_r30 = (uint32_t)(var_r30 + 8);
		// cmplw cr6,r31,r6
		// blt cr6,0x8247f25c
		if (var_r31 < ctx.r6.u32) goto loc_8247F25C;
	}
loc_8247F2B0:
	return;
}

__attribute__((alias("__imp__ke_F2B8"))) PPC_WEAK_FUNC(ke_F2B8);
PPC_FUNC_IMPL(__imp__ke_F2B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,28392
	ctx.r11.s64 = ctx.r11.s64 + 28392;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82480160
	ph_vt5D38_15_0160(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8247f1d0
	phInst_F1D0_p33(ctx, base);
	// lbz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 68);
	// li r28,0
	var_r28 = 0;
	// cmplwi cr6,r11,0
	// mr r29,r28
	var_r29 = (uint32_t)(var_r28);
	// beq cr6,0x8247f348
	if (ctx.r11.u32 != 0) {
		// mr r30,r28
		var_r30 = (uint32_t)(var_r28);
	loc_8247F300:
		// lwz r11,72(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 72);
		// add r11,r11,r30
		ctx.r11.u64 = ctx.r11.u64 + var_r30;
		// lwz r10,4(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// cmplwi cr6,r10,0
		// beq cr6,0x8247f334
		if (ctx.r10.u32 != 0) {
			// rotlwi r3,r10,0
			ctx.r3.u64 = ctx.r10.u32;
			// lwz r8,4(r9)
			// bctrl
			VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
			// lwz r11,72(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 72);
			// add r7,r11,r30
			ctx.r7.u64 = ctx.r11.u64 + var_r30;
			// stw r28,4(r7)
			PPC_STORE_U32(ctx.r7.u32 + 4, var_r28);
		}
	loc_8247F334:
		// lbz r6,68(r31)
		ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 68);
		// addi r29,r29,1
		var_r29 = (uint32_t)(var_r29 + 1);
		// addi r30,r30,12
		var_r30 = (uint32_t)(var_r30 + 12);
		// cmplw cr6,r29,r6
		// blt cr6,0x8247f300
		if (var_r29 < ctx.r6.u32) goto loc_8247F300;
	}
loc_8247F348:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8247f8e8
	util_F8E8(ctx, base);
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_9_F358"))) PPC_WEAK_FUNC(ph_vt5CD8_9_F358);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_9_F358) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=144, savegprlr_26
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// li r26,0
	var_r26 = 0;
	// bl 0x8247f1d0
	phInst_F1D0_p33(ctx, base);
	// cmplwi cr6,r27,0
	// beq cr6,0x8247f3ec
	if (var_r27 != 0) {
		// lbz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 0);
		// li r29,0
		var_r29 = 0;
		// cmplwi cr6,r11,0
		// beq cr6,0x8247f3d8
		if (ctx.r11.u32 != 0) {
			// li r30,0
			var_r30 = 0;
			// li r31,0
			var_r31 = 0;
		loc_8247F394:
			// cmpwi cr6,r26,0
			// blt cr6,0x8247f3e0
			if ((int32_t)var_r26 < 0) goto loc_8247F3E0;
			// lwz r10,72(r28)
			ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 72);
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			// lwz r11,4(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 4);
			// add r5,r10,r31
			ctx.r5.u64 = ctx.r10.u64 + var_r31;
			// add r4,r11,r30
			ctx.r4.u64 = ctx.r11.u64 + var_r30;
			// bl 0x8247f110
			phDemoWorld_F110_p42(ctx, base);
			// lbz r10,0(r27)
			ctx.r10.u64 = PPC_LOAD_U8(var_r27 + 0);
			// addi r29,r29,1
			var_r29 = (uint32_t)(var_r29 + 1);
			// mr r26,r3
			var_r26 = ctx.r3.u32;
			// addi r31,r31,12
			var_r31 = (uint32_t)(var_r31 + 12);
			// addi r30,r30,8
			var_r30 = (uint32_t)(var_r30 + 8);
			// cmplw cr6,r29,r10
			// blt cr6,0x8247f394
			if (var_r29 < ctx.r10.u32) goto loc_8247F394;
			// cmpwi cr6,r26,0
			// blt cr6,0x8247f3e0
			if ((int32_t)var_r26 < 0) goto loc_8247F3E0;
		}
	loc_8247F3D8:
		// lbz r9,0(r27)
		ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 0);
		// stb r9,69(r28)
		PPC_STORE_U8(var_r28 + 69, ctx.r9.u8);
	loc_8247F3E0:
		// mr r3,r26
		ctx.r3.u64 = var_r26;
	loc_8247F3E4:
		return;
	}
loc_8247F3EC:
	// lbz r8,68(r28)
	ctx.r8.u64 = PPC_LOAD_U8(var_r28 + 68);
	// cmplwi cr6,r8,0
	// beq cr6,0x8247f3e0
	if (ctx.r8.u32 == 0) goto loc_8247F3E0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,72(r28)
	ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 72);
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	// bl 0x8247f110
	phDemoWorld_F110_p42(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247f3e4
	if (ctx.r3.s32 < 0) {
		return;
	}
	// li r7,1
	ctx.r7.s64 = 1;
	// stb r7,69(r28)
	PPC_STORE_U8(var_r28 + 69, ctx.r7.u8);
	return;
}

__attribute__((alias("__imp__ph_vt5D38_20_F420"))) PPC_WEAK_FUNC(ph_vt5D38_20_F420);
PPC_FUNC_IMPL(__imp__ph_vt5D38_20_F420) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r28,r4
	var_r28 = ctx.r4.u32;
	// bl 0x824806c0
	ph_vt6E94_63_06C0(ctx, base);
	// lbz r11,69(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 69);
	// li r31,0
	var_r31 = 0;
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f490
	if (ctx.r11.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_8247F44C:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f490
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r11,72(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 72);
		// lwz r4,0(r28)
		ctx.r4.u64 = PPC_LOAD_U32(var_r28 + 0);
		// add r11,r11,r30
		ctx.r11.u64 = ctx.r11.u64 + var_r30;
		// lwz r3,4(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// lwz r10,0(r11)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
		// lwz r5,32(r10)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 32);
		// lwz r8,24(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r3.u32, 6, ctx, base);  // pattern-B slot 6 (byte +24)
		// lbz r7,69(r29)
		ctx.r7.u64 = PPC_LOAD_U8(var_r29 + 69);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r30,r30,12
		var_r30 = (uint32_t)(var_r30 + 12);
		// cmplw cr6,r31,r7
		// blt cr6,0x8247f44c
		if (var_r31 < ctx.r7.u32) goto loc_8247F44C;
	}
loc_8247F490:
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_19_F498"))) PPC_WEAK_FUNC(ph_vt5CD8_19_F498);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_19_F498) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// lis r11,-32162
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r4,r27
	ctx.r4.u64 = var_r27;
	// lwz r3,30860(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// bl 0x8247e2e0
	phDemoWorld_E2E0_g(ctx, base);
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// cmplwi cr6,r28,0
	// beq cr6,0x8247f588
	if (var_r28 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// addi r31,r11,30832
		var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8247f4f0
		if (ctx.r11.s32 != 0) {
			// lwz r8,8(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r8
			// beq cr6,0x8247f508
			if (var_r30 == ctx.r8.u32) goto loc_8247F508;
		}
	loc_8247F4F0:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r8,r30
		ctx.r8.u64 = var_r30;
		// stw r8,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r8.u32);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8247F508:
		// addi r10,r11,1
		ctx.r10.s64 = ctx.r11.s64 + 1;
		// stw r10,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
		// lwz r11,20(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 20);
		// add r11,r11,r27
		ctx.r11.u64 = ctx.r11.u64 + var_r27;
		// lwz r9,0(r11)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// cmplw cr6,r9,r11
		// beq cr6,0x8247f548
		if (ctx.r9.u32 != ctx.r11.u32) {
			// lwz r10,4(r11)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// stw r10,4(r9)
			PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
			// lwz r8,0(r11)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
			// lwz r9,4(r11)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
			// stw r8,0(r9)
			PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
			// stw r11,4(r11)
			PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
			// stw r11,0(r11)
			PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
			// lwz r8,8(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
			// lwz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		}
	loc_8247F548:
		// mr r11,r13
		ctx.r11.u64 = ctx.r13.u64;
		// cmpwi cr6,r10,0
		// beq cr6,0x8247f588
		if (ctx.r10.s32 == 0) goto loc_8247F588;
		// cmplw cr6,r11,r8
		// bne cr6,0x8247f588
		if (ctx.r11.u32 != ctx.r8.u32) goto loc_8247F588;
		// addi r11,r10,-1
		ctx.r11.s64 = ctx.r10.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8247f588
		if (ctx.r11.s32 != 0) goto loc_8247F588;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8247F588:
	// lbz r7,69(r27)
	ctx.r7.u64 = PPC_LOAD_U8(var_r27 + 69);
	// li r3,0
	ctx.r3.s64 = 0;
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r7,0
	// beq cr6,0x8247f5f4
	if (ctx.r7.u32 != 0) {
		// li r31,0
		var_r31 = 0;
	loc_8247F5A0:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f5f4
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r6,72(r27)
		ctx.r6.u64 = PPC_LOAD_U32(var_r27 + 72);
		// lwzx r3,r6,r31
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r31);
		// lwz r5,40(r3)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
		// addic. r11,r5,-1
		ctx.xer.ca = ctx.r5.u32 > 0;
		ctx.r11.s64 = ctx.r5.s64 + -1;
		// stw r11,40(r3)
		PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r11.u32);
		// beq 0x8247f5c8
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x8247f5e0
		} else {
		loc_8247F5C8:
			// lwz r11,0(r3)
  // [ph4a] vtable load collapsed
			// li r5,0
			ctx.r5.s64 = 0;
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r10,60(r11)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 15, ctx, base);  // pattern-B slot 15 (byte +60)
		}
	loc_8247F5E0:
		// lbz r9,69(r27)
		ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 69);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r31,r31,12
		var_r31 = (uint32_t)(var_r31 + 12);
		// cmplw cr6,r30,r9
		// blt cr6,0x8247f5a0
		if (var_r30 < ctx.r9.u32) goto loc_8247F5A0;
	}
loc_8247F5F4:
	return;
}

__attribute__((alias("__imp__phDemoWorld_vfn_42"))) PPC_WEAK_FUNC(phDemoWorld_vfn_42);
PPC_FUNC_IMPL(__imp__phDemoWorld_vfn_42) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8247f2b8
	ke_F2B8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f640
	if (ctx.r11.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_8247F640:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_18_F660"))) PPC_WEAK_FUNC(ph_vt5CD8_18_F660);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_18_F660) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r27 = 0;
	// FRAME: size=144, savegprlr_27
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r31,0
	var_r31 = 0;
	// lbz r11,69(r29)
	ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 69);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f6d0
	if (ctx.r11.u32 != 0) {
		// li r30,0
		var_r30 = 0;
	loc_8247F688:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f7e0
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r10,72(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 72);
		// lwzx r3,r10,r30
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + var_r30);
		// lwz r11,40(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
		// addi r9,r11,1
		ctx.r9.s64 = ctx.r11.s64 + 1;
		// stw r9,40(r3)
		PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r9.u32);
		// lwz r7,56(r8)
		// bctrl
		VCALL(ctx.r3.u32, 14, ctx, base);  // vtable slot 14 (byte +56)
		// lbz r6,69(r29)
		ctx.r6.u64 = PPC_LOAD_U8(var_r29 + 69);
		// addi r31,r31,1
		var_r31 = (uint32_t)(var_r31 + 1);
		// addi r30,r30,12
		var_r30 = (uint32_t)(var_r30 + 12);
		// cmplw cr6,r31,r6
		// blt cr6,0x8247f688
		if (var_r31 < ctx.r6.u32) goto loc_8247F688;
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f7e0
		if (ctx.r3.s32 < 0) {
			return;
		}
	}
loc_8247F6D0:
	// lis r11,-32162
	// lwz r5,0(r29)
  // [ph4a] vtable load collapsed
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lwz r31,30860(r11)
	var_r31 = (uint32_t)(PPC_LOAD_U32(ctx.r11.u32 + 30860));
	// lwz r11,52(r5)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r29, 13, ctx, base);  // pattern-B slot 13 (byte +52)
	// lbz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	// blt cr6,0x8247f71c
	if (ctx.r11.u32 >= 1) {
		// bne cr6,0x8247f7dc
		if (!ctx.cr6.eq) goto loc_8247F7DC;
		// lbz r11,76(r29)
		ctx.r11.u64 = PPC_LOAD_U8(var_r29 + 76);
		// lwz r10,64(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 64);
		// rotlwi r9,r11,1
		ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
		// add r9,r11,r9
		ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
		// rlwinm r11,r9,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
		// add r28,r11,r10
		var_r28 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// b 0x8247f720
	} else {
	loc_8247F71C:
		// addi r28,r31,40
		var_r28 = (uint32_t)(var_r31 + 40);
	}
loc_8247F720:
	// cmplwi cr6,r28,0
	// beq cr6,0x8247f7dc
	if (var_r28 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r27,r3
		var_r27 = ctx.r3.u32;
		// addi r31,r11,30832
		var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8247f754
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8247f768
			if (var_r30 == ctx.r10.u32) goto loc_8247F768;
		}
	loc_8247F754:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r27,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r27);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8247F768:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r28,12
		ctx.r10.s64 = (int64_t)(int32_t)var_r28 + 12;
		// mr r9,r13
		ctx.r9.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lwz r11,8(r10)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		// add r11,r11,r29
		ctx.r11.u64 = ctx.r11.u64 + var_r29;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// lwz r8,4(r10)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
		// stw r8,4(r11)
		PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
		// stw r11,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r11.u32);
		// lwz r7,4(r11)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// stw r11,0(r7)
		PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r11.u32);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x8247f7dc
		if (ctx.r11.s32 == 0) goto loc_8247F7DC;
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r9,r10
		// bne cr6,0x8247f7dc
		if (ctx.r9.u32 != ctx.r10.u32) goto loc_8247F7DC;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8247f7dc
		if (ctx.r11.s32 != 0) goto loc_8247F7DC;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_8247F7DC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_8247F7E0:
	return;
}

__attribute__((alias("__imp__ph_F7E8_h"))) PPC_WEAK_FUNC(ph_F7E8_h);
PPC_FUNC_IMPL(__imp__ph_F7E8_h) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r10,r3,52
	ctx.r10.s64 = ctx.r3.s64 + 52;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_F800"))) PPC_WEAK_FUNC(phInst_F800);
PPC_FUNC_IMPL(__imp__phInst_F800) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	// FRAME: size=176, savegprlr_26
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// li r31,0
	var_r31 = 0;
	// mr r26,r4
	var_r26 = ctx.r4.u32;
	// mr r29,r31
	var_r29 = (uint32_t)(var_r31);
	// mr r30,r31
	var_r30 = (uint32_t)(var_r31);
	// lwz r11,12(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 12);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// bne cr6,0x8247f874
	if (ctx.r11.u32 == 0) {
		// addi r11,r1,96
		ctx.r11.s64 = ctx.r1.s64 + 96;
		// lbz r10,1(r28)
		ctx.r10.u64 = PPC_LOAD_U8(var_r28 + 1);
		// lis r9,0
		ctx.r9.s64 = 0;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// ori r8,r9,48000
		ctx.r8.u64 = ctx.r9.u64 | 48000;
		// addi r3,r1,96
		ctx.r3.s64 = ctx.r1.s64 + 96;
		// std r31,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, var_r31);
		// std r31,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, var_r31);
		// stw r31,16(r11)
		PPC_STORE_U32(ctx.r11.u32 + 16, var_r31);
		// stb r31,100(r1)
		PPC_STORE_U8(ctx.r1.u32 + 100, (uint8_t)var_r31);
		// stb r10,101(r1)
		PPC_STORE_U8(ctx.r1.u32 + 101, ctx.r10.u8);
		// stw r8,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
		// stb r31,96(r1)
		PPC_STORE_U8(ctx.r1.u32 + 96, (uint8_t)var_r31);
		// bl 0x82466030
		phInst_6030_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f8dc
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r29,80(r1)
		var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
	}
loc_8247F874:
	// lwz r11,8(r28)
	ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 8);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f8d0
	if (ctx.r11.u32 != 0) {
		// lbz r7,0(r11)
		ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// cmplwi cr6,r7,0
		// beq cr6,0x8247f8d0
		if (ctx.r7.u32 == 0) goto loc_8247F8D0;
		// lis r27,-32162
		var_r27 = (uint32_t)(-2107768832);
	loc_8247F890:
		// lwz r6,4(r11)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// lwz r11,30860(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 30860);
		// lwzx r4,r6,r31
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + var_r31);
		// lwz r3,20(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
		// bl 0x8246bb20
		phInst_BB20_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8247f8dc
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r11,8(r28)
		ctx.r11.u64 = PPC_LOAD_U32(var_r28 + 8);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// lwz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// add r29,r10,r29
		var_r29 = (uint32_t)(ctx.r10.u64 + var_r29);
		// lbz r5,0(r11)
		ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// cmplw cr6,r30,r5
		// blt cr6,0x8247f890
		if (var_r30 < ctx.r5.u32) goto loc_8247F890;
	}
loc_8247F8D0:
	// rlwinm r11,r30,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 3) & 0xFFFFFFF8;
	// add r4,r11,r29
	ctx.r4.u64 = ctx.r11.u64 + var_r29;
	// stw r4,0(r26)
	PPC_STORE_U32(var_r26 + 0, ctx.r4.u32);
loc_8247F8DC:
	return;
}

__attribute__((alias("__imp__util_F8E8"))) PPC_WEAK_FUNC(util_F8E8);
PPC_FUNC_IMPL(__imp__util_F8E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,28480
	ctx.r11.s64 = ctx.r11.s64 + 28480;
	// li r28,0
	var_r28 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// lbz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 60);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247f95c
	if (ctx.r11.u32 != 0) {
		// mr r29,r28
		var_r29 = (uint32_t)(var_r28);
	loc_8247F918:
		// lwz r10,36(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 36);
		// rlwinm r30,r29,3,0,28
		var_r30 = (uint32_t)(__builtin_rotateleft64(var_r29 | (var_r29 << 32), 3) & 0xFFFFFFF8);
		// lwzx r11,r30,r10
		ctx.r11.u64 = PPC_LOAD_U32(var_r30 + ctx.r10.u32);
		// cmplwi cr6,r11,0
		// beq cr6,0x8247f948
		if (ctx.r11.u32 != 0) {
			// lwz r9,0(r11)
  // [ph4a] vtable load collapsed
			// rotlwi r3,r11,0
			ctx.r3.u64 = ctx.r11.u32;
			// lwz r8,4(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r11.u32, 1, ctx, base);  // pattern-B slot 1 (byte +4)
			// lwz r7,36(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 36);
			// stwx r28,r30,r7
			PPC_STORE_U32(var_r30 + ctx.r7.u32, var_r28);
		}
	loc_8247F948:
		// addi r6,r29,1
		ctx.r6.s64 = (int64_t)(int32_t)var_r29 + 1;
		// lbz r5,60(r31)
		ctx.r5.u64 = PPC_LOAD_U8(var_r31 + 60);
		// clrlwi r29,r6,24
		var_r29 = (uint32_t)(ctx.r6.u32 & 0xFF);
		// cmplw cr6,r29,r5
		// blt cr6,0x8247f918
		if (var_r29 < ctx.r5.u32) goto loc_8247F918;
	}
loc_8247F95C:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 32);
	// cmplwi cr6,r3,0
	// beq cr6,0x8247f970
	if (ctx.r3.u32 != 0) {
		// bl 0x82465d60
		phInst_5D60_p39(ctx, base);
		// stw r28,32(r31)
		PPC_STORE_U32(var_r31 + 32, var_r28);
	}
loc_8247F970:
	// lis r11,-32256
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
	// addi r11,r11,23684
	ctx.r11.s64 = ctx.r11.s64 + 23684;
	// cmplwi cr6,r3,0
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// beq cr6,0x8247f99c
	if (ctx.r3.u32 != 0) {
		// lwz r11,4(r4)
		// bctrl
		VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
		// stw r28,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r28);
	}
loc_8247F99C:
	// lis r11,-32256
	// addi r11,r11,15792
	ctx.r11.s64 = ctx.r11.s64 + 15792;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	return;
}

__attribute__((alias("__imp__phInst_F9B0_2h"))) PPC_WEAK_FUNC(phInst_F9B0_2h);
PPC_FUNC_IMPL(__imp__phInst_F9B0_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_29
	// lis r11,-32162
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lwz r11,30860(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
	// mr r6,r31
	ctx.r6.u64 = var_r31;
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 8);
	// lwz r3,20(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// bl 0x8246bb40
	phInst_BB40_2hr(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x8247fa38
	if (ctx.r3.s32 >= 0) {
		// lwz r3,0(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// cmpwi cr6,r3,0
		// blt cr6,0x8247fa38
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lbz r8,80(r1)
		ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
		// li r9,1
		ctx.r9.s64 = 1;
		// lhz r11,82(r1)
		ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
		// stb r29,4(r31)
		PPC_STORE_U8(var_r31 + 4, (uint8_t)var_r29);
		// mr r7,r11
		ctx.r7.u64 = ctx.r11.u64;
		// stb r9,5(r31)
		PPC_STORE_U8(var_r31 + 5, ctx.r9.u8);
		// stb r8,6(r31)
		PPC_STORE_U8(var_r31 + 6, ctx.r8.u8);
		// lhz r6,62(r30)
		ctx.r6.u64 = PPC_LOAD_U16(var_r30 + 62);
		// cmplw cr6,r7,r6
		// ble cr6,0x8247fa38
		if (ctx.r7.u32 <= ctx.r6.u32) {
			return;
		}
		// sth r11,62(r30)
		PPC_STORE_U16(var_r30 + 62, ctx.r11.u16);
	}
loc_8247FA38:
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_7_FA40"))) PPC_WEAK_FUNC(ph_vt5CD8_7_FA40);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_7_FA40) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmplw cr6,r11,r10
	// blt cr6,0x8247fa78
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// lis r3,-32761
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// b 0x8247fa88
	} else {
	loc_8247FA78:
		// lwz r10,36(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
		// rlwinm r11,r11,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// li r3,0
		ctx.r3.s64 = 0;
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	}
loc_8247FA88:
	// cmpwi cr6,r3,0
	// blt cr6,0x8247faa8
	if (ctx.r3.s32 >= 0) {
		// lwz r3,0(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// mr r6,r7
		ctx.r6.u64 = ctx.r7.u64;
		// lwz r8,12(r9)
		// bctrl
		VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
	}
loc_8247FAA8:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_8_FAB8"))) PPC_WEAK_FUNC(ph_vt5CD8_8_FAB8);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_8_FAB8) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmplw cr6,r11,r10
	// blt cr6,0x8247faf0
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// lis r3,-32761
		// lwz r11,80(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// b 0x8247fb00
	} else {
	loc_8247FAF0:
		// lwz r10,36(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
		// rlwinm r11,r11,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// li r3,0
		ctx.r3.s64 = 0;
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	}
loc_8247FB00:
	// cmpwi cr6,r3,0
	// blt cr6,0x8247fb20
	if (ctx.r3.s32 >= 0) {
		// lwz r3,0(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		// mr r6,r7
		ctx.r6.u64 = ctx.r7.u64;
		// lwz r8,16(r9)
		// bctrl
		VCALL(ctx.r3.u32, 4, ctx, base);  // vtable slot 4 (byte +16)
	}
loc_8247FB20:
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_5_FB30"))) PPC_WEAK_FUNC(ph_vt5CD8_5_FB30);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_5_FB30) {
	PPC_FUNC_PROLOGUE();
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplw cr6,r11,r10
	// blt cr6,0x8247fb4c
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	}
loc_8247FB4C:
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// stb r9,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, ctx.r9.u8);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_6_FB68"))) PPC_WEAK_FUNC(ph_vt5CD8_6_FB68);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_6_FB68) {
	PPC_FUNC_PROLOGUE();
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplw cr6,r11,r10
	// blt cr6,0x8247fb84
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	}
loc_8247FB84:
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stb r5,5(r11)
	PPC_STORE_U8(ctx.r11.u32 + 5, ctx.r5.u8);
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5CD8_17_FBA0"))) PPC_WEAK_FUNC(ph_vt5CD8_17_FBA0);
PPC_FUNC_IMPL(__imp__ph_vt5CD8_17_FBA0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 32);
	// lwz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 44);
	// cmplwi cr6,r11,0
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// beq cr6,0x8247fbdc
	if (ctx.r11.u32 != 0) {
		// lwz r9,48(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 48);
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stw r9,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	}
loc_8247FBDC:
	// lbz r8,61(r31)
	ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 61);
	// rlwinm r7,r8,0,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r7,0
	// beq cr6,0x8247fc04
	if (ctx.r7.u32 != 0) {
		// lhz r11,64(r31)
		ctx.r11.u64 = PPC_LOAD_U16(var_r31 + 64);
		// cmplwi cr6,r11,0
		// beq cr6,0x8247fc30
		if (ctx.r11.u32 == 0) goto loc_8247FC30;
		// addis r6,r11,1
		ctx.r6.s64 = ctx.r11.s64 + 65536;
		// addi r6,r6,-1
		ctx.r6.s64 = ctx.r6.s64 + -1;
		// sth r6,64(r31)
		PPC_STORE_U16(var_r31 + 64, ctx.r6.u16);
	}
loc_8247FC04:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,80(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 20, ctx, base);  // pattern-B slot 20 (byte +80)
	// blr
	return;
loc_8247FC30:
	// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,60(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
	// blr
	return;
}

__attribute__((alias("__imp__phInst_FC60_v12"))) PPC_WEAK_FUNC(phInst_FC60_v12);
PPC_FUNC_IMPL(__imp__phInst_FC60_v12) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32256
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,23684
	ctx.r11.s64 = ctx.r11.s64 + 23684;
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// cmplwi cr6,r4,0
	// stw r4,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r4.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
	// beq cr6,0x8247fcb0
	if (ctx.r4.u32 != 0) {
		// lwz r9,0(r4)
  // [ph4a] vtable load collapsed
		// mr r3,r4
		ctx.r3.u64 = ctx.r4.u64;
		// lwz r8,0(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(ctx.r4.u32, 0, ctx, base);  // pattern-B slot 0 (byte +0)
	}
loc_8247FCB0:
	// lis r9,-32256
	// stb r30,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
	// addi r11,r31,16
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 16;
	// addi r9,r9,28480
	ctx.r9.s64 = ctx.r9.s64 + 28480;
	// addi r10,r31,24
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 24;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r9,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r9.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r11.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r11.u32);
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// stw r7,40(r31)
	PPC_STORE_U32(var_r31 + 40, ctx.r7.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_vt6E44_63"))) PPC_WEAK_FUNC(game_vt6E44_63);
PPC_FUNC_IMPL(__imp__game_vt6E44_63) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// bl 0x8247f8e8
	util_F8E8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// cmplwi cr6,r11,0
	// beq cr6,0x8247fd40
	if (ctx.r11.u32 != 0) {
		// lis r11,-32162
		// lis r5,24962
		ctx.r5.s64 = 1635909632;
		// addi r3,r11,30856
		ctx.r3.s64 = ctx.r11.s64 + 30856;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// bl 0x824667d0
		phDemoWorld_67D0_g(ctx, base);
	}
loc_8247FD40:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_FD60_2h"))) PPC_WEAK_FUNC(phInst_FD60_2h);
PPC_FUNC_IMPL(__imp__phInst_FD60_2h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=160, savegprlr_25
	// mr r27,r4
	var_r27 = ctx.r4.u32;
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r29,0
	var_r29 = 0;
	// lbz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U8(var_r27 + 0);
	// cmplwi cr6,r11,0
	// beq cr6,0x8247fe30
	if (ctx.r11.u32 != 0) {
		// lis r26,-32162
		var_r26 = (uint32_t)(-2107768832);
		// li r25,1
		var_r25 = 1;
	loc_8247FD90:
		// cmpwi cr6,r3,0
		// blt cr6,0x8247fe30
		if (ctx.r3.s32 < 0) {
			return;
		}
		// lwz r11,30860(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 30860);
		// clrlwi r30,r29,24
		var_r30 = (uint32_t)(var_r29 & 0xFF);
		// lwz r10,36(r28)
		ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 36);
		// lwz r9,4(r27)
		ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 4);
		// rlwinm r8,r30,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 2) & 0xFFFFFFFC;
		// lwz r5,8(r28)
		ctx.r5.u64 = PPC_LOAD_U32(var_r28 + 8);
		// lwz r3,20(r11)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
		// rlwinm r11,r30,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 3) & 0xFFFFFFF8;
		// add r31,r11,r10
		var_r31 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
		// lwzx r4,r8,r9
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
		// mr r6,r31
		ctx.r6.u64 = var_r31;
		// bl 0x8246bb40
		phInst_BB40_2hr(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x8247fe18
		if (ctx.r3.s32 >= 0) {
			// lwz r3,0(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwz r6,8(r7)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			// cmpwi cr6,r3,0
			// blt cr6,0x8247fe18
			if (ctx.r3.s32 < 0) goto loc_8247FE18;
			// lbz r5,80(r1)
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
			// lhz r11,82(r1)
			ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
			// stb r29,4(r31)
			PPC_STORE_U8(var_r31 + 4, (uint8_t)var_r29);
			// stb r25,5(r31)
			PPC_STORE_U8(var_r31 + 5, (uint8_t)var_r25);
			// mr r4,r11
			ctx.r4.u64 = ctx.r11.u64;
			// stb r5,6(r31)
			PPC_STORE_U8(var_r31 + 6, ctx.r5.u8);
			// lhz r10,62(r28)
			ctx.r10.u64 = PPC_LOAD_U16(var_r28 + 62);
			// cmplw cr6,r4,r10
			// ble cr6,0x8247fe18
			if (ctx.r4.u32 <= ctx.r10.u32) goto loc_8247FE18;
			// sth r11,62(r28)
			PPC_STORE_U16(var_r28 + 62, ctx.r11.u16);
		}
	loc_8247FE18:
		// addi r9,r30,1
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 1;
		// lbz r8,0(r27)
		ctx.r8.u64 = PPC_LOAD_U8(var_r27 + 0);
		// clrlwi r29,r9,24
		var_r29 = (uint32_t)(ctx.r9.u32 & 0xFF);
		// mr r7,r29
		ctx.r7.u64 = var_r29;
		// cmplw cr6,r7,r8
		// blt cr6,0x8247fd90
		if (ctx.r7.u32 < ctx.r8.u32) goto loc_8247FD90;
	}
loc_8247FE30:
	return;
}

__attribute__((alias("__imp__ph_FE38"))) PPC_WEAK_FUNC(ph_FE38);
PPC_FUNC_IMPL(__imp__ph_FE38) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=96, manual
	// lbz r11,5(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	// bne cr6,0x8247fe68
	if (ctx.r10.u32 == 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// blr
		return;
	}
loc_8247FE68:
	// lbz r11,6(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// rlwinm r9,r11,0,29,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x6;
	// cmplwi cr6,r9,6
	// bne cr6,0x8247fed8
	if (ctx.r9.u32 == 6) {
		// lis r11,-32162
		// lwz r9,0(r5)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
		// cmplwi cr6,r9,0
		// lwz r8,30860(r11)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30860);
		// beq cr6,0x8247feb0
		if (ctx.r9.u32 != 0) {
			// li r11,0
			ctx.r11.s64 = 0;
			// addi r10,r8,12
			ctx.r10.s64 = ctx.r8.s64 + 12;
		loc_8247FE94:
			// lwz r7,0(r10)
			ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
			// cmplw cr6,r9,r7
			// beq cr6,0x8247febc
			if (ctx.r9.u32 == ctx.r7.u32) goto loc_8247FEBC;
			// addi r11,r11,1
			ctx.r11.s64 = ctx.r11.s64 + 1;
			// addi r10,r10,4
			ctx.r10.s64 = ctx.r10.s64 + 4;
			// cmplwi cr6,r11,2
			// blt cr6,0x8247fe94
			if (ctx.r11.u32 < 2) goto loc_8247FE94;
		}
	loc_8247FEB0:
		// lwz r11,12(r8)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
		// stw r11,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
		// b 0x8247ff10
		goto loc_8247FF10;
	loc_8247FEBC:
		// addi r6,r11,-1
		ctx.r6.s64 = ctx.r11.s64 + -1;
		// clrlwi r11,r6,31
		ctx.r11.u64 = ctx.r6.u32 & 0x1;
		// addi r3,r11,3
		ctx.r3.s64 = ctx.r11.s64 + 3;
		// rlwinm r11,r3,2,0,29
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r11,r11,r8
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
		// stw r11,0(r5)
		PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
		// b 0x8247ff10
	} else {
	loc_8247FED8:
		// clrlwi r10,r11,30
		ctx.r10.u64 = ctx.r11.u32 & 0x3;
		// cmplwi cr6,r10,0
		// beq cr6,0x8247fef0
		if (ctx.r10.u32 != 0) {
			// lwz r9,0(r5)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
			// li r11,0
			ctx.r11.s64 = 0;
			// b 0x8247ff10
		} else {
		loc_8247FEF0:
			// rlwinm r9,r11,0,29,29
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
			// cmplwi cr6,r9,0
			// beq cr6,0x8247ff08
			if (ctx.r9.u32 != 0) {
				// lwz r11,0(r5)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
				// li r9,0
				ctx.r9.s64 = 0;
				// b 0x8247ff10
			} else {
			loc_8247FF08:
				// lwz r9,80(r1)
				ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				// lwz r11,80(r1)
				ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			}
		}
	}
loc_8247FF10:
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// lwz r7,24(r8)
	// bctrl
	VCALL(ctx.r3.u32, 6, ctx, base);  // vtable slot 6 (byte +24)
	// blr
	return;
}

__attribute__((alias("__imp__ph_vt5D38_14_FF40"))) PPC_WEAK_FUNC(ph_vt5D38_14_FF40);
PPC_FUNC_IMPL(__imp__ph_vt5D38_14_FF40) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_28
	// mr r28,r3
	var_r28 = ctx.r3.u32;
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x8247ff7c
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x8247ff90
		if (var_r30 == ctx.r10.u32) goto loc_8247FF90;
	}
loc_8247FF7C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_8247FF90:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r11,61(r28)
	ctx.r11.u64 = PPC_LOAD_U8(var_r28 + 61);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	// beq cr6,0x82480058
	if (ctx.r10.u32 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8247ffcc
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x8247ffe0
			if (var_r30 == ctx.r10.u32) goto loc_8247FFE0;
		}
	loc_8247FFCC:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_8247FFE0:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r9,61(r28)
		ctx.r9.u64 = PPC_LOAD_U8(var_r28 + 61);
		// andi. r8,r9,251
		ctx.r8.u64 = ctx.r9.u64 & 251;
		ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
		// stb r8,61(r28)
		PPC_STORE_U8(var_r28 + 61, ctx.r8.u8);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x82480040
		if (ctx.r11.s32 != 0) {
			// lwz r7,8(r31)
			ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r7
			// bne cr6,0x82480040
			if (ctx.r10.u32 != ctx.r7.u32) goto loc_82480040;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x82480040
			if (ctx.r11.s32 != 0) goto loc_82480040;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
			// lwz r11,4(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		}
	loc_82480040:
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82480150
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r6,8(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r6
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
		// b 0x8248011c
	} else {
	loc_82480058:
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x8248007c
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x82480090
			if (var_r30 == ctx.r10.u32) goto loc_82480090;
		}
	loc_8248007C:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_82480090:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r5,61(r28)
		ctx.r5.u64 = PPC_LOAD_U8(var_r28 + 61);
		// andi. r4,r5,190
		ctx.r4.u64 = ctx.r5.u64 & 190;
		ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
		// ori r3,r4,1
		ctx.r3.u64 = ctx.r4.u64 | 1;
		// stb r3,61(r28)
		PPC_STORE_U8(var_r28 + 61, ctx.r3.u8);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x824800f0
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r9
			// bne cr6,0x824800f0
			if (ctx.r10.u32 != ctx.r9.u32) goto loc_824800F0;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x824800f0
			if (ctx.r11.s32 != 0) goto loc_824800F0;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_824800F0:
		// lwz r11,0(r28)
  // [ph4a] vtable load collapsed
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// lwz r10,72(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r28, 18, ctx, base);  // pattern-B slot 18 (byte +72)
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x82480150
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	}
loc_8248011C:
	// bne cr6,0x82480150
	if (ctx.cr6.eq) {
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82480150
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// li r11,0
		ctx.r11.s64 = 0;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82480150:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__ph_vt5D38_15_0160"))) PPC_WEAK_FUNC(ph_vt5D38_15_0160);
PPC_FUNC_IMPL(__imp__ph_vt5D38_15_0160) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=128, savegprlr_27
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// cmplwi cr6,r11,0
	// beq cr6,0x824803e8
	if (ctx.r11.u32 != 0) {
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lis r11,-32162
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// addi r31,r11,30832
		var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
		// mr r29,r13
		var_r29 = ctx.r13.u32;
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r10,0
		// beq cr6,0x824801ac
		if (ctx.r10.s32 != 0) {
			// lwz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r29,r11
			// beq cr6,0x824801c8
			if (var_r29 == ctx.r11.u32) goto loc_824801C8;
		}
	loc_824801AC:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// mr r11,r29
		ctx.r11.u64 = var_r29;
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// stb r30,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		// b 0x824801cc
		goto loc_824801CC;
	loc_824801C8:
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
	loc_824801CC:
		// addi r10,r10,1
		ctx.r10.s64 = ctx.r10.s64 + 1;
		// cmpwi cr6,r28,0
		// stw r10,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
		// bne cr6,0x82480294
		if ((int32_t)var_r28 == 0) {
			// lis r28,-32162
			var_r28 = (uint32_t)(-2107768832);
		loc_824801E0:
			// lwz r9,30860(r28)
			ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 30860);
			// lwz r8,84(r9)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 84);
			// cmplw cr6,r27,r8
			// beq cr6,0x824801fc
			if (var_r27 != ctx.r8.u32) {
				// lwz r7,88(r9)
				ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
				// cmplw cr6,r27,r7
				// bne cr6,0x82480294
				if (var_r27 != ctx.r7.u32) goto loc_82480294;
			}
		loc_824801FC:
			// mr r9,r13
			ctx.r9.u64 = ctx.r13.u64;
			// cmpwi cr6,r10,0
			// beq cr6,0x82480238
			if (ctx.r10.s32 != 0) {
				// cmplw cr6,r9,r11
				// bne cr6,0x82480238
				if (ctx.r9.u32 != ctx.r11.u32) goto loc_82480238;
				// addi r11,r10,-1
				ctx.r11.s64 = ctx.r10.s64 + -1;
				// cmpwi cr6,r11,0
				// stw r11,4(r31)
				PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
				// bne cr6,0x82480238
				if (ctx.r11.s32 != 0) goto loc_82480238;
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// stb r11,12(r31)
				PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
				// stw r11,8(r31)
				PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
				// bl 0x8258631c
				__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// bl 0x8258654c
				__imp__KfLowerIrql(ctx, base);
			}
		loc_82480238:
			// bl 0x8258653c
			__imp__KeRaiseIrqlToDpcLevel(ctx, base);
			// lwz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// mr r30,r13
			var_r30 = ctx.r13.u32;
			// cmpwi cr6,r10,0
			// beq cr6,0x8248025c
			if (ctx.r10.s32 != 0) {
				// lwz r11,8(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
				// cmplw cr6,r30,r11
				// beq cr6,0x82480284
				if (var_r30 == ctx.r11.u32) goto loc_82480284;
			}
		loc_8248025C:
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// bl 0x8258634c
			__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
			// mr r11,r30
			ctx.r11.u64 = var_r30;
			// mr r30,r29
			var_r30 = (uint32_t)(var_r29);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// stb r30,12(r31)
			PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
			// lwz r10,4(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// stw r10,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
			// b 0x824801e0
			goto loc_824801E0;
		loc_82480284:
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// stw r10,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r10.u32);
			// b 0x824801e0
			goto loc_824801E0;
		}
	loc_82480294:
		// lbz r6,61(r27)
		ctx.r6.u64 = PPC_LOAD_U8(var_r27 + 61);
		// clrlwi r5,r6,31
		ctx.r5.u64 = ctx.r6.u32 & 0x1;
		// cmplwi cr6,r5,0
		// bne cr6,0x824802ec
		if (ctx.r5.u32 == 0) {
			// mr r9,r13
			ctx.r9.u64 = ctx.r13.u64;
			// cmpwi cr6,r10,0
			// beq cr6,0x8248057c
			if (ctx.r10.s32 == 0) {
				// li r3,0
				ctx.r3.s64 = 0;
				return;
			}
			// cmplw cr6,r9,r11
			// bne cr6,0x8248057c
			if (ctx.r9.u32 != ctx.r11.u32) {
				// li r3,0
				ctx.r3.s64 = 0;
				return;
			}
			// addi r11,r10,-1
			ctx.r11.s64 = ctx.r10.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x8248057c
			if (ctx.r11.s32 != 0) {
				// li r3,0
				ctx.r3.s64 = 0;
				return;
			}
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
	loc_824802EC:
		// bl 0x8258653c
		__imp__KeRaiseIrqlToDpcLevel(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r30,r13
		var_r30 = ctx.r13.u32;
		// cmpwi cr6,r11,0
		// beq cr6,0x82480310
		if (ctx.r11.s32 != 0) {
			// lwz r10,8(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r30,r10
			// beq cr6,0x82480324
			if (var_r30 == ctx.r10.u32) goto loc_82480324;
		}
	loc_82480310:
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x8258634c
		__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
		// stw r30,8(r31)
		PPC_STORE_U32(var_r31 + 8, var_r30);
		// stb r29,12(r31)
		PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	loc_82480324:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// lbz r4,61(r27)
		ctx.r4.u64 = PPC_LOAD_U8(var_r27 + 61);
		// andi. r3,r4,186
		ctx.r3.u64 = ctx.r4.u64 & 186;
		ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
		// stb r3,61(r27)
		PPC_STORE_U8(var_r27 + 61, ctx.r3.u8);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// cmpwi cr6,r11,0
		// beq cr6,0x82480380
		if (ctx.r11.s32 != 0) {
			// lwz r9,8(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
			// cmplw cr6,r10,r9
			// bne cr6,0x82480380
			if (ctx.r10.u32 != ctx.r9.u32) goto loc_82480380;
			// addi r11,r11,-1
			ctx.r11.s64 = ctx.r11.s64 + -1;
			// cmpwi cr6,r11,0
			// stw r11,4(r31)
			PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
			// bne cr6,0x82480380
			if (ctx.r11.s32 != 0) goto loc_82480380;
			// lbz r30,12(r31)
			var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			// stb r11,12(r31)
			PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
			// stw r11,8(r31)
			PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
			// bl 0x8258631c
			__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x8258654c
			__imp__KfLowerIrql(ctx, base);
		}
	loc_82480380:
		// lwz r11,0(r27)
  // [ph4a] vtable load collapsed
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r10,76(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 19, ctx, base);  // pattern-B slot 19 (byte +76)
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
		// mr r10,r13
		ctx.r10.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x8248057c
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r9,8(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r9
		// bne cr6,0x8248057c
		if (ctx.r10.u32 != ctx.r9.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8248057c
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_824803E8:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-32162
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// addi r31,r11,30832
	var_r31 = (uint32_t)(ctx.r11.s64 + 30832);  // lbl_825E7870 @ 0x825e7870
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82480414
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x82480434
		if (var_r30 == ctx.r10.u32) goto loc_82480434;
	}
loc_82480414:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r30
	ctx.r10.u64 = var_r30;
	// mr r30,r29
	var_r30 = (uint32_t)(var_r29);
	// stw r10,8(r31)
	PPC_STORE_U32(var_r31 + 8, ctx.r10.u32);
	// stb r30,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r30);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// b 0x82480438
	goto loc_82480438;
loc_82480434:
	// lbz r30,12(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
loc_82480438:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r9,61(r27)
	ctx.r9.u64 = PPC_LOAD_U8(var_r27 + 61);
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// cmplwi cr6,r8,0
	// bne cr6,0x82480498
	if (ctx.r8.u32 == 0) {
		// mr r9,r13
		ctx.r9.u64 = ctx.r13.u64;
		// cmpwi cr6,r11,0
		// beq cr6,0x8248057c
		if (ctx.r11.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// cmplw cr6,r9,r10
		// bne cr6,0x8248057c
		if (ctx.r9.u32 != ctx.r10.u32) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x8248057c
		if (ctx.r11.s32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_82480498:
	// bl 0x8258653c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r13
	var_r30 = ctx.r13.u32;
	// cmpwi cr6,r11,0
	// beq cr6,0x824804bc
	if (ctx.r11.s32 != 0) {
		// lwz r10,8(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r30,r10
		// beq cr6,0x824804d0
		if (var_r30 == ctx.r10.u32) goto loc_824804D0;
	}
loc_824804BC:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x8258634c
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stb r29,12(r31)
	PPC_STORE_U8(var_r31 + 12, (uint8_t)var_r29);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
loc_824804D0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
	// lbz r7,61(r27)
	ctx.r7.u64 = PPC_LOAD_U8(var_r27 + 61);
	// ori r6,r7,4
	ctx.r6.u64 = ctx.r7.u64 | 4;
	// stb r6,61(r27)
	PPC_STORE_U8(var_r27 + 61, ctx.r6.u8);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// cmpwi cr6,r11,0
	// beq cr6,0x82480530
	if (ctx.r11.s32 != 0) {
		// lwz r5,8(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r5
		// bne cr6,0x82480530
		if (ctx.r10.u32 != ctx.r5.u32) goto loc_82480530;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82480530
		if (ctx.r11.s32 != 0) goto loc_82480530;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
		// lwz r11,4(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	}
loc_82480530:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	// beq cr6,0x82480574
	if (ctx.r11.s32 != 0) {
		// lwz r4,8(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 8);
		// cmplw cr6,r10,r4
		// bne cr6,0x82480574
		if (ctx.r10.u32 != ctx.r4.u32) goto loc_82480574;
		// addi r11,r11,-1
		ctx.r11.s64 = ctx.r11.s64 + -1;
		// cmpwi cr6,r11,0
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bne cr6,0x82480574
		if (ctx.r11.s32 != 0) goto loc_82480574;
		// lbz r30,12(r31)
		var_r30 = (uint32_t)(PPC_LOAD_U8(var_r31 + 12));
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stb r11,12(r31)
		PPC_STORE_U8(var_r31 + 12, ctx.r11.u8);
		// stw r11,8(r31)
		PPC_STORE_U32(var_r31 + 8, ctx.r11.u32);
		// bl 0x8258631c
		__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8258654c
		__imp__KfLowerIrql(ctx, base);
	}
loc_82480574:
	// lhz r3,62(r27)
	ctx.r3.u64 = PPC_LOAD_U16(var_r27 + 62);
	// sth r3,64(r27)
	PPC_STORE_U16(var_r27 + 64, ctx.r3.u16);
loc_8248057C:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__phInst_0588"))) PPC_WEAK_FUNC(phInst_0588);
PPC_FUNC_IMPL(__imp__phInst_0588) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=144, savegprlr_28
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r29,0
	var_r29 = 0;
	// addi r10,r31,52
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 52;
	// mr r28,r29
	var_r28 = (uint32_t)(var_r29);
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 16);
	// stw r11,44(r31)
	PPC_STORE_U32(var_r31 + 44, ctx.r11.u32);
	// lwz r9,20(r30)
	ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 20);
	// stw r9,48(r31)
	PPC_STORE_U32(var_r31 + 48, ctx.r9.u32);
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 8);
	// cmplwi cr6,r11,0
	// beq cr6,0x824805dc
	if (ctx.r11.u32 != 0) {
		// lbz r11,0(r11)
		ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
		// b 0x824805e0
	} else {
	loc_824805DC:
		// mr r11,r29
		ctx.r11.u64 = var_r29;
	}
loc_824805E0:
	// stb r11,60(r31)
	PPC_STORE_U8(var_r31 + 60, ctx.r11.u8);
	// lwz r11,12(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 12);
	// cmplwi cr6,r11,0
	// beq cr6,0x82480608
	if (ctx.r11.u32 != 0) {
		// stw r11,32(r31)
		PPC_STORE_U32(var_r31 + 32, ctx.r11.u32);
		// lwz r3,12(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 12);
		// cmplwi cr6,r3,0
		// beq cr6,0x82480650
		if (ctx.r3.u32 == 0) goto loc_82480650;
		// bl 0x82465d40
		phInst_5D40_2hr(ctx, base);
		// b 0x82480650
	} else {
	loc_82480608:
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// lwz r4,8(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lis r10,0
		ctx.r10.s64 = 0;
		// addi r5,r31,32
		ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 32;
		// ori r9,r10,48000
		ctx.r9.u64 = ctx.r10.u64 | 48000;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// std r29,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, var_r29);
		// std r29,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, var_r29);
		// stw r29,16(r11)
		PPC_STORE_U32(ctx.r11.u32 + 16, var_r29);
		// lbz r11,1(r30)
		ctx.r11.u64 = PPC_LOAD_U8(var_r30 + 1);
		// stb r29,84(r1)
		PPC_STORE_U8(ctx.r1.u32 + 84, (uint8_t)var_r29);
		// stw r9,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
		// stb r29,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, (uint8_t)var_r29);
		// stb r11,85(r1)
		PPC_STORE_U8(ctx.r1.u32 + 85, ctx.r11.u8);
		// bl 0x82466088
		phInst_6088_2h(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// cmpwi cr6,r28,0
		// blt cr6,0x824806b4
		if ((int32_t)var_r28 < 0) {
			// mr r3,r28
			ctx.r3.u64 = var_r28;
			return;
		}
	}
loc_82480650:
	// lbz r11,60(r31)
	ctx.r11.u64 = PPC_LOAD_U8(var_r31 + 60);
	// cmplwi cr6,r11,0
	// beq cr6,0x82480684
	if (ctx.r11.u32 != 0) {
		// lwz r3,8(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 8);
		// rlwinm r4,r11,3,0,28
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// lwz r7,20(r8)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		// cmplwi cr6,r3,0
		// stw r3,36(r31)
		PPC_STORE_U32(var_r31 + 36, ctx.r3.u32);
		// beq cr6,0x824806a4
		if (ctx.r3.u32 == 0) {
			// lis r3,-32761
			// ori r3,r3,14
			ctx.r3.u64 = ctx.r3.u64 | 14;
			return;
		}
		// mr r28,r29
		var_r28 = (uint32_t)(var_r29);
	}
loc_82480684:
	// lbz r6,60(r31)
	ctx.r6.u64 = PPC_LOAD_U8(var_r31 + 60);
	// cmplwi cr6,r6,0
	// beq cr6,0x824806b4
	if (ctx.r6.u32 != 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r4,8(r30)
		ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 8);
		// bl 0x8247fd60
		phInst_FD60_2h(ctx, base);
		return;
	}
loc_824806B4:
	// mr r3,r28
	ctx.r3.u64 = var_r28;
	return;
}

__attribute__((alias("__imp__ph_vt6E94_63_06C0"))) PPC_WEAK_FUNC(ph_vt6E94_63_06C0);
PPC_FUNC_IMPL(__imp__ph_vt6E94_63_06C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U8(var_r31 + 60);
	// cmplwi cr6,r10,0
	// beq cr6,0x82480724
while (ctx.r7.u32 < ctx.r8.u32) {
	loc_824806E8:
		// cmpwi cr6,r3,0
		// blt cr6,0x82480724
		if (ctx.r3.s32 < 0) {
			return;
		}
		// clrlwi r30,r11,24
		var_r30 = (uint32_t)(ctx.r11.u32 & 0xFF);
		// lwz r10,36(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 36);
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// rlwinm r11,r30,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 3) & 0xFFFFFFF8;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// add r4,r11,r10
		ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
		// bl 0x8247fe38
		ph_FE38(ctx, base);
		// addi r9,r30,1
		ctx.r9.s64 = (int64_t)(int32_t)var_r30 + 1;
		// lbz r8,60(r31)
		ctx.r8.u64 = PPC_LOAD_U8(var_r31 + 60);
		// clrlwi r11,r9,24
		ctx.r11.u64 = ctx.r9.u32 & 0xFF;
		// mr r7,r11
		ctx.r7.u64 = ctx.r11.u64;
		// cmplw cr6,r7,r8
		// blt cr6,0x824806e8
}
loc_82480724:
	return;
}

__attribute__((alias("__imp__phBoundCapsule_0730_wrh"))) PPC_WEAK_FUNC(phBoundCapsule_0730_wrh);
PPC_FUNC_IMPL(__imp__phBoundCapsule_0730_wrh) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,1
	// bgt cr6,0x82480740
	if (ctx.r3.u32 <= 1) {
		// li r3,1
		ctx.r3.s64 = 1;
		// blr
		return;
	}
loc_82480740:
	// cmplwi cr6,r3,9973
	// blt cr6,0x82480750
	if (ctx.r3.u32 >= 9973) {
		// li r3,9973
		ctx.r3.s64 = 9973;
		// blr
		return;
	}
loc_82480750:
	// lis r10,-32256
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,1229
	ctx.r8.s64 = 1229;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r6,r10,28568
	ctx.r6.s64 = ctx.r10.s64 + 28568;
loc_82480764:
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// rlwinm r11,r10,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r5,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r6.u32);
	// cmplw cr6,r10,r3
	// beq cr6,0x82480798
	if (ctx.r10.u32 == ctx.r3.u32) {
		// rlwinm r4,r11,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
		// lwzx r3,r4,r6
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r6.u32);
		// blr
		return;
	}
	// ble cr6,0x8248078c
	if (ctx.cr6.gt) {
		// addi r8,r11,-1
		ctx.r8.s64 = ctx.r11.s64 + -1;
		// b 0x82480790
	} else {
	loc_8248078C:
		// addi r9,r11,1
		ctx.r9.s64 = ctx.r11.s64 + 1;
	}
loc_82480790:
	// cmplw cr6,r11,r7
	// bne cr6,0x82480764
	if (ctx.r11.u32 != ctx.r7.u32) goto loc_82480764;
loc_82480798:
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r4,r6
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__game_07A8"))) PPC_WEAK_FUNC(game_07A8);
PPC_FUNC_IMPL(__imp__game_07A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32162
	// stfs f1,28(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 28, temp.u32);
	// li r10,-1
	// addi r7,r11,30868
	ctx.r7.s64 = ctx.r11.s64 + 30868;
	// lis r11,-32165
	// rlwinm r9,r3,0,28,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x8;
	// addi r11,r11,9024
	ctx.r11.s64 = ctx.r11.s64 + 9024;
	// cmplwi cr6,r9,0
	// stfs f1,8(r7)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// beq cr6,0x824807f0
	if (ctx.r9.u32 != 0) {
		// rlwinm r10,r3,30,31,31
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x1;
		// rlwinm r8,r3,31,31,31
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x1;
		// clrlwi r9,r3,31
		ctx.r9.u64 = ctx.r3.u32 & 0x1;
		// add r10,r10,r8
		ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
		// add r10,r10,r9
		ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	}
loc_824807F0:
	// li r10,0
	ctx.r10.s64 = 0;
loc_824807F4:
	// addi r8,r3,-1
	ctx.r8.s64 = ctx.r3.s64 + -1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// and r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 & ctx.r3.u64;
	// cmplwi cr6,r3,0
	// bne cr6,0x824807f4
	if (ctx.r3.u32 != 0) goto loc_824807F4;
	// stw r10,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r10.u32);
	// lwz r7,28(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 28);
	// addi r10,r7,-1
	ctx.r10.s64 = ctx.r7.s64 + -1;
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_0820_p39"))) PPC_WEAK_FUNC(phInst_0820_p39);
PPC_FUNC_IMPL(__imp__phInst_0820_p39) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// fdivs f0,f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f2.f64 / ctx.f1.f64));
	// addi r10,r8,-1
	ctx.r10.s64 = ctx.r8.s64 + -1;
	// cmpwi cr6,r10,4
	// blt cr6,0x82480884
	if (ctx.r10.s32 >= 4) {
		// lwz r10,0(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// addi r10,r10,24
		ctx.r10.s64 = ctx.r10.s64 + 24;
	loc_82480840:
		// lfs f13,-16(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16);
		ctx.f13.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// bgt cr6,0x824808cc
		if (ctx.f13.f64 > ctx.f0.f64) goto loc_824808CC;
		// lfs f12,-8(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -8);
		ctx.f12.f64 = double(temp.f32);
		// fcmpu cr6,f12,f0
		// bgt cr6,0x824808b8
		if (ctx.f12.f64 > ctx.f0.f64) goto loc_824808B8;
		// lfs f11,0(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f11.f64 = double(temp.f32);
		// fcmpu cr6,f11,f0
		// bgt cr6,0x824808c0
		if (ctx.f11.f64 > ctx.f0.f64) goto loc_824808C0;
		// lfs f10,8(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
		ctx.f10.f64 = double(temp.f32);
		// fcmpu cr6,f10,f0
		// bgt cr6,0x824808c8
		if (ctx.f10.f64 > ctx.f0.f64) goto loc_824808C8;
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// addi r9,r8,-3
		ctx.r9.s64 = ctx.r8.s64 + -3;
		// addi r10,r10,32
		ctx.r10.s64 = ctx.r10.s64 + 32;
		// cmplw cr6,r11,r9
		// blt cr6,0x82480840
		if (ctx.r11.u32 < ctx.r9.u32) goto loc_82480840;
	}
loc_82480884:
	// cmplw cr6,r11,r8
	// bge cr6,0x8248090c
	if (ctx.r11.u32 < ctx.r8.u32) {
		// lwz r9,0(r3)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// rlwinm r10,r11,3,0,28
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// add r10,r10,r9
		ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	loc_82480898:
		// lfs f9,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// fcmpu cr6,f9,f0
		// bgt cr6,0x824808cc
		if (ctx.f9.f64 > ctx.f0.f64) goto loc_824808CC;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,8
		ctx.r10.s64 = ctx.r10.s64 + 8;
		// cmplw cr6,r11,r8
		// blt cr6,0x82480898
		if (ctx.r11.u32 < ctx.r8.u32) goto loc_82480898;
		// b 0x824808cc
		goto loc_824808CC;
	loc_824808B8:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// b 0x824808cc
		goto loc_824808CC;
	loc_824808C0:
		// addi r11,r11,2
		ctx.r11.s64 = ctx.r11.s64 + 2;
		// b 0x824808cc
		goto loc_824808CC;
	loc_824808C8:
		// addi r11,r11,3
		ctx.r11.s64 = ctx.r11.s64 + 3;
	loc_824808CC:
		// cmplw cr6,r11,r8
		// bge cr6,0x8248090c
		if (ctx.r11.u32 >= ctx.r8.u32) {
			// lwz r10,0(r3)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
			// rlwinm r11,r8,3,0,28
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
			// add r8,r11,r10
			ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
			// lfs f1,-4(r8)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
			ctx.f1.f64 = double(temp.f32);
			// blr
			return;
		}
		// lwz r10,0(r3)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// rlwinm r11,r11,3,0,28
		ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// add r11,r11,r10
		ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
		// lfs f13,-8(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
		ctx.f13.f64 = double(temp.f32);
		// lfs f6,-4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
		ctx.f6.f64 = double(temp.f32);
		// fsubs f0,f0,f13
		ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
		// lfs f8,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f8.f64 = double(temp.f32);
		// fmr f5,f6
		ctx.f5.f64 = ctx.f6.f64;
		// lfs f7,4(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
		ctx.f7.f64 = double(temp.f32);
		// fsubs f13,f8,f13
		ctx.f13.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
		// fsubs f12,f7,f6
		ctx.f12.f64 = double(float(ctx.f7.f64 - ctx.f6.f64));
		// fdivs f4,f12,f13
		ctx.f4.f64 = double(float(ctx.f12.f64 / ctx.f13.f64));
		// fmadds f1,f4,f0,f5
		ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f5.f64));
		// blr
		return;
	}
loc_8248090C:
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r11,r8,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lfs f1,-4(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__aud_0920"))) PPC_WEAK_FUNC(aud_0920);
PPC_FUNC_IMPL(__imp__aud_0920) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
	// stfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f29.u64);
	// stfd f30,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f30.u64);
	// stfd f31,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// rlwinm r11,r4,0,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x4;
	// lis r9,-32255
	// cmplwi cr6,r11,0
	// lis r10,-32256
	// lis r11,-32256
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lfs f30,-32044(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -32044);
	var_f30 = double(temp.f32);
	// li r31,2
	var_r31 = 2;
	// lfs f31,15788(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15788);
	var_f31 = double(temp.f32);
	// lfs f3,15784(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
	ctx.f3.f64 = double(temp.f32);
	// beq cr6,0x824809ac
	if (ctx.r11.u32 != 0) {
		// lwz r3,72(r7)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 72);
		// lfs f2,32(r5)
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
		ctx.f2.f64 = double(temp.f32);
		// lfs f1,84(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 84);
		ctx.f1.f64 = double(temp.f32);
		// cmplwi cr6,r3,0
		// beq cr6,0x82480984
		if (ctx.r3.u32 != 0) {
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
			// b 0x824809a8
		} else {
		loc_82480984:
			// addi r10,r1,96
			ctx.r10.s64 = ctx.r1.s64 + 96;
			// stfs f3,96(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stfs f3,100(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
			// stfs f31,104(r1)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
			// stw r31,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
			// stfs f30,108(r1)
			temp.f32 = float(var_f30);
			PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
			// stw r10,80(r1)
			PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
		}
	loc_824809A8:
		// stfs f1,16(r5)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	}
loc_824809AC:
	// rlwinm r9,r4,0,28,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r9,0
	// beq cr6,0x824809fc
	if (ctx.r9.u32 != 0) {
		// lwz r3,76(r7)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 76);
		// lfs f2,32(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
		ctx.f2.f64 = double(temp.f32);
		// lfs f1,84(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 84);
		ctx.f1.f64 = double(temp.f32);
		// cmplwi cr6,r3,0
		// beq cr6,0x824809d4
		if (ctx.r3.u32 != 0) {
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
			// b 0x824809f8
		} else {
		loc_824809D4:
			// addi r8,r1,96
			ctx.r8.s64 = ctx.r1.s64 + 96;
			// stfs f3,96(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stfs f30,100(r1)
			temp.f32 = float(var_f30);
			PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
			// stfs f31,104(r1)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
			// stw r31,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
			// stfs f30,108(r1)
			temp.f32 = float(var_f30);
			PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
			// stw r8,80(r1)
			PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
		}
	loc_824809F8:
		// stfs f1,20(r5)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	}
loc_824809FC:
	// rlwinm r6,r4,0,27,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r6,0
	// beq cr6,0x82480a4c
	if (ctx.r6.u32 != 0) {
		// lwz r3,80(r7)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 80);
		// lfs f2,32(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
		ctx.f2.f64 = double(temp.f32);
		// lfs f1,84(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 84);
		ctx.f1.f64 = double(temp.f32);
		// cmplwi cr6,r3,0
		// beq cr6,0x82480a24
		if (ctx.r3.u32 != 0) {
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
			// b 0x82480a48
		} else {
		loc_82480A24:
			// addi r11,r1,96
			ctx.r11.s64 = ctx.r1.s64 + 96;
			// stfs f3,96(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stfs f31,100(r1)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
			// stfs f31,104(r1)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
			// stw r31,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
			// stfs f3,108(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
			// stw r11,80(r1)
			PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
		}
	loc_82480A48:
		// stfs f1,24(r5)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r5.u32 + 24, temp.u32);
	}
loc_82480A4C:
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// cmplwi cr6,r10,0
	// beq cr6,0x82480b9c
	if (ctx.r10.u32 != 0) {
		// lwz r3,64(r7)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
		// lfs f2,32(r5)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
		ctx.f2.f64 = double(temp.f32);
		// lfs f1,84(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 84);
		ctx.f1.f64 = double(temp.f32);
		// cmplwi cr6,r3,0
		// beq cr6,0x82480a74
		if (ctx.r3.u32 != 0) {
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
			// b 0x82480a98
		} else {
		loc_82480A74:
			// addi r9,r1,96
			ctx.r9.s64 = ctx.r1.s64 + 96;
			// stfs f3,96(r1)
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
			// addi r3,r1,80
			ctx.r3.s64 = ctx.r1.s64 + 80;
			// stfs f31,100(r1)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
			// stfs f31,104(r1)
			temp.f32 = float(var_f31);
			PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
			// stw r31,84(r1)
			PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
			// stfs f3,108(r1)
			temp.f32 = float(ctx.f3.f64);
			PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
			// stw r9,80(r1)
			PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
			// bl 0x82480820
			phInst_0820_p39(ctx, base);
		}
	loc_82480A98:
		// lwz r11,52(r7)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 52);
		// fmr f30,f1
		ctx.fpscr.disableFlushMode();
		var_f30 = ctx.f1.f64;
		// cmplwi cr6,r11,1
		// bne cr6,0x82480ab4
		if (ctx.r11.u32 == 1) {
			// lwz r8,0(r5)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
			// stfs f30,0(r8)
			temp.f32 = float(var_f30);
			PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			// b 0x82480b9c
		} else {
		loc_82480AB4:
			// cmplwi cr6,r11,0
			// lis r11,-32255
			// li r6,0
			ctx.r6.s64 = 0;
			// lfs f29,-32048(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32048);
			var_f29 = double(temp.f32);
			// beq cr6,0x82480af8
			if (ctx.r11.u32 != 0) {
				// li r11,0
				ctx.r11.s64 = 0;
			loc_82480ACC:
				// lwz r4,60(r7)
				ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
				// lfsx f0,r11,r4
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r4.u32);
				ctx.f0.f64 = double(temp.f32);
				// fcmpu cr6,f0,f29
				// beq cr6,0x82480af8
				if (ctx.f0.f64 == var_f29) goto loc_82480AF8;
				// lwz r3,0(r5)
				ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
				// addi r6,r6,1
				ctx.r6.s64 = ctx.r6.s64 + 1;
				// stfsx f30,r11,r3
				temp.f32 = float(var_f30);
				PPC_STORE_U32(ctx.r11.u32 + ctx.r3.u32, temp.u32);
				// lwz r10,52(r7)
				ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 52);
				// addi r11,r11,4
				ctx.r11.s64 = ctx.r11.s64 + 4;
				// cmplw cr6,r6,r10
				// blt cr6,0x82480acc
				if (ctx.r6.u32 < ctx.r10.u32) goto loc_82480ACC;
			}
		loc_82480AF8:
			// lwz r9,52(r7)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 52);
			// cmplw cr6,r6,r9
			// bge cr6,0x82480b9c
			if (ctx.r6.u32 >= ctx.r9.u32) {
				// addi r1,r1,160
				ctx.r1.s64 = ctx.r1.s64 + 160;
				// lwz r12,-8(r1)
				ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
				// mtlr r12
				ctx.lr = ctx.r12.u64;
				// lfd f29,-40(r1)
				ctx.fpscr.disableFlushMode();
				ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
				// lfd f30,-32(r1)
				ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
				// lfd f31,-24(r1)
				ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
				// ld r31,-16(r1)
				var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
				// blr
				return;
			}
			// lwz r3,68(r7)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 68);
			// lfs f2,32(r5)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 32);
			ctx.f2.f64 = double(temp.f32);
			// lfs f1,84(r7)
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 84);
			ctx.f1.f64 = double(temp.f32);
			// cmplwi cr6,r3,0
			// beq cr6,0x82480b20
			if (ctx.r3.u32 != 0) {
				// bl 0x82480820
				phInst_0820_p39(ctx, base);
				// b 0x82480b44
			} else {
			loc_82480B20:
				// addi r8,r1,96
				ctx.r8.s64 = ctx.r1.s64 + 96;
				// stfs f3,96(r1)
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(ctx.f3.f64);
				PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
				// addi r3,r1,80
				ctx.r3.s64 = ctx.r1.s64 + 80;
				// stfs f31,100(r1)
				temp.f32 = float(var_f31);
				PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
				// stfs f31,104(r1)
				temp.f32 = float(var_f31);
				PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
				// stw r31,84(r1)
				PPC_STORE_U32(ctx.r1.u32 + 84, var_r31);
				// stfs f3,108(r1)
				temp.f32 = float(ctx.f3.f64);
				PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
				// stw r8,80(r1)
				PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
				// bl 0x82480820
				phInst_0820_p39(ctx, base);
			}
		loc_82480B44:
			// lwz r3,0(r5)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
			// rlwinm r4,r6,2,0,29
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
			// addi r10,r6,1
			ctx.r10.s64 = ctx.r6.s64 + 1;
			// stfsx f1,r4,r3
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f1.f64);
			PPC_STORE_U32(ctx.r4.u32 + ctx.r3.u32, temp.u32);
			// lwz r11,52(r7)
			ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 52);
			// cmplw cr6,r10,r11
			// bge cr6,0x82480b9c
			if (ctx.r10.u32 >= ctx.r11.u32) {
				// addi r1,r1,160
				ctx.r1.s64 = ctx.r1.s64 + 160;
				// lwz r12,-8(r1)
				ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
				// mtlr r12
				ctx.lr = ctx.r12.u64;
				// lfd f29,-40(r1)
				ctx.fpscr.disableFlushMode();
				ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
				// lfd f30,-32(r1)
				ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
				// lfd f31,-24(r1)
				ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
				// ld r31,-16(r1)
				var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
				// blr
				return;
			}
			// rlwinm r11,r10,2,0,29
			ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
		loc_82480B64:
			// lwz r9,60(r7)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + 60);
			// lfsx f13,r11,r9
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f13,f29
			// bne cr6,0x82480b80
			if (ctx.f13.f64 == var_f29) {
				// lwz r8,0(r5)
				ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
				// stfsx f1,r11,r8
				temp.f32 = float(ctx.f1.f64);
				PPC_STORE_U32(ctx.r11.u32 + ctx.r8.u32, temp.u32);
				// b 0x82480b88
			} else {
			loc_82480B80:
				// lwz r6,0(r5)
				ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
				// stfsx f30,r11,r6
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(var_f30);
				PPC_STORE_U32(ctx.r11.u32 + ctx.r6.u32, temp.u32);
			}
		loc_82480B88:
			// lwz r4,52(r7)
			ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 52);
			// addi r10,r10,1
			ctx.r10.s64 = ctx.r10.s64 + 1;
			// addi r11,r11,4
			ctx.r11.s64 = ctx.r11.s64 + 4;
			// cmplw cr6,r10,r4
			// blt cr6,0x82480b64
			if (ctx.r10.u32 < ctx.r4.u32) goto loc_82480B64;
		}
	}
loc_82480B9C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f30,-32(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
	// blr
	return;
}

__attribute__((alias("__imp__aud_0BC0"))) PPC_WEAK_FUNC(aud_0BC0);
PPC_FUNC_IMPL(__imp__aud_0BC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	double var_f30 = 0.0;
	double var_f31 = 0.0;
	double var_f29 = 0.0;
	PPCRegister temp{};
	// FRAME: size=160, savegprlr_29
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// lis r11,-32256
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// mr r29,r6
	var_r29 = ctx.r6.u32;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
	// lfs f0,15784(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	// bne cr6,0x82480c40
	if (ctx.f13.f64 == ctx.f0.f64) {
		// clrlwi r9,r30,31
		ctx.r9.u64 = var_r30 & 0x1;
		// cmplwi cr6,r9,0
		// beq cr6,0x82480c10
		if (ctx.r9.u32 != 0) {
			// lfs f11,12(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
			ctx.f11.f64 = double(temp.f32);
			// b 0x82480c14
		} else {
		loc_82480C10:
			// lfs f11,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f11.f64 = double(temp.f32);
		}
	loc_82480C14:
		// rlwinm r11,r30,0,28,29
		ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0xC;
		// cmplwi cr6,r11,0
		// beq cr6,0x82480c28
		if (ctx.r11.u32 != 0) {
			// lfs f12,20(r10)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
			ctx.f12.f64 = double(temp.f32);
			// b 0x82480c2c
		} else {
		loc_82480C28:
			// lfs f12,80(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
			ctx.f12.f64 = double(temp.f32);
		}
	loc_82480C2C:
		// rlwinm r11,r30,0,27,27
		ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x10;
		// cmplwi cr6,r11,0
		// beq cr6,0x82480e40
		if (ctx.r11.u32 == 0) goto loc_82480E40;
		// lfs f0,28(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
		ctx.f0.f64 = double(temp.f32);
		// b 0x82480e44
	} else {
	loc_82480C40:
		// lis r11,-32255
		// lfs f12,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f12.f64 = double(temp.f32);
		// lfs f0,-32048(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32048);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f12,f0
		// beq cr6,0x82480dfc
		if (ctx.f12.f64 != ctx.f0.f64) {
			// lis r11,-32248
			// lfs f11,32(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 32);
			ctx.f11.f64 = double(temp.f32);
			// lfs f0,-24984(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24984);
			ctx.f0.f64 = double(temp.f32);
			// fcmpu cr6,f11,f0
			// bso cr6,0x82480c6c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82480C64, "bso");
			// ble cr6,0x82480dfc
			if (ctx.f11.f64 <= ctx.f0.f64) goto loc_82480DFC;
		loc_82480C6C:
			// addi r5,r1,88
			ctx.r5.s64 = ctx.r1.s64 + 88;
			// addi r4,r31,28
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 28;
			// addi r3,r3,24
			ctx.r3.s64 = ctx.r3.s64 + 24;
			// bl 0x82481a68
			aud_1A68(ctx, base);
			// addi r4,r31,4
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 4;
			// addi r3,r1,88
			ctx.r3.s64 = ctx.r1.s64 + 88;
			// bl 0x82481a40
			aud_1A40(ctx, base);
			// fmr f0,f1
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = ctx.f1.f64;
			// lwz r10,0(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
			// lis r11,-32256
			// lfs f10,32(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 32);
			ctx.f10.f64 = double(temp.f32);
			// lfs f9,0(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
			ctx.f9.f64 = double(temp.f32);
			// lfs f30,27200(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);
			var_f30 = double(temp.f32);
			// fmuls f1,f9,f30
			ctx.f1.f64 = double(float(ctx.f9.f64 * var_f30));
			// fdivs f31,f0,f10
			var_f31 = double(float(ctx.f0.f64 / ctx.f10.f64));
			// bl 0x824302b0
			phBoundCapsule_02B0_g(ctx, base);
			// frsp f29,f1
			ctx.fpscr.disableFlushMode();
			var_f29 = double(float(ctx.f1.f64));
			// fcmpu cr6,f31,f29
			// blt cr6,0x82480d0c
			if (var_f31 >= var_f29) {
				// bso cr6,0x82480d0c
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x82480CB8, "bso");
				// clrlwi r9,r30,31
				ctx.r9.u64 = var_r30 & 0x1;
				// cmplwi cr6,r9,0
				// beq cr6,0x82480cd4
				if (ctx.r9.u32 != 0) {
					// lwz r8,0(r31)
					ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
					// lfs f11,8(r8)
					temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
					ctx.f11.f64 = double(temp.f32);
					// b 0x82480cd8
				} else {
				loc_82480CD4:
					// lfs f11,80(r1)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
					ctx.f11.f64 = double(temp.f32);
				}
			loc_82480CD8:
				// rlwinm r7,r30,0,28,29
				ctx.r7.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0xC;
				// cmplwi cr6,r7,0
				// beq cr6,0x82480cf0
				if (ctx.r7.u32 != 0) {
					// lwz r6,0(r31)
					ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
					// lfs f12,16(r6)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16);
					ctx.f12.f64 = double(temp.f32);
					// b 0x82480cf4
				} else {
				loc_82480CF0:
					// lfs f12,80(r1)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
					ctx.f12.f64 = double(temp.f32);
				}
			loc_82480CF4:
				// rlwinm r11,r30,0,27,27
				ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x10;
				// cmplwi cr6,r11,0
				// beq cr6,0x82480e40
				if (ctx.r11.u32 == 0) goto loc_82480E40;
				// lwz r5,0(r31)
				ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 0);
				// lfs f0,24(r5)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 24);
				ctx.f0.f64 = double(temp.f32);
				// b 0x82480e44
				goto loc_82480E44;
			}
		loc_82480D0C:
			// lwz r4,0(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 0);
			// lfs f8,4(r4)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
			ctx.f8.f64 = double(temp.f32);
			// fmuls f1,f8,f30
			ctx.f1.f64 = double(float(ctx.f8.f64 * var_f30));
			// bl 0x824302b0
			phBoundCapsule_02B0_g(ctx, base);
			// frsp f0,f1
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = double(float(ctx.f1.f64));
			// fcmpu cr6,f31,f0
			// bgt cr6,0x82480d7c
			if (var_f31 <= ctx.f0.f64) {
				// bso cr6,0x82480d7c
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x82480D28, "bso");
				// clrlwi r9,r30,31
				ctx.r9.u64 = var_r30 & 0x1;
				// cmplwi cr6,r9,0
				// beq cr6,0x82480d44
				if (ctx.r9.u32 != 0) {
					// lwz r3,0(r31)
					ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
					// lfs f11,12(r3)
					temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
					ctx.f11.f64 = double(temp.f32);
					// b 0x82480d48
				} else {
				loc_82480D44:
					// lfs f11,80(r1)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
					ctx.f11.f64 = double(temp.f32);
				}
			loc_82480D48:
				// rlwinm r11,r30,0,28,29
				ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0xC;
				// cmplwi cr6,r11,0
				// beq cr6,0x82480d60
				if (ctx.r11.u32 != 0) {
					// lwz r10,0(r31)
					ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
					// lfs f12,20(r10)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
					ctx.f12.f64 = double(temp.f32);
					// b 0x82480d64
				} else {
				loc_82480D60:
					// lfs f12,80(r1)
					ctx.fpscr.disableFlushMode();
					temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
					ctx.f12.f64 = double(temp.f32);
				}
			loc_82480D64:
				// rlwinm r11,r30,0,27,27
				ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x10;
				// cmplwi cr6,r11,0
				// beq cr6,0x82480e40
				if (ctx.r11.u32 == 0) goto loc_82480E40;
				// lwz r8,0(r31)
				ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 0);
				// lfs f0,28(r8)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 28);
				ctx.f0.f64 = double(temp.f32);
				// b 0x82480e44
				goto loc_82480E44;
			}
		loc_82480D7C:
			// fsubs f7,f31,f29
			ctx.fpscr.disableFlushMode();
			ctx.f7.f64 = double(float(var_f31 - var_f29));
			// clrlwi r9,r30,31
			ctx.r9.u64 = var_r30 & 0x1;
			// fsubs f6,f0,f29
			ctx.f6.f64 = double(float(ctx.f0.f64 - var_f29));
			// cmplwi cr6,r9,0
			// fdivs f0,f7,f6
			ctx.f0.f64 = double(float(ctx.f7.f64 / ctx.f6.f64));
			// beq cr6,0x82480dac
			if (ctx.r9.u32 != 0) {
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
				// lfs f13,8(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
				ctx.f13.f64 = double(temp.f32);
				// lfs f5,12(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
				ctx.f5.f64 = double(temp.f32);
				// fsubs f4,f5,f13
				ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f13.f64));
				// fmadds f11,f4,f0,f13
				ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f13.f64));
				// b 0x82480db0
			} else {
			loc_82480DAC:
				// lfs f11,80(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				ctx.f11.f64 = double(temp.f32);
			}
		loc_82480DB0:
			// rlwinm r7,r30,0,28,29
			ctx.r7.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0xC;
			// cmplwi cr6,r7,0
			// beq cr6,0x82480dd4
			if (ctx.r7.u32 != 0) {
				// lwz r11,0(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
				// lfs f13,16(r11)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
				ctx.f13.f64 = double(temp.f32);
				// lfs f3,20(r11)
				temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
				ctx.f3.f64 = double(temp.f32);
				// fsubs f2,f3,f13
				ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
				// fmadds f12,f2,f0,f13
				ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f13.f64));
				// b 0x82480dd8
			} else {
			loc_82480DD4:
				// lfs f12,80(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				ctx.f12.f64 = double(temp.f32);
			}
		loc_82480DD8:
			// rlwinm r11,r30,0,27,27
			ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x10;
			// cmplwi cr6,r11,0
			// beq cr6,0x82480e40
			if (ctx.r11.u32 == 0) goto loc_82480E40;
			// lwz r10,0(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
			// lfs f13,24(r10)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
			ctx.f13.f64 = double(temp.f32);
			// lfs f1,28(r10)
			temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
			ctx.f1.f64 = double(temp.f32);
			// fsubs f10,f1,f13
			ctx.f10.f64 = double(float(ctx.f1.f64 - ctx.f13.f64));
			// fmadds f0,f10,f0,f13
			ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f13.f64));
			// b 0x82480e44
		} else {
		loc_82480DFC:
			// clrlwi r9,r30,31
			ctx.r9.u64 = var_r30 & 0x1;
			// cmplwi cr6,r9,0
			// beq cr6,0x82480e10
			if (ctx.r9.u32 != 0) {
				// lfs f11,8(r10)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
				ctx.f11.f64 = double(temp.f32);
				// b 0x82480e14
			} else {
			loc_82480E10:
				// lfs f11,80(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				ctx.f11.f64 = double(temp.f32);
			}
		loc_82480E14:
			// rlwinm r6,r30,0,28,29
			ctx.r6.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0xC;
			// cmplwi cr6,r6,0
			// beq cr6,0x82480e28
			if (ctx.r6.u32 != 0) {
				// lfs f12,16(r10)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
				ctx.f12.f64 = double(temp.f32);
				// b 0x82480e2c
			} else {
			loc_82480E28:
				// lfs f12,80(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				ctx.f12.f64 = double(temp.f32);
			}
		loc_82480E2C:
			// rlwinm r11,r30,0,27,27
			ctx.r11.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x10;
			// cmplwi cr6,r11,0
			// beq cr6,0x82480e40
			if (ctx.r11.u32 != 0) {
				// lfs f0,24(r10)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
				ctx.f0.f64 = double(temp.f32);
				// b 0x82480e44
			} else {
			loc_82480E40:
				// lfs f0,80(r1)
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
				ctx.f0.f64 = double(temp.f32);
			}
		}
	}
loc_82480E44:
	// cmplwi cr6,r9,0
	// beq cr6,0x82480e5c
	if (ctx.r9.u32 != 0) {
		// lwz r10,0(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lfs f9,0(r10)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
		ctx.f9.f64 = double(temp.f32);
		// fmuls f8,f9,f11
		ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
		// stfs f8,0(r10)
		temp.f32 = float(ctx.f8.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	}
loc_82480E5C:
	// rlwinm r5,r30,0,29,29
	ctx.r5.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x4;
	// cmplwi cr6,r5,0
	// beq cr6,0x82480e74
	if (ctx.r5.u32 != 0) {
		// lfs f7,16(r29)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r29 + 16);
		ctx.f7.f64 = double(temp.f32);
		// fmuls f6,f12,f7
		ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
		// stfs f6,16(r29)
		temp.f32 = float(ctx.f6.f64);
		PPC_STORE_U32(var_r29 + 16, temp.u32);
	}
loc_82480E74:
	// rlwinm r4,r30,0,28,28
	ctx.r4.u64 = __builtin_rotateleft64(var_r30 | (var_r30 << 32), 0) & 0x8;
	// cmplwi cr6,r4,0
	// beq cr6,0x82480e8c
	if (ctx.r4.u32 != 0) {
		// lfs f5,20(r29)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r29 + 20);
		ctx.f5.f64 = double(temp.f32);
		// fmuls f4,f5,f12
		ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
		// stfs f4,20(r29)
		temp.f32 = float(ctx.f4.f64);
		PPC_STORE_U32(var_r29 + 20, temp.u32);
	}
loc_82480E8C:
	// cmplwi cr6,r11,0
	// beq cr6,0x82480ea0
	if (ctx.r11.u32 != 0) {
		// lfs f3,24(r29)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r29 + 24);
		ctx.f3.f64 = double(temp.f32);
		// fmuls f2,f3,f0
		ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
		// stfs f2,24(r29)
		temp.f32 = float(ctx.f2.f64);
		PPC_STORE_U32(var_r29 + 24, temp.u32);
	}
loc_82480EA0:
	return;
}

__attribute__((alias("__imp__aud_0EB8"))) PPC_WEAK_FUNC(aud_0EB8);
PPC_FUNC_IMPL(__imp__aud_0EB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_29
	// lis r11,-32256
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lfs f0,15788(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	// lfs f12,32(r31)
	temp.u32 = PPC_LOAD_U32(var_r31 + 32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f31,15784(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);
	var_f31 = double(temp.f32);
	// lis r11,-32248
	// fmr f10,f31
	ctx.f10.f64 = var_f31;
	// lfs f13,-24984(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24984);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	// ble cr6,0x82480f94
	if (ctx.f12.f64 > ctx.f13.f64) {
		// addi r5,r1,80
		ctx.r5.s64 = ctx.r1.s64 + 80;
		// addi r4,r30,28
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 28;
		// addi r3,r29,24
		ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 24;
		// bl 0x82481a68
		aud_1A68(ctx, base);
		// addi r4,r29,36
		ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 36;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x82481a40
		aud_1A40(ctx, base);
		// lfs f11,32(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 32);
		ctx.f11.f64 = double(temp.f32);
		// addi r4,r30,40
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 40;
		// fdivs f31,f1,f11
		var_f31 = double(float(ctx.f1.f64 / ctx.f11.f64));
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// bl 0x82481a40
		aud_1A40(ctx, base);
		// lfs f10,32(r31)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(var_r31 + 32);
		ctx.f10.f64 = double(temp.f32);
		// fdivs f10,f1,f10
		ctx.f10.f64 = double(float(ctx.f1.f64 / ctx.f10.f64));
		// lfs f0,88(r30)
		temp.u32 = PPC_LOAD_U32(var_r30 + 88);
		ctx.f0.f64 = double(temp.f32);
		// fmuls f13,f0,f31
		ctx.f13.f64 = double(float(ctx.f0.f64 * var_f31));
		// lis r11,-32165
		// fmuls f12,f0,f10
		ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
		// lfs f0,9028(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 9028);
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f13,f0
		// ble cr6,0x82480f58
		if (ctx.f13.f64 > ctx.f0.f64) {
			// fmr f13,f0
			ctx.f13.f64 = ctx.f0.f64;
			// b 0x82480f68
		} else {
		loc_82480F58:
			// fneg f11,f0
			ctx.fpscr.disableFlushMode();
			ctx.f11.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// fcmpu cr6,f13,f11
			// bge cr6,0x82480f68
			if (ctx.f13.f64 >= ctx.f11.f64) goto loc_82480F68;
			// fmr f13,f11
			ctx.f13.f64 = ctx.f11.f64;
		}
	loc_82480F68:
		// fcmpu cr6,f12,f0
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x82480f7c
		if (ctx.f12.f64 <= ctx.f0.f64) {
			// fneg f0,f0
			ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
			// fcmpu cr6,f12,f0
			// bge cr6,0x82480f80
			if (ctx.f12.f64 >= ctx.f0.f64) goto loc_82480F80;
		}
	loc_82480F7C:
		// fmr f12,f0
		ctx.fpscr.disableFlushMode();
		ctx.f12.f64 = ctx.f0.f64;
	loc_82480F80:
		// lis r11,-32162
		ctx.r11.s64 = -2107768832;
		// lfs f0,30876(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 30876);  /* glob:lbl_825E789C @ 0x825e789c */
		ctx.f0.f64 = double(temp.f32);
		// fsubs f9,f0,f13
		ctx.f9.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
		// fsubs f8,f0,f12
		ctx.f8.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
		// fdivs f0,f9,f8
		ctx.f0.f64 = double(float(ctx.f9.f64 / ctx.f8.f64));
	}
loc_82480F94:
	// stfs f0,28(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 28, temp.u32);
	// stfs f31,40(r31)
	temp.f32 = float(var_f31);
	PPC_STORE_U32(var_r31 + 40, temp.u32);
	// stfs f10,36(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(var_r31 + 36, temp.u32);
	return;
}

__attribute__((alias("__imp__fiAsciiTokenizer_0FB0_fw"))) PPC_WEAK_FUNC(fiAsciiTokenizer_0FB0_fw);
PPC_FUNC_IMPL(__imp__fiAsciiTokenizer_0FB0_fw) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	// FRAME: size=192, savegprlr_28
	// lis r11,-32248
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// li r6,0
	ctx.r6.s64 = 0;
	// lfd f0,-24800(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -24800);  /* glob:0x825d9f20 */
	// lis r11,-32248
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f0.u64);
	// lfd f7,-24808(r11)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r11.u32 + -24808);  /* glob:lbl_825D9F18 @ 0x825d9f18 */
	// lis r11,-32162
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// lwz r7,30872(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 30872);  /* glob:lbl_825E7898 @ 0x825e7898 */
	// lis r11,-32255
	// cmplwi cr6,r7,2
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 2, ctx.xer);
	// lfd f0,-32008(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -32008);  /* glob:0x825d82f8 */
	// lis r11,-32248
	// stfd f0,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f0.u64);
	// lfd f0,-23832(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -23832);  /* glob:0x825da2e8 */
	// lis r11,-32248
	// stfd f0,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.f0.u64);
	// lfd f12,-25856(r11)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r11.u32 + -25856);  /* glob:0x825d9b00 */
	// lis r11,-32255
	// stfd f12,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.f12.u64);
	// lfd f0,-32016(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -32016);  /* glob:0x825d82f0 */
	// lis r11,-32255
	// stfd f0,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.f0.u64);
	// lfd f0,-32024(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -32024);  /* glob:0x825d82e8 */
	// lis r11,-32255
	// stfd f0,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f0.u64);
	// lfd f0,-32032(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -32032);  /* glob:0x825d82e0 */
	// stfd f0,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.f0.u64);
	// beq cr6,0x8248104c
	if (!(ctx.cr6.eq)) {
		// cmplwi cr6,r7,6
		// bne cr6,0x82481050
		if (ctx.r7.u32 != 6) goto loc_82481050;
		// addi r6,r1,96
		ctx.r6.s64 = ctx.r1.s64 + 96;
		// b 0x82481050
	} else {
	loc_8248104C:
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
	}
loc_82481050:
	// lis r11,-32256
	// lis r8,-32248
	// lis r9,-32248
	// lis r10,-32248
	// li r29,-1
	var_r29 = (uint32_t)(-1);
	// lfd f13,27360(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + 27360);  /* glob:0x825e6ae0 */
	// lis r11,-32248
	// lfd f8,-24456(r8)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r8.u32 + -24456);
	// li r30,-1
	var_r30 = (uint32_t)(-1);
	// lfd f10,-25296(r9)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r9.u32 + -25296);
	// cmpwi cr6,r7,4
	// lfd f9,-24464(r10)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r10.u32 + -24464);
	// lfd f11,-24520(r11)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r11.u32 + -24520);  /* glob:0x825da038 */
	// li r11,0
	ctx.r11.s64 = 0;
	// blt cr6,0x8248122c
	if (ctx.r7.s32 >= 4) {
		// addi r8,r7,-3
		ctx.r8.s64 = ctx.r7.s64 + -3;
		// li r10,2
		ctx.r10.s64 = 2;
		// addi r9,r6,16
		ctx.r9.s64 = ctx.r6.s64 + 16;
	loc_82481098:
		// lfd f0,-16(r9)
		ctx.fpscr.disableFlushMode();
		ctx.f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + -16);
		// fsub f0,f1,f0
		ctx.f0.f64 = ctx.f1.f64 - ctx.f0.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x824810b0
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fsub f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 - ctx.f10.f64;
			// b 0x824810bc
		} else {
		loc_824810B0:
			// fcmpu cr6,f0,f8
			ctx.fpscr.disableFlushMode();
			// bge cr6,0x824810bc
			if (ctx.f0.f64 >= ctx.f8.f64) goto loc_824810BC;
			// fadd f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 + ctx.f10.f64;
		}
	loc_824810BC:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x824810dc
		if (ctx.f0.f64 <= ctx.f12.f64) {
			// bso cr6,0x824810dc
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x824810C4, "bso");
			// fcmpu cr6,f0,f13
			// ble cr6,0x824810dc
			if (ctx.f0.f64 <= ctx.f13.f64) goto loc_824810DC;
			// mr r29,r11
			var_r29 = ctx.r11.u32;
			// fmr f13,f0
			ctx.f13.f64 = ctx.f0.f64;
			// b 0x824810f8
		} else {
		loc_824810DC:
			// fcmpu cr6,f0,f12
			ctx.fpscr.disableFlushMode();
			// blt cr6,0x824810f8
			if (ctx.f0.f64 < ctx.f12.f64) goto loc_824810F8;
			// bso cr6,0x824810f8
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x824810E4, "bso");
			// fcmpu cr6,f0,f11
			// bge cr6,0x824810f8
			if (ctx.f0.f64 >= ctx.f11.f64) goto loc_824810F8;
			// mr r30,r11
			var_r30 = ctx.r11.u32;
			// fmr f11,f0
			ctx.f11.f64 = ctx.f0.f64;
		}
	loc_824810F8:
		// lfd f6,-8(r9)
		ctx.fpscr.disableFlushMode();
		ctx.f6.u64 = PPC_LOAD_U64(ctx.r9.u32 + -8);
		// fsub f0,f1,f6
		ctx.f0.f64 = ctx.f1.f64 - ctx.f6.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x82481110
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fsub f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 - ctx.f10.f64;
			// b 0x8248111c
		} else {
		loc_82481110:
			// fcmpu cr6,f0,f8
			ctx.fpscr.disableFlushMode();
			// bge cr6,0x8248111c
			if (ctx.f0.f64 >= ctx.f8.f64) goto loc_8248111C;
			// fadd f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 + ctx.f10.f64;
		}
	loc_8248111C:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x8248113c
		if (ctx.f0.f64 <= ctx.f12.f64) {
			// bso cr6,0x8248113c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82481124, "bso");
			// fcmpu cr6,f0,f13
			// ble cr6,0x8248113c
			if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8248113C;
			// addi r29,r10,-1
			var_r29 = (uint32_t)(ctx.r10.s64 + -1);  // addr:0x8207ffff
			// fmr f13,f0
			ctx.f13.f64 = ctx.f0.f64;
			// b 0x82481158
		} else {
		loc_8248113C:
			// fcmpu cr6,f0,f12
			ctx.fpscr.disableFlushMode();
			// blt cr6,0x82481158
			if (ctx.f0.f64 < ctx.f12.f64) goto loc_82481158;
			// bso cr6,0x82481158
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82481144, "bso");
			// fcmpu cr6,f0,f11
			// bge cr6,0x82481158
			if (ctx.f0.f64 >= ctx.f11.f64) goto loc_82481158;
			// addi r30,r10,-1
			var_r30 = (uint32_t)(ctx.r10.s64 + -1);  // addr:0x8207ffff
			// fmr f11,f0
			ctx.f11.f64 = ctx.f0.f64;
		}
	loc_82481158:
		// lfd f5,0(r9)
		ctx.fpscr.disableFlushMode();
		ctx.f5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
		// fsub f0,f1,f5
		ctx.f0.f64 = ctx.f1.f64 - ctx.f5.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x82481170
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fsub f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 - ctx.f10.f64;
			// b 0x8248117c
		} else {
		loc_82481170:
			// fcmpu cr6,f0,f8
			ctx.fpscr.disableFlushMode();
			// bge cr6,0x8248117c
			if (ctx.f0.f64 >= ctx.f8.f64) goto loc_8248117C;
			// fadd f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 + ctx.f10.f64;
		}
	loc_8248117C:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x8248119c
		if (ctx.f0.f64 <= ctx.f12.f64) {
			// bso cr6,0x8248119c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82481184, "bso");
			// fcmpu cr6,f0,f13
			// ble cr6,0x8248119c
			if (ctx.f0.f64 <= ctx.f13.f64) goto loc_8248119C;
			// mr r29,r10
			var_r29 = ctx.r10.u32;
			// fmr f13,f0
			ctx.f13.f64 = ctx.f0.f64;
			// b 0x824811b8
		} else {
		loc_8248119C:
			// fcmpu cr6,f0,f12
			ctx.fpscr.disableFlushMode();
			// blt cr6,0x824811b8
			if (ctx.f0.f64 < ctx.f12.f64) goto loc_824811B8;
			// bso cr6,0x824811b8
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x824811A4, "bso");
			// fcmpu cr6,f0,f11
			// bge cr6,0x824811b8
			if (ctx.f0.f64 >= ctx.f11.f64) goto loc_824811B8;
			// mr r30,r10
			var_r30 = ctx.r10.u32;
			// fmr f11,f0
			ctx.f11.f64 = ctx.f0.f64;
		}
	loc_824811B8:
		// lfd f4,8(r9)
		ctx.fpscr.disableFlushMode();
		ctx.f4.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
		// fsub f0,f1,f4
		ctx.f0.f64 = ctx.f1.f64 - ctx.f4.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x824811d0
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fsub f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 - ctx.f10.f64;
			// b 0x824811dc
		} else {
		loc_824811D0:
			// fcmpu cr6,f0,f8
			ctx.fpscr.disableFlushMode();
			// bge cr6,0x824811dc
			if (ctx.f0.f64 >= ctx.f8.f64) goto loc_824811DC;
			// fadd f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 + ctx.f10.f64;
		}
	loc_824811DC:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x824811fc
		if (ctx.f0.f64 <= ctx.f12.f64) {
			// bso cr6,0x824811fc
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x824811E4, "bso");
			// fcmpu cr6,f0,f13
			// ble cr6,0x824811fc
			if (ctx.f0.f64 <= ctx.f13.f64) goto loc_824811FC;
			// addi r29,r10,1
			var_r29 = (uint32_t)(ctx.r10.s64 + 1);  // addr:0x82080001
			// fmr f13,f0
			ctx.f13.f64 = ctx.f0.f64;
			// b 0x82481218
		} else {
		loc_824811FC:
			// fcmpu cr6,f0,f12
			ctx.fpscr.disableFlushMode();
			// blt cr6,0x82481218
			if (ctx.f0.f64 < ctx.f12.f64) goto loc_82481218;
			// bso cr6,0x82481218
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82481204, "bso");
			// fcmpu cr6,f0,f11
			// bge cr6,0x82481218
			if (ctx.f0.f64 >= ctx.f11.f64) goto loc_82481218;
			// addi r30,r10,1
			var_r30 = (uint32_t)(ctx.r10.s64 + 1);  // addr:0x82080001
			// fmr f11,f0
			ctx.f11.f64 = ctx.f0.f64;
		}
	loc_82481218:
		// addi r11,r11,4
		ctx.r11.s64 = ctx.r11.s64 + 4;
		// addi r9,r9,32
		ctx.r9.s64 = ctx.r9.s64 + 32;
		// addi r10,r10,4
		ctx.r10.s64 = ctx.r10.s64 + 4;
		// cmplw cr6,r11,r8
		// blt cr6,0x82481098
		if (ctx.r11.u32 < ctx.r8.u32) goto loc_82481098;
	}
loc_8248122C:
	// cmplw cr6,r11,r7
	// bge cr6,0x824812ac
	if (ctx.r11.u32 < ctx.r7.u32) {
		// rlwinm r10,r11,3,0,28
		ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
		// add r10,r10,r6
		ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	loc_8248123C:
		// lfd f3,0(r10)
		ctx.fpscr.disableFlushMode();
		ctx.f3.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
		// fsub f0,f1,f3
		ctx.f0.f64 = ctx.f1.f64 - ctx.f3.f64;
		// fcmpu cr6,f0,f9
		// ble cr6,0x82481254
		if (ctx.f0.f64 > ctx.f9.f64) {
			// fsub f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 - ctx.f10.f64;
			// b 0x82481260
		} else {
		loc_82481254:
			// fcmpu cr6,f0,f8
			ctx.fpscr.disableFlushMode();
			// bge cr6,0x82481260
			if (ctx.f0.f64 >= ctx.f8.f64) goto loc_82481260;
			// fadd f0,f0,f10
			ctx.f0.f64 = ctx.f0.f64 + ctx.f10.f64;
		}
	loc_82481260:
		// fcmpu cr6,f0,f12
		ctx.fpscr.disableFlushMode();
		// bgt cr6,0x82481280
		if (ctx.f0.f64 <= ctx.f12.f64) {
			// bso cr6,0x82481280
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82481268, "bso");
			// fcmpu cr6,f0,f13
			// ble cr6,0x82481280
			if (ctx.f0.f64 <= ctx.f13.f64) goto loc_82481280;
			// mr r29,r11
			var_r29 = ctx.r11.u32;
			// fmr f13,f0
			ctx.f13.f64 = ctx.f0.f64;
			// b 0x8248129c
		} else {
		loc_82481280:
			// fcmpu cr6,f0,f12
			ctx.fpscr.disableFlushMode();
			// blt cr6,0x8248129c
			if (ctx.f0.f64 < ctx.f12.f64) goto loc_8248129C;
			// bso cr6,0x8248129c
			// UNIMPLEMENTED: bso
			PPC_UNIMPLEMENTED(0x82481288, "bso");
			// fcmpu cr6,f0,f11
			// bge cr6,0x8248129c
			if (ctx.f0.f64 >= ctx.f11.f64) goto loc_8248129C;
			// mr r30,r11
			var_r30 = ctx.r11.u32;
			// fmr f11,f0
			ctx.f11.f64 = ctx.f0.f64;
		}
	loc_8248129C:
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// addi r10,r10,8
		ctx.r10.s64 = ctx.r10.s64 + 8;
		// cmplw cr6,r11,r7
		// blt cr6,0x8248123c
		if (ctx.r11.u32 < ctx.r7.u32) goto loc_8248123C;
	}
loc_824812AC:
	// fcmpu cr6,f13,f12
	ctx.fpscr.disableFlushMode();
	// bge cr6,0x824812b8
	if (ctx.f13.f64 < ctx.f12.f64) {
		// fneg f13,f13
		ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	}
loc_824812B8:
	// fcmpu cr6,f11,f12
	ctx.fpscr.disableFlushMode();
	// bge cr6,0x824812c4
	if (ctx.f11.f64 < ctx.f12.f64) {
		// fneg f11,f11
		ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	}
loc_824812C4:
	// cmpwi cr6,r29,-1
	// beq cr6,0x824812d4
	if ((int32_t)var_r29 != -1) {
		// cmpwi cr6,r30,-1
		// bne cr6,0x824812dc
		if ((int32_t)var_r30 != -1) goto loc_824812DC;
	}
loc_824812D4:
	// li r30,0
	var_r30 = 0;
	// li r29,0
	var_r29 = 0;
loc_824812DC:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0)/* fiAsciiTokenizer::vtable@+0x0 */;
	// rlwinm r9,r28,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 12);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r8,0
	// lfsx f31,r9,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	var_f31 = double(temp.f32);
	// ble cr6,0x82481328
	if (ctx.r8.u32 > 0) {
		// lis r10,-32256
		ctx.r10.s64 = -2113929216;
		// lfs f0,15784(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
		ctx.f0.f64 = double(temp.f32);
	loc_82481300:
		// lwz r7,8(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r6,0(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0)/* fiAsciiTokenizer::vtable@+0x0 */;
		// mullw r10,r11,r7
		ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r7.s32);
		// add r5,r10,r28
		ctx.r5.u64 = ctx.r10.u64 + var_r28;
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// rlwinm r4,r5,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// stfsx f0,r4,r6
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r4.u32 + ctx.r6.u32, temp.u32);
		// lwz r3,12(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 12);
		// cmplw cr6,r11,r3
		// blt cr6,0x82481300
		if (ctx.r11.u32 < ctx.r3.u32) goto loc_82481300;
	}
loc_82481328:
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lfd f0,-32040(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -32040);  /* glob:lbl_820082D8 @ 0x820082d8 */
	// fcmpu cr6,f13,f0
	// bge cr6,0x8248135c
	if (ctx.f13.f64 < ctx.f0.f64) {
		// lwz r11,8(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r10,0(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0)/* fiAsciiTokenizer::vtable@+0x0 */;
		// mullw r11,r11,r29
		ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t((int32_t)var_r29);
		// add r9,r11,r28
		ctx.r9.u64 = ctx.r11.u64 + var_r28;
		// rlwinm r8,r9,2,0,29
		ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
		// stfsx f31,r8,r10
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, temp.u32);
		return;
	}
loc_8248135C:
	// fcmpu cr6,f11,f0
	ctx.fpscr.disableFlushMode();
	// bge cr6,0x82481388
	if (ctx.f11.f64 < ctx.f0.f64) {
		// lwz r7,8(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
		// lwz r6,0(r31)
		ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0)/* fiAsciiTokenizer::vtable@+0x0 */;
		// mullw r11,r7,r30
		ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t((int32_t)var_r30);
		// add r5,r11,r28
		ctx.r5.u64 = ctx.r11.u64 + var_r28;
		// rlwinm r4,r5,2,0,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
		// stfsx f31,r4,r6
		temp.f32 = float(var_f31);
		PPC_STORE_U32(ctx.r4.u32 + ctx.r6.u32, temp.u32);
		return;
	}
loc_82481388:
	// fmul f2,f13,f7
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f13.f64 * ctx.f7.f64;
	// fadd f1,f11,f13
	ctx.f1.f64 = ctx.f11.f64 + ctx.f13.f64;
	// fdiv f1,f2,f1
	ctx.f1.f64 = ctx.f2.f64 / ctx.f1.f64;
	// bl 0x82432628
	fiAsciiTokenizer_2628_g(ctx, base);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0)/* fiAsciiTokenizer::vtable@+0x0 */;
	// mullw r11,r29,r11
	ctx.r11.s64 = int64_t((int32_t)var_r29) * int64_t(ctx.r11.s32);
	// add r10,r11,r28
	ctx.r10.u64 = ctx.r11.u64 + var_r28;
	// lis r11,-32256
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f13,15788(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);
	ctx.f13.f64 = double(temp.f32);
	// fadds f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// fdivs f13,f31,f13
	ctx.f13.f64 = double(float(var_f31 / ctx.f13.f64));
	// stfsx f13,r9,r3
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r3.u32, temp.u32);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 8);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 0)/* fiAsciiTokenizer::vtable@+0x0 */;
	// mullw r11,r30,r8
	ctx.r11.s64 = int64_t((int32_t)var_r30) * int64_t(ctx.r8.s32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// add r6,r11,r28
	ctx.r6.u64 = ctx.r11.u64 + var_r28;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// stfsx f12,r5,r7
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + ctx.r7.u32, temp.u32);
	return;
}

__attribute__((alias("__imp__aud_13F0"))) PPC_WEAK_FUNC(aud_13F0);
PPC_FUNC_IMPL(__imp__aud_13F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r26 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	double var_f25 = 0.0;
	double var_f30 = 0.0;
	double var_f29 = 0.0;
	double var_f28 = 0.0;
	double var_f27 = 0.0;
	double var_f24 = 0.0;
	double var_f26 = 0.0;
	double var_f23 = 0.0;
	double var_f31 = 0.0;
	double var_f22 = 0.0;
	double var_f21 = 0.0;
	double var_f20 = 0.0;
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8242f890
	ctx.lr = 0x824813F8;
	__savegprlr_26(ctx, base);
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x824365f8
	__savefpr_20(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	var_r26 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r31,r5
	var_r31 = ctx.r5.u32;
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// lwz r11,0(r26)
	ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 0);
	// addi r3,r26,12
	ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 12;
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(var_r26 + 4);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(var_r26 + 8);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r11.u32);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// bl 0x82481aa0
	aud_1AA0(ctx, base);
	// addi r5,r1,152
	ctx.r5.s64 = ctx.r1.s64 + 152;
	// addi r4,r1,120
	ctx.r4.s64 = ctx.r1.s64 + 120;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82481aa0
	aud_1AA0(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f25,15784(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
	var_f25 = double(temp.f32);
	// lwz r11,52(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 52);
	// fmr f30,f25
	var_f30 = var_f25;
	// fmr f29,f25
	var_f29 = var_f25;
	// cmplwi cr6,r11,1
	// fmr f28,f25
	var_f28 = var_f25;
	// ble cr6,0x82481484
	if (ctx.r11.u32 > 1) {
		// lfs f0,56(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 56);
		ctx.f0.f64 = double(temp.f32);
		// lfs f13,4(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 4);
		ctx.f13.f64 = double(temp.f32);
		// lfs f12,8(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 8);
		ctx.f12.f64 = double(temp.f32);
		// fmuls f30,f13,f0
		var_f30 = double(float(ctx.f13.f64 * ctx.f0.f64));
		// lfs f11,12(r29)
		temp.u32 = PPC_LOAD_U32(var_r29 + 12);
		ctx.f11.f64 = double(temp.f32);
		// fmuls f29,f12,f0
		var_f29 = double(float(ctx.f12.f64 * ctx.f0.f64));
		// fmuls f28,f11,f0
		var_f28 = double(float(ctx.f11.f64 * ctx.f0.f64));
	}
loc_82481484:
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r11,0
	// beq cr6,0x82481778
	if (ctx.r11.u32 != 0) {
		// lis r9,-32256
		// lis r7,-32255
		// lis r8,-32248
		// lis r10,-32255
		// lis r27,-32165
		var_r27 = (uint32_t)(-2107965440);
		// lfs f27,15788(r9)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 15788);
		var_f27 = double(temp.f32);
		// li r28,0
		var_r28 = 0;
		// lfs f24,-32000(r7)
		temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -32000);
		var_f24 = double(temp.f32);
		// lfs f26,-24984(r8)
		temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -24984);
		var_f26 = double(temp.f32);
		// lfs f23,-32048(r10)
		temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -32048);
		var_f23 = double(temp.f32);
		// lwz r9,9024(r27)
		ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 9024);
	loc_824814BC:
		// cmplwi cr6,r11,1
		// ble cr6,0x824815a4
		if (ctx.r11.u32 > 1) {
			// lwz r8,60(r29)
			ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 60);
			// lfsx f1,r8,r28
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + var_r28);
			ctx.f1.f64 = double(temp.f32);
			// fcmpu cr6,f1,f23
			// beq cr6,0x824815a4
			if (ctx.f1.f64 == var_f23) goto loc_824815A4;
			// bl 0x824302b0
			phBoundCapsule_02B0_g(ctx, base);
			// fmr f0,f1
			ctx.fpscr.disableFlushMode();
			ctx.f0.f64 = ctx.f1.f64;
			// lwz r7,60(r29)
			ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 60);
			// lfsx f1,r7,r28
			temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + var_r28);
			ctx.f1.f64 = double(temp.f32);
			// frsp f31,f0
			var_f31 = double(float(ctx.f0.f64));
			// bl 0x824301d8
			phBoundCapsule_01D8_g(ctx, base);
			// lfs f13,24(r29)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r29 + 24);
			ctx.f13.f64 = double(temp.f32);
			// frsp f10,f1
			ctx.f10.f64 = double(float(ctx.f1.f64));
			// lfs f12,16(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 16);
			ctx.f12.f64 = double(temp.f32);
			// fsubs f0,f27,f31
			ctx.f0.f64 = double(float(var_f27 - var_f31));
			// lfs f11,20(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 20);
			ctx.f11.f64 = double(temp.f32);
			// fmuls f3,f12,f13
			ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
			// fmuls f2,f11,f12
			ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
			// lfs f6,28(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 28);
			ctx.f6.f64 = double(temp.f32);
			// fmuls f1,f11,f13
			ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
			// lfs f5,32(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 32);
			ctx.f5.f64 = double(temp.f32);
			// fmuls f22,f12,f12
			var_f22 = double(float(ctx.f12.f64 * ctx.f12.f64));
			// lfs f4,36(r29)
			temp.u32 = PPC_LOAD_U32(var_r29 + 36);
			ctx.f4.f64 = double(temp.f32);
			// fmuls f21,f11,f11
			var_f21 = double(float(ctx.f11.f64 * ctx.f11.f64));
			// lwz r9,9024(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 9024);
			// fmuls f20,f13,f13
			var_f20 = double(float(ctx.f13.f64 * ctx.f13.f64));
			// fmuls f9,f3,f0
			ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
			// fmuls f8,f2,f0
			ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
			// fmuls f7,f1,f0
			ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
			// fmadds f3,f22,f0,f31
			ctx.f3.f64 = double(float(var_f22 * ctx.f0.f64 + var_f31));
			// fmadds f2,f21,f0,f31
			ctx.f2.f64 = double(float(var_f21 * ctx.f0.f64 + var_f31));
			// fmadds f1,f20,f0,f31
			ctx.f1.f64 = double(float(var_f20 * ctx.f0.f64 + var_f31));
			// fmuls f0,f13,f10
			ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
			// fmuls f13,f12,f10
			ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
			// fmuls f12,f11,f10
			ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
			// fsubs f11,f8,f0
			ctx.f11.f64 = double(float(ctx.f8.f64 - ctx.f0.f64));
			// fsubs f10,f7,f13
			ctx.f10.f64 = double(float(ctx.f7.f64 - ctx.f13.f64));
			// fadds f7,f13,f7
			ctx.f7.f64 = double(float(ctx.f13.f64 + ctx.f7.f64));
			// fadds f8,f0,f8
			ctx.f8.f64 = double(float(ctx.f0.f64 + ctx.f8.f64));
			// fadds f13,f12,f9
			ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f9.f64));
			// fsubs f0,f9,f12
			ctx.f0.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
			// fmuls f12,f11,f29
			ctx.f12.f64 = double(float(ctx.f11.f64 * var_f29));
			// fmuls f11,f10,f28
			ctx.f11.f64 = double(float(ctx.f10.f64 * var_f28));
			// fmuls f10,f7,f29
			ctx.f10.f64 = double(float(ctx.f7.f64 * var_f29));
			// fmadds f9,f3,f30,f12
			ctx.f9.f64 = double(float(ctx.f3.f64 * var_f30 + ctx.f12.f64));
			// fmadds f7,f2,f29,f11
			ctx.f7.f64 = double(float(ctx.f2.f64 * var_f29 + ctx.f11.f64));
			// fmadds f3,f1,f28,f10
			ctx.f3.f64 = double(float(ctx.f1.f64 * var_f28 + ctx.f10.f64));
			// fmadds f2,f13,f28,f9
			ctx.f2.f64 = double(float(ctx.f13.f64 * var_f28 + ctx.f9.f64));
			// fmadds f1,f8,f30,f7
			ctx.f1.f64 = double(float(ctx.f8.f64 * var_f30 + ctx.f7.f64));
			// fmadds f0,f0,f30,f3
			ctx.f0.f64 = double(float(ctx.f0.f64 * var_f30 + ctx.f3.f64));
			// fadds f13,f2,f6
			ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f6.f64));
			// stfs f13,104(r1)
			temp.f32 = float(ctx.f13.f64);
			PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
			// fadds f12,f1,f5
			ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
			// stfs f12,108(r1)
			temp.f32 = float(ctx.f12.f64);
			PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
			// fadds f11,f0,f4
			ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f4.f64));
			// stfs f11,112(r1)
			temp.f32 = float(ctx.f11.f64);
			PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
			// b 0x824815bc
		} else {
		loc_824815A4:
			// lwz r6,28(r29)
			ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 28);
			// lwz r5,32(r29)
			ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 32);
			// lwz r4,36(r29)
			ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 36);
			// stw r6,104(r1)
			PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r6.u32);
			// stw r5,108(r1)
			PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
			// stw r4,112(r1)
			PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
		}
	loc_824815BC:
		// lwz r3,52(r29)
		ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 52);
		// cmplwi cr6,r3,1
		// ble cr6,0x8248163c
		if (ctx.r3.u32 > 1) {
			// lwz r11,60(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 60);
			// lfsx f10,r11,r28
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + var_r28);
			ctx.f10.f64 = double(temp.f32);
			// fcmpu cr6,f10,f23
			// bne cr6,0x8248163c
			if (ctx.f10.f64 != var_f23) goto loc_8248163C;
			// lwz r10,0(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
			// li r11,0
			ctx.r11.s64 = 0;
			// lwz r8,12(r31)
			ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 12);
			// cmplwi cr6,r8,0
			ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
			// lfsx f0,r28,r10
			temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r10.u32);
			ctx.f0.f64 = double(temp.f32);
			// ble cr6,0x82481618
		while (ctx.r11.u32 < ctx.r3.u32) {
			loc_824815F0:
				// lwz r7,8(r31)
				ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
				// lwz r6,0(r31)
				ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
				// mullw r10,r7,r11
				ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
				// add r5,r10,r30
				ctx.r5.u64 = ctx.r10.u64 + var_r30;
				// addi r11,r11,1
				ctx.r11.s64 = ctx.r11.s64 + 1;
				// rlwinm r4,r5,2,0,29
				ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
				// stfsx f25,r4,r6
				ctx.fpscr.disableFlushMode();
				temp.f32 = float(var_f25);
				PPC_STORE_U32(ctx.r4.u32 + ctx.r6.u32, temp.u32);
				// lwz r3,12(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 12);
				// cmplw cr6,r11,r3
				// blt cr6,0x824815f0
		}
		loc_82481618:
			// cmpwi cr6,r9,-1
			// beq cr6,0x82481764
			if (ctx.r9.s32 == -1) goto loc_82481764;
			// lwz r11,8(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 8);
			// lwz r10,0(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 0);
			// mullw r11,r11,r9
			ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
			// add r8,r11,r30
			ctx.r8.u64 = ctx.r11.u64 + var_r30;
			// rlwinm r7,r8,2,0,29
			ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
			// stfsx f0,r7,r10
			ctx.fpscr.disableFlushMode();
			temp.f32 = float(ctx.f0.f64);
			PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, temp.u32);
			// b 0x82481764
		} else {
		loc_8248163C:
			// addi r5,r1,136
			ctx.r5.s64 = ctx.r1.s64 + 136;
			// addi r4,r26,24
			ctx.r4.s64 = (int64_t)(int32_t)var_r26 + 24;
			// addi r3,r1,104
			ctx.r3.s64 = ctx.r1.s64 + 104;
			// bl 0x82481a68
			aud_1A68(ctx, base);
			// lfs f13,140(r1)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
			ctx.f13.f64 = double(temp.f32);
			// lfs f8,124(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
			ctx.f8.f64 = double(temp.f32);
			// fmuls f7,f13,f8
			ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
			// lfs f0,144(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
			ctx.f0.f64 = double(temp.f32);
			// lfs f9,128(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
			ctx.f9.f64 = double(temp.f32);
			// lfs f4,92(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
			ctx.f4.f64 = double(temp.f32);
			// fmuls f3,f4,f13
			ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
			// lfs f12,136(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
			ctx.f12.f64 = double(temp.f32);
			// lfs f6,120(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
			ctx.f6.f64 = double(temp.f32);
			// lfs f5,96(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
			ctx.f5.f64 = double(temp.f32);
			// lfs f2,88(r1)
			temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
			ctx.f2.f64 = double(temp.f32);
			// fmadds f1,f0,f9,f7
			ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f9.f64 + ctx.f7.f64));
			// fmadds f0,f5,f0,f3
			ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f3.f64));
			// fmadds f1,f12,f6,f1
			ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f1.f64));
			// fmadds f2,f2,f12,f0
			ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f12.f64 + ctx.f0.f64));
			// fcmpu cr6,f1,f26
			// bgt cr6,0x8248174c
			if (ctx.f1.f64 <= var_f26) {
				// bso cr6,0x8248174c
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x82481690, "bso");
				// fcmpu cr6,f2,f26
				// bgt cr6,0x8248174c
				if (ctx.f2.f64 > var_f26) goto loc_8248174C;
				// bso cr6,0x8248174c
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x8248169C, "bso");
				// fcmpu cr6,f1,f24
				// blt cr6,0x8248174c
				if (ctx.f1.f64 < var_f24) goto loc_8248174C;
				// bso cr6,0x8248174c
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x824816A8, "bso");
				// fcmpu cr6,f2,f24
				// blt cr6,0x8248174c
				if (ctx.f2.f64 < var_f24) goto loc_8248174C;
				// bso cr6,0x8248174c
				// UNIMPLEMENTED: bso
				PPC_UNIMPLEMENTED(0x824816B4, "bso");
				// lwz r9,9024(r27)
				ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 9024);
				// lwz r10,12(r31)
				ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 12);
				// cmpwi cr6,r9,-1
				// mr r11,r10
				ctx.r11.u64 = ctx.r10.u64;
				// beq cr6,0x824816d0
				if (ctx.r9.s32 != -1) {
					// addi r11,r11,-1
					ctx.r11.s64 = ctx.r11.s64 + -1;
				}
			loc_824816D0:
				// clrldi r6,r11,32
				ctx.r6.u64 = ctx.r11.u64 & 0xFFFFFFFF;
				// lwz r5,0(r31)
				ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 0);
				// li r11,0
				ctx.r11.s64 = 0;
				// cmplwi cr6,r10,0
				// std r6,80(r1)
				PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
				// lfsx f13,r28,r5
				ctx.fpscr.disableFlushMode();
				temp.u32 = PPC_LOAD_U32(var_r28 + ctx.r5.u32);
				ctx.f13.f64 = double(temp.f32);
				// lfd f12,80(r1)
				ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
				// fcfid f11,f12
				ctx.f11.f64 = double(ctx.f12.s64);
				// frsp f10,f11
				ctx.f10.f64 = double(float(ctx.f11.f64));
				// fdivs f0,f13,f10
				ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f10.f64));
				// beq cr6,0x82481764
				if (ctx.r10.u32 == 0) goto loc_82481764;
			loc_824816FC:
				// cmplw cr6,r11,r9
				// beq cr6,0x82481720
				if (ctx.r11.u32 != ctx.r9.u32) {
					// lwz r4,8(r31)
					ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 8);
					// lwz r3,0(r31)
					ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
					// mullw r10,r4,r11
					ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r11.s32);
					// add r10,r10,r30
					ctx.r10.u64 = ctx.r10.u64 + var_r30;
					// rlwinm r8,r10,2,0,29
					ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
					// stfsx f0,r8,r3
					ctx.fpscr.disableFlushMode();
					temp.f32 = float(ctx.f0.f64);
					PPC_STORE_U32(ctx.r8.u32 + ctx.r3.u32, temp.u32);
					// b 0x82481738
				} else {
				loc_82481720:
					// lwz r7,8(r31)
					ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 8);
					// lwz r6,0(r31)
					ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 0);
					// mullw r10,r7,r11
					ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
					// add r5,r10,r30
					ctx.r5.u64 = ctx.r10.u64 + var_r30;
					// rlwinm r4,r5,2,0,29
					ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
					// stfsx f25,r4,r6
					ctx.fpscr.disableFlushMode();
					temp.f32 = float(var_f25);
					PPC_STORE_U32(ctx.r4.u32 + ctx.r6.u32, temp.u32);
				}
			loc_82481738:
				// lwz r3,12(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 12);
				// addi r11,r11,1
				ctx.r11.s64 = ctx.r11.s64 + 1;
				// cmplw cr6,r11,r3
				// blt cr6,0x824816fc
				if (ctx.r11.u32 < ctx.r3.u32) goto loc_824816FC;
				// b 0x82481764
			} else {
			loc_8248174C:
				// bl 0x82430fe0
				phBoundCapsule_0FE0_g(ctx, base);
				// mr r5,r30
				ctx.r5.u64 = var_r30;
				// mr r4,r31
				ctx.r4.u64 = var_r31;
				// frsp f1,f1
				ctx.fpscr.disableFlushMode();
				ctx.f1.f64 = double(float(ctx.f1.f64));
				// bl 0x82480fb0
				fiAsciiTokenizer_0FB0_fw(ctx, base);
				// lwz r9,9024(r27)
				ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 9024);
			}
		}
	loc_82481764:
		// lwz r11,52(r29)
		ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 52);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// addi r28,r28,4
		var_r28 = (uint32_t)(var_r28 + 4);
		// cmplw cr6,r30,r11
		// blt cr6,0x824814bc
		if (var_r30 < ctx.r11.u32) goto loc_824814BC;
	}
loc_82481778:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82436644
	__restfpr_20(ctx, base);
	// b 0x8242f8e0
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__aud_1788"))) PPC_WEAK_FUNC(aud_1788);
PPC_FUNC_IMPL(__imp__aud_1788) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	// FRAME: size=208, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// mr r30,r5
	var_r30 = ctx.r5.u32;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
	// addi r3,r31,12
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 12;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 8);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// bl 0x82481aa0
	aud_1AA0(ctx, base);
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82481aa0
	aud_1AA0(ctx, base);
	// addi r8,r29,28
	ctx.r8.s64 = (int64_t)(int32_t)var_r29 + 28;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r31,24
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 24;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r11,8(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// stw r7,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r7.u32);
	// stw r6,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r6.u32);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r11.u32);
	// bl 0x82481a68
	aud_1A68(ctx, base);
	// lfs f13,132(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32248
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f0,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// lfs f6,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f12,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f12.f64 = double(temp.f32);
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f3,f0,f11,f9
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f9.f64));
	// fmadds f2,f7,f0,f5
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + ctx.f5.f64));
	// lfs f0,-24984(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24984);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f31,f12,f8,f3
	var_f31 = double(float(ctx.f12.f64 * ctx.f8.f64 + ctx.f3.f64));
	// fmadds f2,f4,f12,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 + ctx.f2.f64));
	// fcmpu cr6,f31,f0
	// bgt cr6,0x824818b0
	if (var_f31 <= ctx.f0.f64) {
		// bso cr6,0x824818b0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82481858, "bso");
		// fcmpu cr6,f2,f0
		// bgt cr6,0x824818b0
		if (ctx.f2.f64 > ctx.f0.f64) goto loc_824818B0;
		// bso cr6,0x824818b0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82481864, "bso");
		// lis r11,-32255
		ctx.r11.s64 = -2113863680;
		// lfs f0,-32000(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32000);  /* glob:lbl_82008300 @ 0x82008300 */
		ctx.f0.f64 = double(temp.f32);
		// fcmpu cr6,f31,f0
		// blt cr6,0x824818b0
		if (var_f31 < ctx.f0.f64) goto loc_824818B0;
		// bso cr6,0x824818b0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82481878, "bso");
		// fcmpu cr6,f2,f0
		// blt cr6,0x824818b0
		if (ctx.f2.f64 < ctx.f0.f64) goto loc_824818B0;
		// bso cr6,0x824818b0
		// UNIMPLEMENTED: bso
		PPC_UNIMPLEMENTED(0x82481884, "bso");
		// lis r11,-32256
		// lwz r10,4(r30)
		ctx.r10.u64 = PPC_LOAD_U32(var_r30 + 4);
		// lfs f0,15784(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:0x82013da8 */
		ctx.f0.f64 = double(temp.f32);
		// stfs f0,0(r10)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
		// lwz r9,4(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 4);
		// stfs f0,4(r9)
		temp.f32 = float(ctx.f0.f64);
		PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
		return;
	}
loc_824818B0:
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = var_f31;
	// bl 0x82430fe0
	phBoundCapsule_0FE0_g(ctx, base);
	// lis r11,-32256
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// lfs f30,15784(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:0x82013da8 */
	var_f30 = double(temp.f32);
	// fcmpu cr6,f31,f30
	// ble cr6,0x824818f4
	if (var_f31 > var_f30) {
		// bl 0x824301d8
		phBoundCapsule_01D8_g(ctx, base);
		// lwz r8,4(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 4);
		// frsp f1,f1
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = double(float(ctx.f1.f64));
		// stfs f1,0(r8)
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
		// lwz r7,4(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 4);
		// stfs f30,4(r7)
		temp.f32 = float(var_f30);
		PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
		return;
	}
loc_824818F4:
	// lwz r6,4(r30)
	ctx.r6.u64 = PPC_LOAD_U32(var_r30 + 4);
	// fneg f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f30,0(r6)
	temp.f32 = float(var_f30);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// bl 0x824301d8
	phBoundCapsule_01D8_g(ctx, base);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(var_r30 + 4);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,4(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	return;
}

__attribute__((alias("__imp__aud_1920"))) PPC_WEAK_FUNC(aud_1920);
PPC_FUNC_IMPL(__imp__aud_1920) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	PPCRegister temp{};
	// FRAME: size=144, savegprlr_28
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r28,r5
	var_r28 = ctx.r5.u32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r31,28
	ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 28;
	// addi r3,r29,24
	ctx.r3.s64 = (int64_t)(int32_t)var_r29 + 24;
	// mr r30,r6
	var_r30 = ctx.r6.u32;
	// bl 0x82481a68
	aud_1A68(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82481a00
	aud_1A00(ctx, base);
	// stfs f1,32(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(var_r30 + 32, temp.u32);
	// andi. r11,r28,29
	ctx.r11.u64 = var_r28 & 29;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// cmplwi cr6,r11,0
	// beq cr6,0x824819bc
	if (ctx.r11.u32 != 0) {
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// mr r4,r28
		ctx.r4.u64 = var_r28;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x82480920
		aud_0920(ctx, base);
		// lwz r10,52(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 52);
		// cmplwi cr6,r10,1
		// bne cr6,0x824819a0
		if (ctx.r10.u32 == 1) {
			// lwz r9,0(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0);
			// cmplwi cr6,r9,0
			// beq cr6,0x824819a0
			if (ctx.r9.u32 == 0) goto loc_824819A0;
			// mr r6,r30
			ctx.r6.u64 = var_r30;
			// mr r5,r28
			ctx.r5.u64 = var_r28;
			// mr r4,r31
			ctx.r4.u64 = var_r31;
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x82480bc0
			aud_0BC0(ctx, base);
		}
	loc_824819A0:
		// clrlwi r8,r28,31
		ctx.r8.u64 = var_r28 & 0x1;
		// cmplwi cr6,r8,0
		// beq cr6,0x824819bc
		if (ctx.r8.u32 == 0) goto loc_824819BC;
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x824813f0
		aud_13F0(ctx, base);
	}
loc_824819BC:
	// rlwinm r7,r28,0,30,30
	ctx.r7.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 0) & 0x2;
	// cmplwi cr6,r7,0
	// beq cr6,0x824819d8
	if (ctx.r7.u32 != 0) {
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x82481788
		aud_1788(ctx, base);
	}
loc_824819D8:
	// rlwinm r6,r28,0,26,26
	ctx.r6.u64 = __builtin_rotateleft64(var_r28 | (var_r28 << 32), 0) & 0x20;
	// cmplwi cr6,r6,0
	// beq cr6,0x824819f4
	if (ctx.r6.u32 != 0) {
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// mr r4,r31
		ctx.r4.u64 = var_r31;
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x82480eb8
		aud_0EB8(ctx, base);
	}
loc_824819F4:
	return;
}

__attribute__((alias("__imp__aud_1A00"))) PPC_WEAK_FUNC(aud_1A00);
PPC_FUNC_IMPL(__imp__aud_1A00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=96, manual
	// lfs f13,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmul f13,f13,f13
	ctx.f13.f64 = ctx.f13.f64 * ctx.f13.f64;
	// lfs f0,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmadd f11,f0,f0,f13
	ctx.f11.f64 = ctx.f0.f64 * ctx.f0.f64 + ctx.f13.f64;
	// fmadd f1,f12,f12,f11
	ctx.f1.f64 = ctx.f12.f64 * ctx.f12.f64 + ctx.f11.f64;
	// bl 0x824301d0
	phBoundCapsule_01D0_g(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// blr
	return;
}

__attribute__((alias("__imp__aud_1A40"))) PPC_WEAK_FUNC(aud_1A40);
PPC_FUNC_IMPL(__imp__aud_1A40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f7,f11,f10,f12
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f12.f64));
	// fmadds f1,f9,f8,f7
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f7.f64));
	// blr
	return;
}

__attribute__((alias("__imp__aud_1A68"))) PPC_WEAK_FUNC(aud_1A68);
PPC_FUNC_IMPL(__imp__aud_1A68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// stfs f12,0(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// stfs f9,4(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lfs f8,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// stfs f6,8(r5)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__aud_1AA0"))) PPC_WEAK_FUNC(aud_1AA0);
PPC_FUNC_IMPL(__imp__aud_1AA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f9,f11,f10,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 - ctx.f12.f64));
	// stfs f9,0(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f8,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f5,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f3,f5,f4,f6
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 - ctx.f6.f64));
	// stfs f3,4(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lfs f2,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f2,f1
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmsubs f11,f13,f12,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 - ctx.f0.f64));
	// stfs f11,8(r5)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__util_1AF8"))) PPC_WEAK_FUNC(util_1AF8);
PPC_FUNC_IMPL(__imp__util_1AF8) {
	PPC_FUNC_PROLOGUE();
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f2c
	__imp__NetDll_WSAStartup(ctx, base);
	return;
}

__attribute__((alias("__imp__rage_1B08"))) PPC_WEAK_FUNC(rage_1B08);
PPC_FUNC_IMPL(__imp__rage_1B08) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f3c
	__imp__NetDll_WSACleanup(ctx, base);
	return;
}

__attribute__((alias("__imp__util_1B10"))) PPC_WEAK_FUNC(util_1B10);
PPC_FUNC_IMPL(__imp__util_1B10) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f4c
	__imp__NetDll_socket(ctx, base);
	return;
}

__attribute__((alias("__imp__rage_1B28"))) PPC_WEAK_FUNC(rage_1B28);
PPC_FUNC_IMPL(__imp__rage_1B28) {
	PPC_FUNC_PROLOGUE();
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f5c
	__imp__NetDll_closesocket(ctx, base);
	return;
}

__attribute__((alias("__imp__netHardware_1B38_v12"))) PPC_WEAK_FUNC(netHardware_1B38_v12);
PPC_FUNC_IMPL(__imp__netHardware_1B38_v12) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f6c
	__imp__NetDll_ioctlsocket(ctx, base);
	return;
}

__attribute__((alias("__imp__netHardware_1B50_2h"))) PPC_WEAK_FUNC(netHardware_1B50_2h);
PPC_FUNC_IMPL(__imp__netHardware_1B50_2h) {
	PPC_FUNC_PROLOGUE();
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f7c
	__imp__NetDll_setsockopt(ctx, base);
	return;
}

__attribute__((alias("__imp__fiDeviceTcpIp_1B70_h"))) PPC_WEAK_FUNC(fiDeviceTcpIp_1B70_h);
PPC_FUNC_IMPL(__imp__fiDeviceTcpIp_1B70_h) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f8c
	__imp__NetDll_getsockname(ctx, base);
	return;
}

__attribute__((alias("__imp__util_1B88"))) PPC_WEAK_FUNC(util_1B88);
PPC_FUNC_IMPL(__imp__util_1B88) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585f9c
	__imp__NetDll_bind(ctx, base);
	return;
}

__attribute__((alias("__imp__fiDeviceTcpIp_1BA0_2hr"))) PPC_WEAK_FUNC(fiDeviceTcpIp_1BA0_2hr);
PPC_FUNC_IMPL(__imp__fiDeviceTcpIp_1BA0_2hr) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585fac
	__imp__NetDll_connect(ctx, base);
	return;
}

__attribute__((alias("__imp__fiDeviceTcpIp_1BB8_h"))) PPC_WEAK_FUNC(fiDeviceTcpIp_1BB8_h);
PPC_FUNC_IMPL(__imp__fiDeviceTcpIp_1BB8_h) {
	PPC_FUNC_PROLOGUE();
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585fbc
	__imp__NetDll_listen(ctx, base);
	return;
}

__attribute__((alias("__imp__fiDeviceTcpIp_1BC8_h"))) PPC_WEAK_FUNC(fiDeviceTcpIp_1BC8_h);
PPC_FUNC_IMPL(__imp__fiDeviceTcpIp_1BC8_h) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585fcc
	__imp__NetDll_accept(ctx, base);
	return;
}

__attribute__((alias("__imp__fiDeviceTcpIp_1BE0_h"))) PPC_WEAK_FUNC(fiDeviceTcpIp_1BE0_h);
PPC_FUNC_IMPL(__imp__fiDeviceTcpIp_1BE0_h) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585fdc
	__imp__NetDll_recv(ctx, base);
	return;
}

__attribute__((alias("__imp__SinglesNetworkClient_1BF8_w"))) PPC_WEAK_FUNC(SinglesNetworkClient_1BF8_w);
PPC_FUNC_IMPL(__imp__SinglesNetworkClient_1BF8_w) {
	PPC_FUNC_PROLOGUE();
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585fec
	__imp__NetDll_recvfrom(ctx, base);
	return;
}

__attribute__((alias("__imp__fiDeviceTcpIp_1C18_h"))) PPC_WEAK_FUNC(fiDeviceTcpIp_1C18_h);
PPC_FUNC_IMPL(__imp__fiDeviceTcpIp_1C18_h) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82585ffc
	__imp__NetDll_send(ctx, base);
	return;
}

__attribute__((alias("__imp__SinglesNetworkClient_1C30_g_1C30_1"))) PPC_WEAK_FUNC(SinglesNetworkClient_1C30_g_1C30_1);
PPC_FUNC_IMPL(__imp__SinglesNetworkClient_1C30_g_1C30_1) {
	PPC_FUNC_PROLOGUE();
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258600c
	__imp__NetDll_sendto(ctx, base);
	return;
}

__attribute__((alias("__imp__thunk_NetDll_inet_addr"))) PPC_WEAK_FUNC(thunk_NetDll_inet_addr);
PPC_FUNC_IMPL(__imp__thunk_NetDll_inet_addr) {
	PPC_FUNC_PROLOGUE();
	// b 0x8258601c
	__imp__NetDll_inet_addr(ctx, base);
	return;
}

__attribute__((alias("__imp__thunk_NetDll_WSAGetLastError"))) PPC_WEAK_FUNC(thunk_NetDll_WSAGetLastError);
PPC_FUNC_IMPL(__imp__thunk_NetDll_WSAGetLastError) {
	PPC_FUNC_PROLOGUE();
	// b 0x8258602c
	__imp__NetDll_WSAGetLastError(ctx, base);
	return;
}

__attribute__((alias("__imp__netHardware_1C60_v12"))) PPC_WEAK_FUNC(netHardware_1C60_v12);
PPC_FUNC_IMPL(__imp__netHardware_1C60_v12) {
	PPC_FUNC_PROLOGUE();
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258603c
	__imp__NetDll_XNetStartup(ctx, base);
	return;
}

__attribute__((alias("__imp__rage_1C70_h"))) PPC_WEAK_FUNC(rage_1C70_h);
PPC_FUNC_IMPL(__imp__rage_1C70_h) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258604c
	__imp__NetDll_XNetCleanup(ctx, base);
	return;
}

__attribute__((alias("__imp__game_1C78_h"))) PPC_WEAK_FUNC(game_1C78_h);
PPC_FUNC_IMPL(__imp__game_1C78_h) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258605c
	__imp__NetDll_XNetXnAddrToInAddr(ctx, base);
	return;
}

__attribute__((alias("__imp__game_1C90_h"))) PPC_WEAK_FUNC(game_1C90_h);
PPC_FUNC_IMPL(__imp__game_1C90_h) {
	PPC_FUNC_PROLOGUE();
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258606c
	__imp__NetDll_XNetInAddrToXnAddr(ctx, base);
	return;
}

__attribute__((alias("__imp__jumptable_1CA8_h"))) PPC_WEAK_FUNC(jumptable_1CA8_h);
PPC_FUNC_IMPL(__imp__jumptable_1CA8_h) {
	PPC_FUNC_PROLOGUE();
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258607c
	__imp__NetDll_XNetQosListen(ctx, base);
	return;
}

__attribute__((alias("__imp__net_1CC8"))) PPC_WEAK_FUNC(net_1CC8);
PPC_FUNC_IMPL(__imp__net_1CC8) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=128, manual
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// lwz r9,236(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// lwz r8,228(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// lwz r7,220(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// lwz r6,212(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x8258608c
	__imp__NetDll_XNetQosLookup(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__pg_1D30_w"))) PPC_WEAK_FUNC(pg_1D30_w);
PPC_FUNC_IMPL(__imp__pg_1D30_w) {
	PPC_FUNC_PROLOGUE();
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x8258609c
	__imp__NetDll_XNetQosRelease(ctx, base);
	return;
}

__attribute__((alias("__imp__netHardware_1D40_2h"))) PPC_WEAK_FUNC(netHardware_1D40_2h);
PPC_FUNC_IMPL(__imp__netHardware_1D40_2h) {
	PPC_FUNC_PROLOGUE();
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x825860ac
	__imp__NetDll_XNetGetTitleXnAddr(ctx, base);
	return;
}

__attribute__((alias("__imp__pongXMVMovie_1D50_h"))) PPC_WEAK_FUNC(pongXMVMovie_1D50_h);
PPC_FUNC_IMPL(__imp__pongXMVMovie_1D50_h) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__rage_1D60"))) PPC_WEAK_FUNC(rage_1D60);
PPC_FUNC_IMPL(__imp__rage_1D60) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=128, savegprlr_29
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
	// li r29,0
	var_r29 = 0;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,112(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r29);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, var_r29);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r31,0
	// blt cr6,0x82481df4
	if ((int32_t)var_r31 >= 0) {
		// cmplwi cr6,r3,0
		// beq cr6,0x82481dcc
		if (ctx.r3.u32 != 0) {
			// lwz r9,0(r3)
			ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0)/* rage_GameObject::vtable@+0x0 */;
			// li r6,0
			ctx.r6.s64 = 0;
			// addi r5,r1,84
			ctx.r5.s64 = ctx.r1.s64 + 84;
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r8,64(r9)
			ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
			// bctrl
			PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
			// mr r31,r3
			var_r31 = ctx.r3.u32;
			// lwz r3,80(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		}
	loc_82481DCC:
		// cmpwi cr6,r31,0
		// blt cr6,0x82481df4
		if ((int32_t)var_r31 < 0) goto loc_82481DF4;
		// lwz r11,84(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// cmplwi cr6,r11,0
		// beq cr6,0x82481df4
		if (ctx.r11.u32 == 0) goto loc_82481DF4;
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// mr r3,r11
		ctx.r3.u64 = ctx.r11.u64;
		// bl 0x82482ef0
		rage_2EF0(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// lwz r3,80(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	}
loc_82481DF4:
	// cmplwi cr6,r3,0
	// beq cr6,0x82481e10
	if (ctx.r3.u32 != 0) {
		// lwz r6,8(r7)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// stw r29,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r29);
	}
loc_82481E10:
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r3,0
	// beq cr6,0x82481e2c
	if (ctx.r3.u32 != 0) {
		// lwz r4,8(r5)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_82481E2C:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__ph_1E38"))) PPC_WEAK_FUNC(ph_1E38);
PPC_FUNC_IMPL(__imp__ph_1E38) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	PPCRegister temp{};
	// FRAME: size=128, savegprlr_29
	// li r30,0
	var_r30 = 0;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
	// stw r30,0(r31)
	PPC_STORE_U32(var_r31 + 0, var_r30);
	// stw r30,4(r31)
	PPC_STORE_U32(var_r31 + 4, var_r30);
	// stw r30,8(r31)
	PPC_STORE_U32(var_r31 + 8, var_r30);
	// stw r30,12(r31)
	PPC_STORE_U32(var_r31 + 12, var_r30);
	// lwz r11,0(r3)
  // [ph4a] vtable load collapsed
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, var_r30);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, var_r30);
	// lwz r10,112(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(ctx.r3.u32, 28, ctx, base);  // pattern-B slot 28 (byte +112)
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r29,0
	// blt cr6,0x82481f3c
	if ((int32_t)var_r29 >= 0) {
		// cmplwi cr6,r3,0
		// beq cr6,0x82481eb8
		if (ctx.r3.u32 != 0) {
			// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
			// li r6,0
			ctx.r6.s64 = 0;
			// addi r5,r1,88
			ctx.r5.s64 = ctx.r1.s64 + 88;
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r8,60(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r3.u32, 15, ctx, base);  // pattern-B slot 15 (byte +60)
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// lwz r3,80(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		}
	loc_82481EB8:
		// cmpwi cr6,r29,0
		// blt cr6,0x82481f3c
		if ((int32_t)var_r29 < 0) goto loc_82481F3C;
		// lwz r11,88(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
		// cmplwi cr6,r11,0
		// beq cr6,0x82481eec
		if (ctx.r11.u32 != 0) {
			// lwz r7,0(r11)
  // [ph4a] vtable load collapsed
			// addi r4,r1,84
			ctx.r4.s64 = ctx.r1.s64 + 84;
			// mr r3,r11
			ctx.r3.u64 = ctx.r11.u64;
			// lwz r6,92(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r11.u32, 23, ctx, base);  // pattern-B slot 23 (byte +92)
			// mr r29,r3
			var_r29 = ctx.r3.u32;
			// lwz r3,80(r1)
			ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		}
	loc_82481EEC:
		// cmpwi cr6,r29,0
		// blt cr6,0x82481f3c
		if ((int32_t)var_r29 < 0) goto loc_82481F3C;
		// lwz r11,84(r1)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// cmplwi cr6,r11,0
		// beq cr6,0x82481f3c
		if (ctx.r11.u32 == 0) goto loc_82481F3C;
		// mr r3,r11
		ctx.r3.u64 = ctx.r11.u64;
		// bl 0x824834d0
		phInst_34D0_g(ctx, base);
		// mr r5,r3
		ctx.r5.u64 = ctx.r3.u64;
		// lwz r3,84(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// stw r5,0(r31)
		PPC_STORE_U32(var_r31 + 0, ctx.r5.u32);
		// bl 0x820f0730
		grmShaderFx_vfn_16(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// lwz r3,84(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// stw r11,4(r31)
		PPC_STORE_U32(var_r31 + 4, ctx.r11.u32);
		// bl 0x82483358
		game_3358(ctx, base);
		// lwz r3,84(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// stfs f1,8(r31)
		ctx.fpscr.disableFlushMode();
		temp.f32 = float(ctx.f1.f64);
		PPC_STORE_U32(var_r31 + 8, temp.u32);
		// bl 0x82483350
		atSingleton_vfn_27(ctx, base);
		// stw r3,12(r31)
		PPC_STORE_U32(var_r31 + 12, ctx.r3.u32);
		// lwz r3,80(r1)
		ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	}
loc_82481F3C:
	// cmplwi cr6,r3,0
	// beq cr6,0x82481f58
	if (ctx.r3.u32 != 0) {
		// lwz r11,8(r4)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// stw r30,80(r1)
		PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
	}
loc_82481F58:
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	// beq cr6,0x82481f74
	if (ctx.r3.u32 != 0) {
		// lwz r9,8(r10)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
	}
loc_82481F74:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	return;
}

__attribute__((alias("__imp__phInstStatic_1F80_g"))) PPC_WEAK_FUNC(phInstStatic_1F80_g);
PPC_FUNC_IMPL(__imp__phInstStatic_1F80_g) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r19 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=336, savegprlr_17
	// mr r19,r3
	var_r19 = ctx.r3.u32;
	// li r3,276
	ctx.r3.s64 = 276;
	// mr r25,r4
	var_r25 = ctx.r4.u32;
	// mr r22,r5
	var_r22 = ctx.r5.u32;
	// mr r20,r6
	var_r20 = ctx.r6.u32;
	// mr r17,r7
	var_r17 = ctx.r7.u32;
	// bl 0x824871f0
	game_71F0_h(ctx, base);
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r3,0
	// beq cr6,0x82481fc0
	if (ctx.r3.u32 != 0) {
		// bl 0x82487200
		CCalMoviePlayer_7200_w(ctx, base);
		// mr r18,r3
		var_r18 = ctx.r3.u32;
		// b 0x82481fc4
	} else {
	loc_82481FC0:
		// mr r18,r30
		var_r18 = (uint32_t)(var_r30);
	}
loc_82481FC4:
	// li r3,336
	ctx.r3.s64 = 336;
	// bl 0x82486540
	game_6540_h(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82481fe0
	if (ctx.r3.u32 != 0) {
		// bl 0x82486bf0
		phInstStatic_6BF0_w(ctx, base);
		// mr r21,r3
		var_r21 = ctx.r3.u32;
		// b 0x82481fe4
	} else {
	loc_82481FE0:
		// mr r21,r30
		var_r21 = (uint32_t)(var_r30);
	}
loc_82481FE4:
	// li r3,676
	ctx.r3.s64 = 676;
	// bl 0x824852d8
	game_52D8_h(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482000
	if (ctx.r3.u32 != 0) {
		// bl 0x82486448
		phInstStatic_6448_w(ctx, base);
		// mr r26,r3
		var_r26 = ctx.r3.u32;
		// b 0x82482004
	} else {
	loc_82482000:
		// mr r26,r30
		var_r26 = (uint32_t)(var_r30);
	}
loc_82482004:
	// li r3,536
	ctx.r3.s64 = 536;
	// bl 0x82484748
	game_4748_h(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482020
	if (ctx.r3.u32 != 0) {
		// bl 0x824851d0
		phInstStatic_51D0_w(ctx, base);
		// mr r27,r3
		var_r27 = ctx.r3.u32;
		// b 0x82482024
	} else {
	loc_82482020:
		// mr r27,r30
		var_r27 = (uint32_t)(var_r30);
	}
loc_82482024:
	// li r3,1488
	ctx.r3.s64 = 1488;
	// bl 0x824835b8
	game_35B8_h(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482040
	if (ctx.r3.u32 != 0) {
		// bl 0x824837f0
		phInstStatic_37F0_w(ctx, base);
		// mr r23,r3
		var_r23 = ctx.r3.u32;
		// b 0x82482044
	} else {
	loc_82482040:
		// mr r23,r30
		var_r23 = (uint32_t)(var_r30);
	}
loc_82482044:
	// li r3,376
	ctx.r3.s64 = 376;
	// bl 0x82483e20
	game_3E20_h(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482060
	if (ctx.r3.u32 != 0) {
		// bl 0x82483f90
		game_3F90_h(ctx, base);
		// mr r24,r3
		var_r24 = ctx.r3.u32;
		// b 0x82482064
	} else {
	loc_82482060:
		// mr r24,r30
		var_r24 = (uint32_t)(var_r30);
	}
loc_82482064:
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, var_r30);
	// cmplwi cr6,r26,0
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r30);
	// beq cr6,0x8248209c
	if (var_r26 != 0) {
		// cmplwi cr6,r27,0
		// beq cr6,0x8248209c
		if (var_r27 == 0) goto loc_8248209C;
		// cmplwi cr6,r23,0
		// beq cr6,0x8248209c
		if (var_r23 == 0) goto loc_8248209C;
		// cmplwi cr6,r24,0
		// beq cr6,0x8248209c
		if (var_r24 == 0) goto loc_8248209C;
		// cmplwi cr6,r21,0
		// beq cr6,0x8248209c
		if (var_r21 == 0) goto loc_8248209C;
		// cmplwi cr6,r18,0
		// bne cr6,0x824820a8
		if (var_r18 != 0) goto loc_824820A8;
	}
loc_8248209C:
	// lis r31,-32761
	var_r31 = (uint32_t)(-2147024896);
	// ori r31,r31,14
	var_r31 = (uint32_t)(var_r31 | 14);
	// b 0x82482300
	goto loc_82482300;
loc_824820A8:
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r29,8
	var_r29 = 8;
	// li r28,1
	var_r28 = 1;
	// cmplw cr6,r22,r20
	// std r30,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
	// std r30,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
	// stw r30,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, var_r30);
	// stw r29,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, var_r29);
	// stw r28,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, var_r28);
	// beq cr6,0x824820d4
	if (var_r22 != var_r20) {
		// stw r30,120(r1)
		PPC_STORE_U32(ctx.r1.u32 + 120, var_r30);
	}
loc_824820D4:
	// lwz r11,0(r26)
  // [ph4a] vtable load collapsed
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r26
	ctx.r3.u64 = var_r26;
	// lwz r10,44(r11)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r26, 11, ctx, base);  // pattern-B slot 11 (byte +44)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmpwi cr6,r31,0
	// blt cr6,0x82482300
	if ((int32_t)var_r31 >= 0) {
		// addi r11,r1,144
		ctx.r11.s64 = ctx.r1.s64 + 144;
		// cmplw cr6,r22,r20
		// std r30,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
		// std r30,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
		// stw r30,16(r11)
		PPC_STORE_U32(ctx.r11.u32 + 16, var_r30);
		// stw r29,160(r1)
		PPC_STORE_U32(ctx.r1.u32 + 160, var_r29);
		// stw r28,152(r1)
		PPC_STORE_U32(ctx.r1.u32 + 152, var_r28);
		// beq cr6,0x8248211c
		if (var_r22 != var_r20) {
			// stw r30,152(r1)
			PPC_STORE_U32(ctx.r1.u32 + 152, var_r30);
		}
	loc_8248211C:
		// lwz r9,0(r27)
  // [ph4a] vtable load collapsed
		// addi r4,r1,144
		ctx.r4.s64 = ctx.r1.s64 + 144;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r8,44(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 11, ctx, base);  // pattern-B slot 11 (byte +44)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// addi r11,r1,96
		ctx.r11.s64 = ctx.r1.s64 + 96;
		// lwz r7,4(r25)
		ctx.r7.u64 = PPC_LOAD_U32(var_r25 + 4);
		// clrlwi r6,r7,31
		ctx.r6.u64 = ctx.r7.u32 & 0x1;
		// cmplwi cr6,r6,0
		// std r30,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
		// std r30,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
		// stw r28,100(r1)
		PPC_STORE_U32(ctx.r1.u32 + 100, var_r28);
		// bne cr6,0x82482164
		if (ctx.r6.u32 == 0) {
			// stw r30,100(r1)
			PPC_STORE_U32(ctx.r1.u32 + 100, var_r30);
		}
	loc_82482164:
		// li r5,3
		ctx.r5.s64 = 3;
		// addi r4,r1,96
		ctx.r4.s64 = ctx.r1.s64 + 96;
		// mr r3,r23
		ctx.r3.u64 = var_r23;
		// stw r5,104(r1)
		PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r5.u32);
		// bl 0x82483860
		game_3860(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// mr r4,r19
		ctx.r4.u64 = var_r19;
		// mr r3,r24
		ctx.r3.u64 = var_r24;
		// bl 0x82483278
		phInstStatic_3278_wrh(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// lwz r4,0(r21)
  // [ph4a] vtable load collapsed
		// li r9,0
		ctx.r9.s64 = 0;
		// li r8,0
		ctx.r8.s64 = 0;
		// mr r7,r24
		ctx.r7.u64 = var_r24;
		// mr r6,r27
		ctx.r6.u64 = var_r27;
		// mr r5,r23
		ctx.r5.u64 = var_r23;
		// lwz r11,36(r4)
  // [ph4a] slot load collapsed
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		// mr r4,r26
		ctx.r4.u64 = var_r26;
		// bctrl
		VCALL(var_r21, 9, ctx, base);  // pattern-B slot 9 (byte +36)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// addi r11,r1,176
		ctx.r11.s64 = ctx.r1.s64 + 176;
		// lwz r10,4(r25)
		ctx.r10.u64 = PPC_LOAD_U32(var_r25 + 4);
		// rlwinm r9,r10,0,30,30
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
		// cmplwi cr6,r9,0
		// std r30,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
		// std r30,8(r11)
		PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
		// std r30,16(r11)
		PPC_STORE_U64(ctx.r11.u32 + 16, var_r30);
		// stw r30,24(r11)
		PPC_STORE_U32(ctx.r11.u32 + 24, var_r30);
		// stw r19,176(r1)
		PPC_STORE_U32(ctx.r1.u32 + 176, var_r19);
		// stw r21,180(r1)
		PPC_STORE_U32(ctx.r1.u32 + 180, var_r21);
		// stw r28,184(r1)
		PPC_STORE_U32(ctx.r1.u32 + 184, var_r28);
		// bne cr6,0x82482208
		if (ctx.r9.u32 == 0) {
			// stw r30,184(r1)
			PPC_STORE_U32(ctx.r1.u32 + 184, var_r30);
		}
	loc_82482208:
		// lwz r8,8(r25)
		ctx.r8.u64 = PPC_LOAD_U32(var_r25 + 8);
		// addi r4,r1,176
		ctx.r4.s64 = ctx.r1.s64 + 176;
		// lwz r7,12(r25)
		ctx.r7.u64 = PPC_LOAD_U32(var_r25 + 12);
		// mr r3,r18
		ctx.r3.u64 = var_r18;
		// lwz r6,16(r25)
		ctx.r6.u64 = PPC_LOAD_U32(var_r25 + 16);
		// lwz r5,20(r25)
		ctx.r5.u64 = PPC_LOAD_U32(var_r25 + 20);
		// stw r8,188(r1)
		PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r8.u32);
		// stw r7,192(r1)
		PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r7.u32);
		// stw r6,196(r1)
		PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r6.u32);
		// stw r5,200(r1)
		PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r5.u32);
		// lwz r10,36(r11)
		// bctrl
		VCALL(ctx.r3.u32, 9, ctx, base);  // vtable slot 9 (byte +36)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// lwz r9,0(r26)
  // [ph4a] vtable load collapsed
		// mr r4,r22
		ctx.r4.u64 = var_r22;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// lwz r8,68(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r26, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// lwz r7,0(r27)
  // [ph4a] vtable load collapsed
		// mr r4,r20
		ctx.r4.u64 = var_r20;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r6,68(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 17, ctx, base);  // pattern-B slot 17 (byte +68)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// lwz r5,0(r26)
  // [ph4a] vtable load collapsed
		// addi r4,r1,84
		ctx.r4.s64 = ctx.r1.s64 + 84;
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// lwz r11,96(r5)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r26, 24, ctx, base);  // pattern-B slot 24 (byte +96)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// lwz r10,0(r27)
  // [ph4a] vtable load collapsed
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r9,92(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 23, ctx, base);  // pattern-B slot 23 (byte +92)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82482300
		if ((int32_t)var_r31 < 0) goto loc_82482300;
		// lwz r8,0(r21)
  // [ph4a] vtable load collapsed
		// li r6,0
		ctx.r6.s64 = 0;
		// lwz r5,80(r1)
		ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		// lwz r4,84(r1)
		ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// lwz r7,56(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r21, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
	}
loc_82482300:
	// cmplwi cr6,r26,0
	// beq cr6,0x8248231c
	if (var_r26 != 0) {
		// lwz r6,0(r26)
  // [ph4a] vtable load collapsed
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// lwz r5,8(r6)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r26, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	}
loc_8248231C:
	// cmplwi cr6,r27,0
	// beq cr6,0x82482338
	if (var_r27 != 0) {
		// lwz r4,0(r27)
  // [ph4a] vtable load collapsed
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r11,8(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	}
loc_82482338:
	// cmplwi cr6,r23,0
	// beq cr6,0x82482354
	if (var_r23 != 0) {
		// lwz r10,0(r23)
  // [ph4a] vtable load collapsed
		// mr r3,r23
		ctx.r3.u64 = var_r23;
		// lwz r9,8(r10)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r23, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	}
loc_82482354:
	// cmplwi cr6,r24,0
	// beq cr6,0x82482370
	if (var_r24 != 0) {
		// lwz r8,0(r24)
  // [ph4a] vtable load collapsed
		// mr r3,r24
		ctx.r3.u64 = var_r24;
		// lwz r7,8(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r24, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	}
loc_82482370:
	// cmplwi cr6,r21,0
	// beq cr6,0x8248238c
	if (var_r21 != 0) {
		// lwz r6,0(r21)
  // [ph4a] vtable load collapsed
		// mr r3,r21
		ctx.r3.u64 = var_r21;
		// lwz r5,8(r6)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r21, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	}
loc_8248238C:
	// cmpwi cr6,r31,0
	// blt cr6,0x824823a4
	if ((int32_t)var_r31 >= 0) {
		// stw r18,0(r17)
		PPC_STORE_U32(var_r17 + 0, var_r18);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		return;
	}
loc_824823A4:
	// cmplwi cr6,r18,0
	// beq cr6,0x824823c0
	if (var_r18 != 0) {
		// lwz r4,0(r18)
  // [ph4a] vtable load collapsed
		// mr r3,r18
		ctx.r3.u64 = var_r18;
		// lwz r11,8(r4)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r18, 2, ctx, base);  // pattern-B slot 2 (byte +8)
	}
loc_824823C0:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__game_23D0"))) PPC_WEAK_FUNC(game_23D0);
PPC_FUNC_IMPL(__imp__game_23D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r29 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=176, savegprlr_22
	// li r29,0
	var_r29 = 0;
	// mr r22,r5
	var_r22 = ctx.r5.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r23,r29
	var_r23 = (uint32_t)(var_r29);
	// mr r24,r3
	var_r24 = ctx.r3.u32;
	// stw r29,0(r22)
	PPC_STORE_U32(var_r22 + 0, var_r29);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0);
	// stw r23,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r23);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r11,3
	// bgt cr6,0x824826dc
	if (ctx.r11.u32 > 3) goto loc_824826DC;
	// lis r12,-32184
	// addi r12,r12,9248
	ctx.r12.s64 = ctx.r12.s64 + 9248;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		goto loc_824826DC;
	case 1:
		goto loc_82482430;
	case 2:
		goto loc_82482584;
	case 3:
		goto loc_824825FC;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82482430:
	// li r3,128
	ctx.r3.s64 = 128;
	// bl 0x82487830
	game_7830(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482450
	if (ctx.r3.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
	// bl 0x82488488
	game_8488(ctx, base);
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// cmplwi cr6,r25,0
	// bne cr6,0x82482460
	if (var_r25 != 0) goto loc_82482460;
loc_82482450:
	// lis r3,-32761
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	return;
loc_82482460:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	// beq cr6,0x82482558
	if (ctx.r10.u32 == 0) goto loc_82482558;
	// lwz r9,0(r25)
  // [ph4a] vtable load collapsed
	// mr r29,r25
	var_r29 = (uint32_t)(var_r25);
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// lwz r8,4(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r25, 1, ctx, base);  // pattern-B slot 1 (byte +4)
loc_82482488:
	// lwz r26,48(r31)
	var_r26 = (uint32_t)(PPC_LOAD_U32(var_r31 + 48));
	// lwz r27,52(r31)
	var_r27 = (uint32_t)(PPC_LOAD_U32(var_r31 + 52));
	// lwz r28,56(r31)
	var_r28 = (uint32_t)(PPC_LOAD_U32(var_r31 + 56));
	// cmplwi cr6,r26,0
	// bne cr6,0x824824a0
	if (var_r26 != 0) goto loc_824824A0;
	// lis r26,2
	var_r26 = 131072;
loc_824824A0:
	// cmplwi cr6,r27,0
	// bne cr6,0x824824c0
	if (!(var_r27 != 0)) {
		// lwz r7,4(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 4);
		// li r27,64
		var_r27 = 64;
		// rlwinm r6,r7,0,29,29
		ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x4;
	} else {
		if (!(ctx.r6.u32 != 0)) {
			// li r27,8
			var_r27 = 8;
		}
	}
loc_824824C0:
	// cmplwi cr6,r28,0
	// bne cr6,0x824824e0
	if (!(var_r28 != 0)) {
		// lwz r5,4(r31)
		ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 4);
		// li r28,32
		var_r28 = 32;
		// rlwinm r4,r5,0,29,29
		ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x4;
	} else {
		if (!(ctx.r4.u32 != 0)) {
			// li r28,2
			var_r28 = 2;
		}
	}
loc_824824E0:
	// lwz r3,0(r25)
	ctx.r3.u64 = PPC_LOAD_U32(var_r25 + 0);
	// mr r10,r28
	ctx.r10.u64 = var_r28;
	// ld r6,40(r31)
	ctx.r6.u64 = PPC_LOAD_U64(var_r31 + 40);
	// mr r9,r27
	ctx.r9.u64 = var_r27;
	// ld r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U64(var_r31 + 32);
	// mr r8,r26
	ctx.r8.u64 = var_r26;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r11,96(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82482810
	if ((int32_t)var_r30 < 0) goto loc_82482810;
	// cmplw cr6,r25,r29
	// beq cr6,0x824827e8
	if (var_r25 == var_r29) goto loc_824827E8;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
	// mr r10,r28
	ctx.r10.u64 = var_r28;
	// ld r6,40(r31)
	ctx.r6.u64 = PPC_LOAD_U64(var_r31 + 40);
	// mr r9,r27
	ctx.r9.u64 = var_r27;
	// ld r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U64(var_r31 + 32);
	// mr r8,r26
	ctx.r8.u64 = var_r26;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r11,96(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 96);
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	// b 0x824827e4
	goto loc_824827E4;
loc_82482558:
	// li r3,128
	ctx.r3.s64 = 128;
	// bl 0x82487830
	game_7830(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482578
	if (ctx.r3.u32 == 0) goto loc_82482578;
	// bl 0x82488488
	game_8488(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmplwi cr6,r29,0
	// bne cr6,0x82482488
	if (var_r29 != 0) goto loc_82482488;
loc_82482578:
	// lis r30,-32761
	var_r30 = (uint32_t)(-2147024896);
	// ori r30,r30,14
	var_r30 = (uint32_t)(var_r30 | 14);
	// b 0x82482810
	goto loc_82482810;
loc_82482584:
	// li r3,60
	ctx.r3.s64 = 60;
	// bl 0x824874e0
	game_74E0(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x824825a0
	if (ctx.r3.u32 == 0) goto loc_824825A0;
	// bl 0x82487770
	game_7770(ctx, base);
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// b 0x824825a4
	goto loc_824825A4;
loc_824825A0:
	// mr r25,r29
	var_r25 = (uint32_t)(var_r29);
loc_824825A4:
	// li r3,60
	ctx.r3.s64 = 60;
	// bl 0x824874e0
	game_74E0(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x824825bc
	if (ctx.r3.u32 == 0) goto loc_824825BC;
	// bl 0x82487770
	game_7770(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
loc_824825BC:
	// cmplwi cr6,r25,0
	// beq cr6,0x82482578
	if (var_r25 == 0) goto loc_82482578;
	// cmplwi cr6,r29,0
	// beq cr6,0x82482578
	if (var_r29 == 0) goto loc_82482578;
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// lwz r5,28(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// bl 0x82487500
	game_7500(ctx, base);
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82482810
	if ((int32_t)var_r30 < 0) goto loc_82482810;
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lwz r5,28(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// bl 0x82487500
	game_7500(ctx, base);
	// b 0x824827e4
	goto loc_824827E4;
loc_824825FC:
	// li r3,64
	ctx.r3.s64 = 64;
	// bl 0x824872a8
	game_72A8(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482450
	if (ctx.r3.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
	// bl 0x82487410
	jumptable_7410_h(ctx, base);
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// cmplwi cr6,r25,0
	// beq cr6,0x82482450
	if (var_r25 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rlwinm r9,r10,0,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r9,0
	// beq cr6,0x824826b0
	if (ctx.r9.u32 == 0) goto loc_824826B0;
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lwz r7,28(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 28);
	// cmplw cr6,r8,r7
	// bne cr6,0x824826b0
	if (ctx.r8.u32 != ctx.r7.u32) goto loc_824826B0;
	// lwz r6,32(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 32);
	// lwz r5,36(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 36);
	// cmplw cr6,r6,r5
	// bne cr6,0x824826b0
	if (ctx.r6.u32 != ctx.r5.u32) goto loc_824826B0;
	// lwz r4,0(r25)
  // [ph4a] vtable load collapsed
	// mr r29,r25
	var_r29 = (uint32_t)(var_r25);
	// lwz r11,4(r4)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r25, 1, ctx, base);  // pattern-B slot 1 (byte +4)
loc_82482660:
	// lwz r10,0(r25)
  // [ph4a] vtable load collapsed
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 32);
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// lwz r9,96(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r25, 24, ctx, base);  // pattern-B slot 24 (byte +96)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82482810
	if ((int32_t)var_r30 < 0) goto loc_82482810;
	// cmplw cr6,r25,r29
	// beq cr6,0x824827e8
	if (var_r25 == var_r29) goto loc_824827E8;
	// lwz r8,0(r29)
  // [ph4a] vtable load collapsed
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lwz r5,36(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 36);
	// lwz r4,28(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 28);
	// lwz r7,96(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r29, 24, ctx, base);  // pattern-B slot 24 (byte +96)
	// b 0x824827e4
	goto loc_824827E4;
loc_824826B0:
	// li r3,64
	ctx.r3.s64 = 64;
	// bl 0x824872a8
	game_72A8(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x824826d0
	if (ctx.r3.u32 == 0) goto loc_824826D0;
	// bl 0x82487410
	jumptable_7410_h(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmplwi cr6,r29,0
	// bne cr6,0x82482660
	if (var_r29 != 0) goto loc_82482660;
loc_824826D0:
	// lis r30,-32761
	var_r30 = (uint32_t)(-2147024896);
	// ori r30,r30,14
	var_r30 = (uint32_t)(var_r30 | 14);
	// b 0x82482810
	goto loc_82482810;
loc_824826DC:
	// li r3,128
	ctx.r3.s64 = 128;
	// bl 0x82487830
	game_7830(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482450
	if (ctx.r3.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
	// bl 0x82488488
	game_8488(ctx, base);
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// cmplwi cr6,r25,0
	// beq cr6,0x82482450
	if (var_r25 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		return;
	}
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(var_r31 + 4);
	// rlwinm r5,r6,0,29,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r5,0
	// beq cr6,0x82482860
	if (ctx.r5.u32 == 0) goto loc_82482860;
	// lwz r4,0(r25)
  // [ph4a] vtable load collapsed
	// mr r29,r25
	var_r29 = (uint32_t)(var_r25);
	// lwz r11,4(r4)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r25, 1, ctx, base);  // pattern-B slot 1 (byte +4)
loc_82482720:
	// lwz r26,28(r31)
	var_r26 = (uint32_t)(PPC_LOAD_U32(var_r31 + 28));
	// lwz r27,32(r31)
	var_r27 = (uint32_t)(PPC_LOAD_U32(var_r31 + 32));
	// lwz r28,36(r31)
	var_r28 = (uint32_t)(PPC_LOAD_U32(var_r31 + 36));
	// cmplwi cr6,r26,0
	// bne cr6,0x82482738
	if (var_r26 != 0) goto loc_82482738;
	// lis r26,2
	var_r26 = 131072;
loc_82482738:
	// cmplwi cr6,r27,0
	// bne cr6,0x82482758
	if (!(var_r27 != 0)) {
		// lwz r10,4(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 4);
		// li r27,64
		var_r27 = 64;
		// rlwinm r9,r10,0,29,29
		ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	} else {
		if (!(ctx.r9.u32 != 0)) {
			// li r27,8
			var_r27 = 8;
		}
	}
loc_82482758:
	// cmplwi cr6,r28,0
	// bne cr6,0x82482778
	if (!(var_r28 != 0)) {
		// lwz r8,4(r31)
		ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 4);
		// li r28,32
		var_r28 = 32;
		// rlwinm r7,r8,0,29,29
		ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x4;
	} else {
		if (!(ctx.r7.u32 != 0)) {
			// li r28,2
			var_r28 = 2;
		}
	}
loc_82482778:
	// lwz r5,0(r25)
  // [ph4a] vtable load collapsed
	// mr r9,r28
	ctx.r9.u64 = var_r28;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// mr r8,r27
	ctx.r8.u64 = var_r27;
	// mr r7,r26
	ctx.r7.u64 = var_r26;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// lwz r11,104(r5)
  // [ph4a] slot load collapsed
	// li r5,1
	ctx.r5.s64 = 1;
	// bctrl
	VCALL(var_r25, 26, ctx, base);  // pattern-B slot 26 (byte +104)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82482810
	if ((int32_t)var_r30 < 0) goto loc_82482810;
	// cmplw cr6,r25,r29
	// beq cr6,0x824827e8
	if (var_r25 == var_r29) goto loc_824827E8;
	// lwz r10,0(r29)
  // [ph4a] vtable load collapsed
	// mr r9,r28
	ctx.r9.u64 = var_r28;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 24);
	// mr r8,r27
	ctx.r8.u64 = var_r27;
	// mr r7,r26
	ctx.r7.u64 = var_r26;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r11,104(r10)
  // [ph4a] slot load collapsed
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// bctrl
	VCALL(var_r29, 26, ctx, base);  // pattern-B slot 26 (byte +104)
loc_824827E4:
	// mr r30,r3
	var_r30 = ctx.r3.u32;
loc_824827E8:
	// cmpwi cr6,r30,0
	// blt cr6,0x82482810
	if ((int32_t)var_r30 < 0) goto loc_82482810;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r6,r29
	ctx.r6.u64 = var_r29;
	// mr r5,r25
	ctx.r5.u64 = var_r25;
	// mr r4,r31
	ctx.r4.u64 = var_r31;
	// mr r3,r24
	ctx.r3.u64 = var_r24;
	// bl 0x82481f80
	phInstStatic_1F80_g(ctx, base);
	// lwz r23,80(r1)
	var_r23 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 80));
	// mr r30,r3
	var_r30 = ctx.r3.u32;
loc_82482810:
	// cmplwi cr6,r25,0
	// beq cr6,0x8248282c
	if (var_r25 == 0) goto loc_8248282C;
	// lwz r10,0(r25)
  // [ph4a] vtable load collapsed
	// mr r3,r25
	ctx.r3.u64 = var_r25;
	// lwz r9,8(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r25, 2, ctx, base);  // pattern-B slot 2 (byte +8)
loc_8248282C:
	// cmplwi cr6,r29,0
	// beq cr6,0x82482848
	if (var_r29 == 0) goto loc_82482848;
	// lwz r8,0(r29)
  // [ph4a] vtable load collapsed
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	// lwz r7,8(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r29, 2, ctx, base);  // pattern-B slot 2 (byte +8)
loc_82482848:
	// cmpwi cr6,r30,0
	// blt cr6,0x8248288c
	if ((int32_t)var_r30 < 0) goto loc_8248288C;
	// stw r23,0(r22)
	PPC_STORE_U32(var_r22 + 0, var_r23);
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
loc_82482860:
	// li r3,128
	ctx.r3.s64 = 128;
	// bl 0x82487830
	game_7830(ctx, base);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482880
	if (ctx.r3.u32 == 0) goto loc_82482880;
	// bl 0x82488488
	game_8488(ctx, base);
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmplwi cr6,r29,0
	// bne cr6,0x82482720
	if (var_r29 != 0) goto loc_82482720;
loc_82482880:
	// lis r30,-32761
	var_r30 = (uint32_t)(-2147024896);
	// ori r30,r30,14
	var_r30 = (uint32_t)(var_r30 | 14);
	// b 0x82482810
	goto loc_82482810;
loc_8248288C:
	// cmplwi cr6,r23,0
	// beq cr6,0x824828a8
	if (var_r23 == 0) {
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		return;
	}
	// lwz r6,0(r23)
  // [ph4a] vtable load collapsed
	// mr r3,r23
	ctx.r3.u64 = var_r23;
	// lwz r5,8(r6)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r23, 2, ctx, base);  // pattern-B slot 2 (byte +8)
loc_824828A8:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__pongXMVMovie_28B8_h"))) PPC_WEAK_FUNC(pongXMVMovie_28B8_h);
PPC_FUNC_IMPL(__imp__pongXMVMovie_28B8_h) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=160, manual
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_824828D8:
	// std r9,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x824828d8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_824828D8;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// bl 0x824823d0
	game_23D0(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__game_2908"))) PPC_WEAK_FUNC(game_2908);
PPC_FUNC_IMPL(__imp__game_2908) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x824887e0
	util_87E0(ctx, base);
	// lis r10,-32255
	// lis r11,-32248
	// li r9,30
	ctx.r9.s64 = 30;
	// lfs f13,-31996(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -31996);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32256
	// lfs f0,-24400(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24400);
	ctx.f0.f64 = double(temp.f32);
	// li r11,0
	ctx.r11.s64 = 0;
	// stfs f0,132(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 132, temp.u32);
	// stfs f13,136(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 136, temp.u32);
	// lfs f12,15788(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15788);
	ctx.f12.f64 = double(temp.f32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stfs f12,140(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 140, temp.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r11.u32);
	// stw r11,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r11.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
	// stw r10,112(r31)
	PPC_STORE_U32(var_r31 + 112, ctx.r10.u32);
	// addi r10,r31,144
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 144;
	// stw r11,72(r31)
	PPC_STORE_U32(var_r31 + 72, ctx.r11.u32);
	// stw r11,76(r31)
	PPC_STORE_U32(var_r31 + 76, ctx.r11.u32);
	// stw r11,116(r31)
	PPC_STORE_U32(var_r31 + 116, ctx.r11.u32);
	// stw r11,120(r31)
	PPC_STORE_U32(var_r31 + 120, ctx.r11.u32);
	// stw r11,124(r31)
	PPC_STORE_U32(var_r31 + 124, ctx.r11.u32);
	// stw r11,128(r31)
	PPC_STORE_U32(var_r31 + 128, ctx.r11.u32);
	// stw r11,264(r31)
	PPC_STORE_U32(var_r31 + 264, ctx.r11.u32);
	// stw r11,268(r31)
	PPC_STORE_U32(var_r31 + 268, ctx.r11.u32);
	// stw r11,272(r31)
	PPC_STORE_U32(var_r31 + 272, ctx.r11.u32);
	// stw r11,276(r31)
	PPC_STORE_U32(var_r31 + 276, ctx.r11.u32);
	// stw r11,280(r31)
	PPC_STORE_U32(var_r31 + 280, ctx.r11.u32);
	// stw r11,284(r31)
	PPC_STORE_U32(var_r31 + 284, ctx.r11.u32);
	// stw r11,80(r31)
	PPC_STORE_U32(var_r31 + 80, ctx.r11.u32);
	// stw r11,84(r31)
	PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
	// stw r11,88(r31)
	PPC_STORE_U32(var_r31 + 88, ctx.r11.u32);
	// stw r11,92(r31)
	PPC_STORE_U32(var_r31 + 92, ctx.r11.u32);
	// stw r11,96(r31)
	PPC_STORE_U32(var_r31 + 96, ctx.r11.u32);
	// stw r11,100(r31)
	PPC_STORE_U32(var_r31 + 100, ctx.r11.u32);
	// stw r11,104(r31)
	PPC_STORE_U32(var_r31 + 104, ctx.r11.u32);
	// stw r11,108(r31)
	PPC_STORE_U32(var_r31 + 108, ctx.r11.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_824829BC:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x824829bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_824829BC;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_vfn_10"))) PPC_WEAK_FUNC(phInst_vfn_10);
PPC_FUNC_IMPL(__imp__phInst_vfn_10) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r10,44(r11)
	// bctrl
	phInst_vfn_11(ctx, base);  // vtable slot 11 (byte +44)  // phInst::vfn_11
	// cmpwi cr6,r3,0
	// blt cr6,0x82482a14
	if (ctx.r3.s32 >= 0) {
		// li r9,1
		ctx.r9.s64 = 1;
		// stw r9,116(r31)
		PPC_STORE_U32(var_r31 + 116, ctx.r9.u32);
	}
loc_82482A14:
	// blr
	return;
}

__attribute__((alias("__imp__phInst_45"))) PPC_WEAK_FUNC(phInst_45);
PPC_FUNC_IMPL(__imp__phInst_45) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInst_vfn_12"))) PPC_WEAK_FUNC(phInst_vfn_12);
PPC_FUNC_IMPL(__imp__phInst_vfn_12) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmpwi cr6,r4,1
	// beq cr6,0x82482a7c
	if (ctx.r4.s32 != 1) {
		// cmpwi cr6,r4,3
		// beq cr6,0x82482a6c
		if (ctx.r4.s32 != 3) {
			// cmpwi cr6,r4,4
			// bne cr6,0x82482a6c
			if (ctx.r4.s32 != 4) {
				// stw r5,264(r11)
				PPC_STORE_U32(ctx.r11.u32 + 264, ctx.r5.u32);
				// li r3,0
				ctx.r3.s64 = 0;
				// stw r6,276(r11)
				PPC_STORE_U32(ctx.r11.u32 + 276, ctx.r6.u32);
				// blr
				return;
			}
			// stw r5,268(r11)
			PPC_STORE_U32(ctx.r11.u32 + 268, ctx.r5.u32);
			// li r3,0
			ctx.r3.s64 = 0;
			// stw r6,280(r11)
			PPC_STORE_U32(ctx.r11.u32 + 280, ctx.r6.u32);
			// blr
			return;
		}
	loc_82482A6C:
		// stw r5,264(r11)
		PPC_STORE_U32(ctx.r11.u32 + 264, ctx.r5.u32);
		// li r3,0
		ctx.r3.s64 = 0;
		// stw r6,276(r11)
		PPC_STORE_U32(ctx.r11.u32 + 276, ctx.r6.u32);
		// blr
		return;
	}
loc_82482A7C:
	// stw r5,272(r11)
	PPC_STORE_U32(ctx.r11.u32 + 272, ctx.r5.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r6,284(r11)
	PPC_STORE_U32(ctx.r11.u32 + 284, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_vfn_14"))) PPC_WEAK_FUNC(phInst_vfn_14);
PPC_FUNC_IMPL(__imp__phInst_vfn_14) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,60(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,44(r31)
	PPC_STORE_U32(var_r31 + 44, var_r30);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_47"))) PPC_WEAK_FUNC(phInst_47);
PPC_FUNC_IMPL(__imp__phInst_47) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 44);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482b30
	if (ctx.r3.u32 != 0) {
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// li r9,0
		ctx.r9.s64 = 0;
		// stw r9,44(r31)
		PPC_STORE_U32(var_r31 + 44, ctx.r9.u32);
	}
loc_82482B30:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_2B48_p45"))) PPC_WEAK_FUNC(phInst_2B48_p45);
PPC_FUNC_IMPL(__imp__phInst_2B48_p45) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 92);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__game_2B58_h"))) PPC_WEAK_FUNC(game_2B58_h);
PPC_FUNC_IMPL(__imp__game_2B58_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488580
	util_8580(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82482908
	game_2908(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__rage_2B90"))) PPC_WEAK_FUNC(rage_2B90);
PPC_FUNC_IMPL(__imp__rage_2B90) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r30,0
	var_r30 = 0;
	// lwz r3,76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 76);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482bc0
	if (ctx.r3.u32 != 0) {
		// bl 0x82356cf0
		rage_6CF0(ctx, base);
		// stw r30,76(r31)
		PPC_STORE_U32(var_r31 + 76, var_r30);
	}
loc_82482BC0:
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 72);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482bd4
	if (ctx.r3.u32 != 0) {
		// bl 0x82356da8
		rage_6DA8(ctx, base);
		// stw r30,72(r31)
		PPC_STORE_U32(var_r31 + 72, var_r30);
	}
loc_82482BD4:
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 68);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482be8
	if (ctx.r3.u32 != 0) {
		// bl 0x82357540
		rage_7540_h(ctx, base);
		// stw r30,68(r31)
		PPC_STORE_U32(var_r31 + 68, var_r30);
	}
loc_82482BE8:
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482bfc
	if (ctx.r3.u32 != 0) {
		// bl 0x82352ee0
		rage_2EE0(ctx, base);
		// stw r30,64(r31)
		PPC_STORE_U32(var_r31 + 64, var_r30);
	}
loc_82482BFC:
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 60);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482c10
	if (ctx.r3.u32 != 0) {
		// bl 0x82352ee0
		rage_2EE0(ctx, base);
		// stw r30,60(r31)
		PPC_STORE_U32(var_r31 + 60, var_r30);
	}
loc_82482C10:
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482c24
	if (ctx.r3.u32 != 0) {
		// bl 0x8235ead8
		rage_EAD8(ctx, base);
		// stw r30,56(r31)
		PPC_STORE_U32(var_r31 + 56, var_r30);
	}
loc_82482C24:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 60);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* rage_GameObject::vtable@+0x0 */;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__msgMsgSink_2C68_w"))) PPC_WEAK_FUNC(msgMsgSink_2C68_w);
PPC_FUNC_IMPL(__imp__msgMsgSink_2C68_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=192, manual
	// li r30,0
	var_r30 = 0;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// mr r9,r30
	ctx.r9.u64 = var_r30;
	// li r10,6
	ctx.r10.s64 = 6;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_82482C94:
	// std r9,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x82482c94
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82482C94;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82585f0c
	__imp__XGetVideoMode(ctx, base);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r4,0
	ctx.r4.s64 = 0;
	// std r30,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, var_r30);
	// std r30,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, var_r30);
	// bl 0x8235ef60
	msgMsgSink_EF60_wrh(ctx, base);
	// lwz r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r7,124(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f0
	ctx.f12.f64 = double(ctx.f0.s64);
	// fcfid f11,f13
	ctx.f11.f64 = double(ctx.f13.s64);
	// frsp f13,f12
	ctx.f13.f64 = double(float(ctx.f12.f64));
	// stfs f13,132(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(var_r31 + 132, temp.u32);
	// frsp f12,f11
	ctx.f12.f64 = double(float(ctx.f11.f64));
	// stfs f12,136(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(var_r31 + 136, temp.u32);
	// beq cr6,0x82482d0c
	if (!(ctx.cr6.eq)) {
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfs f0,-24676(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -24676);  /* glob:lbl_82079F9C @ 0x82079f9c */
		ctx.f0.f64 = double(temp.f32);
		// b 0x82482d14
	} else {
	loc_82482D0C:
		// lis r11,-32248
		ctx.r11.s64 = -2113404928;
		// lfs f0,-25648(r11)
		ctx.fpscr.disableFlushMode();
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25648);  /* glob:lbl_82079BD0 @ 0x82079bd0 */
		ctx.f0.f64 = double(temp.f32);
	}
loc_82482D14:
	// fdivs f10,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f13.f64 / ctx.f12.f64));
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,12
	ctx.r3.s64 = 12;
	// fdivs f9,f0,f10
	ctx.f9.f64 = double(float(ctx.f0.f64 / ctx.f10.f64));
	// stfs f9,140(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(var_r31 + 140, temp.u32);
	// bl 0x82353140
	msgMsgSink_3140_w(ctx, base);
	// cmplwi cr6,r3,0
	// stw r3,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r3.u32);
	// bne cr6,0x82482d4c
	if (ctx.r3.u32 == 0) {
	loc_82482D40:
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		// b 0x82482ddc
		// blr
		return;
	}
loc_82482D4C:
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 60);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823531f0
	grc_31F0(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,2
	ctx.r5.s64 = 2;
	// sth r30,0(r3)
	PPC_STORE_U16(ctx.r3.u32 + 0, (uint16_t)var_r30);
	// li r4,3
	ctx.r4.s64 = 3;
	// li r11,4
	ctx.r11.s64 = 4;
	// li r10,5
	ctx.r10.s64 = 5;
	// sth r6,2(r3)
	PPC_STORE_U16(ctx.r3.u32 + 2, ctx.r6.u16);
	// sth r5,4(r3)
	PPC_STORE_U16(ctx.r3.u32 + 4, ctx.r5.u16);
	// sth r4,6(r3)
	PPC_STORE_U16(ctx.r3.u32 + 6, ctx.r4.u16);
	// sth r11,8(r3)
	PPC_STORE_U16(ctx.r3.u32 + 8, ctx.r11.u16);
	// sth r10,10(r3)
	PPC_STORE_U16(ctx.r3.u32 + 10, ctx.r10.u16);
	// lwz r3,60(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 60);
	// bl 0x82353258
	msgMsgSink_3258_wrh(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,120
	ctx.r3.s64 = 120;
	// bl 0x82353010
	util_3010(ctx, base);
	// cmplwi cr6,r3,0
	// stw r3,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r3.u32);
	// beq cr6,0x82482d40
	if (ctx.r3.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		// b 0x82482ddc
		// blr
		return;
	}
	// lis r11,-32165
	ctx.r11.s64 = -2107965440;
	// addi r3,r11,9040
	ctx.r3.s64 = ctx.r11.s64 + 9040;
	// bl 0x82357480
	rage_7480(ctx, base);
	// cmplwi cr6,r3,0
	// stw r3,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r3.u32);
	// beq cr6,0x82482d40
	if (ctx.r3.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
		// b 0x82482ddc
		// blr
		return;
	}
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 0)/* msgMsgSink::vtable@+0x0 */;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,96(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 96);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
loc_82482DDC:
	// blr
	return;
}

__attribute__((alias("__imp__rage_2DF8"))) PPC_WEAK_FUNC(rage_2DF8);
PPC_FUNC_IMPL(__imp__rage_2DF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// cmplwi cr6,r3,0
	// beq cr6,0x82482e98
	if (ctx.r3.u32 != 0) {
		// addi r6,r1,80
		ctx.r6.s64 = ctx.r1.s64 + 80;
		// addi r5,r1,84
		ctx.r5.s64 = ctx.r1.s64 + 84;
		// li r4,0
		ctx.r4.s64 = 0;
		// bl 0x823561b8
		rage_61B8_h(ctx, base);
		// li r7,0
		ctx.r7.s64 = 0;
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,0
		ctx.r5.s64 = 0;
		// li r4,0
		ctx.r4.s64 = 0;
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// lwz r3,56(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
		// bl 0x82356118
		rage_6118(ctx, base);
		// li r6,0
		ctx.r6.s64 = 0;
		// li r5,120
		ctx.r5.s64 = 120;
		// lwz r3,64(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
		// li r4,0
		ctx.r4.s64 = 0;
		// bl 0x823530c0
		grc_30C0(ctx, base);
		// addi r4,r31,144
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 144;
		// li r5,120
		ctx.r5.s64 = 120;
		// bl 0x82434100
		memcpy(ctx, base);
		// lwz r3,64(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 64);
		// bl 0x82353130
		rage_3130_h(ctx, base);
		// mr r5,r30
		ctx.r5.u64 = var_r30;
		// li r4,0
		ctx.r4.s64 = 0;
		// lwz r7,80(r1)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// lwz r6,84(r1)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
		// lwz r3,56(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
		// bl 0x82356118
		rage_6118(ctx, base);
		// cmplwi cr6,r30,0
		// beq cr6,0x82482e98
		if (var_r30 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// blr
			return;
		}
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x82352ee0
		rage_2EE0(ctx, base);
	}
loc_82482E98:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__rage_2EB8"))) PPC_WEAK_FUNC(rage_2EB8);
PPC_FUNC_IMPL(__imp__rage_2EB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82482b90
	rage_2B90(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x824885c8
	util_85C8(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__rage_2EF0"))) PPC_WEAK_FUNC(rage_2EF0);
PPC_FUNC_IMPL(__imp__rage_2EF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=96, manual
	// cmplwi cr6,r4,0
	// bne cr6,0x82482f14
	if (ctx.r4.u32 == 0) {
		// li r11,1
		ctx.r11.s64 = 1;
		// addi r4,r3,96
		ctx.r4.s64 = ctx.r3.s64 + 96;
		// stw r11,112(r3)
		PPC_STORE_U32(ctx.r3.u32 + 112, ctx.r11.u32);
		// b 0x82482f3c
	} else {
	loc_82482F14:
		// li r10,0
		ctx.r10.s64 = 0;
		// stw r10,112(r3)
		PPC_STORE_U32(ctx.r3.u32 + 112, ctx.r10.u32);
		// lwz r9,0(r4)
		ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
		// stw r9,80(r3)
		PPC_STORE_U32(ctx.r3.u32 + 80, ctx.r9.u32);
		// lwz r8,4(r4)
		ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
		// stw r8,84(r3)
		PPC_STORE_U32(ctx.r3.u32 + 84, ctx.r8.u32);
		// lwz r7,8(r4)
		ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
		// stw r7,88(r3)
		PPC_STORE_U32(ctx.r3.u32 + 88, ctx.r7.u32);
		// lwz r6,12(r4)
		ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
		// stw r6,92(r3)
		PPC_STORE_U32(ctx.r3.u32 + 92, ctx.r6.u32);
	}
loc_82482F3C:
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// extsw r11,r5
	ctx.r11.s64 = ctx.r5.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,144(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 144, temp.u32);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// stfs f9,148(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 148, temp.u32);
	// lwz r8,8(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f8,80(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// stfs f6,164(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 164, temp.u32);
	// lwz r6,4(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// extsw r5,r6
	ctx.r5.s64 = ctx.r6.s32;
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f5,80(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// stfs f3,168(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 168, temp.u32);
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f2,80(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f0,f1
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,184(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 184, temp.u32);
	// lwz r9,12(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stfs f11,188(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 188, temp.u32);
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f10,80(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// stfs f8,204(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 204, temp.u32);
	// lwz r5,12(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// extsw r11,r5
	ctx.r11.s64 = ctx.r5.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f7,80(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// stfs f5,208(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 208, temp.u32);
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f4,80(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// stfs f2,224(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 224, temp.u32);
	// lwz r8,4(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f1,80(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f1
	ctx.f0.f64 = double(ctx.f1.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// stfs f13,228(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 228, temp.u32);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// extsw r5,r6
	ctx.r5.s64 = ctx.r6.s32;
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,244(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 244, temp.u32);
	// lwz r4,12(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// extsw r11,r4
	ctx.r11.s64 = ctx.r4.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f9,80(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// stfs f7,248(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 248, temp.u32);
	// bl 0x82482df8
	rage_2DF8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__rage_30A8"))) PPC_WEAK_FUNC(rage_30A8);
PPC_FUNC_IMPL(__imp__rage_30A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// FRAME: size=112, manual
	// lwz r9,124(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	// lfs f10,140(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	ctx.f10.f64 = double(temp.f32);
	// lwz r8,128(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	// lfs f9,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f0,f9,f10
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f13,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// addi r11,r11,15784
	ctx.r11.s64 = ctx.r11.s64 + 15784;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// fdivs f6,f0,f13
	ctx.f6.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f8,f12
	ctx.f8.f64 = double(ctx.f12.s64);
	// fcfid f7,f11
	ctx.f7.f64 = double(ctx.f11.s64);
	// frsp f11,f8
	ctx.f11.f64 = double(float(ctx.f8.f64));
	// frsp f12,f7
	ctx.f12.f64 = double(float(ctx.f7.f64));
	// fdivs f5,f11,f12
	ctx.f5.f64 = double(float(ctx.f11.f64 / ctx.f12.f64));
	// fcmpu cr6,f6,f5
	// ble cr6,0x82483130
	if (ctx.f6.f64 > ctx.f5.f64) {
		// fmuls f4,f11,f13
		ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
		// lfs f0,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// lis r11,-32256
		// fmr f11,f0
		ctx.f11.f64 = ctx.f0.f64;
		// fdivs f3,f4,f12
		ctx.f3.f64 = double(float(ctx.f4.f64 / ctx.f12.f64));
		// lfs f12,27200(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);
		ctx.f12.f64 = double(temp.f32);
		// fdivs f10,f3,f10
		ctx.f10.f64 = double(float(ctx.f3.f64 / ctx.f10.f64));
		// fsubs f2,f9,f10
		ctx.f2.f64 = double(float(ctx.f9.f64 - ctx.f10.f64));
		// fmuls f12,f2,f12
		ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
		// fadds f9,f12,f10
		ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
		// b 0x82483154
	} else {
	loc_82483130:
		// fmuls f1,f12,f0
		ctx.fpscr.disableFlushMode();
		ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
		// lfs f0,0(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
		ctx.f0.f64 = double(temp.f32);
		// lis r11,-32256
		// fmr f12,f0
		ctx.f12.f64 = ctx.f0.f64;
		// fdivs f10,f1,f11
		ctx.f10.f64 = double(float(ctx.f1.f64 / ctx.f11.f64));
		// fsubs f11,f13,f10
		ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f10.f64));
		// lfs f13,27200(r11)
		temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 27200);
		ctx.f13.f64 = double(temp.f32);
		// fmuls f11,f11,f13
		ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
		// fadds f13,f10,f11
		ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	}
loc_82483154:
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// fctiwz f10,f12
	ctx.fpscr.disableFlushMode();
	ctx.f10.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f12.f64));
	// lis r11,-32256
	// fctiwz f6,f13
	ctx.f6.s64 = (ctx.f13.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f13.f64));
	// fctiwz f8,f11
	ctx.f8.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f11.f64));
	// stfs f0,156(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 156, temp.u32);
	// fctiwz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f9.f64));
	// stfs f0,160(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 160, temp.u32);
	// stfs f0,180(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 180, temp.u32);
	// stfiwx f10,0,r7
	PPC_STORE_U32(ctx.r7.u32, ctx.f10.u32);
	// lfs f13,15788(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15788);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,176(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 176, temp.u32);
	// stfs f0,196(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 196, temp.u32);
	// stfs f13,200(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 200, temp.u32);
	// stfs f0,216(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 216, temp.u32);
	// stfs f13,220(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 220, temp.u32);
	// stfs f13,236(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 236, temp.u32);
	// stfs f0,240(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 240, temp.u32);
	// stfs f13,256(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 256, temp.u32);
	// stfs f13,260(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 260, temp.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// extsw r5,r11
	ctx.r5.s64 = ctx.r11.s32;
	// stfiwx f8,0,r6
	PPC_STORE_U32(ctx.r6.u32, ctx.f8.u32);
	// stw r11,96(r3)
	PPC_STORE_U32(ctx.r3.u32 + 96, ctx.r11.u32);
	// std r5,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r5.u64);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// lfd f5,88(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// stfiwx f7,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f7.u32);
	// stw r10,100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 100, ctx.r10.u32);
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// frsp f12,f4
	ctx.f12.f64 = double(float(ctx.f4.f64));
	// stfs f12,144(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 144, temp.u32);
	// stfs f12,184(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 184, temp.u32);
	// stfs f12,204(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 204, temp.u32);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// extsw r7,r11
	ctx.r7.s64 = ctx.r11.s32;
	// lfd f3,88(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f2,f3
	ctx.f2.f64 = double(ctx.f3.s64);
	// stfiwx f6,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f6.u32);
	// stw r11,104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 104, ctx.r11.u32);
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// frsp f12,f2
	ctx.f12.f64 = double(float(ctx.f2.f64));
	// stfs f12,148(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 148, temp.u32);
	// stfs f12,168(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 168, temp.u32);
	// stfs f12,228(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 228, temp.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// lfd f1,88(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// stw r10,108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 108, ctx.r10.u32);
	// std r6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r6.u64);
	// frsp f12,f11
	ctx.f12.f64 = double(float(ctx.f11.f64));
	// stfs f12,164(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 164, temp.u32);
	// stfs f12,224(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 224, temp.u32);
	// stfs f12,244(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 244, temp.u32);
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f12,f9
	ctx.f12.f64 = double(float(ctx.f9.f64));
	// stfs f12,188(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 188, temp.u32);
	// stfs f12,208(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 208, temp.u32);
	// stfs f12,248(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 248, temp.u32);
	// bl 0x82482df8
	rage_2DF8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phInstStatic_3278_wrh"))) PPC_WEAK_FUNC(phInstStatic_3278_wrh);
PPC_FUNC_IMPL(__imp__phInstStatic_3278_wrh) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// lwz r10,40(r11)
	// bctrl
	VCALL(ctx.r3.u32, 10, ctx, base);  // vtable slot 10 (byte +40)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x824832c8
	if ((int32_t)var_r30 >= 0) {
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8235eac0
		rage_EAC0(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// stw r29,56(r31)
		PPC_STORE_U32(var_r31 + 56, var_r29);
		// bl 0x82482c68
		msgMsgSink_2C68_w(ctx, base);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmpwi cr6,r30,0
		// bge cr6,0x824832dc
		if ((int32_t)var_r30 >= 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			return;
		}
	}
loc_824832C8:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,44(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 11, ctx, base);  // pattern-B slot 11 (byte +44)
loc_824832DC:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__ph_ctor_32E8"))) PPC_WEAK_FUNC(ph_ctor_32E8);
PPC_FUNC_IMPL(__imp__ph_ctor_32E8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32255
	// addi r11,r11,-31992
	ctx.r11.s64 = ctx.r11.s64 + -31992;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// b 0x824887f8
	phInst_87F8(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_15"))) PPC_WEAK_FUNC(phInst_15);
PPC_FUNC_IMPL(__imp__phInst_15) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488808
	phInstStatic_15(ctx, base);
	// lis r10,-32248
	// li r11,0
	ctx.r11.s64 = 0;
	// lfs f0,-25668(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25668);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,72(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 72, temp.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r11.u32);
	// stw r11,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r11.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__set_3348"))) PPC_WEAK_FUNC(set_3348);
PPC_FUNC_IMPL(__imp__set_3348) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// stfs f1,72(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 72, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_vfn_27"))) PPC_WEAK_FUNC(atSingleton_vfn_27);
PPC_FUNC_IMPL(__imp__atSingleton_vfn_27) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,64(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 64);
	// blr
	return;
}

__attribute__((alias("__imp__game_3358"))) PPC_WEAK_FUNC(game_3358);
PPC_FUNC_IMPL(__imp__game_3358) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f1,72(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	ctx.f1.f64 = double(temp.f32);
	// blr
	return;
}

__attribute__((alias("__imp__phInstStatic_3360_w"))) PPC_WEAK_FUNC(phInstStatic_3360_w);
PPC_FUNC_IMPL(__imp__phInstStatic_3360_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488840
	phInstStatic_8840(ctx, base);
	// lis r11,-32255
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,-31992
	ctx.r11.s64 = ctx.r11.s64 + -31992;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82488808
	phInstStatic_15(ctx, base);
	// lis r10,-32248
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lfs f0,-25668(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25668);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,72(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 72, temp.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r11.u32);
	// stw r11,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r11.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_0"))) PPC_WEAK_FUNC(phInst_0);
PPC_FUNC_IMPL(__imp__phInst_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32255
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-31992
	ctx.r11.s64 = ctx.r11.s64 + -31992;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x824887f8
	phInst_87F8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x82483414
	if (ctx.r11.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32773
		ctx.r4.u64 = ctx.r4.u64 | 32773;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82483414:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_3430_h"))) PPC_WEAK_FUNC(crAnimDofQuaternion_3430_h);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_3430_h) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32255
	// addi r11,r11,-31928
	ctx.r11.s64 = ctx.r11.s64 + -31928;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// b 0x824887f8
	phInst_87F8(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_31"))) PPC_WEAK_FUNC(phInst_31);
PPC_FUNC_IMPL(__imp__phInst_31) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488808
	phInstStatic_15(ctx, base);
	// lis r10,-32256
	// li r11,0
	ctx.r11.s64 = 0;
	// lfs f0,15784(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15784);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r11.u32);
	// stw r11,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r11.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
	// stw r11,72(r31)
	PPC_STORE_U32(var_r31 + 72, ctx.r11.u32);
	// stw r11,76(r31)
	PPC_STORE_U32(var_r31 + 76, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__set_3498"))) PPC_WEAK_FUNC(set_3498);
PPC_FUNC_IMPL(__imp__set_3498) {
	PPC_FUNC_PROLOGUE();
	// stw r4,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_vfn_25_34A0_1"))) PPC_WEAK_FUNC(atSingleton_vfn_25_34A0_1);
PPC_FUNC_IMPL(__imp__atSingleton_vfn_25_34A0_1) {
	PPC_FUNC_PROLOGUE();
	// stw r4,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__set_34A8"))) PPC_WEAK_FUNC(set_34A8);
PPC_FUNC_IMPL(__imp__set_34A8) {
	PPC_FUNC_PROLOGUE();
	// stw r4,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__set_34B0"))) PPC_WEAK_FUNC(set_34B0);
PPC_FUNC_IMPL(__imp__set_34B0) {
	PPC_FUNC_PROLOGUE();
	// stw r4,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_vfn_57"))) PPC_WEAK_FUNC(atSingleton_vfn_57);
PPC_FUNC_IMPL(__imp__atSingleton_vfn_57) {
	PPC_FUNC_PROLOGUE();
	// stw r4,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_34C0_h"))) PPC_WEAK_FUNC(crAnimDofQuaternion_34C0_h);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_34C0_h) {
	PPC_FUNC_PROLOGUE();
	// stw r4,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, ctx.r4.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_34C8_h"))) PPC_WEAK_FUNC(crAnimDofQuaternion_34C8_h);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_34C8_h) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// stfs f1,80(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r3.u32 + 80, temp.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_34D0_g"))) PPC_WEAK_FUNC(phInst_34D0_g);
PPC_FUNC_IMPL(__imp__phInst_34D0_g) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,56(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	// blr
	return;
}

__attribute__((alias("__imp__atSingleton_vfn_46"))) PPC_WEAK_FUNC(atSingleton_vfn_46);
PPC_FUNC_IMPL(__imp__atSingleton_vfn_46) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,72(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// blr
	return;
}

__attribute__((alias("__imp__phInstStatic_34E0_w"))) PPC_WEAK_FUNC(phInstStatic_34E0_w);
PPC_FUNC_IMPL(__imp__phInstStatic_34E0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	PPCRegister temp{};
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488840
	phInstStatic_8840(ctx, base);
	// lis r11,-32255
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,-31928
	ctx.r11.s64 = ctx.r11.s64 + -31928;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82488808
	phInstStatic_15(ctx, base);
	// lis r10,-32256
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lfs f0,15784(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 15784);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(var_r31 + 80, temp.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,60(r31)
	PPC_STORE_U32(var_r31 + 60, ctx.r11.u32);
	// stw r11,64(r31)
	PPC_STORE_U32(var_r31 + 64, ctx.r11.u32);
	// stw r11,68(r31)
	PPC_STORE_U32(var_r31 + 68, ctx.r11.u32);
	// stw r11,72(r31)
	PPC_STORE_U32(var_r31 + 72, ctx.r11.u32);
	// stw r11,76(r31)
	PPC_STORE_U32(var_r31 + 76, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_16"))) PPC_WEAK_FUNC(phInst_16);
PPC_FUNC_IMPL(__imp__phInst_16) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32255
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-31928
	ctx.r11.s64 = ctx.r11.s64 + -31928;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x824887f8
	phInst_87F8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x8248359c
	if (ctx.r11.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32772
		ctx.r4.u64 = ctx.r4.u64 | 32772;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8248359C:
	// blr
	return;
}

__attribute__((alias("__imp__game_35B8_h"))) PPC_WEAK_FUNC(game_35B8_h);
PPC_FUNC_IMPL(__imp__game_35B8_h) {
	PPC_FUNC_PROLOGUE();
	// lis r4,8332
	ctx.r4.s64 = 546045952;
	// ori r4,r4,32808
	ctx.r4.u64 = ctx.r4.u64 | 32808;
	// b 0x820c01b8
	rage_01B8(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_54"))) PPC_WEAK_FUNC(phInst_54);
PPC_FUNC_IMPL(__imp__phInst_54) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x824887e0
	util_87E0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r5,1408
	ctx.r5.s64 = 1408;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,60
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 60;
	// stw r11,1472(r31)
	PPC_STORE_U32(var_r31 + 1472, ctx.r11.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,1468(r31)
	PPC_STORE_U32(var_r31 + 1468, ctx.r11.u32);
	// stw r11,1476(r31)
	PPC_STORE_U32(var_r31 + 1476, ctx.r11.u32);
	// stw r11,1480(r31)
	PPC_STORE_U32(var_r31 + 1480, ctx.r11.u32);
	// stw r11,1484(r31)
	PPC_STORE_U32(var_r31 + 1484, ctx.r11.u32);
	// bl 0x8242fed0
	memset(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_44"))) PPC_WEAK_FUNC(phInst_44);
PPC_FUNC_IMPL(__imp__phInst_44) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmpwi cr6,r4,1
	// li r3,0
	ctx.r3.s64 = 0;
	// bnelr cr6
	if (ctx.r4.s32 != 1) return;
	// stw r5,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r5.u32);
	// stw r6,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_46"))) PPC_WEAK_FUNC(phInst_46);
PPC_FUNC_IMPL(__imp__phInst_46) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmplwi cr6,r31,0
	// beq cr6,0x82483678
	if (var_r31 != 0) {
		// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r10,4(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 1, ctx, base);  // pattern-B slot 1 (byte +4)
	}
loc_82483678:
	// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r8,60(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r30, 15, ctx, base);  // pattern-B slot 15 (byte +60)
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r31,44(r30)
	PPC_STORE_U32(var_r30 + 44, var_r31);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_36B0_wrh"))) PPC_WEAK_FUNC(phInst_36B0_wrh);
PPC_FUNC_IMPL(__imp__phInst_36B0_wrh) {
	PPC_FUNC_PROLOGUE();
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r10,14
	ctx.r10.s64 = 14;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_824836C4:
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x824836c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_824836C4;
	// addi r11,r4,-8
	ctx.r11.s64 = ctx.r4.s64 + -8;
	// cmplwi cr6,r11,24
	// bgt cr6,0x824837a0
	if (ctx.r11.u32 > 24) {
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	}
	// lis r12,-32184
	// addi r12,r12,14068
	ctx.r12.s64 = ctx.r12.s64 + 14068;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr
	switch (ctx.r11.u64) {
	case 0:
		goto loc_82483758;
	case 1:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 2:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 3:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 4:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 5:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 6:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 7:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 8:
		goto loc_82483764;
	case 9:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 10:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 11:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 12:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 13:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 14:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 15:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 16:
		goto loc_82483770;
	case 17:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 18:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 19:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 20:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 21:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 22:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 23:
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	case 24:
		goto loc_82483770;
	default:
		__builtin_trap(); // Switch case out of range
	}
loc_82483758:
	// li r11,2
	ctx.r11.s64 = 2;
	// stb r11,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r11.u8);
	// b 0x82483774
	goto loc_82483774;
loc_82483764:
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r10,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r10.u8);
	// b 0x82483774
	goto loc_82483774;
loc_82483770:
	// stb r3,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r3.u8);
loc_82483774:
	// addi r9,r5,-1
	ctx.r9.s64 = ctx.r5.s64 + -1;
	// cmplwi cr6,r9,7
	// bgt cr6,0x824837a0
	if (ctx.r9.u32 > 7) {
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	}
	// cmplwi cr6,r5,8
	// bne cr6,0x8248378c
	if (ctx.r5.u32 != 8) {
		// stb r5,4(r7)
		PPC_STORE_U8(ctx.r7.u32 + 4, ctx.r5.u8);
		// cmplwi cr6,r6,0
		// beq cr6,0x824837a0
		if (ctx.r6.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
		}
		// stw r6,8(r7)
		PPC_STORE_U32(ctx.r7.u32 + 8, ctx.r6.u32);
		// blr
		return;
	}
	// li r5,6
	ctx.r5.s64 = 6;
loc_8248378C:
	// stb r5,4(r7)
	PPC_STORE_U8(ctx.r7.u32 + 4, ctx.r5.u8);
	// cmplwi cr6,r6,0
	// beq cr6,0x824837a0
	if (ctx.r6.u32 == 0) {
		// lis r3,-32761
		// ori r3,r3,87
		ctx.r3.u64 = ctx.r3.u64 | 87;
		// blr
		return;
	}
	// stw r6,8(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8, ctx.r6.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_37B0_p33"))) PPC_WEAK_FUNC(phInst_37B0_p33);
PPC_FUNC_IMPL(__imp__phInst_37B0_p33) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r8,r11,1472
	ctx.r8.s64 = ctx.r11.s64 + 1472;
loc_824837B8:
	// mfmsr r9
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.r9.u64 = PPC_CHECK_GLOBAL_LOCK();
	// mtmsrd r13,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_ENTER_GLOBAL_LOCK();
	// lwarx r10,0,r8
	ea = ctx.r8.u32;
	ctx.reserved.u32 = *(uint32_t*)PPC_RAW_ADDR(ea);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	ea = ctx.r8.u32;
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(PPC_RAW_ADDR(ea)), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_LEAVE_GLOBAL_LOCK();
	// bne 0x824837b8
	if (!ctx.cr0.eq) goto loc_824837B8;
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// cmplwi cr6,r10,0
	// beqlr cr6
	if (ctx.r10.u32 == 0) return;
	// lwz r3,52(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInstStatic_37F0_w"))) PPC_WEAK_FUNC(phInstStatic_37F0_w);
PPC_FUNC_IMPL(__imp__phInstStatic_37F0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488580
	util_8580(ctx, base);
	// lis r11,-32255
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,-31864
	ctx.r11.s64 = ctx.r11.s64 + -31864;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x824887e0
	util_87E0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r5,1408
	ctx.r5.s64 = 1408;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,60
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 60;
	// stw r11,1472(r31)
	PPC_STORE_U32(var_r31 + 1472, ctx.r11.u32);
	// stw r11,56(r31)
	PPC_STORE_U32(var_r31 + 56, ctx.r11.u32);
	// stw r11,1468(r31)
	PPC_STORE_U32(var_r31 + 1468, ctx.r11.u32);
	// stw r11,1476(r31)
	PPC_STORE_U32(var_r31 + 1476, ctx.r11.u32);
	// stw r11,1480(r31)
	PPC_STORE_U32(var_r31 + 1480, ctx.r11.u32);
	// stw r11,1484(r31)
	PPC_STORE_U32(var_r31 + 1484, ctx.r11.u32);
	// bl 0x8242fed0
	memset(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__game_3860"))) PPC_WEAK_FUNC(game_3860);
PPC_FUNC_IMPL(__imp__game_3860) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=208, savegprlr_29
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// lwz r10,44(r11)
	// bctrl
	VCALL(ctx.r3.u32, 11, ctx, base);  // vtable slot 11 (byte +44)
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// cmpwi cr6,r31,0
	// blt cr6,0x82483984
	if ((int32_t)var_r31 >= 0) {
		// li r3,0
		ctx.r3.s64 = 0;
		// bl 0x82461270
		game_1270(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82483984
		if ((int32_t)var_r31 < 0) goto loc_82483984;
		// li r7,1
		ctx.r7.s64 = 1;
		// stw r7,1484(r30)
		PPC_STORE_U32(var_r30 + 1484, ctx.r7.u32);
		// lwz r8,8(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 8);
		// cmplwi cr6,r8,0
		// bne cr6,0x824838bc
		if (ctx.r8.u32 == 0) {
			// li r8,16
			ctx.r8.s64 = 16;
		}
	loc_824838BC:
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// stw r8,1468(r30)
		PPC_STORE_U32(var_r30 + 1468, ctx.r8.u32);
		// li r10,0
		ctx.r10.s64 = 0;
		// li r9,11
		ctx.r9.s64 = 11;
		// mtctr r9
		ctx.ctr.u64 = ctx.r9.u64;
	loc_824838D0:
		// std r10,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// bdnz 0x824838d0
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_824838D0;
		// lis r9,-32184
		// lwz r4,12(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 12);
		// lwz r6,4(r29)
		ctx.r6.u64 = PPC_LOAD_U32(var_r29 + 4);
		// addi r3,r9,14256
		ctx.r3.s64 = ctx.r9.s64 + 14256;
		// stw r10,0(r11)
		PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
		// cmpwi cr6,r6,0
		// stb r8,139(r1)
		PPC_STORE_U8(ctx.r1.u32 + 139, ctx.r8.u8);
		// stb r7,144(r1)
		PPC_STORE_U8(ctx.r1.u32 + 144, ctx.r7.u8);
		// mr r11,r7
		ctx.r11.u64 = ctx.r7.u64;
		// stw r4,148(r1)
		PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r4.u32);
		// stw r3,160(r1)
		PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r3.u32);
		// bne cr6,0x82483910
		if (ctx.r6.s32 == 0) {
			// li r11,0
			ctx.r11.s64 = 0;
		}
	loc_82483910:
		// stb r11,136(r1)
		PPC_STORE_U8(ctx.r1.u32 + 136, ctx.r11.u8);
		// li r9,0
		ctx.r9.s64 = 0;
		// addi r11,r1,80
		ctx.r11.s64 = ctx.r1.s64 + 80;
		// li r10,7
		ctx.r10.s64 = 7;
		// mtctr r10
		ctx.ctr.u64 = ctx.r10.u64;
	loc_82483924:
		// std r9,0(r11)
		PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
		// addi r11,r11,8
		ctx.r11.s64 = ctx.r11.s64 + 8;
		// bdnz 0x82483924
		--ctx.ctr.u64;
		if (ctx.ctr.u32 != 0) goto loc_82483924;
		// lis r9,0
		ctx.r9.s64 = 0;
		// stb r7,80(r1)
		PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r7.u8);
		// li r10,2
		ctx.r10.s64 = 2;
		// ori r8,r9,44100
		ctx.r8.u64 = ctx.r9.u64 | 44100;
		// addi r4,r30,56
		ctx.r4.s64 = (int64_t)(int32_t)var_r30 + 56;
		// addi r3,r1,80
		ctx.r3.s64 = ctx.r1.s64 + 80;
		// stb r10,84(r1)
		PPC_STORE_U8(ctx.r1.u32 + 84, ctx.r10.u8);
		// stw r8,88(r1)
		PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
		// bl 0x82461278
		game_1278(ctx, base);
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// blt cr6,0x82483984
		if ((int32_t)var_r31 < 0) goto loc_82483984;
		// lwz r7,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r4,0(r29)
		ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r6,56(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 14, ctx, base);  // pattern-B slot 14 (byte +56)
		// mr r31,r3
		var_r31 = ctx.r3.u32;
		// cmpwi cr6,r31,0
		// bge cr6,0x82483998
		if ((int32_t)var_r31 >= 0) {
			// mr r3,r31
			ctx.r3.u64 = var_r31;
			return;
		}
	}
loc_82483984:
	// lwz r5,0(r30)
  // [ph4a] vtable load collapsed
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r4,44(r5)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r30, 11, ctx, base);  // pattern-B slot 11 (byte +44)
loc_82483998:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	return;
}

__attribute__((alias("__imp__phInst_43"))) PPC_WEAK_FUNC(phInst_43);
PPC_FUNC_IMPL(__imp__phInst_43) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r10,84(r11)
	// bctrl
	phInst_vfn_21(ctx, base);  // vtable slot 21 (byte +84)  // phInst::vfn_21
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r30,0
	var_r30 = 0;
	// cmplwi cr6,r3,0
	// beq cr6,0x824839ec
	if (ctx.r3.u32 != 0) {
		// bl 0x824608b8
		phInst_08B8(ctx, base);
		// stw r30,56(r31)
		PPC_STORE_U32(var_r31 + 56, var_r30);
	}
loc_824839EC:
	// lwz r9,1484(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 1484);
	// cmpwi cr6,r9,0
	// beq cr6,0x82483a00
	if (ctx.r9.s32 != 0) {
		// bl 0x82460fe8
		phInst_0FE8_v12(ctx, base);
		// stw r30,1484(r31)
		PPC_STORE_U32(var_r31 + 1484, var_r30);
	}
loc_82483A00:
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r7,60(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
	// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r5,88(r6)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 22, ctx, base);  // pattern-B slot 22 (byte +88)
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_51"))) PPC_WEAK_FUNC(phInst_51);
PPC_FUNC_IMPL(__imp__phInst_51) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=176, savegprlr_28
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// li r4,4
	ctx.r4.s64 = 4;
	// lwz r10,84(r11)
	// bctrl
	phInst_vfn_21(ctx, base);  // vtable slot 21 (byte +84)  // phInst::vfn_21
	// cmpwi cr6,r3,0
	// blt cr6,0x82483acc
	if (ctx.r3.s32 >= 0) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f0730
		grmShaderFx_vfn_16(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x824834d0
		phInst_34D0_g(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x824834d8
		atSingleton_vfn_46(ctx, base);
		// mr r4,r3
		ctx.r4.u64 = ctx.r3.u64;
		// addi r7,r1,80
		ctx.r7.s64 = ctx.r1.s64 + 80;
		// mr r6,r29
		ctx.r6.u64 = var_r29;
		// mr r5,r28
		ctx.r5.u64 = var_r28;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x824836b0
		phInst_36B0_wrh(ctx, base);
		// cmpwi cr6,r3,0
		// blt cr6,0x82483acc
		if (ctx.r3.s32 < 0) {
			return;
		}
		// li r6,0
		ctx.r6.s64 = 0;
		// lwz r3,56(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 56);
		// li r5,0
		ctx.r5.s64 = 0;
		// addi r4,r1,80
		ctx.r4.s64 = ctx.r1.s64 + 80;
		// bl 0x82460af8
		phInst_0AF8_h(ctx, base);
	}
loc_82483ACC:
	return;
}

__attribute__((alias("__imp__CCalMoviePlayer_3AD8_h"))) PPC_WEAK_FUNC(CCalMoviePlayer_3AD8_h);
PPC_FUNC_IMPL(__imp__CCalMoviePlayer_3AD8_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// bl 0x82460850
	util_0850(ctx, base);
	// lwz r11,56(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r30)
	PPC_STORE_U32(var_r30 + 0, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_52"))) PPC_WEAK_FUNC(phInst_52);
PPC_FUNC_IMPL(__imp__phInst_52) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	uint32_t ea{};
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// addi r30,r31,1472
	var_r30 = (uint32_t)(var_r31 + 1472);
	// lwz r10,1468(r31)
	ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 1468);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 0);
	// cmplw cr6,r11,r10
	// blt cr6,0x82483b78
	if (ctx.r11.u32 >= ctx.r10.u32) {
		// lwz r9,1480(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 1480);
		// cmpwi cr6,r9,0
		// bne cr6,0x82483b68
		if (ctx.r9.s32 == 0) {
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r3,56(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
			// bl 0x82460c08
			xam_0C08_g(ctx, base);
			// li r8,1
			ctx.r8.s64 = 1;
			// stw r8,1480(r31)
			PPC_STORE_U32(var_r31 + 1480, ctx.r8.u32);
		}
	loc_82483B68:
		// lis r3,-32768
		// ori r3,r3,10
		ctx.r3.u64 = ctx.r3.u64 | 10;
		return;
	}
loc_82483B78:
	// mfmsr r6
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.r6.u64 = PPC_CHECK_GLOBAL_LOCK();
	// mtmsrd r13,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_ENTER_GLOBAL_LOCK();
	// lwarx r7,0,r30
	ea = var_r30;
	ctx.reserved.u32 = *(uint32_t*)PPC_RAW_ADDR(ea);
	ctx.r7.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stwcx. r7,0,r30
	ea = var_r30;
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(PPC_RAW_ADDR(ea)), ctx.reserved.s32, __builtin_bswap32(ctx.r7.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r6,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r6.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_LEAVE_GLOBAL_LOCK();
	// bne 0x82483b78
	if (!ctx.cr0.eq) goto loc_82483B78;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 44);
	// lwz r4,80(r5)
	// bctrl
	VCALL(ctx.r3.u32, 20, ctx, base);  // vtable slot 20 (byte +80)
	// lwz r11,1476(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 1476);
	// li r5,0
	ctx.r5.s64 = 0;
	// mulli r11,r11,88
	ctx.r11.s64 = static_cast<int64_t>(ctx.r11.u64 * static_cast<uint64_t>(88));
	// add r10,r11,r31
	ctx.r10.u64 = ctx.r11.u64 + var_r31;
	// stw r3,60(r10)
	PPC_STORE_U32(ctx.r10.u32 + 60, ctx.r3.u32);
	// lwz r9,1476(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 1476);
	// mulli r11,r9,88
	ctx.r11.s64 = static_cast<int64_t>(ctx.r9.u64 * static_cast<uint64_t>(88));
	// add r8,r11,r31
	ctx.r8.u64 = ctx.r11.u64 + var_r31;
	// stw r29,64(r8)
	PPC_STORE_U32(ctx.r8.u32 + 64, var_r29);
	// lwz r7,1476(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 1476);
	// mulli r11,r7,88
	ctx.r11.s64 = static_cast<int64_t>(ctx.r7.u64 * static_cast<uint64_t>(88));
	// add r6,r11,r31
	ctx.r6.u64 = ctx.r11.u64 + var_r31;
	// stw r31,144(r6)
	PPC_STORE_U32(ctx.r6.u32 + 144, var_r31);
	// lwz r4,1476(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 1476);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// mulli r11,r4,88
	ctx.r11.s64 = static_cast<int64_t>(ctx.r4.u64 * static_cast<uint64_t>(88));
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + var_r31;
	// addi r4,r11,60
	ctx.r4.s64 = ctx.r11.s64 + 60;
	// bl 0x82460b50
	xam_0B50_g(ctx, base);
	// cmpwi cr6,r3,0
	// blt cr6,0x82483c28
	if (ctx.r3.s32 >= 0) {
		// lwz r11,1480(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 1480);
		// cmpwi cr6,r11,0
		// bne cr6,0x82483c14
		if (ctx.r11.s32 == 0) {
			// li r4,0
			ctx.r4.s64 = 0;
			// lwz r3,56(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
			// bl 0x82460c08
			xam_0C08_g(ctx, base);
		}
	loc_82483C14:
		// cmpwi cr6,r3,0
		// blt cr6,0x82483c84
		if (ctx.r3.s32 < 0) {
			return;
		}
		// li r10,1
		ctx.r10.s64 = 1;
		// stw r10,1480(r31)
		PPC_STORE_U32(var_r31 + 1480, ctx.r10.u32);
		// b 0x82483c44
	} else {
	loc_82483C28:
		// mfmsr r8
		std::atomic_thread_fence(std::memory_order_seq_cst);
		ctx.r8.u64 = PPC_CHECK_GLOBAL_LOCK();
		// mtmsrd r13,1
		std::atomic_thread_fence(std::memory_order_seq_cst);
		ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
		PPC_ENTER_GLOBAL_LOCK();
		// lwarx r9,0,r30
		ea = var_r30;
		ctx.reserved.u32 = *(uint32_t*)PPC_RAW_ADDR(ea);
		ctx.r9.u64 = __builtin_bswap32(ctx.reserved.u32);
		// addi r9,r9,-1
		ctx.r9.s64 = ctx.r9.s64 + -1;
		// stwcx. r9,0,r30
		ea = var_r30;
		ctx.cr0.lt = 0;
		ctx.cr0.gt = 0;
		ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(PPC_RAW_ADDR(ea)), ctx.reserved.s32, __builtin_bswap32(ctx.r9.s32));
		ctx.cr0.so = ctx.xer.so;
		// mtmsrd r8,1
		std::atomic_thread_fence(std::memory_order_seq_cst);
		ctx.msr = (ctx.r8.u32 & 0x8020) | (ctx.msr & ~0x8020);
		PPC_LEAVE_GLOBAL_LOCK();
		// bne 0x82483c28
		if (!ctx.cr0.eq) goto loc_82483C28;
	}
loc_82483C44:
	// cmpwi cr6,r3,0
	// blt cr6,0x82483c84
	if (ctx.r3.s32 >= 0) {
		// lwz r11,1476(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 1476);
		// addi r11,r11,1
		ctx.r11.s64 = ctx.r11.s64 + 1;
		// stw r11,1476(r31)
		PPC_STORE_U32(var_r31 + 1476, ctx.r11.u32);
		// lwz r7,1468(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 1468);
		// cmplw cr6,r11,r7
		// blt cr6,0x82483c6c
		if (ctx.r11.u32 >= ctx.r7.u32) {
			// li r6,0
			ctx.r6.s64 = 0;
			// stw r6,1476(r31)
			PPC_STORE_U32(var_r31 + 1476, ctx.r6.u32);
		}
	loc_82483C6C:
		// lwz r3,44(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 44);
		// clrldi r4,r29,32
		ctx.r4.u64 = var_r29 & 0xFFFFFFFF;
		// lwz r11,52(r5)
		// bctrl
		VCALL(ctx.r3.u32, 13, ctx, base);  // vtable slot 13 (byte +52)
	}
loc_82483C84:
	return;
}

__attribute__((alias("__imp__phInst_53"))) PPC_WEAK_FUNC(phInst_53);
PPC_FUNC_IMPL(__imp__phInst_53) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r30,0
	var_r30 = 0;
	// mr r11,r30
	ctx.r11.u64 = var_r30;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// cmplwi cr6,r3,0
	// beq cr6,0x82483d50
	if (ctx.r3.u32 != 0) {
		// lwz r10,1480(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 1480);
		// cmpwi cr6,r10,0
		// bne cr6,0x82483ce4
		if (ctx.r10.s32 == 0) {
			// cmpwi cr6,r4,3
			// beq cr6,0x82483cdc
			if (ctx.r4.s32 != 3) {
				// bl 0x82460bb0
				phInst_0BB0_h(ctx, base);
				// mr r11,r3
				ctx.r11.u64 = ctx.r3.u64;
				// stw r30,1472(r31)
				PPC_STORE_U32(var_r31 + 1472, var_r30);
			}
		loc_82483CDC:
			// mr r3,r11
			ctx.r3.u64 = ctx.r11.u64;
			// b 0x82483d50
		} else {
		loc_82483CE4:
			// cmpwi cr6,r4,2
			// beq cr6,0x82483cfc
			if (ctx.r4.s32 != 2) {
				// cmpwi cr6,r4,3
				// beq cr6,0x82483d68
				if (ctx.r4.s32 == 3) goto loc_82483D68;
				// cmpwi cr6,r4,4
				// beq cr6,0x82483d44
				if (ctx.r4.s32 == 4) goto loc_82483D44;
			}
		loc_82483CFC:
			// bl 0x82460cb0
			phInst_0CB0_p28(ctx, base);
			// addi r4,r1,80
			ctx.r4.s64 = ctx.r1.s64 + 80;
			// lwz r3,56(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
			// bl 0x82460aa0
			phInst_0AA0_v12(ctx, base);
			// lbz r9,80(r1)
			ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
			// clrlwi r8,r9,31
			ctx.r8.u64 = ctx.r9.u32 & 0x1;
			// cmplwi cr6,r8,0
			// beq cr6,0x82483d40
		while (ctx.r6.u32 != 0) {
			loc_82483D1C:
				// li r3,10
				ctx.r3.s64 = 10;
				// bl 0x82566c80
				pg_6C80_g(ctx, base);
				// addi r4,r1,80
				ctx.r4.s64 = ctx.r1.s64 + 80;
				// lwz r3,56(r31)
				ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
				// bl 0x82460aa0
				phInst_0AA0_v12(ctx, base);
				// lbz r7,80(r1)
				ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
				// clrlwi r6,r7,31
				ctx.r6.u64 = ctx.r7.u32 & 0x1;
				// cmplwi cr6,r6,0
				// bne cr6,0x82483d1c
		}
		loc_82483D40:
			// lwz r3,56(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
		loc_82483D44:
			// bl 0x82460bb0
			phInst_0BB0_h(ctx, base);
			// stw r30,1472(r31)
			PPC_STORE_U32(var_r31 + 1472, var_r30);
		loc_82483D4C:
			// stw r30,1480(r31)
			PPC_STORE_U32(var_r31 + 1480, var_r30);
		}
	}
loc_82483D50:
	// blr
	return;
loc_82483D68:
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x82460c58
	phInst_0C58(ctx, base);
	// b 0x82483d4c
	goto loc_82483D4C;
}

__attribute__((alias("__imp__phInst_42"))) PPC_WEAK_FUNC(phInst_42);
PPC_FUNC_IMPL(__imp__phInst_42) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=112, manual
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// std r10,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// bl 0x82483860
	game_3860(ctx, base);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_32"))) PPC_WEAK_FUNC(phInst_32);
PPC_FUNC_IMPL(__imp__phInst_32) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32255
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-31864
	ctx.r11.s64 = ctx.r11.s64 + -31864;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x824839a8
	phInst_43(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x824885c8
	util_85C8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x82483e04
	if (ctx.r11.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32808
		ctx.r4.u64 = ctx.r4.u64 | 32808;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_82483E04:
	// blr
	return;
}

__attribute__((alias("__imp__game_3E20_h"))) PPC_WEAK_FUNC(game_3E20_h);
PPC_FUNC_IMPL(__imp__game_3E20_h) {
	PPC_FUNC_PROLOGUE();
	// lis r4,8332
	ctx.r4.s64 = 546045952;
	// ori r4,r4,32810
	ctx.r4.u64 = ctx.r4.u64 | 32810;
	// b 0x820c01b8
	rage_01B8(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_vfn_22"))) PPC_WEAK_FUNC(phInst_vfn_22);
PPC_FUNC_IMPL(__imp__phInst_vfn_22) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82482908
	game_2908(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r10,r31,288
	ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 288;
	// li r9,9
	ctx.r9.s64 = 9;
	// stw r11,368(r31)
	PPC_STORE_U32(var_r31 + 368, ctx.r11.u32);
	// stw r11,324(r31)
	PPC_STORE_U32(var_r31 + 324, ctx.r11.u32);
	// stw r11,328(r31)
	PPC_STORE_U32(var_r31 + 328, ctx.r11.u32);
	// stw r11,372(r31)
	PPC_STORE_U32(var_r31 + 372, ctx.r11.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_82483E68:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82483e68
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_82483E68;
	// stw r11,332(r31)
	PPC_STORE_U32(var_r31 + 332, ctx.r11.u32);
	// stw r11,336(r31)
	PPC_STORE_U32(var_r31 + 336, ctx.r11.u32);
	// stw r11,340(r31)
	PPC_STORE_U32(var_r31 + 340, ctx.r11.u32);
	// stw r11,344(r31)
	PPC_STORE_U32(var_r31 + 344, ctx.r11.u32);
	// stw r11,348(r31)
	PPC_STORE_U32(var_r31 + 348, ctx.r11.u32);
	// stw r11,352(r31)
	PPC_STORE_U32(var_r31 + 352, ctx.r11.u32);
	// stw r11,356(r31)
	PPC_STORE_U32(var_r31 + 356, ctx.r11.u32);
	// stw r11,360(r31)
	PPC_STORE_U32(var_r31 + 360, ctx.r11.u32);
	// stw r11,364(r31)
	PPC_STORE_U32(var_r31 + 364, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phInst_vfn_21"))) PPC_WEAK_FUNC(phInst_vfn_21);
PPC_FUNC_IMPL(__imp__phInst_vfn_21) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r11,368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 368);
	// cmplwi cr6,r11,0
	// beq cr6,0x82483ee4
while (ctx.r10.u32 != 0) {
	loc_82483ED0:
		// li r3,50
		ctx.r3.s64 = 50;
		// bl 0x82566c80
		pg_6C80_g(ctx, base);
		// lwz r10,368(r31)
		ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 368);
		// cmplwi cr6,r10,0
		// bne cr6,0x82483ed0
}
loc_82483EE4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__CCalMoviePlayer_3F00_h"))) PPC_WEAK_FUNC(CCalMoviePlayer_3F00_h);
PPC_FUNC_IMPL(__imp__CCalMoviePlayer_3F00_h) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,124(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	// lwz r11,128(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,80(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 80);
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r4,r7,31,3,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x1FFFFFFF;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInst_vfn_25"))) PPC_WEAK_FUNC(phInst_vfn_25);
PPC_FUNC_IMPL(__imp__phInst_vfn_25) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r30,124(r31)
	var_r30 = (uint32_t)(PPC_LOAD_U32(var_r31 + 124));
	// cmplwi cr6,r30,1440
	// bne cr6,0x82483f60
	if (var_r30 == 1440) {
		// lwz r11,128(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 128);
		// cmplwi cr6,r11,1080
		// bne cr6,0x82483f60
		if (ctx.r11.u32 != 1080) goto loc_82483F60;
		// li r10,1920
		ctx.r10.s64 = 1920;
		// stw r10,124(r31)
		PPC_STORE_U32(var_r31 + 124, ctx.r10.u32);
	}
loc_82483F60:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x824830a8
	rage_30A8(ctx, base);
	// lwz r9,124(r31)
	ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 124);
	// cmplw cr6,r30,r9
	// beq cr6,0x82483f78
	if (var_r30 != ctx.r9.u32) {
		// stw r30,124(r31)
		PPC_STORE_U32(var_r31 + 124, var_r30);
	}
loc_82483F78:
	// blr
	return;
}

__attribute__((alias("__imp__game_3F90_h"))) PPC_WEAK_FUNC(game_3F90_h);
PPC_FUNC_IMPL(__imp__game_3F90_h) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82482b58
	game_2B58_h(ctx, base);
	// lis r11,-32255
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,-30552
	ctx.r11.s64 = ctx.r11.s64 + -30552;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82483e30
	phInst_vfn_22(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// blr
	return;
}

__attribute__((alias("__imp__phInst_vfn_23"))) PPC_WEAK_FUNC(phInst_vfn_23);
PPC_FUNC_IMPL(__imp__phInst_vfn_23) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// addi r11,r3,368
	ctx.r11.s64 = ctx.r3.s64 + 368;
loc_82483FDC:
	// mfmsr r9
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.r9.u64 = PPC_CHECK_GLOBAL_LOCK();
	// mtmsrd r13,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_ENTER_GLOBAL_LOCK();
	// lwarx r10,0,r11
	ea = ctx.r11.u32;
	ctx.reserved.u32 = *(uint32_t*)PPC_RAW_ADDR(ea);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r11
	ea = ctx.r11.u32;
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(PPC_RAW_ADDR(ea)), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_LEAVE_GLOBAL_LOCK();
	// bne 0x82483fdc
	if (!ctx.cr0.eq) goto loc_82483FDC;
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// cmplwi cr6,r11,0
	// beqlr cr6
	if (ctx.r11.u32 == 0) return;
	// lwz r3,284(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 284);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctr
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__phInst_vfn_11"))) PPC_WEAK_FUNC(phInst_vfn_11);
PPC_FUNC_IMPL(__imp__phInst_vfn_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=128, savegprlr_27
	// mr r27,r3
	var_r27 = ctx.r3.u32;
	// li r29,3
	var_r29 = 3;
	// addi r31,r27,288
	var_r31 = (uint32_t)(var_r27 + 288);
	// li r28,0
	var_r28 = 0;
loc_82484034:
	// li r30,3
	var_r30 = 3;
loc_82484038:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// cmplwi cr6,r3,0
	// beq cr6,0x8248404c
	if (ctx.r3.u32 != 0) {
		// bl 0x82352ee0
		rage_2EE0(ctx, base);
		// stw r28,0(r31)
		PPC_STORE_U32(var_r31 + 0, var_r28);
	}
loc_8248404C:
	// addi r30,r30,-1
	var_r30 = (uint32_t)(var_r30 + -1);
	// addi r31,r31,4
	var_r31 = (uint32_t)(var_r31 + 4);
	// cmplwi cr6,r30,0
	// bne cr6,0x82484038
	if (var_r30 != 0) goto loc_82484038;
	// addi r29,r29,-1
	var_r29 = (uint32_t)(var_r29 + -1);
	// cmplwi cr6,r29,0
	// bne cr6,0x82484034
	if (var_r29 != 0) goto loc_82484034;
	// lwz r3,324(r27)
	ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 324);
	// cmplwi cr6,r3,0
	// beq cr6,0x8248407c
	if (ctx.r3.u32 != 0) {
		// bl 0x82356cf0
		rage_6CF0(ctx, base);
		// stw r28,324(r27)
		PPC_STORE_U32(var_r27 + 324, var_r28);
	}
loc_8248407C:
	// lwz r3,328(r27)
	ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 328);
	// cmplwi cr6,r3,0
	// beq cr6,0x82484090
	if (ctx.r3.u32 != 0) {
		// bl 0x82356cf0
		rage_6CF0(ctx, base);
		// stw r28,328(r27)
		PPC_STORE_U32(var_r27 + 328, var_r28);
	}
loc_82484090:
	// mr r3,r27
	ctx.r3.u64 = var_r27;
	// bl 0x82482b90
	rage_2B90(ctx, base);
	return;
}

__attribute__((alias("__imp__phInst_vfn_18"))) PPC_WEAK_FUNC(phInst_vfn_18);
PPC_FUNC_IMPL(__imp__phInst_vfn_18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	// FRAME: size=160, savegprlr_23
	// mr r31,r4
	var_r31 = ctx.r4.u32;
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// li r25,0
	var_r25 = 0;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// mr r30,r25
	var_r30 = (uint32_t)(var_r25);
	// bl 0x824834d0
	phInst_34D0_g(ctx, base);
	// lwz r11,124(r29)
	ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 124);
	// cmplw cr6,r11,r3
	// bne cr6,0x824840e4
	if (ctx.r11.u32 == ctx.r3.u32) {
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// bl 0x820f0730
		grmShaderFx_vfn_16(ctx, base);
		// lwz r10,128(r29)
		ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 128);
		// cmplw cr6,r10,r3
		// beq cr6,0x82484258
		if (ctx.r10.u32 == ctx.r3.u32) goto loc_82484258;
	}
loc_824840E4:
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x824834d0
	phInst_34D0_g(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r9,124(r29)
	PPC_STORE_U32(var_r29 + 124, ctx.r9.u32);
	// bl 0x820f0730
	grmShaderFx_vfn_16(ctx, base);
	// lwz r10,124(r29)
	ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 124);
	// rlwinm r9,r3,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r24,r29,332
	var_r24 = (uint32_t)(var_r29 + 332);
	// stw r3,128(r29)
	PPC_STORE_U32(var_r29 + 128, ctx.r3.u32);
	// rlwinm r11,r10,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r3,356(r29)
	PPC_STORE_U32(var_r29 + 356, ctx.r3.u32);
	// lis r8,10240
	ctx.r8.s64 = 671088640;
	// addi r26,r29,288
	var_r26 = (uint32_t)(var_r29 + 288);
	// li r23,3
	var_r23 = 3;
	// stw r10,344(r29)
	PPC_STORE_U32(var_r29 + 344, ctx.r10.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(var_r24 + 0, ctx.r10.u32);
	// ori r27,r8,2
	var_r27 = (uint32_t)(ctx.r8.u64 | 2);
	// stw r11,336(r29)
	PPC_STORE_U32(var_r29 + 336, ctx.r11.u32);
	// stw r11,340(r29)
	PPC_STORE_U32(var_r29 + 340, ctx.r11.u32);
	// stw r11,348(r29)
	PPC_STORE_U32(var_r29 + 348, ctx.r11.u32);
	// stw r11,352(r29)
	PPC_STORE_U32(var_r29 + 352, ctx.r11.u32);
	// stw r9,360(r29)
	PPC_STORE_U32(var_r29 + 360, ctx.r9.u32);
	// stw r9,364(r29)
	PPC_STORE_U32(var_r29 + 364, ctx.r9.u32);
loc_82484144:
	// mr r28,r25
	var_r28 = (uint32_t)(var_r25);
	// mr r30,r24
	var_r30 = (uint32_t)(var_r24);
	// mr r31,r26
	var_r31 = (uint32_t)(var_r26);
loc_82484150:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 0);
	// cmplwi cr6,r3,0
	// beq cr6,0x82484164
	if (ctx.r3.u32 != 0) {
		// bl 0x82352ee0
		rage_2EE0(ctx, base);
		// stw r25,0(r31)
		PPC_STORE_U32(var_r31 + 0, var_r25);
	}
loc_82484164:
	// li r10,3
	ctx.r10.s64 = 3;
	// lwz r4,24(r30)
	ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 24);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 0);
	// mr r8,r27
	ctx.r8.u64 = var_r27;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,1
	ctx.r5.s64 = 1;
	// bl 0x823593c0
	rage_93C0(ctx, base);
	// cmplwi cr6,r3,0
	// stw r3,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r3.u32);
	// beq cr6,0x824841b0
	if (ctx.r3.u32 != 0) {
		// addi r28,r28,1
		var_r28 = (uint32_t)(var_r28 + 1);
		// mr r3,r25
		ctx.r3.u64 = var_r25;
		// addi r31,r31,4
		var_r31 = (uint32_t)(var_r31 + 4);
		// addi r30,r30,4
		var_r30 = (uint32_t)(var_r30 + 4);
		// cmplwi cr6,r28,3
		// blt cr6,0x82484150
		if (var_r28 < 3) goto loc_82484150;
		// b 0x824841b8
	} else {
	loc_824841B0:
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
	}
loc_824841B8:
	// addi r23,r23,-1
	var_r23 = (uint32_t)(var_r23 + -1);
	// addi r26,r26,12
	var_r26 = (uint32_t)(var_r26 + 12);
	// cmplwi cr6,r23,0
	// bne cr6,0x82484144
	if (var_r23 != 0) goto loc_82484144;
	// cmpwi cr6,r3,0
	// blt cr6,0x8248425c
	if (ctx.r3.s32 >= 0) {
		// lwz r7,0(r29)
  // [ph4a] vtable load collapsed
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r6,100(r7)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 25, ctx, base);  // pattern-B slot 25 (byte +100)
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// cmpwi cr6,r30,0
		// blt cr6,0x82484258
		if ((int32_t)var_r30 >= 0) {
			// lwz r5,112(r29)
			ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 112);
			// cmpwi cr6,r5,0
			// bne cr6,0x8248420c
			if (ctx.r5.s32 == 0) {
				// addi r4,r29,80
				ctx.r4.s64 = (int64_t)(int32_t)var_r29 + 80;
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// bl 0x82482ef0
				rage_2EF0(ctx, base);
				// mr r30,r3
				var_r30 = ctx.r3.u32;
			}
		loc_8248420C:
			// cmpwi cr6,r30,0
			// blt cr6,0x82484258
			if ((int32_t)var_r30 < 0) goto loc_82484258;
			// lwz r4,128(r29)
			ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 128);
			// lwz r31,76(r29)
			var_r31 = (uint32_t)(PPC_LOAD_U32(var_r29 + 76));
			// cmplwi cr6,r4,576
			// ble cr6,0x82484230
			if (ctx.r4.u32 > 576) {
				// lwz r3,328(r29)
				ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 328);
				// stw r3,76(r29)
				PPC_STORE_U32(var_r29 + 76, ctx.r3.u32);
				// b 0x82484238
			} else {
			loc_82484230:
				// lwz r11,324(r29)
				ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 324);
				// stw r11,76(r29)
				PPC_STORE_U32(var_r29 + 76, ctx.r11.u32);
			}
		loc_82484238:
			// lwz r3,76(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 76);
			// bl 0x821add88
			CPeakMeterEffect_vfn_0(ctx, base);
			// cmplwi cr6,r31,0
			// beq cr6,0x82484250
			if (var_r31 != 0) {
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// bl 0x82356cf0
				rage_6CF0(ctx, base);
			}
		loc_82484250:
			// li r10,1
			ctx.r10.s64 = 1;
			// stw r10,120(r29)
			PPC_STORE_U32(var_r29 + 120, ctx.r10.u32);
		}
	loc_82484258:
		// mr r3,r30
		ctx.r3.u64 = var_r30;
	}
loc_8248425C:
	return;
}

__attribute__((alias("__imp__phInst_vfn_20"))) PPC_WEAK_FUNC(phInst_vfn_20);
PPC_FUNC_IMPL(__imp__phInst_vfn_20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r26 = 0;
	uint32_t ea{};
	// FRAME: size=192, savegprlr_25
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lis r10,256
	ctx.r10.s64 = 16777216;
	// addi r11,r31,368
	ctx.r11.s64 = (int64_t)(int32_t)var_r31 + 368;
	// lis r9,512
	ctx.r9.s64 = 33554432;
	// lis r8,1024
	ctx.r8.s64 = 67108864;
	// lis r7,2048
	ctx.r7.s64 = 134217728;
	// lis r6,4096
	ctx.r6.s64 = 268435456;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// lis r5,8192
	ctx.r5.s64 = 536870912;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// cmplwi cr6,r4,3
	// stw r7,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r7.u32);
	// stw r6,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r6.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// blt cr6,0x824842c8
	if (ctx.r4.u32 >= 3) {
		// lis r3,-32768
		// ori r3,r3,10
		ctx.r3.u64 = ctx.r3.u64 | 10;
		return;
	}
loc_824842C8:
	// mfmsr r10
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.r10.u64 = PPC_CHECK_GLOBAL_LOCK();
	// mtmsrd r13,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_ENTER_GLOBAL_LOCK();
	// lwarx r3,0,r11
	ea = ctx.r11.u32;
	ctx.reserved.u32 = *(uint32_t*)PPC_RAW_ADDR(ea);
	ctx.r3.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// stwcx. r3,0,r11
	ea = ctx.r11.u32;
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(PPC_RAW_ADDR(ea)), ctx.reserved.s32, __builtin_bswap32(ctx.r3.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	std::atomic_thread_fence(std::memory_order_seq_cst);
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	PPC_LEAVE_GLOBAL_LOCK();
	// bne 0x824842c8
	if (!ctx.cr0.eq) goto loc_824842C8;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 44);
	// lwz r8,80(r9)
	// bctrl
	VCALL(ctx.r3.u32, 20, ctx, base);  // vtable slot 20 (byte +80)
	// mr r25,r3
	var_r25 = ctx.r3.u32;
	// li r27,0
	var_r27 = 0;
	// addi r28,r31,356
	var_r28 = (uint32_t)(var_r31 + 356);
loc_82484304:
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 372);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r3,r11,r27
	ctx.r3.u64 = ctx.r11.u64 + var_r27;
	// rlwinm r11,r3,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + var_r31);
	// bl 0x823592f8
	phInst_92F8_p42(ctx, base);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(var_r28 + 0);
	// li r30,0
	var_r30 = 0;
	// lwz r29,84(r1)
	var_r29 = (uint32_t)(PPC_LOAD_U32(ctx.r1.u32 + 84));
	// cmplwi cr6,r10,0
	// ble cr6,0x8248437c
	if (ctx.r10.u32 > 0) {
		// addi r26,r28,-12
		var_r26 = (uint32_t)(var_r28 + -12);
	loc_8248434C:
		// mr r4,r25
		ctx.r4.u64 = var_r25;
		// lwz r5,-12(r26)
		ctx.r5.u64 = PPC_LOAD_U32(var_r26 + -12);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x82434100
		memcpy(ctx, base);
		// lwz r11,0(r26)
		ctx.r11.u64 = PPC_LOAD_U32(var_r26 + 0);
		// lwz r10,80(r1)
		ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
		// addi r30,r30,1
		var_r30 = (uint32_t)(var_r30 + 1);
		// lwz r9,0(r28)
		ctx.r9.u64 = PPC_LOAD_U32(var_r28 + 0);
		// add r25,r25,r11
		var_r25 = (uint32_t)(var_r25 + ctx.r11.u64);
		// add r29,r10,r29
		var_r29 = (uint32_t)(ctx.r10.u64 + var_r29);
		// cmplw cr6,r30,r9
		// blt cr6,0x8248434c
		if (var_r30 < ctx.r9.u32) goto loc_8248434C;
	}
loc_8248437C:
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 372);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r8,r11,r27
	ctx.r8.u64 = ctx.r11.u64 + var_r27;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r7,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + var_r31);
	// bl 0x82359310
	phInst_9310_p42(ctx, base);
	// addi r27,r27,1
	var_r27 = (uint32_t)(var_r27 + 1);
	// addi r28,r28,4
	var_r28 = (uint32_t)(var_r28 + 4);
	// cmplwi cr6,r27,3
	// blt cr6,0x82484304
	if (var_r27 < 3) goto loc_82484304;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 264);
	// cmplwi cr6,r11,0
	// beq cr6,0x824843c8
	if (ctx.r11.u32 != 0) {
		// lwz r3,276(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 276);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	}
loc_824843C8:
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 372);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r11,r10
	ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r5,r5,r31
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + var_r31);
	// bl 0x82359940
	rage_9940(ctx, base);
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 372);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r31
	ctx.r10.u64 = ctx.r11.u64 + var_r31;
	// lwz r5,292(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 292);
	// bl 0x82359940
	rage_9940(ctx, base);
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 372);
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r31
	ctx.r8.u64 = ctx.r11.u64 + var_r31;
	// lwz r5,296(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 296);
	// bl 0x82359940
	rage_9940(ctx, base);
	// lwz r4,60(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 60);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// bl 0x82356210
	rage_6210_1(ctx, base);
	// lwz r4,68(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 68);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// bl 0x82357400
	rage_7400(ctx, base);
	// lwz r4,72(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 72);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// bl 0x82357398
	rage_7398(ctx, base);
	// lwz r4,76(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 76);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// bl 0x82356f50
	phInst_6F50_w(ctx, base);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r7,484(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,488(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 488);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,504(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 504);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,500(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 500);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r9,484(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r8,488(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 488);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r7,504(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 504);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r6,500(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 500);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r11,484(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 484);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r10,488(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 488);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r10.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r9,504(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 504);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r9.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r8,500(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 500);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r8.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r7,136(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r7.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,152(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r6.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,404(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 404);
	// bctrl
	PPC_CALL_INDIRECT_FUNC(ctx.r5.u32);
	// li r7,20
	ctx.r7.s64 = 20;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r5,64(r31)
	ctx.r5.u64 = PPC_LOAD_U32(var_r31 + 64);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// bl 0x82356118
	rage_6118(ctx, base);
	// li r6,6
	ctx.r6.s64 = 6;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// li r4,4
	ctx.r4.s64 = 4;
	// bl 0x823578f0
	rage_78F0(ctx, base);
	// bl 0x8242b958
	game_B958_h(ctx, base);
	// rlwinm r4,r3,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 56);
	// lis r11,-32184
	// mr r6,r31
	ctx.r6.u64 = var_r31;
	// addi r5,r11,11080
	ctx.r5.s64 = ctx.r11.s64 + 11080;
	// lwzx r4,r4,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// bl 0x8235aeb0
	phInst_AEB0_v12(ctx, base);
	// lwz r11,268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 268);
	// cmplwi cr6,r11,0
	// beq cr6,0x82484628
	if (ctx.r11.u32 != 0) {
		// lwz r3,280(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 280);
		// bctrl
		PPC_CALL_INDIRECT_FUNC(ctx.r11.u32);
	}
loc_82484628:
	// lwz r11,372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 372);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmplwi cr6,r11,3
	// stw r11,372(r31)
	PPC_STORE_U32(var_r31 + 372, ctx.r11.u32);
	// blt cr6,0x82484644
	if (ctx.r11.u32 >= 3) {
		// li r9,0
		ctx.r9.s64 = 0;
		// stw r9,372(r31)
		PPC_STORE_U32(var_r31 + 372, ctx.r9.u32);
	}
loc_82484644:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
}

__attribute__((alias("__imp__phInst_vfn_24"))) PPC_WEAK_FUNC(phInst_vfn_24);
PPC_FUNC_IMPL(__imp__phInst_vfn_24) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32255
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r30,r11,-31768
	var_r30 = (uint32_t)(ctx.r11.s64 + -31768);  // lbl_820083E8 @ 0x820083e8
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// bl 0x82357168
	rage_7168(ctx, base);
	// cmplwi cr6,r3,0
	// stw r3,72(r31)
	PPC_STORE_U32(var_r31 + 72, ctx.r3.u32);
	// beq cr6,0x824846b4
	if (ctx.r3.u32 != 0) {
		// addi r3,r30,272
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 272;
		// bl 0x82356e60
		msgMsgSink_6E60_w(ctx, base);
		// cmplwi cr6,r3,0
		// stw r3,324(r31)
		PPC_STORE_U32(var_r31 + 324, ctx.r3.u32);
		// beq cr6,0x824846b4
		if (ctx.r3.u32 != 0) {
			// addi r3,r30,744
			ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 744;
			// bl 0x82356e60
			msgMsgSink_6E60_w(ctx, base);
			// cmplwi cr6,r3,0
			// stw r3,328(r31)
			PPC_STORE_U32(var_r31 + 328, ctx.r3.u32);
		} else {
			if (ctx.r3.u32 != 0) {
				// li r3,0
				ctx.r3.s64 = 0;
				// b 0x824846bc
				} else {
			}
		}
	loc_824846B4:
		// lis r3,-32761
		// ori r3,r3,14
		ctx.r3.u64 = ctx.r3.u64 | 14;
	}
loc_824846BC:
	// blr
	return;
}

__attribute__((alias("__imp__phInst_vfn_0_1"))) PPC_WEAK_FUNC(phInst_vfn_0_1);
PPC_FUNC_IMPL(__imp__phInst_vfn_0_1) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32255
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-30552
	ctx.r11.s64 = ctx.r11.s64 + -30552;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82484018
	phInst_vfn_11(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x82482eb8
	rage_2EB8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x8248472c
	if (ctx.r11.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32810
		ctx.r4.u64 = ctx.r4.u64 | 32810;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_8248472C:
	// blr
	return;
}

__attribute__((alias("__imp__game_4748_h"))) PPC_WEAK_FUNC(game_4748_h);
PPC_FUNC_IMPL(__imp__game_4748_h) {
	PPC_FUNC_PROLOGUE();
	// lis r4,8332
	ctx.r4.s64 = 546045952;
	// ori r4,r4,32797
	ctx.r4.u64 = ctx.r4.u64 | 32797;
	// b 0x820c01b8
	rage_01B8(ctx, base);
	return;
}

__attribute__((alias("__imp__phObject_32"))) PPC_WEAK_FUNC(phObject_32);
PPC_FUNC_IMPL(__imp__phObject_32) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x824888f8
	phInstStatic_88F8_fw(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,500(r31)
	PPC_STORE_U32(var_r31 + 500, ctx.r11.u32);
	// stw r11,508(r31)
	PPC_STORE_U32(var_r31 + 508, ctx.r11.u32);
	// stw r10,504(r31)
	PPC_STORE_U32(var_r31 + 504, ctx.r10.u32);
	// std r11,512(r31)
	PPC_STORE_U64(var_r31 + 512, ctx.r11.u64);
	// stw r11,520(r31)
	PPC_STORE_U32(var_r31 + 520, ctx.r11.u32);
	// stw r11,524(r31)
	PPC_STORE_U32(var_r31 + 524, ctx.r11.u32);
	// stw r10,528(r31)
	PPC_STORE_U32(var_r31 + 528, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phObject_11"))) PPC_WEAK_FUNC(phObject_11);
PPC_FUNC_IMPL(__imp__phObject_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r29,r4
	var_r29 = ctx.r4.u32;
	// lwz r10,60(r11)
	// bctrl
	VCALL(ctx.r3.u32, 15, ctx, base);  // vtable slot 15 (byte +60)
	// mr r30,r3
	var_r30 = ctx.r3.u32;
	// cmpwi cr6,r30,0
	// blt cr6,0x82484870
	if ((int32_t)var_r30 >= 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// lis r3,1
		ctx.r3.s64 = 65536;
		// ori r4,r4,32797
		ctx.r4.u64 = ctx.r4.u64 | 32797;
		// bl 0x820c01b8
		rage_01B8(ctx, base);
		// cmplwi cr6,r3,0
		// stw r3,508(r31)
		PPC_STORE_U32(var_r31 + 508, ctx.r3.u32);
		// bne cr6,0x82484800
		if (ctx.r3.u32 == 0) {
			// lis r30,-32761
			var_r30 = (uint32_t)(-2147024896);
			// ori r30,r30,14
			var_r30 = (uint32_t)(var_r30 | 14);
			// b 0x82484870
		} else {
		loc_82484800:
			// lwz r9,8(r29)
			ctx.r9.u64 = PPC_LOAD_U32(var_r29 + 8);
			// stw r9,112(r31)
			PPC_STORE_U32(var_r31 + 112, ctx.r9.u32);
			// lwz r8,12(r29)
			ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 12);
			// stw r8,116(r31)
			PPC_STORE_U32(var_r31 + 116, ctx.r8.u32);
			// lwz r7,16(r29)
			ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 16);
			// stw r7,120(r31)
			PPC_STORE_U32(var_r31 + 120, ctx.r7.u32);
			// lwz r4,0(r29)
			ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 0);
			// cmplwi cr6,r4,0
			// beq cr6,0x8248483c
			if (ctx.r4.u32 != 0) {
				// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r5,68(r6)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 17, ctx, base);  // pattern-B slot 17 (byte +68)
				// mr r30,r3
				var_r30 = ctx.r3.u32;
			}
		loc_8248483C:
			// cmpwi cr6,r30,0
			// blt cr6,0x82484870
			if ((int32_t)var_r30 < 0) goto loc_82484870;
			// lwz r4,4(r29)
			ctx.r4.u64 = PPC_LOAD_U32(var_r29 + 4);
			// cmplwi cr6,r4,0
			// beq cr6,0x82484868
			if (ctx.r4.u32 != 0) {
				// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r10,72(r11)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 18, ctx, base);  // pattern-B slot 18 (byte +72)
				// mr r30,r3
				var_r30 = ctx.r3.u32;
			}
		loc_82484868:
			// cmpwi cr6,r30,0
			// bge cr6,0x82484884
			if ((int32_t)var_r30 >= 0) {
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				return;
			}
		}
	}
loc_82484870:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,60(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
loc_82484884:
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	return;
}

__attribute__((alias("__imp__phObject_15"))) PPC_WEAK_FUNC(phObject_15);
PPC_FUNC_IMPL(__imp__phObject_15) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,508(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 508);
	// cmplwi cr6,r3,0
	// beq cr6,0x824848c4
	if (ctx.r3.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32797
		ctx.r4.u64 = ctx.r4.u64 | 32797;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,508(r31)
		PPC_STORE_U32(var_r31 + 508, ctx.r11.u32);
	}
loc_824848C4:
	// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r9,76(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 19, ctx, base);  // pattern-B slot 19 (byte +76)
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r7,80(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 20, ctx, base);  // pattern-B slot 20 (byte +80)
	// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r5,128(r6)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 32, ctx, base);  // pattern-B slot 32 (byte +128)
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phObject_18"))) PPC_WEAK_FUNC(phObject_18);
PPC_FUNC_IMPL(__imp__phObject_18) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	// FRAME: size=112, manual
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r3,r30
	ctx.r3.u64 = var_r30;
	// lwz r10,4(r11)
	// bctrl
	VCALL(ctx.r3.u32, 1, ctx, base);  // vtable slot 1 (byte +4)
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,80(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 20, ctx, base);  // pattern-B slot 20 (byte +80)
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,48(r31)
	PPC_STORE_U32(var_r31 + 48, var_r30);
	// blr
	return;
}

__attribute__((alias("__imp__phObject_19"))) PPC_WEAK_FUNC(phObject_19);
PPC_FUNC_IMPL(__imp__phObject_19) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r30 = 0;
	uint32_t var_r31 = 0;
	uint32_t ea{};
	SEH_TRY {
	// mflr r12
		ctx.r12.u64 = ctx.lr;
		// stw r12,-8(r1)
		PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
		// std r30,-24(r1)
		PPC_STORE_U64(ctx.r1.u32 + -24, var_r30);
		// std r31,-16(r1)
		PPC_STORE_U64(ctx.r1.u32 + -16, var_r31);
		// addi r31,r1,-112
		var_r31 = (uint32_t)(ctx.r1.s64 + -112);
		// stwu r1,-112(r1)
		ea = -112 + ctx.r1.u32;
		PPC_STORE_U32(ea, ctx.r1.u32);
		ctx.r1.u32 = ea;
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// stw r30,132(r31)
		PPC_STORE_U32(var_r31 + 132, var_r30);
		// addi r3,r30,500
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 500;
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// cmplwi cr6,r11,0
		// beq cr6,0x824849dc
		if (ctx.r11.u32 != 0) {
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// bl 0x82489a48
			phObject_9A48_h(ctx, base);
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// b 0x824849d4
			goto loc_824849D4;
	loc_824849D4:
			// li r10,0
			ctx.r10.s64 = 0;
			// stw r10,500(r30)
			PPC_STORE_U32(var_r30 + 500, ctx.r10.u32);
		}
loc_824849DC:
		// lwz r3,52(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 52);
		// cmplwi cr6,r3,0
		// beq cr6,0x82484a00
		if (ctx.r3.u32 != 0) {
			// lwz r8,8(r9)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			// li r7,0
			ctx.r7.s64 = 0;
			// stw r7,52(r30)
			PPC_STORE_U32(var_r30 + 52, ctx.r7.u32);
		}
loc_82484A00:
		// li r3,0
		ctx.r3.s64 = 0;
		// addi r1,r31,112
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 112;
		// lwz r12,-8(r1)
		ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
		// mtlr r12
		ctx.lr = ctx.r12.u64;
		// ld r30,-24(r1)
		var_r30 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -24));
		// ld r31,-16(r1)
		var_r31 = (uint32_t)(PPC_LOAD_U64(ctx.r1.u32 + -16));
		// blr
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in phObject_19");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 112;  // Establisher frame pointer
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_AD1C_2"))) PPC_WEAK_FUNC(except_AD1C_2);
PPC_FUNC_IMPL(__imp__except_AD1C_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phObject_30"))) PPC_WEAK_FUNC(phObject_30);
PPC_FUNC_IMPL(__imp__phObject_30) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r24 = 0;
	SEH_TRY {
		// FRAME: size=208, savegprlr_22
		// addi r31,r1,-208
		var_r31 = (uint32_t)(ctx.r1.s64 + -208);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// mr r26,r4
		var_r26 = ctx.r4.u32;
		// mr r27,r5
		var_r27 = ctx.r5.u32;
		// li r23,0
		var_r23 = 0;
		// mr r28,r23
		var_r28 = (uint32_t)(var_r23);
		// stw r23,0(r27)
		PPC_STORE_U32(var_r27 + 0, var_r23);
		// lwz r3,48(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 48);
		// lwz r10,80(r11)
		// bctrl
		VCALL(ctx.r3.u32, 20, ctx, base);  // vtable slot 20 (byte +80)
		// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
		// mr r22,r3
		var_r22 = ctx.r3.u32;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r8,116(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 29, ctx, base);  // pattern-B slot 29 (byte +116)
		// add r25,r3,r22
		var_r25 = (uint32_t)(ctx.r3.u64 + var_r22);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// addi r29,r30,524
		var_r29 = (uint32_t)(var_r30 + 524);
		// lwz r5,0(r29)
		ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 0);
		// cmplwi cr6,r5,0
		// beq cr6,0x82484b20
		if (ctx.r5.u32 != 0) {
			// stw r23,80(r31)
			PPC_STORE_U32(var_r31 + 80, var_r23);
			// cmplw cr6,r26,r5
			// bge cr6,0x82484ab0
			if (var_r26 < ctx.r5.u32) {
				// mr r5,r26
				ctx.r5.u64 = var_r26;
			}
	loc_82484AB0:
			// lwz r11,0(r27)
			ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
			// li r9,2
			ctx.r9.s64 = 2;
			// addi r8,r31,88
			ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 88;
			// lwz r3,500(r30)
			ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 500);
			// addi r7,r31,104
			ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 104;
			// addi r6,r31,80
			ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 80;
			// add r4,r11,r25
			ctx.r4.u64 = ctx.r11.u64 + var_r25;
			// bl 0x8248b620
			phObject_B620(ctx, base);
			// mr r28,r3
			var_r28 = ctx.r3.u32;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// stw r28,84(r31)
			PPC_STORE_U32(var_r31 + 84, var_r28);
			// lwz r7,0(r29)
			ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 0);
			// lwz r11,80(r31)
			ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 80);
			// subf r6,r11,r7
			ctx.r6.s64 = ctx.r7.s64 - ctx.r11.s64;
			// stw r6,0(r29)
			PPC_STORE_U32(var_r29 + 0, ctx.r6.u32);
			// subf r26,r11,r26
			var_r26 = (uint32_t)((int64_t)(int32_t)var_r26 - ctx.r11.s64);
			// stw r26,236(r31)
			PPC_STORE_U32(var_r31 + 236, var_r26);
			// lwz r10,0(r27)
			ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 0);
			// add r5,r10,r11
			ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
			// stw r5,0(r27)
			PPC_STORE_U32(var_r27 + 0, ctx.r5.u32);
			// lwz r11,116(r4)
			// bctrl
			VCALL(ctx.r3.u32, 29, ctx, base);  // vtable slot 29 (byte +116)
			// cmplwi cr6,r3,8
			// blt cr6,0x82484b20
			if (ctx.r3.u32 < 8) goto loc_82484B20;
			// ld r10,104(r31)
			ctx.r10.u64 = PPC_LOAD_U64(var_r31 + 104);
			// std r10,0(r22)
			PPC_STORE_U64(var_r22 + 0, ctx.r10.u64);
		}
loc_82484B20:
		// li r24,1
		var_r24 = 1;
loc_82484B24:
		// cmpwi cr6,r28,0
		// bne cr6,0x82484c18
		if ((int32_t)var_r28 != 0) goto loc_82484C18;
		// cmplwi cr6,r26,0
		// beq cr6,0x82484c18
		if (var_r26 == 0) goto loc_82484C18;
		// lwz r9,528(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 528);
		// cmpwi cr6,r9,0
		// bne cr6,0x82484c18
		if (ctx.r9.s32 != 0) goto loc_82484C18;
loc_82484B40:
		// lwz r8,0(r29)
		ctx.r8.u64 = PPC_LOAD_U32(var_r29 + 0);
		// cmplwi cr6,r8,0
		// bne cr6,0x82484b84
		if (ctx.r8.u32 != 0) goto loc_82484B84;
		// li r6,-1
		// mr r5,r29
		ctx.r5.u64 = var_r29;
		// lwz r3,500(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 500);
		// addi r4,r31,96
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 96;
		// bl 0x8248b730
		phObject_B730_h(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// stw r28,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r28);
		// cmpwi cr6,r28,0
		// bne cr6,0x82484b78
		if ((int32_t)var_r28 != 0) goto loc_82484B78;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82484b40
		goto loc_82484B40;
loc_82484B78:
		// cmpwi cr6,r28,33
		// bne cr6,0x82484b84
		if ((int32_t)var_r28 == 33) {
			// stw r24,528(r30)
			PPC_STORE_U32(var_r30 + 528, var_r24);
		}
loc_82484B84:
		// lwz r5,0(r29)
		ctx.r5.u64 = PPC_LOAD_U32(var_r29 + 0);
		// cmplwi cr6,r5,0
		// beq cr6,0x82484c18
		if (ctx.r5.u32 == 0) goto loc_82484C18;
		// stw r23,80(r31)
		PPC_STORE_U32(var_r31 + 80, var_r23);
		// cmplw cr6,r26,r5
		// bge cr6,0x82484ba0
		if (var_r26 < ctx.r5.u32) {
			// mr r5,r26
			ctx.r5.u64 = var_r26;
		}
loc_82484BA0:
		// lwz r11,0(r27)
		ctx.r11.u64 = PPC_LOAD_U32(var_r27 + 0);
		// li r9,2
		ctx.r9.s64 = 2;
		// addi r8,r31,88
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 88;
		// lwz r3,500(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 500);
		// addi r7,r31,104
		ctx.r7.s64 = (int64_t)(int32_t)var_r31 + 104;
		// addi r6,r31,80
		ctx.r6.s64 = (int64_t)(int32_t)var_r31 + 80;
		// add r4,r11,r25
		ctx.r4.u64 = ctx.r11.u64 + var_r25;
		// bl 0x8248b620
		phObject_B620(ctx, base);
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// stw r28,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r28);
		// lwz r7,0(r29)
		ctx.r7.u64 = PPC_LOAD_U32(var_r29 + 0);
		// lwz r11,80(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 80);
		// subf r6,r11,r7
		ctx.r6.s64 = ctx.r7.s64 - ctx.r11.s64;
		// stw r6,0(r29)
		PPC_STORE_U32(var_r29 + 0, ctx.r6.u32);
		// subf r26,r11,r26
		var_r26 = (uint32_t)((int64_t)(int32_t)var_r26 - ctx.r11.s64);
		// stw r26,236(r31)
		PPC_STORE_U32(var_r31 + 236, var_r26);
		// lwz r10,0(r27)
		ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 0);
		// add r5,r10,r11
		ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
		// stw r5,0(r27)
		PPC_STORE_U32(var_r27 + 0, ctx.r5.u32);
		// lwz r11,116(r4)
		// bctrl
		VCALL(ctx.r3.u32, 29, ctx, base);  // vtable slot 29 (byte +116)
		// cmplwi cr6,r3,8
		// blt cr6,0x82484c10
		if (ctx.r3.u32 >= 8) {
			// ld r10,104(r31)
			ctx.r10.u64 = PPC_LOAD_U64(var_r31 + 104);
			// std r10,0(r22)
			PPC_STORE_U64(var_r22 + 0, ctx.r10.u64);
		}
loc_82484C10:
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82484b24
		goto loc_82484B24;
loc_82484C18:
		// cmpwi cr6,r28,33
		// bne cr6,0x82484c28
		if ((int32_t)var_r28 == 33) {
			// mr r28,r23
			var_r28 = (uint32_t)(var_r23);
			// stw r28,84(r31)
			PPC_STORE_U32(var_r31 + 84, var_r28);
		}
loc_82484C28:
		// cmpwi cr6,r28,0
		// bne cr6,0x82484c38
		if ((int32_t)var_r28 == 0) {
			// mr r11,r23
			ctx.r11.u64 = var_r23;
			// b 0x82484c40
		} else {
	loc_82484C38:
			// lis r11,-32768
			// ori r11,r11,65535
			ctx.r11.u64 = ctx.r11.u64 | 65535;
		}
loc_82484C40:
		// mr r29,r11
		var_r29 = ctx.r11.u32;
		// stw r29,92(r31)
		PPC_STORE_U32(var_r31 + 92, var_r29);
		// cmpwi cr6,r29,0
		// blt cr6,0x82484c94
		if ((int32_t)var_r29 >= 0) {
			// lwz r9,0(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 0);
			// cmplwi cr6,r9,0
			// beq cr6,0x82484c94
			if (ctx.r9.u32 == 0) goto loc_82484C94;
			// lwz r8,0(r30)
  // [ph4a] vtable load collapsed
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r7,116(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 29, ctx, base);  // pattern-B slot 29 (byte +116)
			// lwz r11,48(r30)
			ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 48);
			// mr r10,r3
			ctx.r10.u64 = ctx.r3.u64;
			// lwz r9,0(r27)
			ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 0);
			// lwz r6,0(r11)
  // [ph4a] vtable load collapsed
			// mr r3,r11
			ctx.r3.u64 = ctx.r11.u64;
			// add r4,r9,r10
			ctx.r4.u64 = ctx.r9.u64 + ctx.r10.u64;
			// lwz r5,72(r6)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(ctx.r11.u32, 18, ctx, base);  // pattern-B slot 18 (byte +72)
		}
loc_82484C94:
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82484cac
		goto loc_82484CAC;
loc_82484CAC:
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// addi r1,r31,208
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 208;
		// b 0x8242f8d0
		__restgprlr_22(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in phObject_30");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 208;  // Establisher frame pointer
				__restgprlr_22(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_AD34_2"))) PPC_WEAK_FUNC(except_AD34_2);
PPC_FUNC_IMPL(__imp__except_AD34_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__rage_4CD0"))) PPC_WEAK_FUNC(rage_4CD0);
PPC_FUNC_IMPL(__imp__rage_4CD0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	double var_f31 = 0.0;
	PPCRegister temp{};
	SEH_TRY {
		// FRAME: size=2016, savegprlr_25
		// addi r31,r1,-2016
		var_r31 = (uint32_t)(ctx.r1.s64 + -2016);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// addi r26,r30,124
		var_r26 = (uint32_t)(var_r30 + 124);
		// mr r3,r26
		ctx.r3.u64 = var_r26;
		// bl 0x82483358
		game_3358(ctx, base);
		// lwz r3,52(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 52);
		// li r25,1
		var_r25 = 1;
		// fmr f31,f1
		ctx.fpscr.disableFlushMode();
		var_f31 = ctx.f1.f64;
		// li r27,0
		var_r27 = 0;
		// stw r25,504(r30)
		PPC_STORE_U32(var_r30 + 504, var_r25);
		// li r4,1
		ctx.r4.s64 = 1;
		// std r27,512(r30)
		PPC_STORE_U64(var_r30 + 512, var_r27);
		// stw r27,520(r30)
		PPC_STORE_U32(var_r30 + 520, var_r27);
		// stw r27,524(r30)
		PPC_STORE_U32(var_r30 + 524, var_r27);
		// stw r27,528(r30)
		PPC_STORE_U32(var_r30 + 528, var_r27);
		// lwz r10,56(r11)
		// bctrl
		VCALL(ctx.r3.u32, 14, ctx, base);  // vtable slot 14 (byte +56)
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// cmpwi cr6,r29,0
		// blt cr6,0x82484dec
		if ((int32_t)var_r29 >= 0) {
			// li r7,0
			ctx.r7.s64 = 0;
			// li r6,-1
			// li r5,1
			ctx.r5.s64 = 1;
			// li r4,9
			ctx.r4.s64 = 9;
			// addi r3,r30,500
			ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 500;
			// bl 0x8248adf8
			rage_ADF8_1(ctx, base);
			// cmpwi cr6,r3,0
			// bne cr6,0x82484d68
			if (ctx.r3.s32 == 0) {
				// mr r29,r27
				var_r29 = (uint32_t)(var_r27);
				// b 0x82484d70
			} else {
		loc_82484D68:
				// lis r29,-32768
				var_r29 = (uint32_t)(-2147483648);
				// ori r29,r29,65535
				var_r29 = (uint32_t)(var_r29 | 65535);
			}
	loc_82484D70:
			// stw r29,80(r31)
			PPC_STORE_U32(var_r31 + 80, var_r29);
			// cmpwi cr6,r29,0
			// blt cr6,0x82484dec
			if (!((int32_t)var_r29 < 0)) {
				// addi r4,r31,96
				ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 96;
				// lwz r3,500(r30)
				ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 500);
				// bl 0x824899b0
				phObject_99B0_p33(ctx, base);
				// cmpwi cr6,r3,0
				// bne cr6,0x82484d98
				if (ctx.r3.s32 == 0) {
				// mr r29,r27
				var_r29 = (uint32_t)(var_r27);
				// b 0x82484da0
				} else {
				loc_82484D98:
				// lis r29,-32768
				var_r29 = (uint32_t)(-2147483648);
				// ori r29,r29,65535
				var_r29 = (uint32_t)(var_r29 | 65535);
				}
				loc_82484DA0:
				// stw r29,80(r31)
				PPC_STORE_U32(var_r31 + 80, var_r29);
			} else {
				if (!((int32_t)var_r29 < 0)) {
					// addi r5,r31,128
					ctx.r5.s64 = (int64_t)(int32_t)var_r31 + 128;
					// li r4,2
					ctx.r4.s64 = 2;
					// lwz r3,500(r30)
					ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 500);
					// bl 0x8248b040
					phObject_B040_w(ctx, base);
					// cmpwi cr6,r3,0
					// bne cr6,0x82484dcc
					if (ctx.r3.s32 == 0) {
					// mr r29,r27
					var_r29 = (uint32_t)(var_r27);
					// b 0x82484dd4
					} else {
					loc_82484DCC:
					// lis r29,-32768
					var_r29 = (uint32_t)(-2147483648);
					// ori r29,r29,65535
					var_r29 = (uint32_t)(var_r29 | 65535);
					}
					loc_82484DD4:
					// stw r29,80(r31)
					PPC_STORE_U32(var_r31 + 80, var_r29);
				} else {
					if (!((int32_t)var_r29 < 0)) {
						// li r28,2
						var_r28 = 2;
						// stw r28,496(r30)
						PPC_STORE_U32(var_r30 + 496, var_r28);
						// b 0x82484df0
						} else {
					}
				}
			}
	loc_82484DEC:
			// li r28,2
			var_r28 = 2;
		}
loc_82484DF0:
		// cmpwi cr6,r29,0
		// blt cr6,0x82484ef0
		if ((int32_t)var_r29 >= 0) {
			// li r5,1016
			ctx.r5.s64 = 1016;
			// li r4,0
			ctx.r4.s64 = 0;
			// addi r3,r31,928
			ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 928;
			// bl 0x8242fed0
			memset(ctx, base);
			// sth r25,928(r31)
			PPC_STORE_U16(var_r31 + 928, (uint16_t)var_r25);
			// stw r27,932(r31)
			PPC_STORE_U32(var_r31 + 932, var_r27);
			// addi r4,r31,928
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 928;
			// addi r3,r30,500
			ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 500;
			// sth r28,936(r31)
			PPC_STORE_U16(var_r31 + 936, (uint16_t)var_r28);
			// stw r28,940(r31)
			PPC_STORE_U32(var_r31 + 940, var_r28);
			// bl 0x8248b220
			rage_B220(ctx, base);
			// cmpwi cr6,r3,0
			// bne cr6,0x82484e34
			if (ctx.r3.s32 == 0) {
				// mr r29,r27
				var_r29 = (uint32_t)(var_r27);
				// b 0x82484e3c
			} else {
		loc_82484E34:
				// lis r29,-32768
				var_r29 = (uint32_t)(-2147483648);
				// ori r29,r29,65535
				var_r29 = (uint32_t)(var_r29 | 65535);
			}
	loc_82484E3C:
			// stw r29,80(r31)
			PPC_STORE_U32(var_r31 + 80, var_r29);
			// cmpwi cr6,r29,0
			// blt cr6,0x82484ef0
			if ((int32_t)var_r29 < 0) goto loc_82484EF0;
			// lwz r9,0(r26)
  // [ph4a] vtable load collapsed
			// li r4,2
			ctx.r4.s64 = 2;
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// lwz r8,36(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r26, 9, ctx, base);  // pattern-B slot 9 (byte +36)
			// lwz r7,0(r26)
  // [ph4a] vtable load collapsed
			// li r4,8
			ctx.r4.s64 = 8;
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// lwz r6,40(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r26, 10, ctx, base);  // pattern-B slot 10 (byte +40)
			// lwz r5,0(r26)
  // [ph4a] vtable load collapsed
			// li r4,0
			ctx.r4.s64 = 0;
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// lwz r11,44(r5)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r26, 11, ctx, base);  // pattern-B slot 11 (byte +44)
			// lwz r4,140(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 140);
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// bl 0x82483498
			set_3498(ctx, base);
			// lwz r4,144(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 144);
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// bl 0x824834a0
			atSingleton_vfn_25_34A0_1(ctx, base);
			// lwz r4,96(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 96);
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// bl 0x824834a8
			set_34A8(ctx, base);
			// lwz r10,144(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 144);
			// lwz r9,140(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 140);
			// mullw r8,r10,r9
			ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
			// rlwinm r4,r8,5,0,26
			ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// bl 0x824834b0
			set_34B0(ctx, base);
			// lfs f0,136(r31)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(var_r31 + 136);
			ctx.f0.f64 = double(temp.f32);
			// lis r11,-32256
			ctx.r11.s64 = -2113929216;
			// lfs f13,15784(r11)
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
			ctx.f13.f64 = double(temp.f32);
			// fcmpu cr6,f0,f13
			// beq cr6,0x82484ee4
			if (ctx.f0.f64 != ctx.f13.f64) {
				// fmr f31,f0
				var_f31 = ctx.f0.f64;
			}
	loc_82484EE4:
			// mr r3,r26
			ctx.r3.u64 = var_r26;
			// fmr f1,f31
			ctx.fpscr.disableFlushMode();
			ctx.f1.f64 = var_f31;
			// bl 0x82483348
			set_3348(ctx, base);
		}
loc_82484EF0:
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82484f08
		goto loc_82484F08;
loc_82484F08:
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// addi r1,r31,2016
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 2016;
		// lfd f31,-72(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
		// b 0x8242f8dc
		__restgprlr_25(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in rage_4CD0");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 2016;  // Establisher frame pointer
				__restgprlr_25(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_AD4C_2"))) PPC_WEAK_FUNC(except_AD4C_2);
PPC_FUNC_IMPL(__imp__except_AD4C_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phObject_4F28_p46"))) PPC_WEAK_FUNC(phObject_4F28_p46);
PPC_FUNC_IMPL(__imp__phObject_4F28_p46) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r27 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=144, savegprlr_27
	// mr r27,r6
	var_r27 = ctx.r6.u32;
	// li r28,0
	var_r28 = 0;
	// mr r29,r5
	var_r29 = ctx.r5.u32;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// cmplw cr6,r29,r11
	// stw r28,0(r27)
	PPC_STORE_U32(var_r27 + 0, var_r28);
	// bgt cr6,0x82484fdc
	if (var_r29 <= ctx.r11.u32) {
		// ld r11,512(r31)
		ctx.r11.u64 = PPC_LOAD_U64(var_r31 + 512);
		// cmpld cr6,r30,r11
		// blt cr6,0x82484f7c
		if (var_r30 >= ctx.r11.u64) {
			// clrldi r10,r29,32
			ctx.r10.u64 = var_r29 & 0xFFFFFFFF;
			// add r9,r10,r30
			ctx.r9.u64 = ctx.r10.u64 + var_r30;
			// lwz r10,520(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 520);
			// add r8,r10,r11
			ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
			// cmpld cr6,r9,r8
			// ble cr6,0x824850b8
			if (ctx.r9.u64 <= ctx.r8.u64) goto loc_824850B8;
		}
	loc_82484F7C:
		// lwz r7,112(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 112);
		// cmpwi cr6,r7,0
		// beq cr6,0x82484f9c
		if (ctx.r7.s32 != 0) {
			// lwz r3,52(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
			// lwz r5,12(r6)
			// bctrl
			VCALL(ctx.r3.u32, 3, ctx, base);  // vtable slot 3 (byte +12)
		}
	loc_82484F9C:
		// lwz r3,52(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r10,56(r11)
		// bctrl
		VCALL(ctx.r3.u32, 14, ctx, base);  // vtable slot 14 (byte +56)
		// cmpwi cr6,r3,0
		// bge cr6,0x82484fe8
		if (ctx.r3.s32 >= 0) goto loc_82484FE8;
		// lwz r9,112(r31)
		ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 112);
		// cmpwi cr6,r9,0
		// beq cr6,0x82484fdc
		if (ctx.r9.s32 == 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			return;
		}
		// lwz r3,52(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
		// lwz r7,20(r8)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	}
loc_82484FDC:
	// li r3,0
	ctx.r3.s64 = 0;
	return;
loc_82484FE8:
	// lwz r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
	// mr r4,r30
	ctx.r4.u64 = var_r30;
	// lwz r5,52(r6)
	// bctrl
	VCALL(ctx.r3.u32, 13, ctx, base);  // vtable slot 13 (byte +52)
	// cmpwi cr6,r3,0
	// bge cr6,0x82485034
	if (ctx.r3.s32 < 0) {
		// lwz r4,112(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 112);
		// cmpwi cr6,r4,0
		// beq cr6,0x82485028
		if (ctx.r4.s32 != 0) {
			// lwz r3,52(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
			// lwz r10,20(r11)
			// bctrl
			VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		}
	loc_82485028:
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_82485034:
	// lwz r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, var_r28);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// lwz r4,508(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 508);
	// lwz r8,60(r9)
	// bctrl
	VCALL(ctx.r3.u32, 15, ctx, base);  // vtable slot 15 (byte +60)
	// cmpwi cr6,r3,0
	// bge cr6,0x8248508c
	if (ctx.r3.s32 < 0) {
		// lwz r7,112(r31)
		ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 112);
		// cmpwi cr6,r7,0
		// beq cr6,0x82485080
		if (ctx.r7.s32 != 0) {
			// lwz r3,52(r31)
			ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
			// lwz r5,20(r6)
			// bctrl
			VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
		}
	loc_82485080:
		// li r3,0
		ctx.r3.s64 = 0;
		return;
	}
loc_8248508C:
	// lwz r4,112(r31)
	ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 112);
	// cmpwi cr6,r4,0
	// beq cr6,0x824850ac
	if (ctx.r4.s32 != 0) {
		// lwz r3,52(r31)
		ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 52);
		// lwz r10,20(r11)
		// bctrl
		VCALL(ctx.r3.u32, 5, ctx, base);  // vtable slot 5 (byte +20)
	}
loc_824850AC:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// std r30,512(r31)
	PPC_STORE_U64(var_r31 + 512, var_r30);
	// stw r9,520(r31)
	PPC_STORE_U32(var_r31 + 520, ctx.r9.u32);
loc_824850B8:
	// lwz r8,520(r31)
	ctx.r8.u64 = PPC_LOAD_U32(var_r31 + 520);
	// clrldi r10,r29,32
	ctx.r10.u64 = var_r29 & 0xFFFFFFFF;
	// ld r11,512(r31)
	ctx.r11.u64 = PPC_LOAD_U64(var_r31 + 512);
	// subf r9,r30,r8
	ctx.r9.s64 = ctx.r8.s64 - (int64_t)(int32_t)var_r30;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// cmpld cr6,r10,r9
	// blt cr6,0x824850d8
	if (ctx.r10.u64 >= ctx.r9.u64) {
		// mr r10,r9
		ctx.r10.u64 = ctx.r9.u64;
	}
loc_824850D8:
	// lwz r7,508(r31)
	ctx.r7.u64 = PPC_LOAD_U32(var_r31 + 508);
	// rotlwi r6,r11,0
	ctx.r6.u64 = ctx.r11.u32;
	// rotlwi r8,r30,0
	ctx.r8.u64 = var_r30;
	// subf r11,r6,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r6.s64;
	// rotlwi r3,r10,0
	ctx.r3.u64 = ctx.r10.u32;
	// add r5,r11,r8
	ctx.r5.u64 = ctx.r11.u64 + ctx.r8.u64;
	// stw r5,0(r27)
	PPC_STORE_U32(var_r27 + 0, ctx.r5.u32);
	return;
}

__attribute__((alias("__imp__phObject_5100_p46"))) PPC_WEAK_FUNC(phObject_5100_p46);
PPC_FUNC_IMPL(__imp__phObject_5100_p46) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// b 0x82484f28
	phObject_4F28_p46(ctx, base);
	return;
}

__attribute__((alias("__imp__phObject_29"))) PPC_WEAK_FUNC(phObject_29);
PPC_FUNC_IMPL(__imp__phObject_29) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,120(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	// blr
	return;
}

__attribute__((alias("__imp__phObject_17"))) PPC_WEAK_FUNC(phObject_17);
PPC_FUNC_IMPL(__imp__phObject_17) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	SEH_TRY {
		// FRAME: size=128, savegprlr_29
		// addi r31,r1,-128
		var_r31 = (uint32_t)(ctx.r1.s64 + -128);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// stw r30,148(r31)
		PPC_STORE_U32(var_r31 + 148, var_r30);
		// mr r29,r4
		var_r29 = ctx.r4.u32;
		// lwz r11,0(r29)
  // [ph4a] vtable load collapsed
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// lwz r10,4(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r29, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r8,76(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 19, ctx, base);  // pattern-B slot 19 (byte +76)
		// stw r29,52(r30)
		PPC_STORE_U32(var_r30 + 52, var_r29);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x8248ad78
		phObject_AD78_h(ctx, base);
		// cmplwi cr6,r3,0
		// stw r3,500(r30)
		PPC_STORE_U32(var_r30 + 500, ctx.r3.u32);
		// beq cr6,0x82485184
		if (ctx.r3.u32 != 0) {
			// li r3,0
			ctx.r3.s64 = 0;
			// b 0x8248518c
		} else {
	loc_82485184:
			// lis r3,-32768
			// ori r3,r3,65535
			ctx.r3.u64 = ctx.r3.u64 | 65535;
		}
loc_8248518C:
		// stw r3,80(r31)
		PPC_STORE_U32(var_r31 + 80, ctx.r3.u32);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x824851ac
		goto loc_824851AC;
loc_824851AC:
		// cmpwi cr6,r3,0
		// blt cr6,0x824851bc
		if (ctx.r3.s32 >= 0) {
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x82484cd0
			rage_4CD0(ctx, base);
		}
loc_824851BC:
		// addi r1,r31,128
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 128;
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in phObject_17");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 128;  // Establisher frame pointer
				__restgprlr_29(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_AD64_2"))) PPC_WEAK_FUNC(except_AD64_2);
PPC_FUNC_IMPL(__imp__except_AD64_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phInstStatic_51D0_w"))) PPC_WEAK_FUNC(phInstStatic_51D0_w);
PPC_FUNC_IMPL(__imp__phInstStatic_51D0_w) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x82488580
	util_8580(ctx, base);
	// addi r3,r31,56
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 56;
	// bl 0x82488840
	phInstStatic_8840(ctx, base);
	// addi r3,r31,124
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 124;
	// bl 0x82483360
	phInstStatic_3360_w(ctx, base);
	// addi r3,r31,200
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 200;
	// bl 0x8248b8d8
	phInstStatic_B8D8_h(ctx, base);
	// lis r11,-32255
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// addi r11,r11,-30440
	ctx.r11.s64 = ctx.r11.s64 + -30440;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x824888f8
	phInstStatic_88F8_fw(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// stw r11,500(r31)
	PPC_STORE_U32(var_r31 + 500, ctx.r11.u32);
	// stw r11,508(r31)
	PPC_STORE_U32(var_r31 + 508, ctx.r11.u32);
	// stw r10,504(r31)
	PPC_STORE_U32(var_r31 + 504, ctx.r10.u32);
	// std r11,512(r31)
	PPC_STORE_U64(var_r31 + 512, ctx.r11.u64);
	// stw r11,520(r31)
	PPC_STORE_U32(var_r31 + 520, ctx.r11.u32);
	// stw r11,524(r31)
	PPC_STORE_U32(var_r31 + 524, ctx.r11.u32);
	// stw r10,528(r31)
	PPC_STORE_U32(var_r31 + 528, ctx.r10.u32);
	// blr
	return;
}

__attribute__((alias("__imp__phObject_0"))) PPC_WEAK_FUNC(phObject_0);
PPC_FUNC_IMPL(__imp__phObject_0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	// FRAME: size=112, manual
	// lis r11,-32255
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// addi r11,r11,-30440
	ctx.r11.s64 = ctx.r11.s64 + -30440;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// stw r11,0(r31)
	PPC_STORE_U32(var_r31 + 0, ctx.r11.u32);
	// bl 0x82484890
	phObject_15(ctx, base);
	// addi r3,r31,200
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 200;
	// bl 0x8248b878
	phObject_B878_h(ctx, base);
	// addi r3,r31,124
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 124;
	// bl 0x824832e8
	ph_ctor_32E8(ctx, base);
	// addi r3,r31,56
	ctx.r3.s64 = (int64_t)(int32_t)var_r31 + 56;
	// bl 0x824887f8
	phInst_87F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// bl 0x824885c8
	util_85C8(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = var_r30 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// cmplwi cr6,r11,0
	// beq cr6,0x824852bc
	if (ctx.r11.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32797
		ctx.r4.u64 = ctx.r4.u64 | 32797;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// mr r3,r31
		ctx.r3.u64 = var_r31;
	}
loc_824852BC:
	// blr
	return;
}

__attribute__((alias("__imp__game_52D8_h"))) PPC_WEAK_FUNC(game_52D8_h);
PPC_FUNC_IMPL(__imp__game_52D8_h) {
	PPC_FUNC_PROLOGUE();
	// lis r4,8332
	ctx.r4.s64 = 546045952;
	// ori r4,r4,32791
	ctx.r4.u64 = ctx.r4.u64 | 32791;
	// b 0x820c01b8
	rage_01B8(ctx, base);
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_32"))) PPC_WEAK_FUNC(crAnimDofQuaternion_32);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_32) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// bl 0x8248ba30
	phInstStatic_BA30_fw(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,668(r31)
	PPC_STORE_U32(var_r31 + 668, ctx.r11.u32);
	// stw r11,672(r31)
	PPC_STORE_U32(var_r31 + 672, ctx.r11.u32);
	// stw r11,660(r31)
	PPC_STORE_U32(var_r31 + 660, ctx.r11.u32);
	// stw r11,664(r31)
	PPC_STORE_U32(var_r31 + 664, ctx.r11.u32);
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_31"))) PPC_WEAK_FUNC(crAnimDofQuaternion_31);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_31) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	SEH_TRY {
		// FRAME: size=304, savegprlr_18
		// addi r31,r1,-304
		var_r31 = (uint32_t)(ctx.r1.s64 + -304);
		// mr r26,r3
		var_r26 = ctx.r3.u32;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// addi r30,r26,208
		var_r30 = (uint32_t)(var_r26 + 208);
		// lwz r11,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r10,36(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 9, ctx, base);  // pattern-B slot 9 (byte +36)
		// addi r4,r31,84
		ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 84;
		// li r18,0
		var_r18 = 0;
		// stw r18,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r18);
		// lwz r3,668(r26)
		ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 668);
		// bl 0x8248cd70
		crAnimDofQuaternion_CD70_h(ctx, base);
		// cmpwi cr6,r3,0
		// bne cr6,0x82485388
		if (ctx.r3.s32 == 0) {
			// mr r11,r18
			ctx.r11.u64 = var_r18;
			// b 0x82485390
		} else {
	loc_82485388:
			// lis r11,-32768
			// ori r11,r11,16389
			ctx.r11.u64 = ctx.r11.u64 | 16389;
		}
loc_82485390:
		// mr r19,r11
		var_r19 = ctx.r11.u32;
		// stw r19,80(r31)
		PPC_STORE_U32(var_r31 + 80, var_r19);
		// cmpwi cr6,r19,0
		// lwz r11,84(r31)
		ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 84);
		// blt cr6,0x82485408
		if ((int32_t)var_r19 >= 0) {
			// cmplwi cr6,r11,0
			// bne cr6,0x824853bc
			if (ctx.r11.u32 == 0) {
				// lis r19,-32768
				var_r19 = (uint32_t)(-2147483648);
				// ori r19,r19,16387
				var_r19 = (uint32_t)(var_r19 | 16387);
				// stw r19,80(r31)
				PPC_STORE_U32(var_r31 + 80, var_r19);
				// b 0x82485408
			} else {
		loc_824853BC:
				// cmpwi cr6,r19,0
				// blt cr6,0x82485408
				if ((int32_t)var_r19 < 0) goto loc_82485408;
				// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
				// lhz r8,0(r11)
				ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// lwz r4,12(r11)
				ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
				// rlwinm r5,r8,31,1,31
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
				// lwz r7,40(r9)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r30, 10, ctx, base);  // pattern-B slot 10 (byte +40)
				// lwz r6,0(r30)
  // [ph4a] vtable load collapsed
				// lwz r11,84(r31)
				ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 84);
				// lhz r5,2(r11)
				ctx.r5.u64 = PPC_LOAD_U16(ctx.r11.u32 + 2);
				// mr r3,r30
				ctx.r3.u64 = var_r30;
				// lwz r4,16(r11)
				ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
				// rlwinm r5,r5,31,1,31
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 31) & 0x7FFFFFFF;
				// lwz r11,56(r6)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r30, 14, ctx, base);  // pattern-B slot 14 (byte +56)
			}
		}
loc_82485408:
		// stw r18,88(r31)
		PPC_STORE_U32(var_r31 + 88, var_r18);
		// cmpwi cr6,r19,0
		// blt cr6,0x82485464
		if ((int32_t)var_r19 >= 0) {
			// addi r4,r31,88
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 88;
			// lwz r3,668(r26)
			ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 668);
			// bl 0x8248cd98
			crAnimDofQuaternion_CD98_h(ctx, base);
			// cmpwi cr6,r3,0
			// bne cr6,0x82485430
			if (ctx.r3.s32 == 0) {
				// mr r11,r18
				ctx.r11.u64 = var_r18;
				// b 0x82485438
			} else {
		loc_82485430:
				// lis r11,-32768
				// ori r11,r11,16389
				ctx.r11.u64 = ctx.r11.u64 | 16389;
			}
	loc_82485438:
			// mr r19,r11
			var_r19 = ctx.r11.u32;
			// stw r19,80(r31)
			PPC_STORE_U32(var_r31 + 80, var_r19);
			// cmpwi cr6,r19,0
			// blt cr6,0x82485464
			if ((int32_t)var_r19 < 0) goto loc_82485464;
			// lwz r9,88(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 88);
			// cmplwi cr6,r9,0
			// bne cr6,0x82485468
			if (ctx.r9.u32 != 0) goto loc_82485468;
			// lis r19,-32768
			var_r19 = (uint32_t)(-2147483648);
			// ori r19,r19,16387
			var_r19 = (uint32_t)(var_r19 | 16387);
			// stw r19,80(r31)
			PPC_STORE_U32(var_r31 + 80, var_r19);
			// b 0x824856f0
		} else {
	loc_82485464:
			// lwz r9,88(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 88);
	loc_82485468:
			// cmpwi cr6,r19,0
			// blt cr6,0x824856f0
			if ((int32_t)var_r19 < 0) goto loc_824856F0;
			// mr r10,r18
			ctx.r10.u64 = var_r18;
			// lis r11,-32255
			ctx.r11.s64 = -2113863680;
			// addi r23,r11,-30164
			var_r23 = (uint32_t)(ctx.r11.s64 + -30164);  // lbl_82008A2C @ 0x82008a2c
			// lis r11,-32255
			ctx.r11.s64 = -2113863680;
			// addi r25,r11,-30196
			var_r25 = (uint32_t)(ctx.r11.s64 + -30196);  // lbl_82008A0C @ 0x82008a0c
			// lis r11,-32255
			ctx.r11.s64 = -2113863680;
			// addi r22,r11,-30220
			var_r22 = (uint32_t)(ctx.r11.s64 + -30220);  // lbl_820089F4 @ 0x820089f4
			// lis r11,-32255
			ctx.r11.s64 = -2113863680;
			// addi r21,r11,-30240
			var_r21 = (uint32_t)(ctx.r11.s64 + -30240);  // lbl_820089E0 @ 0x820089e0
			// lis r11,-32255
			ctx.r11.s64 = -2113863680;
			// addi r20,r11,-30268
			var_r20 = (uint32_t)(ctx.r11.s64 + -30268);  // lbl_820089C4 @ 0x820089c4
			// lis r11,-32255
			ctx.r11.s64 = -2113863680;
			// addi r24,r11,-30300
			var_r24 = (uint32_t)(ctx.r11.s64 + -30300);  // lbl_820089A4 @ 0x820089a4
	loc_824854A4:
			// clrlwi r27,r10,16
			var_r27 = (uint32_t)(ctx.r10.u32 & 0xFFFF);
			// lhz r10,0(r9)
			ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
			// cmplw cr6,r27,r10
			// bge cr6,0x8248569c
			if (var_r27 >= ctx.r10.u32) goto loc_8248569C;
			// lwz r10,4(r9)
			ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
			// rlwinm r11,r27,4,0,27
			ctx.r11.u64 = __builtin_rotateleft64(var_r27 | (var_r27 << 32), 4) & 0xFFFFFFF0;
			// add r30,r11,r10
			var_r30 = (uint32_t)(ctx.r11.u64 + ctx.r10.u64);
			// lhz r10,0(r30)
			ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 0);
			// lhz r11,8(r30)
			ctx.r11.u64 = PPC_LOAD_U16(var_r30 + 8);
			// lwz r29,4(r30)
			var_r29 = (uint32_t)(PPC_LOAD_U32(var_r30 + 4));
			// rlwinm r10,r10,31,1,31
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
			// cmplwi cr6,r11,0
			// bne cr6,0x824855f0
			if (ctx.r11.u32 == 0) {
				// clrlwi r28,r10,16
				var_r28 = (uint32_t)(ctx.r10.u32 & 0xFFFF);
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// mr r3,r24
				ctx.r3.u64 = var_r24;
				// mr r5,r28
				ctx.r5.u64 = var_r28;
				// bl 0x825693e8
				crAnimDofQuaternion_93E8_h(ctx, base);
				// cmpwi cr6,r3,0
				// bne cr6,0x82485518
				if (ctx.r3.s32 == 0) {
					// addi r3,r26,208
					ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 208;
					// lhz r9,10(r30)
					ctx.r9.u64 = PPC_LOAD_U16(var_r30 + 10);
					// lwz r4,12(r30)
					ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 12);
					// rlwinm r5,r9,31,1,31
					ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
					// lwz r7,60(r8)
					// bctrl
					VCALL(ctx.r3.u32, 15, ctx, base);  // vtable slot 15 (byte +60)
					// b 0x82485688
					goto loc_82485688;
				}
		loc_82485518:
				// mr r5,r28
				ctx.r5.u64 = var_r28;
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// mr r3,r20
				ctx.r3.u64 = var_r20;
				// bl 0x825693e8
				crAnimDofQuaternion_93E8_h(ctx, base);
				// cmpwi cr6,r3,0
				// bne cr6,0x82485554
				if (ctx.r3.s32 == 0) {
					// addi r3,r26,208
					ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 208;
					// lhz r6,10(r30)
					ctx.r6.u64 = PPC_LOAD_U16(var_r30 + 10);
					// lwz r4,12(r30)
					ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 12);
					// rlwinm r5,r6,31,1,31
					ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
					// lwz r10,64(r11)
					// bctrl
					VCALL(ctx.r3.u32, 16, ctx, base);  // vtable slot 16 (byte +64)
					// b 0x82485688
					goto loc_82485688;
				}
		loc_82485554:
				// mr r5,r28
				ctx.r5.u64 = var_r28;
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// mr r3,r21
				ctx.r3.u64 = var_r21;
				// bl 0x825693e8
				crAnimDofQuaternion_93E8_h(ctx, base);
				// cmpwi cr6,r3,0
				// bne cr6,0x82485590
				if (ctx.r3.s32 == 0) {
					// addi r3,r26,208
					ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 208;
					// lhz r9,10(r30)
					ctx.r9.u64 = PPC_LOAD_U16(var_r30 + 10);
					// lwz r4,12(r30)
					ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 12);
					// rlwinm r5,r9,31,1,31
					ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
					// lwz r7,68(r8)
					// bctrl
					VCALL(ctx.r3.u32, 17, ctx, base);  // vtable slot 17 (byte +68)
					// b 0x82485688
					goto loc_82485688;
				}
		loc_82485590:
				// mr r5,r28
				ctx.r5.u64 = var_r28;
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// mr r3,r22
				ctx.r3.u64 = var_r22;
				// bl 0x825693e8
				crAnimDofQuaternion_93E8_h(ctx, base);
				// cmpwi cr6,r3,0
				// bne cr6,0x82485688
				if (ctx.r3.s32 != 0) goto loc_82485688;
				// addi r29,r26,208
				var_r29 = (uint32_t)(var_r26 + 208);
				// lwz r6,0(r29)
  // [ph4a] vtable load collapsed
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// lwz r5,92(r6)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r29, 23, ctx, base);  // pattern-B slot 23 (byte +92)
				// bl 0x82433748
				util_3748(ctx, base);
				// cmplwi cr6,r3,0
				// bne cr6,0x82485688
				if (ctx.r3.u32 != 0) goto loc_82485688;
				// lwz r11,0(r29)
  // [ph4a] vtable load collapsed
				// lhz r10,10(r30)
				ctx.r10.u64 = PPC_LOAD_U16(var_r30 + 10);
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				// lwz r4,12(r30)
				ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 12);
				// rlwinm r5,r10,31,1,31
				ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
				// lwz r9,68(r11)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r29, 17, ctx, base);  // pattern-B slot 17 (byte +68)
				// b 0x82485688
			} else {
		loc_824855F0:
				// cmplwi cr6,r11,3
				// bne cr6,0x82485688
				if (ctx.r11.u32 != 3) goto loc_82485688;
				// clrlwi r28,r10,16
				var_r28 = (uint32_t)(ctx.r10.u32 & 0xFFFF);
				// mr r4,r29
				ctx.r4.u64 = var_r29;
				// mr r3,r25
				ctx.r3.u64 = var_r25;
				// mr r5,r28
				ctx.r5.u64 = var_r28;
				// bl 0x825693e8
				crAnimDofQuaternion_93E8_h(ctx, base);
				// cmpwi cr6,r3,0
				// bne cr6,0x82485630
				if (ctx.r3.s32 == 0) {
					// addi r3,r26,208
					ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 208;
					// lwz r8,12(r30)
					ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 12);
					// lwz r4,0(r8)
					ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
					// lwz r7,0(r3)
					ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
					// lwz r6,76(r7)
					ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 76);
					// mtctr r6
					ctx.ctr.u64 = ctx.r6.u64;
					// b 0x82485684
				} else {
			loc_82485630:
					// mr r5,r28
					ctx.r5.u64 = var_r28;
					// mr r4,r29
					ctx.r4.u64 = var_r29;
					// mr r3,r23
					ctx.r3.u64 = var_r23;
					// bl 0x825693e8
					crAnimDofQuaternion_93E8_h(ctx, base);
					// cmpwi cr6,r3,0
					// bne cr6,0x82485688
					if (ctx.r3.s32 != 0) goto loc_82485688;
					// addi r29,r26,208
					var_r29 = (uint32_t)(var_r26 + 208);
					// lwz r5,0(r29)
  // [ph4a] vtable load collapsed
					// mr r3,r29
					ctx.r3.u64 = var_r29;
					// lwz r4,100(r5)
  // [ph4a] slot load collapsed
					// bctrl
					VCALL(var_r29, 25, ctx, base);  // pattern-B slot 25 (byte +100)
					// cmplwi cr6,r3,0
					// bne cr6,0x82485688
					if (ctx.r3.u32 != 0) goto loc_82485688;
					// lwz r11,12(r30)
					ctx.r11.u64 = PPC_LOAD_U32(var_r30 + 12);
					// lwz r10,0(r29)
					ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
					// mr r3,r29
					ctx.r3.u64 = var_r29;
					// lwz r11,0(r11)
					ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
					// lwz r9,76(r10)
					ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
					// addi r4,r11,1
					ctx.r4.s64 = ctx.r11.s64 + 1;
					// mtctr r9
					ctx.ctr.u64 = ctx.r9.u64;
				}
		loc_82485684:
				// bctrl
				ctx.lr = 0x82485688;
				PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
			}
	loc_82485688:
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// addi r8,r27,1
			ctx.r8.s64 = (int64_t)(int32_t)var_r27 + 1;
			// clrlwi r10,r8,16
			ctx.r10.u64 = ctx.r8.u32 & 0xFFFF;
			// lwz r9,88(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 88);
			// b 0x824854a4
			goto loc_824854A4;
	loc_8248569C:
			// cmpwi cr6,r19,0
			// blt cr6,0x824856f0
			if ((int32_t)var_r19 < 0) goto loc_824856F0;
			// addi r4,r31,96
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 96;
			// lwz r3,668(r26)
			ctx.r3.u64 = PPC_LOAD_U32(var_r26 + 668);
			// bl 0x8248cca8
			crAnimDofQuaternion_CCA8_wrh(ctx, base);
			// cmpwi cr6,r3,0
			// bne cr6,0x824856c0
			if (ctx.r3.s32 == 0) {
				// mr r11,r18
				ctx.r11.u64 = var_r18;
				// b 0x824856c8
			} else {
		loc_824856C0:
				// lis r11,-32768
				// ori r11,r11,16389
				ctx.r11.u64 = ctx.r11.u64 | 16389;
			}
	loc_824856C8:
			// mr r19,r11
			var_r19 = ctx.r11.u32;
			// stw r19,80(r31)
			PPC_STORE_U32(var_r31 + 80, var_r19);
			// cmpwi cr6,r19,0
			// blt cr6,0x824856f0
			if ((int32_t)var_r19 < 0) goto loc_824856F0;
			// addi r3,r26,208
			ctx.r3.s64 = (int64_t)(int32_t)var_r26 + 208;
			// lwz r4,108(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 108);
			// lwz r6,72(r7)
			// bctrl
			VCALL(ctx.r3.u32, 18, ctx, base);  // vtable slot 18 (byte +72)
		}
loc_824856F0:
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82485708
		goto loc_82485708;
loc_82485708:
		// mr r3,r19
		ctx.r3.u64 = var_r19;
		// addi r1,r31,304
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 304;
		// b 0x8242f8c0
		__restgprlr_18(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in crAnimDofQuaternion_31");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 304;  // Establisher frame pointer
				__restgprlr_18(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_AD7C_2"))) PPC_WEAK_FUNC(except_AD7C_2);
PPC_FUNC_IMPL(__imp__except_AD7C_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phObject_14"))) PPC_WEAK_FUNC(phObject_14);
PPC_FUNC_IMPL(__imp__phObject_14) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=112, manual
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lwz r9,0(r3)
  // [ph4a] vtable load collapsed
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r8,44(r9)
  // [ph4a] slot load collapsed
	// std r10,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// bctrl
	VCALL(ctx.r3.u32, 11, ctx, base);  // pattern-B slot 11 (byte +44)
	// blr
	return;
}

__attribute__((alias("__imp__phObject_13"))) PPC_WEAK_FUNC(phObject_13);
PPC_FUNC_IMPL(__imp__phObject_13) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=112, manual
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lwz r8,0(r3)
  // [ph4a] vtable load collapsed
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r7,44(r8)
  // [ph4a] slot load collapsed
	// std r10,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// bctrl
	VCALL(ctx.r3.u32, 11, ctx, base);  // pattern-B slot 11 (byte +44)
	// blr
	return;
}

__attribute__((alias("__imp__phObject_12"))) PPC_WEAK_FUNC(phObject_12);
PPC_FUNC_IMPL(__imp__phObject_12) {
	PPC_FUNC_PROLOGUE();
	// FRAME: size=112, manual
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lwz r8,0(r3)
  // [ph4a] vtable load collapsed
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r7,44(r8)
  // [ph4a] slot load collapsed
	// std r10,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r10.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// bctrl
	VCALL(ctx.r3.u32, 11, ctx, base);  // pattern-B slot 11 (byte +44)
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_11"))) PPC_WEAK_FUNC(crAnimDofQuaternion_11);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_11) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	// FRAME: size=112, savegprlr_29
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// mr r30,r4
	var_r30 = ctx.r4.u32;
	// lwz r10,60(r11)
	// bctrl
	VCALL(ctx.r3.u32, 15, ctx, base);  // crAnimDofQuaternion::vfn_15 (unnamed)  // vtable slot 15 (byte +60)
	// mr r29,r3
	var_r29 = ctx.r3.u32;
	// cmpwi cr6,r29,0
	// blt cr6,0x824858d0
	if ((int32_t)var_r29 >= 0) {
		// lwz r9,8(r30)
		ctx.r9.u64 = PPC_LOAD_U32(var_r30 + 8);
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// lis r3,1
		ctx.r3.s64 = 65536;
		// ori r4,r4,32791
		ctx.r4.u64 = ctx.r4.u64 | 32791;
		// stw r9,112(r31)
		PPC_STORE_U32(var_r31 + 112, ctx.r9.u32);
		// lwz r8,12(r30)
		ctx.r8.u64 = PPC_LOAD_U32(var_r30 + 12);
		// stw r8,116(r31)
		PPC_STORE_U32(var_r31 + 116, ctx.r8.u32);
		// lwz r7,16(r30)
		ctx.r7.u64 = PPC_LOAD_U32(var_r30 + 16);
		// stw r7,120(r31)
		PPC_STORE_U32(var_r31 + 120, ctx.r7.u32);
		// bl 0x820c01b8
		rage_01B8(ctx, base);
		// cmplwi cr6,r3,0
		// stw r3,44(r31)
		PPC_STORE_U32(var_r31 + 44, ctx.r3.u32);
		// bne cr6,0x82485878
		if (ctx.r3.u32 == 0) {
			// lis r29,-32761
			var_r29 = (uint32_t)(-2147024896);
			// ori r29,r29,14
			var_r29 = (uint32_t)(var_r29 | 14);
			// b 0x824858d0
		} else {
		loc_82485878:
			// lwz r4,0(r30)
			ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 0);
			// cmplwi cr6,r4,0
			// beq cr6,0x8248589c
			if (ctx.r4.u32 != 0) {
				// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r5,68(r6)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 17, ctx, base);  // pattern-B slot 17 (byte +68)
				// mr r29,r3
				var_r29 = ctx.r3.u32;
			}
		loc_8248589C:
			// cmpwi cr6,r29,0
			// blt cr6,0x824858d0
			if ((int32_t)var_r29 < 0) goto loc_824858D0;
			// lwz r4,4(r30)
			ctx.r4.u64 = PPC_LOAD_U32(var_r30 + 4);
			// cmplwi cr6,r4,0
			// beq cr6,0x824858c8
			if (ctx.r4.u32 != 0) {
				// lwz r11,0(r31)
  // [ph4a] vtable load collapsed
				// mr r3,r31
				ctx.r3.u64 = var_r31;
				// lwz r10,72(r11)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r31, 18, ctx, base);  // pattern-B slot 18 (byte +72)
				// mr r29,r3
				var_r29 = ctx.r3.u32;
			}
		loc_824858C8:
			// cmpwi cr6,r29,0
			// bge cr6,0x824858e4
			if ((int32_t)var_r29 >= 0) {
				// mr r3,r29
				ctx.r3.u64 = var_r29;
				return;
			}
		}
	}
loc_824858D0:
	// lwz r9,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r8,60(r9)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 15, ctx, base);  // pattern-B slot 15 (byte +60)
loc_824858E4:
	// mr r3,r29
	ctx.r3.u64 = var_r29;
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_15"))) PPC_WEAK_FUNC(crAnimDofQuaternion_15);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_15) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 44);
	// cmplwi cr6,r3,0
	// beq cr6,0x82485924
	if (ctx.r3.u32 != 0) {
		// lis r4,8332
		ctx.r4.s64 = 546045952;
		// ori r4,r4,32791
		ctx.r4.u64 = ctx.r4.u64 | 32791;
		// bl 0x820c02d0
		_locale_register(ctx, base);
		// li r11,0
		ctx.r11.s64 = 0;
		// stw r11,44(r31)
		PPC_STORE_U32(var_r31 + 44, ctx.r11.u32);
	}
loc_82485924:
	// lwz r10,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r9,76(r10)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 19, ctx, base);  // pattern-B slot 19 (byte +76)
	// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r7,80(r8)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 20, ctx, base);  // pattern-B slot 20 (byte +80)
	// lwz r6,0(r31)
  // [ph4a] vtable load collapsed
	// mr r3,r31
	ctx.r3.u64 = var_r31;
	// lwz r5,128(r6)
  // [ph4a] slot load collapsed
	// bctrl
	VCALL(var_r31, 32, ctx, base);  // pattern-B slot 32 (byte +128)
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__phObject_16"))) PPC_WEAK_FUNC(phObject_16);
PPC_FUNC_IMPL(__imp__phObject_16) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r11,52(r31)
	ctx.r11.u64 = PPC_LOAD_U32(var_r31 + 52);
	// cmplwi cr6,r11,0
	// beq cr6,0x824859d4
	if (ctx.r11.u32 != 0) {
		// rotlwi r3,r11,0
		ctx.r3.u64 = ctx.r11.u32;
		// li r4,1
		ctx.r4.s64 = 1;
		// lwz r9,56(r10)
		// bctrl
		VCALL(ctx.r3.u32, 14, ctx, base);  // vtable slot 14 (byte +56)
		// cmpwi cr6,r3,0
		// blt cr6,0x824859d4
		if (ctx.r3.s32 < 0) {
			// blr
			return;
		}
		// lwz r8,0(r31)
  // [ph4a] vtable load collapsed
		// mr r3,r31
		ctx.r3.u64 = var_r31;
		// lwz r4,52(r31)
		ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 52);
		// lwz r7,68(r8)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r31, 17, ctx, base);  // pattern-B slot 17 (byte +68)
	}
loc_824859D4:
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_17"))) PPC_WEAK_FUNC(crAnimDofQuaternion_17);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_17) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r29 = 0;
	uint32_t var_r28 = 0;
	PPCRegister temp{};
	SEH_TRY {
		// FRAME: size=176, savegprlr_27
		// addi r31,r1,-176
		var_r31 = (uint32_t)(ctx.r1.s64 + -176);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// mr r27,r4
		var_r27 = ctx.r4.u32;
		// lwz r11,0(r27)
  // [ph4a] vtable load collapsed
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r10,4(r11)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 1, ctx, base);  // pattern-B slot 1 (byte +4)
		// lwz r9,0(r30)
  // [ph4a] vtable load collapsed
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// lwz r8,76(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r30, 19, ctx, base);  // pattern-B slot 19 (byte +76)
		// stw r27,52(r30)
		PPC_STORE_U32(var_r30 + 52, var_r27);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// addi r29,r30,668
		var_r29 = (uint32_t)(var_r30 + 668);
		// mr r3,r29
		ctx.r3.u64 = var_r29;
		// bl 0x8248c270
		crAnimDofQuaternion_C270_w(ctx, base);
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// stw r11,84(r31)
		PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
		// cmpwi cr6,r11,0
		// li r28,0
		var_r28 = 0;
		// bne cr6,0x82485abc
		if (ctx.r11.s32 == 0) {
			// lwz r10,0(r29)
			ctx.r10.u64 = PPC_LOAD_U32(var_r29 + 0);
			// stw r30,608(r10)
			PPC_STORE_U32(ctx.r10.u32 + 608, var_r30);
			// stw r28,88(r31)
			PPC_STORE_U32(var_r31 + 88, var_r28);
			// lwz r7,0(r27)
  // [ph4a] vtable load collapsed
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// lwz r6,36(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r27, 9, ctx, base);  // pattern-B slot 9 (byte +36)
			// cmpwi cr6,r3,0
			// beq cr6,0x82485aa8
			if (ctx.r3.s32 != 0) {
				// lwz r5,0(r27)
  // [ph4a] vtable load collapsed
				// addi r4,r31,88
				ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 88;
				// mr r3,r27
				ctx.r3.u64 = var_r27;
				// lwz r11,40(r5)
  // [ph4a] slot load collapsed
				// bctrl
				VCALL(var_r27, 10, ctx, base);  // pattern-B slot 10 (byte +40)
				// cmpwi cr6,r3,0
				// beq cr6,0x82485aa8
				if (ctx.r3.s32 == 0) goto loc_82485AA8;
				// stw r28,88(r31)
				PPC_STORE_U32(var_r31 + 88, var_r28);
			}
	loc_82485AA8:
			// lwz r4,88(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 88);
			// lwz r3,0(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
			// bl 0x8248c300
			crAnimDofQuaternion_C300_h(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
		}
loc_82485ABC:
		// addi r10,r31,100
		ctx.r10.s64 = (int64_t)(int32_t)var_r31 + 100;
		// stw r28,96(r31)
		PPC_STORE_U32(var_r31 + 96, var_r28);
		// cmpwi cr6,r11,0
		// stw r28,0(r10)
		PPC_STORE_U32(ctx.r10.u32 + 0, var_r28);
		// stw r28,4(r10)
		PPC_STORE_U32(ctx.r10.u32 + 4, var_r28);
		// stw r28,8(r10)
		PPC_STORE_U32(ctx.r10.u32 + 8, var_r28);
		// stw r28,12(r10)
		PPC_STORE_U32(ctx.r10.u32 + 12, var_r28);
		// stw r28,16(r10)
		PPC_STORE_U32(ctx.r10.u32 + 16, var_r28);
		// bne cr6,0x82485bac
		if (ctx.r11.s32 == 0) {
			// li r10,1
			ctx.r10.s64 = 1;
			// addi r9,r31,96
			ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 96;
			// lwz r3,0(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
			// li r8,0
			ctx.r8.s64 = 0;
			// li r7,0
			ctx.r7.s64 = 0;
			// li r6,0
			ctx.r6.s64 = 0;
			// li r5,0
			ctx.r5.s64 = 0;
			// li r4,0
			ctx.r4.s64 = 0;
			// bl 0x8248c330
			atSingleton_C330_h(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
			// cmpwi cr6,r11,0
			// bne cr6,0x82485bac
			if (ctx.r11.s32 != 0) goto loc_82485BAC;
			// lwz r10,100(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 100);
			// cmplwi cr6,r10,6
			// ble cr6,0x82485bac
			if (ctx.r10.u32 <= 6) goto loc_82485BAC;
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// bl 0x8248bfd8
			crAnimDofQuaternion_BFD8(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
			// cmpwi cr6,r11,0
			// bne cr6,0x82485bac
			if (ctx.r11.s32 != 0) goto loc_82485BAC;
			// lwz r9,0(r27)
  // [ph4a] vtable load collapsed
			// li r4,1
			ctx.r4.s64 = 1;
			// mr r3,r27
			ctx.r3.u64 = var_r27;
			// lwz r8,56(r9)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r27, 14, ctx, base);  // pattern-B slot 14 (byte +56)
			// mr r3,r29
			ctx.r3.u64 = var_r29;
			// stw r28,672(r30)
			PPC_STORE_U32(var_r30 + 672, var_r28);
			// stw r28,660(r30)
			PPC_STORE_U32(var_r30 + 660, var_r28);
			// stw r28,664(r30)
			PPC_STORE_U32(var_r30 + 664, var_r28);
			// stw r28,0(r29)
			PPC_STORE_U32(var_r29 + 0, var_r28);
			// bl 0x8248c270
			crAnimDofQuaternion_C270_w(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
			// cmpwi cr6,r11,0
			// bne cr6,0x82485bac
			if (ctx.r11.s32 != 0) goto loc_82485BAC;
			// lwz r11,0(r29)
			ctx.r11.u64 = PPC_LOAD_U32(var_r29 + 0);
			// li r10,1
			ctx.r10.s64 = 1;
			// addi r9,r31,96
			ctx.r9.s64 = (int64_t)(int32_t)var_r31 + 96;
			// li r8,0
			ctx.r8.s64 = 0;
			// li r7,63
			ctx.r7.s64 = 63;
			// li r6,0
			ctx.r6.s64 = 0;
			// li r5,0
			ctx.r5.s64 = 0;
			// li r4,1
			ctx.r4.s64 = 1;
			// stw r30,608(r11)
			PPC_STORE_U32(ctx.r11.u32 + 608, var_r30);
			// lwz r3,0(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
			// bl 0x8248c330
			atSingleton_C330_h(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
		}
loc_82485BAC:
		// sth r28,80(r31)
		PPC_STORE_U16(var_r31 + 80, (uint16_t)var_r28);
		// cmpwi cr6,r11,0
		// bne cr6,0x82485cb8
		if (ctx.r11.s32 == 0) {
			// addi r30,r30,124
			var_r30 = (uint32_t)(var_r30 + 124);
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r7,0(r30)
  // [ph4a] vtable load collapsed
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r6,36(r7)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 9, ctx, base);  // pattern-B slot 9 (byte +36)
			// lwz r5,0(r30)
  // [ph4a] vtable load collapsed
			// li r4,6
			ctx.r4.s64 = 6;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r11,40(r5)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 10, ctx, base);  // pattern-B slot 10 (byte +40)
			// lwz r4,100(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 100);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x82483498
			set_3498(ctx, base);
			// lwz r4,96(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 96);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x824834a0
			atSingleton_vfn_25_34A0_1(ctx, base);
			// lwz r4,108(r31)
			ctx.r4.u64 = PPC_LOAD_U32(var_r31 + 108);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x824834b8
			atSingleton_vfn_57(ctx, base);
			// lwz r10,108(r31)
			ctx.r10.u64 = PPC_LOAD_U32(var_r31 + 108);
			// lwz r9,96(r31)
			ctx.r9.u64 = PPC_LOAD_U32(var_r31 + 96);
			// mullw r4,r10,r9
			ctx.r4.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x824834a8
			set_34A8(ctx, base);
			// lwz r8,0(r30)
  // [ph4a] vtable load collapsed
			// li r4,0
			ctx.r4.s64 = 0;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lwz r7,44(r8)
  // [ph4a] slot load collapsed
			// bctrl
			VCALL(var_r30, 11, ctx, base);  // pattern-B slot 11 (byte +44)
			// li r4,0
			ctx.r4.s64 = 0;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x824834b0
			set_34B0(ctx, base);
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// lis r11,-32256
			ctx.r11.s64 = -2113929216;
			// lfs f1,15784(r11)
			ctx.fpscr.disableFlushMode();
			temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 15784);  /* glob:lbl_82003DA8 @ 0x82003da8 */
			ctx.f1.f64 = double(temp.f32);
			// bl 0x824834c8
			crAnimDofQuaternion_34C8_h(ctx, base);
			// li r4,0
			ctx.r4.s64 = 0;
			// mr r3,r30
			ctx.r3.u64 = var_r30;
			// bl 0x824834c0
			crAnimDofQuaternion_34C0_h(ctx, base);
			// addi r4,r31,80
			ctx.r4.s64 = (int64_t)(int32_t)var_r31 + 80;
			// lwz r3,0(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
			// bl 0x8248bf80
			crAnimDofQuaternion_BF80_h(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
			// cmpwi cr6,r11,0
			// bne cr6,0x82485cb8
			if (ctx.r11.s32 != 0) goto loc_82485CB8;
			// lhz r5,80(r31)
			ctx.r5.u64 = PPC_LOAD_U16(var_r31 + 80);
			// cmplwi cr6,r5,0
			// bne cr6,0x82485c94
			if (ctx.r5.u32 == 0) {
				// li r11,1
				ctx.r11.s64 = 1;
				// stw r11,84(r31)
				PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
			}
	loc_82485C94:
			// cmpwi cr6,r11,0
			// bne cr6,0x82485cb8
			if (ctx.r11.s32 != 0) goto loc_82485CB8;
			// li r4,1
			ctx.r4.s64 = 1;
			// lwz r3,0(r29)
			ctx.r3.u64 = PPC_LOAD_U32(var_r29 + 0);
			// bl 0x8248bfa8
			crAnimDofQuaternion_BFA8_h(ctx, base);
			// mr r11,r3
			ctx.r11.u64 = ctx.r3.u64;
			// stw r11,84(r31)
			PPC_STORE_U32(var_r31 + 84, ctx.r11.u32);
			// cmpwi cr6,r11,0
			// beq cr6,0x82485cc0
			if (ctx.r11.s32 == 0) goto loc_82485CC0;
		}
loc_82485CB8:
		// lis r28,-32768
		var_r28 = (uint32_t)(-2147483648);
		// ori r28,r28,16389
		var_r28 = (uint32_t)(var_r28 | 16389);
loc_82485CC0:
		// mr r3,r28
		ctx.r3.u64 = var_r28;
		// stw r3,92(r31)
		PPC_STORE_U32(var_r31 + 92, ctx.r3.u32);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82485ce0
		goto loc_82485CE0;
loc_82485CE0:
		// addi r1,r31,176
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 176;
		// b 0x8242f8e4
		__restgprlr_27(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in crAnimDofQuaternion_17");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 176;  // Establisher frame pointer
				__restgprlr_27(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_AD94_2"))) PPC_WEAK_FUNC(except_AD94_2);
PPC_FUNC_IMPL(__imp__except_AD94_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_19"))) PPC_WEAK_FUNC(crAnimDofQuaternion_19);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_19) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r29 = 0;
	SEH_TRY {
		// FRAME: size=112, savegprlr_29
		// addi r31,r1,-112
		var_r31 = (uint32_t)(ctx.r1.s64 + -112);
		// mr r30,r3
		var_r30 = ctx.r3.u32;
		// stw r30,132(r31)
		PPC_STORE_U32(var_r31 + 132, var_r30);
		// addi r3,r30,668
		ctx.r3.s64 = (int64_t)(int32_t)var_r30 + 668;
		// lwz r11,0(r3)
		ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
		// cmplwi cr6,r11,0
		// beq cr6,0x82485d50
		if (ctx.r11.u32 != 0) {
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// bl 0x8248bfd8
			crAnimDofQuaternion_BFD8(ctx, base);
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// b 0x82485d44
			goto loc_82485D44;
	loc_82485D44:
			// li r29,0
			var_r29 = 0;
			// stw r29,668(r30)
			PPC_STORE_U32(var_r30 + 668, var_r29);
			// b 0x82485d54
		} else {
	loc_82485D50:
			// li r29,0
			var_r29 = 0;
		}
loc_82485D54:
		// lwz r3,52(r30)
		ctx.r3.u64 = PPC_LOAD_U32(var_r30 + 52);
		// cmplwi cr6,r3,0
		// beq cr6,0x82485d74
		if (ctx.r3.u32 != 0) {
			// lwz r9,8(r10)
			// bctrl
			VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
			// stw r29,52(r30)
			PPC_STORE_U32(var_r30 + 52, var_r29);
		}
loc_82485D74:
		// li r3,0
		ctx.r3.s64 = 0;
		// stw r29,672(r30)
		PPC_STORE_U32(var_r30 + 672, var_r29);
		// stw r29,664(r30)
		PPC_STORE_U32(var_r30 + 664, var_r29);
		// stw r29,660(r30)
		PPC_STORE_U32(var_r30 + 660, var_r29);
		// addi r1,r31,112
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 112;
		// b 0x8242f8ec
		__restgprlr_29(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in crAnimDofQuaternion_19");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 112;  // Establisher frame pointer
				__restgprlr_29(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_ADAC_2"))) PPC_WEAK_FUNC(except_ADAC_2);
PPC_FUNC_IMPL(__imp__except_ADAC_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

__attribute__((alias("__imp__phObject_20"))) PPC_WEAK_FUNC(phObject_20);
PPC_FUNC_IMPL(__imp__phObject_20) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	// FRAME: size=96, manual
	// mr r31,r3
	var_r31 = ctx.r3.u32;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(var_r31 + 48);
	// cmplwi cr6,r3,0
	// beq cr6,0x82485dd0
	if (ctx.r3.u32 != 0) {
		// lwz r10,8(r11)
		// bctrl
		VCALL(ctx.r3.u32, 2, ctx, base);  // vtable slot 2 (byte +8)
		// li r9,0
		ctx.r9.s64 = 0;
		// stw r9,48(r31)
		PPC_STORE_U32(var_r31 + 48, ctx.r9.u32);
	}
loc_82485DD0:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr
	return;
}

__attribute__((alias("__imp__crAnimDofQuaternion_5DF0"))) PPC_WEAK_FUNC(crAnimDofQuaternion_5DF0);
PPC_FUNC_IMPL(__imp__crAnimDofQuaternion_5DF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t var_r31 = 0;
	uint32_t var_r27 = 0;
	uint32_t var_r21 = 0;
	uint32_t var_r22 = 0;
	uint32_t var_r17 = 0;
	uint32_t var_r18 = 0;
	uint32_t var_r30 = 0;
	uint32_t var_r26 = 0;
	uint32_t var_r19 = 0;
	uint32_t var_r24 = 0;
	uint32_t var_r20 = 0;
	uint32_t var_r25 = 0;
	uint32_t var_r23 = 0;
	uint32_t var_r28 = 0;
	uint32_t var_r29 = 0;
	double var_f31 = 0.0;
	double var_f30 = 0.0;
	PPCRegister temp{};
	SEH_TRY {
		// FRAME: size=272, savegprlr_17
		// addi r31,r1,-272
		var_r31 = (uint32_t)(ctx.r1.s64 + -272);
		// mr r27,r3
		var_r27 = ctx.r3.u32;
		// mr r21,r4
		var_r21 = ctx.r4.u32;
		// mr r22,r5
		var_r22 = ctx.r5.u32;
		// mr r17,r6
		var_r17 = ctx.r6.u32;
		// li r18,0
		var_r18 = 0;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// addi r30,r27,124
		var_r30 = (uint32_t)(var_r27 + 124);
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x824834d0
		phInst_34D0_g(ctx, base);
		// mr r26,r3
		var_r26 = ctx.r3.u32;
		// mr r3,r30
		ctx.r3.u64 = var_r30;
		// bl 0x824834d8
		atSingleton_vfn_46(ctx, base);
		// li r19,0
		var_r19 = 0;
		// rlwinm r24,r3,29,3,31
		var_r24 = (uint32_t)(__builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFFF);
		// cmplwi cr6,r24,3
		// bne cr6,0x82485e54
		if (var_r24 == 3) {
			// li r24,4
			var_r24 = 4;
			// li r19,1
			var_r19 = 1;
		}
loc_82485E54:
		// mullw r11,r24,r26
		ctx.r11.s64 = int64_t((int32_t)var_r24) * int64_t((int32_t)var_r26);
		// lwz r10,0(r22)
		ctx.r10.u64 = PPC_LOAD_U32(var_r22 + 0);
		// divwu r20,r21,r11
		var_r20 = ctx.r11.u32 ? var_r21 / ctx.r11.u32 : 0;
		// twllei r11,0
		if (ctx.r11.s32 == 0 || ctx.r11.u32 < 0u) __builtin_trap();
		// divwu r25,r10,r11
		var_r25 = ctx.r11.u32 ? ctx.r10.u32 / ctx.r11.u32 : 0;
		// twllei r11,0
		if (ctx.r11.s32 == 0 || ctx.r11.u32 < 0u) __builtin_trap();
		// stw r25,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r25);
		// subf r23,r25,r20
		var_r23 = var_r20 - var_r25;
		// stw r23,80(r31)
		PPC_STORE_U32(var_r31 + 80, var_r23);
		// lis r10,-32248
		// lis r11,-32255
		ctx.r11.s64 = -2113863680;
		// lfd f31,-30144(r11)
		ctx.fpscr.disableFlushMode();
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r11.u32 + -30144);  /* glob:lbl_82008A40 @ 0x82008a40 */
		// lfd f30,-25848(r10)
		ctx.f30.u64 = PPC_LOAD_U64(ctx.r10.u32 + -25848);
loc_82485E88:
		// lwz r9,672(r27)
		ctx.r9.u64 = PPC_LOAD_U32(var_r27 + 672);
		// cmplwi cr6,r9,0
		// beq cr6,0x82486058
		if (ctx.r9.u32 == 0) goto loc_82486058;
		// subf r23,r25,r20
		var_r23 = var_r20 - var_r25;
		// stw r23,80(r31)
		PPC_STORE_U32(var_r31 + 80, var_r23);
		// cmplwi cr6,r23,16
		// blt cr6,0x82486060
		if (var_r23 < 16) goto loc_82486060;
		// lwz r3,48(r27)
		ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 48);
		// lwz r8,0(r22)
		ctx.r8.u64 = PPC_LOAD_U32(var_r22 + 0);
		// subf r30,r8,r21
		var_r30 = (uint32_t)((int64_t)(int32_t)var_r21 - ctx.r8.s64);
		// lwz r6,80(r7)
		// bctrl
		VCALL(ctx.r3.u32, 20, ctx, base);  // vtable slot 20 (byte +80)
		// lwz r5,0(r27)
  // [ph4a] vtable load collapsed
		// mr r28,r3
		var_r28 = ctx.r3.u32;
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r4,116(r5)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 29, ctx, base);  // pattern-B slot 29 (byte +116)
		// mr r11,r3
		ctx.r11.u64 = ctx.r3.u64;
		// mullw r10,r25,r24
		ctx.r10.s64 = int64_t((int32_t)var_r25) * int64_t((int32_t)var_r24);
		// lwz r3,668(r27)
		ctx.r3.u64 = PPC_LOAD_U32(var_r27 + 668);
		// mullw r10,r10,r26
		ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t((int32_t)var_r26);
		// addi r8,r31,104
		ctx.r8.s64 = (int64_t)(int32_t)var_r31 + 104;
		// mr r7,r23
		ctx.r7.u64 = var_r23;
		// mr r6,r30
		ctx.r6.u64 = var_r30;
		// li r5,0
		ctx.r5.s64 = 0;
		// add r11,r10,r11
		ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
		// add r30,r11,r28
		var_r30 = (uint32_t)(ctx.r11.u64 + var_r28);
		// mr r4,r30
		ctx.r4.u64 = var_r30;
		// bl 0x8248d5b8
		crAnimDofQuaternion_D5B8_h(ctx, base);
		// mr r29,r3
		var_r29 = ctx.r3.u32;
		// cmplwi cr6,r29,0
		// beq cr6,0x8248604c
		if (var_r29 == 0) goto loc_8248604C;
		// lwz r9,0(r27)
  // [ph4a] vtable load collapsed
		// mr r3,r27
		ctx.r3.u64 = var_r27;
		// lwz r8,116(r9)
  // [ph4a] slot load collapsed
		// bctrl
		VCALL(var_r27, 29, ctx, base);  // pattern-B slot 29 (byte +116)
		// cmplwi cr6,r3,8
		// blt cr6,0x82485f64
		if (ctx.r3.u32 >= 8) {
			// cmplwi cr6,r25,0
			// bne cr6,0x82485f64
			if (var_r25 != 0) goto loc_82485F64;
			// ld r11,104(r31)
			ctx.r11.u64 = PPC_LOAD_U64(var_r31 + 104);
			// lis r7,14470
			ctx.r7.s64 = 948305920;
			// ori r6,r7,22859
			ctx.r6.u64 = ctx.r7.u64 | 22859;
			// lis r5,13421
			ctx.r5.s64 = 879558656;
			// ori r4,r5,50646
			ctx.r4.u64 = ctx.r5.u64 | 50646;
			// rldimi r6,r4,32,0
			ctx.r6.u64 = (__builtin_rotateleft64(ctx.r4.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r6.u64 & 0xFFFFFFFF);
			// mulhd r3,r11,r6
			ctx.r3.s64 = static_cast<int64_t>((static_cast<__int128>(static_cast<int64_t>(ctx.r11.s64)) * static_cast<__int128>(static_cast<int64_t>(ctx.r6.s64))) >> 64);
			// sradi r11,r3,11
			ctx.xer.ca = (ctx.r3.s64 < 0) & ((ctx.r3.u64 & 0x7FF) != 0);
			ctx.r11.s64 = ctx.r3.s64 >> 11;
			// rldicl r10,r11,1,63
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
			// add r11,r11,r10
			ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
			// std r11,0(r28)
			PPC_STORE_U64(var_r28 + 0, ctx.r11.u64);
		}
loc_82485F64:
		// lwz r10,672(r27)
		ctx.r10.u64 = PPC_LOAD_U32(var_r27 + 672);
		// add r25,r25,r29
		var_r25 = (uint32_t)(var_r25 + var_r29);
		// stw r25,84(r31)
		PPC_STORE_U32(var_r31 + 84, var_r25);
		// cmpwi cr6,r19,0
		// subf r9,r29,r10
		ctx.r9.s64 = ctx.r10.s64 - (int64_t)(int32_t)var_r29;
		// stw r9,672(r27)
		PPC_STORE_U32(var_r27 + 672, ctx.r9.u32);
		// beq cr6,0x82486038
		if ((int32_t)var_r19 != 0) {
			// mullw r11,r26,r29
			ctx.r11.s64 = int64_t((int32_t)var_r26) * int64_t((int32_t)var_r29);
			// li r7,0
			ctx.r7.s64 = 0;
			// rlwinm r9,r11,1,0,30
			ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
			// rlwinm r10,r11,2,0,29
			ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
			// add r9,r11,r9
			ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
			// add r10,r10,r30
			ctx.r10.u64 = ctx.r10.u64 + var_r30;
			// add r9,r9,r30
			ctx.r9.u64 = ctx.r9.u64 + var_r30;
			// addi r8,r10,-4
			ctx.r8.s64 = ctx.r10.s64 + -4;
			// stw r8,92(r31)
			PPC_STORE_U32(var_r31 + 92, ctx.r8.u32);
			// addi r10,r9,-3
			ctx.r10.s64 = ctx.r9.s64 + -3;
			// stw r10,88(r31)
			PPC_STORE_U32(var_r31 + 88, ctx.r10.u32);
	loc_82485FAC:
			// stw r7,96(r31)
			PPC_STORE_U32(var_r31 + 96, ctx.r7.u32);
			// cmplw cr6,r7,r11
			// bge cr6,0x82486038
			if (ctx.r7.u32 >= ctx.r11.u32) goto loc_82486038;
			// lbz r6,0(r10)
			ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
			// lbz r5,1(r10)
			ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
			// rotlwi r4,r6,8
			ctx.r4.u64 = __builtin_rotateleft32(ctx.r6.u32, 8);
			// lbz r3,2(r10)
			ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
			// or r9,r4,r5
			ctx.r9.u64 = ctx.r4.u64 | ctx.r5.u64;
			// rlwinm r6,r9,8,0,23
			ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
			// or r9,r6,r3
			ctx.r9.u64 = ctx.r6.u64 | ctx.r3.u64;
			// rlwinm r5,r9,0,8,8
			ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x800000;
			// cmplwi cr6,r5,0
			// beq cr6,0x82486000
			if (ctx.r5.u32 != 0) {
				// clrldi r3,r9,41
				ctx.r3.u64 = ctx.r9.u64 & 0x7FFFFF;
				// std r3,112(r31)
				PPC_STORE_U64(var_r31 + 112, ctx.r3.u64);
				// lfd f0,112(r31)
				ctx.fpscr.disableFlushMode();
				ctx.f0.u64 = PPC_LOAD_U64(var_r31 + 112);
				// fcfid f13,f0
				ctx.f13.f64 = double(ctx.f0.s64);
				// fmsub f12,f13,f31,f30
				ctx.f12.f64 = ctx.f13.f64 * var_f31 - var_f30;
				// frsp f11,f12
				ctx.f11.f64 = double(float(ctx.f12.f64));
				// stfs f11,0(r8)
				temp.f32 = float(ctx.f11.f64);
				PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
				// b 0x8248601c
			} else {
		loc_82486000:
				// clrldi r9,r9,32
				ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
				// std r9,120(r31)
				PPC_STORE_U64(var_r31 + 120, ctx.r9.u64);
				// lfd f10,120(r31)
				ctx.fpscr.disableFlushMode();
				ctx.f10.u64 = PPC_LOAD_U64(var_r31 + 120);
				// fcfid f9,f10
				ctx.f9.f64 = double(ctx.f10.s64);
				// fmul f8,f9,f31
				ctx.f8.f64 = ctx.f9.f64 * var_f31;
				// frsp f7,f8
				ctx.f7.f64 = double(float(ctx.f8.f64));
				// stfs f7,0(r8)
				temp.f32 = float(ctx.f7.f64);
				PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
			}
	loc_8248601C:
			// addi r10,r10,-3
			ctx.r10.s64 = ctx.r10.s64 + -3;
			// stw r10,88(r31)
			PPC_STORE_U32(var_r31 + 88, ctx.r10.u32);
			// addi r8,r8,-4
			ctx.r8.s64 = ctx.r8.s64 + -4;
			// stw r8,92(r31)
			PPC_STORE_U32(var_r31 + 92, ctx.r8.u32);
			// mr r8,r8
			ctx.r8.u64 = ctx.r8.u64;
			// addi r7,r7,1
			ctx.r7.s64 = ctx.r7.s64 + 1;
			// b 0x82485fac
			goto loc_82485FAC;
		}
loc_82486038:
		// mullw r8,r25,r24
		ctx.r8.s64 = int64_t((int32_t)var_r25) * int64_t((int32_t)var_r24);
		// mullw r7,r8,r26
		ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t((int32_t)var_r26);
		// stw r7,0(r22)
		PPC_STORE_U32(var_r22 + 0, ctx.r7.u32);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82485e88
		goto loc_82485E88;
loc_8248604C:
		// lis r18,-32768
		var_r18 = (uint32_t)(-2147483648);
		// ori r18,r18,16389
		var_r18 = (uint32_t)(var_r18 | 16389);
		// stw r18,100(r31)
		PPC_STORE_U32(var_r31 + 100, var_r18);
loc_82486058:
		// cmplwi cr6,r23,16
		// bge cr6,0x82486068
		if (var_r23 < 16) {
	loc_82486060:
			// li r11,1
			ctx.r11.s64 = 1;
			// b 0x8248606c
		} else {
	loc_82486068:
			// li r11,0
			ctx.r11.s64 = 0;
		}
loc_8248606C:
		// stw r11,0(r17)
		PPC_STORE_U32(var_r17 + 0, ctx.r11.u32);
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// mr r8,r8
		ctx.r8.u64 = ctx.r8.u64;
		// b 0x82486088
		goto loc_82486088;
loc_82486088:
		// mr r3,r18
		ctx.r3.u64 = var_r18;
		// addi r1,r31,272
		ctx.r1.s64 = (int64_t)(int32_t)var_r31 + 272;
		// lfd f30,-144(r1)
		ctx.fpscr.disableFlushMode();
		ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
		// lfd f31,-136(r1)
		ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
		// b 0x8242f8bc
		__restgprlr_17(ctx, base);
		return;
			} SEH_CATCH_ALL {
				REXLOG_WARN("SEH exception caught in crAnimDofQuaternion_5DF0");
				ctx.r12.s64 = (int64_t)(int32_t)var_r31 + 272;  // Establisher frame pointer
				__restgprlr_17(ctx, base);  // Restore caller registers
				SEH_RETHROW;
			} SEH_END
		}

__attribute__((alias("__imp__except_ADC4_2"))) PPC_WEAK_FUNC(except_ADC4_2);
PPC_FUNC_IMPL(__imp__except_ADC4_2) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// mr r8,r8
	ctx.r8.u64 = ctx.r8.u64;
	// blr
	return;
}

